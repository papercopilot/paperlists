[
    {
        "title": "2-MAP: Aligned Visualizations for Comparison of High-Dimensional Point Sets",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Liu_2-MAP_Aligned_Visualizations_for_Comparison_of_High-Dimensional_Point_Sets_WACV_2020_paper.html",
        "author": "Xiaotong Liu;  Zeyu Zhang;  Hong Xuan;  Roxana Leontie;  Abby Stylianou;  Robert Pless",
        "abstract": "Visualization tools like t-SNE and UMAP give insight into the high-dimensional structure of datasets. When there are related datasets (such as the high-dimensional representations of image data created by two different Deep Learning architectures), roughly aligning those visualizations helps to highlight both the similarities and differences. In this paper we propose a method to align multiple low dimensional visualizations by adding an alignment term to the UMAP loss function. We provide an automated procedure to find a weight for this term that encourages the alignment but only minimally changes the fidelity of the underlying embedding.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Liu_2-MAP_Aligned_Visualizations_for_Comparison_of_High-Dimensional_Point_Sets_WACV_2020_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7539779,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17897418287338399839&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "360 Panorama Synthesis from a Sparse Set of Images with Unknown Field of View",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sumantri_360_Panorama_Synthesis_from_a_Sparse_Set_of_Images_with_WACV_2020_paper.html",
        "author": "Julius Surya  Sumantri;  In Kyu Park",
        "abstract": "360 images represent scenes captured in all possible viewing directions and enable viewers to navigate freely around the scene thereby providing an immersive experience. Conversely, conventional images represent scenes in a single viewing direction with a small or limited field of view (FOV). As a result, only certain parts of the scenes are observed, and valuable information about the surroundings is lost. In this paper, a learning-based approach that reconstructs the scene in 360 x 180 from a sparse set of conventional images (typically 4 images) is proposed. The proposed approach first estimates the FOV of input images relative to the panorama. The estimated FOV is then used as the prior for synthesizing a high-resolution 360 panoramic output. The proposed method overcomes the difficulty of learning-based approach in synthesizing high resolution images (up to 512x1024). Experimental results demonstrate that the proposed method produces 360 panorama with reasonable quality. Results also show that the proposed method outperforms the alternative method and can be generalized for non-panoramic scenes and images captured by a smartphone camera.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sumantri_360_Panorama_Synthesis_from_a_Sparse_Set_of_Images_with_WACV_2020_paper.pdf",
        "aff": "Dept. of Information and Communication Engineering, Inha University, Incheon 22212, Korea; Dept. of Information and Communication Engineering, Inha University, Incheon 22212, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2829190,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2092102078274668202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;inha.ac.kr",
        "email": "gmail.com;inha.ac.kr",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Inha University",
        "aff_unique_dep": "Dept. of Information and Communication Engineering",
        "aff_unique_url": "http://www.inha.ac.kr",
        "aff_unique_abbr": "Inha U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Incheon",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "360-Indoor: Towards Learning Real-World Objects in 360deg Indoor Equirectangular Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chou_360-Indoor_Towards_Learning_Real-World_Objects_in_360deg_Indoor_Equirectangular_Images_WACV_2020_paper.html",
        "author": "Shih-Han Chou;  Cheng Sun;  Wen-Yen Chang;  Wan-Ting Hsu;  Min Sun;  Jianlong Fu",
        "abstract": "While there are several widely used object detection datasets, current computer vision algorithms are still limited in conventional images. Such images narrow our vision in a restricted region. On the other hand, 360deg images provide a thorough sight. In this paper, our goal is to provide a standard dataset to facilitate the vision and machine learning communities in 360deg domain. To facilitate the research, we present a real-world 360deg panoramic object detection dataset, 360-Indoor, which is a new benchmark for visual object detection and class recognition in 360deg indoor images. It is achieved by gathering images of complex indoor scenes containing common objects and the intensive annotated bounding field-of-view. In addition, 360-Indoor has several distinct properties: (1) the largest category number (37 labels in total). (2) the most complete annotations on average (27 bounding boxes per image). The selected 37 objects are all common in indoor scene. With around 3k images and 90k labels in total, 360-Indoor achieves the largest dataset for detection in 360deg images. In the end, extensive experiments on the state-of-the-art methods for both classification and detection are provided. We will release this dataset in the near future.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chou_360-Indoor_Towards_Learning_Real-World_Objects_in_360deg_Indoor_Equirectangular_Images_WACV_2020_paper.pdf",
        "aff": "National Tsing Hua University; National Tsing Hua University; National Tsing Hua University; National Tsing Hua University+Microsoft Research; National Tsing Hua University; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Chou_360-Indoor_Towards_Learning_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4426650,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14624210147604010681&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;gapp.nthu.edu.tw;gmail.com;gmail.com;ee.nthu.edu.tw;microsoft.com",
        "email": "gmail.com;gapp.nthu.edu.tw;gmail.com;gmail.com;ee.nthu.edu.tw;microsoft.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1;0;1",
        "aff_unique_norm": "National Tsing Hua University;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.nthu.edu.tw;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "NTHU;MSR",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Taiwan;",
        "aff_country_unique_index": "0;0;0;0+1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "3D Hand Pose Estimation with Disentangled Cross-Modal Latent Space",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Gu_3D_Hand_Pose_Estimation_with_Disentangled_Cross-Modal_Latent_Space_WACV_2020_paper.html",
        "author": "Jiajun Gu;  Zhiyong Wang;  Wanli Ouyang;  weichen zhang;  Jiafeng Li;  Li Zhuo",
        "abstract": "Estimating 3D hand pose from a single RGB image is a challenging task because of its ill-posed nature (i.e., depth ambiguity). Recently, various generative-based approaches have been proposed to predict the 3D joints by learning a unified latent space between two modalities (i.e., RGB image and 3D joints).  However, projecting multi-modal data(i.e., RGB images and 3D joints) into a unified latent space is  difficult  as  the  modality-specific  features  usually  inter-fere the learning of the optimal latent space.  Hence in this paper, we propose to disentangle the latent space into two sub-latent spaces: modality-specific latent space and pose-specific latent space for 3D hand pose estimation. Our proposed  method,  namely  Disentangled  Cross-Modal  Latent Space  (DCMLS),  consists  of  two  variational  autoencoder networks and auxiliary components which connects the two VAEs to align underlying hand poses and transfer modality context from RGB to 3D. For the hand pose latent space,we align the hand pose latent space from the two modalities by using a cross-modal discriminator with the adversarial learning strategy.  For the context latent space, we learn acontext  translator  to  gain  access  to  the  cross-modal  con-text. Experimental results on two widely used public bench-mark datasets RHD and STB demonstrate that our proposed DCMLS method is able to outperform the state-of-the-artones on single image based 3D hand pose estimation.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Gu_3D_Hand_Pose_Estimation_with_Disentangled_Cross-Modal_Latent_Space_WACV_2020_paper.pdf",
        "aff": "The University of Sydney; The University of Sydney; The University of Sydney; The University of Sydney; Beijing University of Technology; Beijing University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1147758,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5793462885520414128&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff_domain": "uni.sydney.edu.au;sydney.edu.au;sydney.edu.au;sydney.edu.au;bjut.edu.cn;bjut.edu.cn",
        "email": "uni.sydney.edu.au;sydney.edu.au;sydney.edu.au;sydney.edu.au;bjut.edu.cn;bjut.edu.cn",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "University of Sydney;Beijing University of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sydney.edu.au;http://www.bjut.edu.cn",
        "aff_unique_abbr": "USYD;BJUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;1",
        "aff_country_unique": "Australia;China"
    },
    {
        "title": "3D semi-supervised learning with uncertainty-aware multi-view co-training",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xia_3D_semi-supervised_learning_with_uncertainty-aware_multi-view_co-training_WACV_2020_paper.html",
        "author": "Yingda Xia;  Fengze Liu;  Dong Yang;  Jinzheng Cai;  Lequan Yu;  Zhuotun Zhu;  Daguang Xu;  Alan Yuille;  Holger Roth",
        "abstract": "While making a tremendous impact in various fields, deep neural networks usually require large amounts of labeled data for training which are expensive to collect in many applications, especially in the medical domain. Unlabeled data, on the other hand, is much more abundant. Semi-supervised learning techniques, such as co-training, could provide a powerful tool to leverage unlabeled data. In this paper, we propose a novel framework, uncertainty-aware multi-view co-training (UMCT), to address semi-supervised learning on 3D data, such as volumetric data from medical imaging. In our work, co-training is achieved by exploiting multi-viewpoint consistency of 3D data. We generate different views by rotating or permuting the 3D data and utilize asymmetrical 3D kernels to encourage diversified features in different sub-networks. In addition, we propose an uncertainty-weighted label fusion mechanism to estimate the reliability of each view's prediction with Bayesian deep learning. As one view requires the supervision from other views in co-training, our self-adaptive approach computes a confidence score for the prediction of each unlabeled sample in order to assign a reliable pseudo label. Thus, our approach can take advantage of unlabeled data during training. We show the effectiveness of our proposed semi-supervised method on several public datasets from medical image segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile, a fully-supervised method based on our approach achieved state-of-the-art performances on both the LiTS liver tumor segmentation and the Medical Segmentation Decathlon (MSD) challenge, demonstrating the robustness and value of our framework, even when fully supervised training is feasible.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xia_3D_semi-supervised_learning_with_uncertainty-aware_multi-view_co-training_WACV_2020_paper.pdf",
        "aff": "Johns Hopkins University; Johns Hopkins University; NVIDIA; University of Florida; The Chinese University of Hong Kong; Johns Hopkins University; NVIDIA; Johns Hopkins University; NVIDIA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1006314,
        "gs_citation": 193,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5644782658632085073&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "jhu.edu;jhu.edu;nvidia.com;ufl.edu;cuhk.edu.hk;jhu.edu;nvidia.com;jhu.edu;nvidia.com",
        "email": "jhu.edu;jhu.edu;nvidia.com;ufl.edu;cuhk.edu.hk;jhu.edu;nvidia.com;jhu.edu;nvidia.com",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;3;0;1;0;1",
        "aff_unique_norm": "Johns Hopkins University;NVIDIA Corporation;University of Florida;The Chinese University of Hong Kong",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.jhu.edu;https://www.nvidia.com;https://www.ufl.edu;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "JHU;NVIDIA;UF;CUHK",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;1;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "A \"Network Pruning Network'' Approach to Deep Model Compression",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Verma_A_Network_Pruning_Network_Approach_to_Deep_Model_Compression_WACV_2020_paper.html",
        "author": "Vinay Kumar Verma;  Pravendra Singh;  Vinay Namboodri;  Piyush Rai",
        "abstract": "We present a filter pruning approach for deep model compression, using a multitask network. Our approach is based on learning a a pruner network to prune a pre-trained target network. The pruner is essentially a multitask deep neural network with binary outputs that help identify the filters from each layer of the original network that do not have any significant contribution to the model and can therefore be pruned. The pruner network has the same architecture as the original network except that it has a multitask/multi-output last layer containing binary-valued outputs (one per filter), which indicate which filters have to be pruned. The pruner's goal is to minimize the number of filters from the original network by assigning zero weights to the corresponding output feature-maps. In contrast to most of the existing methods, instead of relying on iterative pruning, our approach can prune the network (original network) in one go and, moreover, does not require specifying the degree of pruning for each layer (and can learn it instead). The compressed model produced by our approach is generic and does not need any special hardware/software support. Moreover, augmenting with other methods such as knowledge distillation, quantization, and connection pruning can increase the degree of compression for the proposed approach. We show the efficacy of our proposed approach for classification and object detection tasks.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Verma_A_Network_Pruning_Network_Approach_to_Deep_Model_Compression_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science and Engineering, IIT Kanpur, India; Department of Computer Science and Engineering, IIT Kanpur, India; Department of Computer Science and Engineering, IIT Kanpur, India; Department of Computer Science and Engineering, IIT Kanpur, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 506242,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10161241179193644865&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "iitk.ac.in;iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "email": "iitk.ac.in;iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Kanpur",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Kanpur",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "A Flexible Selection Scheme for Minimum-Effort Transfer Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Royer_A_Flexible_Selection_Scheme_for_Minimum-Effort_Transfer_Learning_WACV_2020_paper.html",
        "author": "Amelie Royer;  Christoph Lampert",
        "abstract": "Fine-tuning is a popular way of exploiting knowledge contained in a pre-trained convolutional network for a new visual recognition task. However, the orthogonal setting of transferring knowledge from a pretrained network from a visually different yet semantically close source is rarely considered: This commonly happens with real-life data which is not necessarily as clean as the training source (noise, ge- ometric transformations, different modalities, etc.).  To tackle such scenarios, we introduce a new, generalized form of fine-tuning, called flex-tuning, in which any individual unit (e.g. layer) of a network can be tuned, and the most promising one is chosen automatically. In order to make the method appealing for practical use, we propose two lightweight and faster selection procedures that prove to be good approximations in practice. We study these selection criteria empirically across a variety of domain shifts and data scarcity scenarios, and show that fine-tuning individual units, despite its simplicity, yields very good results as an adaptation technique. As it turns out, in contrast to common practice, rather than the last fully-connected unit it is best to tune an intermediate or early one in many domain-shift scenarios, which is accurately detected by flex-tuning.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Royer_A_Flexible_Selection_Scheme_for_Minimum-Effort_Transfer_Learning_WACV_2020_paper.pdf",
        "aff": "IST Austria; IST Austria",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Royer_A_Flexible_Selection_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2344654,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9895704797052786235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ist.ac.at;ist.ac.at",
        "email": "ist.ac.at;ist.ac.at",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Institute of Science and Technology Austria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ist.ac.at",
        "aff_unique_abbr": "IST Austria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "A GAN-based Tunable Image Compression System",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wu_A_GAN-based_Tunable_Image_Compression_System_WACV_2020_paper.html",
        "author": "Lirong Wu;  Kejie Huang;  Haibin Shen",
        "abstract": "The method of importance map has been widely adopted in DNN-based lossy image compression to achieve bit allocation according to the importance of image contents. However, insufficient allocation of bits in non-important regions often leads to severe distortion at low bpp (bits per pixel), which hampers the development of efficient content-weighted image compression systems. This paper rethinks content-based compression by using Generative Adversarial Network (GAN) to reconstruct the non-important regions. Moreover, multiscale pyramid decomposition is applied to both the encoder and the discriminator to achieve global compression of high-resolution images. A tunable compression scheme is also proposed in this paper to compress an image to any specific compression ratio without retraining the model. The experimental results show that our proposed method improves MS-SSIM by more than 10.3% compared to the recently reported GAN-based method to achieve the same low bpp (0.05) on the Kodak dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wu_A_GAN-based_Tunable_Image_Compression_System_WACV_2020_paper.pdf",
        "aff": "Zhejiang University; Zhejiang University; Zhejiang University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Wu_A_GAN-based_Tunable_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3473768,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=97655086820859264&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Generative Framework for Zero Shot Learning with Adversarial Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Khare_A_Generative_Framework_for_Zero_Shot_Learning_with_Adversarial_Domain_WACV_2020_paper.html",
        "author": "Varun Khare;  Divyat Mahajan;  Homanga Bharadhwaj;  Vinay Kumar Verma;  Piyush Rai",
        "abstract": "We present a domain adaptation based generative framework for zero shot learning. We address the problem of domain shift between the seen and unseen class distribution in Zero-Shot Learning (ZSL) and seek to minimize it by developing a generative model and training it via adversarial domain adaptation. Our approach is based on end-to-end learning of the class distributions of seen classes and unseen classes. To enable the model to learn the class distributions of unseen classes, we parameterize these class distributions in terms of the class attribute information (which is available for both seen and unseen classes). This provides a very simple way to learn the class distribution of any unseen class, given only its class attribute information, and no labeled training data. Training this model with adversarial domain adaptation provides robustness against the distribution mismatch between the data from seen and unseen classes. It also engenders a novel way for training neural net based classifiers to overcome the hubness problem in Zero-Shot learning. Through a comprehensive set of experiments, we show that our model yields superior accuracies as compared to various state-of-the-art zero shot learning models, on a variety of benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Khare_A_Generative_Framework_for_Zero_Shot_Learning_with_Adversarial_Domain_WACV_2020_paper.pdf",
        "aff": "IIT Kanpur; Microsoft Research, India + IIT Kanpur; University of Toronto + IIT Kanpur; IIT Kanpur; IIT Kanpur",
        "project": "",
        "github": "github.com/vkkhare/ZSL-ADA",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Khare_A_Generative_Framework_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 687494,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12078634749132535945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gmail.com;microsoft.com;cs.toronto.edu;iitk.ac.in;iitk.ac.in",
        "email": "gmail.com;microsoft.com;cs.toronto.edu;iitk.ac.in;iitk.ac.in",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;2+0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;Microsoft Research;University of Toronto",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.iitk.ac.in;https://www.microsoft.com/en-us/research/group/india.aspx;https://www.utoronto.ca",
        "aff_unique_abbr": "IITK;MSR India;U of T",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Kanpur;",
        "aff_country_unique_index": "0;0+0;1+0;0;0",
        "aff_country_unique": "India;Canada"
    },
    {
        "title": "A Little Fog for a Large Turn",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Machiraju_A_Little_Fog_for_a_Large_Turn_WACV_2020_paper.html",
        "author": "Harshitha Machiraju;  Vineeth N Balasubramanian",
        "abstract": "Small, carefully crafted perturbations called adversarial perturbations can easily fool neural networks. However, these perturbations are largely additive and not naturally found. We turn our attention to the field of Autonomous navigation wherein adverse weather conditions such as fog have a drastic effect on the predictions of these systems. These weather conditions are capable of acting like natural adversaries that can help in testing models. To this end, we introduce a general notion of adversarial perturbations, which can be created using generative models and provide a methodology inspired by Cycle-Consistent Generative Adversarial Networks to generate adversarial weather conditions for a given image. Our formulation and results show that these images provide a suitable testbed for steering models used in Autonomous navigation models. Our work also presents a more natural and general definition of Adversarial perturbations based on Perceptual Similarity.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Machiraju_A_Little_Fog_for_a_Large_Turn_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Technology, Hyderabad, India; Indian Institute of Technology, Hyderabad, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Machiraju_A_Little_Fog_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 967016,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10050159148433126267&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "iith.ac.in;iith.ac.in",
        "email": "iith.ac.in;iith.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Hyderabad",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iith.ac.in",
        "aff_unique_abbr": "IIT Hyderabad",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "A Multi-Scale Guided Cascade Hourglass Network for Depth Completion",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Li_A_Multi-Scale_Guided_Cascade_Hourglass_Network_for_Depth_Completion_WACV_2020_paper.html",
        "author": "Ang Li;  Zejian Yuan;  Yonggen Ling;  Wanchao Chi;  shenghao zhang;  Chong Zhang",
        "abstract": "Depth completion, a task to estimate the dense depth map from sparse measurement under the guidance from the high-resolution image, is essential to many computer vision applications. Most previous methods building on fully convolutional networks can not handle diverse patterns in the depth map efficiently and effectively. We propose a multi-scale guided cascade hourglass network to tackle this problem. Structures at different levels are captured by specialized hourglasses in the cascade network with sparse inputs in various sizes. An encoder extracts multiscale features from color image to provide deep guidance for all the hourglasses. A multi-scale training strategy further activates the effect of cascade stages. With the role of each sub-module divided explicitly, we can implement components with simple architectures. Extensive experiments show that our lightweight model achieves competitive results compared with state-of-the-art in KITTI depth completion benchmark, with low complexity in run-time.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Li_A_Multi-Scale_Guided_Cascade_Hourglass_Network_for_Depth_Completion_WACV_2020_paper.pdf",
        "aff": "Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, China; Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, China; Tencent Robotics X, China; Tencent Robotics X, China; Tencent Robotics X, China; Tencent Robotics X, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1864792,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10633778187235416124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "stu.xjtu.edu.cn;xjtu.edu.cn;connect.ust.hk;tencent.com;pku.edu.cn;gmail.com",
        "email": "stu.xjtu.edu.cn;xjtu.edu.cn;connect.ust.hk;tencent.com;pku.edu.cn;gmail.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;1;1",
        "aff_unique_norm": "Xian Jiaotong University;Tencent Robotics X",
        "aff_unique_dep": "Institute of Artificial Intelligence and Robotics;Tencent Robotics X",
        "aff_unique_url": "http://www.xjtu.edu.cn;https://robotics.tencent.com",
        "aff_unique_abbr": "XJTU;Tencent Robotics X",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Xian;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Multi-Space Approach to Zero-Shot Object Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Gupta_A_Multi-Space_Approach_to_Zero-Shot_Object_Detection_WACV_2020_paper.html",
        "author": "Dikshant Gupta;  Aditya Anantharaman;  Nehal Mamgain;  Sowmya Kamath S;  Vineeth N Balasubramanian;  C.V. Jawahar",
        "abstract": "Object detection has been at the forefront for higher level vision tasks such as scene understanding and contextual reasoning. Therefore, solving object detection for a large number of visual categories is paramount. Zero-Shot Object Detection (ZSD) - where training data is not available for some of the target classes - provides semantic scalability to object detection and reduces dependence on large amount of annotations, thus enabling a large number of applications in real-life scenarios. In this paper, we propose a novel multi-space approach to solve ZSD where we combine predictions obtained in two different search spaces. We learn the projection of visual features of proposals to the semantic embedding space and class labels in the semantic embedding space to visual space. We predict similarity scores in the individual spaces and combine them. We present promising results on two datasets, PASCAL VOC and MS COCO. We further discuss the problem of hubness and show that our approach alleviates hubness with a performance superior to previously proposed methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Gupta_A_Multi-Space_Approach_to_Zero-Shot_Object_Detection_WACV_2020_paper.pdf",
        "aff": "International Institute of Information Technology, Hyderabad; National Institute of Technology Karnataka, Surathkal; Indian Institute of Technology, Hyderabad; National Institute of Technology Karnataka, Surathkal; Indian Institute of Technology, Hyderabad; International Institute of Information Technology, Hyderabad",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Gupta_A_Multi-Space_Approach_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2957687,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9319869563456260860&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;gmail.com;gmail.com;nitk.edu.in;iith.ac.in;iiit.ac.in",
        "email": "gmail.com;gmail.com;gmail.com;nitk.edu.in;iith.ac.in;iiit.ac.in",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1;2;0",
        "aff_unique_norm": "International Institute of Information Technology;National Institute of Technology Karnataka;Indian Institute of Technology Hyderabad",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://iiit Hyderabad.ac.in;https://www.nitk.edu.in;https://www.iith.ac.in",
        "aff_unique_abbr": "IIIT Hyderabad;NITK;IIT Hyderabad",
        "aff_campus_unique_index": "0;1;0;1;0;0",
        "aff_campus_unique": "Hyderabad;Surathkal",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "A Novel Inspection System For Variable Data Printing Using Deep Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Haik_A_Novel_Inspection_System_For_Variable_Data_Printing_Using_Deep_WACV_2020_paper.html",
        "author": "Oren Haik;  Oded Perry;  Eli Chen;  Peter Klammer",
        "abstract": "We present a novel approach for inspecting variable data prints (VDP) with an ultra-low false alarm rate (0.005%) and potential applicability to other real-world problems.  The system is based on a comparison between two images: a reference image and an image captured by low-cost scanners. The comparison task is challenging as low-cost imaging systems create artifacts that may erroneously be classified as true (genuine) defects. To address this challenge we introduce two new fusion methods, for change detection applications, which are both fast and efficient. The first is an early fusion method that combines the two input images into a single pseudo-color image. The second, called Change-Detection Single Shot Detector (CD-SSD) leverages the SSD by fusing features in the middle of the network. We demonstrate the effectiveness of the proposed deep learning-based approach with a large dataset from real-world printing scenarios. Finally, we evaluate our models on a different domain of aerial imagery change detection (AICD). Our best method clearly outperforms the state-of-the-art baseline on this dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Haik_A_Novel_Inspection_System_For_Variable_Data_Printing_Using_Deep_WACV_2020_paper.pdf",
        "aff": "HP Inc.; HP Inc.; HP Inc.; HP Inc.",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Haik_A_Novel_Inspection_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 987062,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12198380326236710398&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "HP Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hp.com",
        "aff_unique_abbr": "HP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Novel Self-Supervised Re-labeling Approach for Training with Noisy Labels",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mandal_A_Novel_Self-Supervised_Re-labeling_Approach_for_Training_with_Noisy_Labels_WACV_2020_paper.html",
        "author": "Devraj Mandal;  Shrisha Bharadwaj;  Soma  Biswas",
        "abstract": "The major driving force behind the immense success of deep learning models is the availability of large datasets along with their clean labels. This is very difficult to obtain and thus has motivated research on training deep neural networks in the presence of label noise. In this work, we build upon the seminal work in this area, Co-teaching and propose a simple, yet efficient approach termed mCT- S2R (modified co-teaching with self-supervision and re-labeling) for this task. Firstly, to deal with significant amount of noise in the labels, we propose to use self- supervision to generate robust features without using any labels. Furthermore, using a parallel network architecture, an estimate of the clean labeled portion of the data is obtained. Finally, using this data, a portion of the estimated noisy labeled portion is re-labeled, before resuming the network training with the augmented data. Extensive experiments on three standard datasets show the effectiveness of the proposed framework.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mandal_A_Novel_Self-Supervised_Re-labeling_Approach_for_Training_with_Noisy_Labels_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Science; University of T\u00fcbingen + Indian Institute of Science; Indian Institute of Science",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1087119,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15904719582259251847&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iisc.ac.in;gmail.com;iisc.ac.in",
        "email": "iisc.ac.in;gmail.com;iisc.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Indian Institute of Science;University of T\u00fcbingen",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.uni-tuebingen.de/",
        "aff_unique_abbr": "IISc;Uni T\u00fcbingen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "India;Germany"
    },
    {
        "title": "A one-and-half stage pedestrian detector",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ujjwal_A_one-and-half_stage_pedestrian_detector_WACV_2020_paper.html",
        "author": "Ujjwal Ujjwal;  Aziz Dziri;  Bertrand Leroy;  Francois Bremond",
        "abstract": "Pedestrian  detection  is  a  specific  instance  of  the  more general problem of object detection in computer vision.  A balance between detection accuracy and speed is a desirable trait for pedestrian detection systems in many applications such as self-driving cars.  In this paper, we follow the  wisdom  of  \"  and  less  is  often  more\"  to  achieve  this balance.   We  propose  a  lightweight  mechanism  based  on semantic segmentation to reduce the number of anchors to be processed.  We furthermore unify this selection with the intra-anchor feature pooling strategy adopted in high performance two-stage detectors such as Faster-RCNN. Such astrategy is avoided in one-stage detectors like SSD in favourof faster inference but at the cost of reducing the accuracy vis-`a-vis two-stage detectors.   However our anchor selection renders it practical to use feature pooling without giving up the inference speed. Our  proposed  approach  succeeds  in  detecting  pedestrians with state-of-art performance on caltech-reasonable and ciypersons datasets with inference speeds of 32fps.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ujjwal_A_one-and-half_stage_pedestrian_detector_WACV_2020_paper.pdf",
        "aff": "Institut VEDECOM+INRIA; Institut VEDECOM; Institut VEDECOM; INRIA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1621981,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15415219887490452122&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "inria.fr;vedecom.fr;vedecom.fr;inria.fr",
        "email": "inria.fr;vedecom.fr;vedecom.fr;inria.fr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;1",
        "aff_unique_norm": "Institut VEDECOM;INRIA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.vedecom.fr;https://www.inria.fr",
        "aff_unique_abbr": ";INRIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "ADNet: Adaptively Dense Convolutional Neural Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wang_ADNet_Adaptively_Dense_Convolutional_Neural_Networks_WACV_2020_paper.html",
        "author": "Mingjie Wang;  Hao Cai;  Xin Huang;  Minglun Gong",
        "abstract": "Convolutional neural networks (CNNs) have demonstrated great success in vision tasks. However, most existing architectures still suffer from low feature reuse efficiency. In this paper, we present a layer attention based Adaptively Dense Network (ADNet) by adaptively determining the reuse status of hierarchical preceding features. Specifically, a dense residual aggregation strategy is developed to fuse multi-level internal representations in an effective manner. Furthermore, a novel layer attention mechanism is proposed to explicitly model the interrelationship among layers to automatically adjust the density of the network. It is worth noting that existing ResNets and DenseNets are both special cases of our ADNet. Extensive experiments demonstrate that the proposed architecture consistently and indubitably achieves competitive results in accuracy on benchmark datasets (CIFAR10, CIFAR100, and SVHN), while at the same time remarkably reduces computational costs and memory space. Visualization and analysis on layer-wise attention further provide better understanding on the density of feature reuse in Deep Networks.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_ADNet_Adaptively_Dense_Convolutional_Neural_Networks_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science, Memorial University of Newfoundland, Canada+School of Computer Science, University of Guelph, Canada; Department of Computer Science, Memorial University of Newfoundland, Canada+School of Computer Science, University of Guelph, Canada; Department of Computer Science, Memorial University of Newfoundland, Canada+School of Computer Science, University of Guelph, Canada; School of Computer Science, University of Guelph, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1372520,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12579578430279821078&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "mun.ca;mun.ca;mun.ca;uoguelph.ca",
        "email": "mun.ca;mun.ca;mun.ca;uoguelph.ca",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;1",
        "aff_unique_norm": "Memorial University of Newfoundland;University of Guelph",
        "aff_unique_dep": "Department of Computer Science;School of Computer Science",
        "aff_unique_url": "https://www.mun.ca;https://www.uoguelph.ca",
        "aff_unique_abbr": "MUN;U of G",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Ablation-CAM: Visual Explanations for Deep Convolutional Network via Gradient-free Localization",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Desai_Ablation-CAM_Visual_Explanations_for_Deep_Convolutional_Network_via_Gradient-free_Localization_WACV_2020_paper.html",
        "author": "saurabh desai;  Harish Guruprasad Ramaswamy",
        "abstract": "In response to recent criticism of gradient-based visualization techniques, we propose a new methodology to generate visual explanations for deep Convolutional Neural Networks (CNN) - based models. Our approach - Ablation-based Class Activation Mapping (Ablation CAM) uses ablation analysis to determine the importance (weights) of individual feature map units w.r.t. class. Further, this is used to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Our objective and subjective evaluations show that this gradient-free approach works better than state-of-the-art Grad-CAM technique. Moreover, further experiments are carried out to show that Ablation-CAM is class discriminative as well as can be used to evaluate trust in a model.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Desai_Ablation-CAM_Visual_Explanations_for_Deep_Convolutional_Network_via_Gradient-free_Localization_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Indian Institute of Technology Madras + Robert Bosch Centre for Data Science and Arti\ufb01cial Intelligence (RBC-DSAI), Indian Institute of Technology, Madras; Department of Computer Science and Engineering, Indian Institute of Technology Madras + Robert Bosch Centre for Data Science and Arti\ufb01cial Intelligence (RBC-DSAI), Indian Institute of Technology, Madras",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Desai_Ablation-CAM_Visual_Explanations_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3577970,
        "gs_citation": 561,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18086110688722824760&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;cse.iitm.ac.in",
        "email": "gmail.com;cse.iitm.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Indian Institute of Technology Madras;Indian Institute of Technology, Madras",
        "aff_unique_dep": "Department of Computer Science and Engineering;Robert Bosch Centre for Data Science and Arti\ufb01cial Intelligence",
        "aff_unique_url": "https://www.iitm.ac.in;https://www.iitm.ac.in",
        "aff_unique_abbr": "IIT Madras;IIT Madras",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Madras",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "India"
    },
    {
        "title": "Accuracy Booster: Performance Boosting using Feature Map Re-calibration",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Singh_Accuracy_Booster_Performance_Boosting_using_Feature_Map_Re-calibration_WACV_2020_paper.html",
        "author": "Pravendra Singh;  PRATIK MAZUMDER;  Vinay Namboodiri",
        "abstract": "Convolution Neural Networks (CNN) have been extremely successful in solving intensive computer vision tasks. The convolutional filters used in CNNs have played a major role in this success, by extracting useful features from the inputs. Recently researchers have tried to boost the performance of CNNs by re-calibrating the feature maps produced by these filters, e.g., Squeeze-and-Excitation Networks (SENets). These approaches have achieved better performance by Exciting up the important channels or feature maps while diminishing the rest. However, in the process, architectural complexity has increased. We propose an architectural block that introduces much lower complexity than the existing methods of CNN performance boosting while performing significantly better than them. We carry out experiments on the CIFAR, ImageNet and MS-COCO datasets, and show that the proposed block can challenge the state-of-the-art results. Our method boosts the ResNet-50 architecture to perform comparably to the ResNet-152 architecture, which is a three times deeper network, on classification. We also show experimentally that our method is not limited to classification but also generalizes well to other tasks such as object detection.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Singh_Accuracy_Booster_Performance_Boosting_using_Feature_Map_Re-calibration_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science and Engineering, IIT Kanpur, India; Department of Computer Science and Engineering, IIT Kanpur, India; Department of Computer Science and Engineering, IIT Kanpur, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 545737,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=695460845786994068&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "email": "iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Kanpur",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kanpur",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Action Graphs: Weakly-supervised Action Localization with Graph Convolution Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Rashid_Action_Graphs_Weakly-supervised_Action_Localization_with_Graph_Convolution_Networks_WACV_2020_paper.html",
        "author": "Maheen Rashid;  Hedvig Kjellstrom;  Yong Jae Lee",
        "abstract": "We present a method for weakly-supervised action localization based on graph convolutions. In order to find and classify video time segments that correspond to relevant action classes, a system must be able to both identify discriminative time segments in each video, and identify the full extent of each action. Achieving this with weak video level labels requires the system to use similarity and dissimilarity between moments across videos in the training data to understand both how an action appears, as well as the sub-actions that comprise the action's full extent. However, current methods do not make explicit use of similarity between video moments to inform the localization and classification predictions. We present a novel method that uses graph convolutions to explicitly model similarity between video moments. Our method utilizes similarity graphs that encode appearance and motion, and pushes the state of the art on THUMOS `14, ActivityNet 1.2, and Charades for weakly-supervised action localization.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Rashid_Action_Graphs_Weakly-supervised_Action_Localization_with_Graph_Convolution_Networks_WACV_2020_paper.pdf",
        "aff": "University of California, Davis; KTH Royal Institute of Technology; University of California, Davis",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Rashid_Action_Graphs_Weakly-supervised_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1345322,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5233405710739860305&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ucdavis.edu;kth.se;ucdavis.edu",
        "email": "ucdavis.edu;kth.se;ucdavis.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Davis;KTH Royal Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucdavis.edu;https://www.kth.se",
        "aff_unique_abbr": "UC Davis;KTH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Davis;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Sweden"
    },
    {
        "title": "Action Segmentation with Mixed Temporal Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.html",
        "author": "Min-Hung Chen;  Baopu Li;  Yingze Bao;  Ghassan AlRegib",
        "abstract": "The main progress for action segmentation comes from densely-annotated data for fully-supervised learning. Since manual annotation for frame-level actions is timeconsuming and challenging, we propose to exploit auxiliary unlabeled videos, which are much easier to obtain, by shaping this problem as a domain adaptation (DA) problem. Although various DA techniques have been proposed in recent years, most of them have been developed only for the spatial direction. Therefore, we propose Mixed Temporal Domain Adaptation (MTDA) to jointly align frame- and video-level embedded feature spaces across domains, and further integrate with the domain attention mechanism to focus on aligning the frame-level features with higher domain discrepancy, leading to more effective domain adaptation. Finally, we evaluate our proposed methods on three challenging datasets (GTEA, 50Salads, and Breakfast), and validate that MTDA outperforms the current state-of-the-art methods on all three datasets by large margins (e.g. 6.4% gain on F1@50 and 6.8% gain on the edit score for GTEA).",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.pdf",
        "aff": "Georgia Institute of Technology; Baidu USA; Baidu USA; Georgia Institute of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Chen_Action_Segmentation_with_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1649700,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9174022661585325349&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gatech.edu;baidu.com;baidu.com;gatech.edu",
        "email": "gatech.edu;baidu.com;baidu.com;gatech.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Baidu",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://www.baidu.com",
        "aff_unique_abbr": "Georgia Tech;Baidu",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";USA",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Active Adversarial Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Su_Active_Adversarial_Domain_Adaptation_WACV_2020_paper.html",
        "author": "Jong-Chyi Su;  Yi-Hsuan Tsai;  Kihyuk Sohn;  Buyu Liu;  Subhransu Maji;  Manmohan Chandraker",
        "abstract": "We propose an active learning approach for transferring representations across domains. Our approach, active adversarial domain adaptation (AADA), explores a duality between two related problems: adversarial domain alignment and importance sampling for adapting models across domains. The former uses a domain discriminative model to align domains, while the latter utilizes the model to weigh samples to account for distribution shifts. Specifically, our importance weight promotes unlabeled samples with large uncertainty in classification and diversity compared to labeled examples, thus serving as a sample selection scheme for active learning. We show that these two views can be unified in one framework for domain adaptation and transfer learning when the source domain has many labeled examples while the target domain does not. AADA provides significant improvements over fine-tuning based approaches and other sampling methods when the two domains are closely related. Results on challenging domain adaptation tasks such as object detection demonstrate that the advantage over baseline approaches is retained even after hundreds of examples being actively annotated.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Su_Active_Adversarial_Domain_Adaptation_WACV_2020_paper.pdf",
        "aff": "UMass Amherst; NEC Laboratories America; NEC Laboratories America; NEC Laboratories America; UMass Amherst; NEC Laboratories America+UC San Diego",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Su_Active_Adversarial_Domain_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 880558,
        "gs_citation": 198,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6764668325015846087&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0;1+2",
        "aff_unique_norm": "University of Massachusetts Amherst;NEC Laboratories America;University of California, San Diego",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.umass.edu;https://www.nec-labs.com;https://www.ucsd.edu",
        "aff_unique_abbr": "UMass Amherst;NEC Labs America;UCSD",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Amherst;;San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Active Learning for Imbalanced Datasets",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Aggarwal_Active_Learning_for_Imbalanced_Datasets_WACV_2020_paper.html",
        "author": "Umang Aggarwal;  Adrian Popescu;  Celine Hudelot",
        "abstract": "Active learning increases the effectiveness of labeling when only subsets of unlabeled datasets can be processed manually. To our knowledge, existing algorithms are designed under the assumption that datasets are balanced.  However, many real-life datasets are actually imbalanced and we propose two adaptations of active learning to tackle imbalance. First, we modify acquisition functions to select samples by taking advantage of a deep model pretrained on a source domain. Second, we introduce a balancing step in the acquisition process to reduce the imbalance of the labeled subset. Evaluation is done with four imbalanced datasets using existing active learning methods and their modifications introduced here.  Results show that our adaptations are useful as long as knowledge from the source domain is transferable to target domains.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Aggarwal_Active_Learning_for_Imbalanced_Datasets_WACV_2020_paper.pdf",
        "aff": "Universit\u00e9 Paris-Saclay, CEA, D\u00e9partement Intelligence Ambiante et Syst\u00e8mes Interactifs+Universit\u00e9 Paris-Saclay, CentraleSup\u00e9lec, Math\u00e9matiques et Informatique pour la Complexit\u00e9 et les Syst\u00e8mes; Universit\u00e9 Paris-Saclay, CEA, D\u00e9partement Intelligence Ambiante et Syst\u00e8mes Interactifs; Universit\u00e9 Paris-Saclay, CentraleSup\u00e9lec, Math\u00e9matiques et Informatique pour la Complexit\u00e9 et les Syst\u00e8mes",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Aggarwal_Active_Learning_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 433066,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2297321455378597055&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cea.fr;cea.fr;centralesupelec.fr",
        "email": "cea.fr;cea.fr;centralesupelec.fr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0;0",
        "aff_unique_norm": "Universit\u00e9 Paris-Saclay",
        "aff_unique_dep": "D\u00e9partement Intelligence Ambiante et Syst\u00e8mes Interactifs",
        "aff_unique_url": "https://www.universite-paris-saclay.fr",
        "aff_unique_abbr": "UPS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Actor Conditioned Attention Maps for Video Action Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ulutan_Actor_Conditioned_Attention_Maps_for_Video_Action_Detection_WACV_2020_paper.html",
        "author": "Oytun Ulutan;  Swati Rallapalli;  Mudhakar Srivatsa;  Carlos Torres;  B. S. Manjunath",
        "abstract": "While observing complex events with multiple actors, humans do not assess each actor separately, but infer from the context. The surrounding context provides essential information for understanding actions. To this end, we propose to replace region of interest(RoI) pooling with an attention module, which ranks each spatio-temporal region's relevance to a detected actor instead of cropping. We refer to these as Actor-Conditioned Attention Maps (ACAM), which amplify/dampen the features extracted from the entire scene. The resulting actor-conditioned features focus the model on regions that are relevant to the conditioned actor. For actor localization, we leverage pre-trained object detectors, which transfer better. The proposed model is efficient and our action detection pipeline achieves near real-time performance. Experimental results on AVA 2.1 and JHMDB demonstrate the effectiveness of attention maps, with improvements of 7 mAP on AVA and 4 mAP on JHMDB.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ulutan_Actor_Conditioned_Attention_Maps_for_Video_Action_Detection_WACV_2020_paper.pdf",
        "aff": "University of California, Santa Barbara; IBM T. J. Watson Research Centre; IBM T. J. Watson Research Centre; University of California, Santa Barbara; University of California, Santa Barbara",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Ulutan_Actor_Conditioned_Attention_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 816743,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17909359670580037836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "University of California, Santa Barbara;IBM",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsb.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "UCSB;IBM",
        "aff_campus_unique_index": "0;1;1;0;0",
        "aff_campus_unique": "Santa Barbara;T. J. Watson",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adapting Grad-CAM for Embedding Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_Adapting_Grad-CAM_for_Embedding_Networks_WACV_2020_paper.html",
        "author": "Lei Chen;  Jianhui Chen;  Hossein Hajimirsadeghi;  Greg Mori",
        "abstract": "The gradient-weighted class activation mapping (Grad-CAM) method can faithfully highlight important regions in images for deep model prediction in image classification, image captioning and many other tasks. It uses the gradients in back-propagation as weights (grad-weights) to explain network decisions. However, applying Grad-CAM to embedding networks raises significant challenges because embedding networks are trained by millions of dynamically paired examples (e.g. triplets). To overcome these challenges, we propose an adaptation of the Grad-CAM method for embedding networks. First, we aggregate grad-weights from multiple training examples to improve the stability of Grad-CAM. Then, we develop an efficient weight-transfer method to explain decisions for any image without back-propagation. We extensively validate the method on the standard CUB200 dataset in which our method produces more accurate visual attention than the original Grad-CAM method. We also apply the method to a house price estimation application using images. The method produces convincing qualitative results, showcasing the practicality of our approach.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Adapting_Grad-CAM_for_Embedding_Networks_WACV_2020_paper.pdf",
        "aff": "Borealis AI+Simon Fraser University; Borealis AI; Borealis AI; Borealis AI+Simon Fraser University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Chen_Adapting_Grad-CAM_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 631899,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10143513509243346481&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "borealisai.com;borealisai.com;borealisai.com;borealisai.com",
        "email": "borealisai.com;borealisai.com;borealisai.com;borealisai.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0+1",
        "aff_unique_norm": "Borealis AI;Simon Fraser University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.borealisai.com;https://www.sfu.ca",
        "aff_unique_abbr": "Borealis AI;SFU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Adapting Style and Content for Attended Text Sequence Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Schwarcz_Adapting_Style_and_Content_for_Attended_Text_Sequence_Recognition_WACV_2020_paper.html",
        "author": "Steven Schwarcz;  Alexander Gorban;  Xavier Gibert;  Dar-Shyang Lee",
        "abstract": "In this paper, we address the problem of learning to perform sequential OCR on photos of street name signs in a language for which no labeled data exists. Our approach leverages easily-generated synthetic data and existing labeled data in other languages to achieve reasonable performance on these unlabeled images, through a combination of a novel domain adaptation technique based on gradient reversal and a multi-task learning scheme. In order to accomplish this, we introduce and release two new datasets - Hebrew Street Name Signs (HSNS) and Synthetic Hebrew Street Name Signs (SynHSNS) - while also making use of the existing French Street Name Signs (FSNS) dataset.  We demonstrate that by using a synthetic dataset of Hebrew characters and a labeled dataset of French street name signs in natural images, it is possible to achieve a significant improvement on real Hebrew street name sign transcription, where the synthetic Hebrew data and real French data each overlap with different features of the images we wish to transcribe.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Schwarcz_Adapting_Style_and_Content_for_Attended_Text_Sequence_Recognition_WACV_2020_paper.pdf",
        "aff": "University of Marlyand, College Park; Google Research; Google Research; Google Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Schwarcz_Adapting_Style_and_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 843135,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6561071919374094630&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "umiacs.umd.edu;google.com;google.com;google.com",
        "email": "umiacs.umd.edu;google.com;google.com;google.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Maryland;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www/umd.edu;https://research.google",
        "aff_unique_abbr": "UMD;Google Research",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "College Park;Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adaptive Aggregation of Arbitrary Online Trackers with a Regret Bound",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Song_Adaptive_Aggregation_of_Arbitrary_Online_Trackers_with_a_Regret_Bound_WACV_2020_paper.html",
        "author": "Heon Song;  Daiki Suehiro;  Seiichi Uchida",
        "abstract": "We propose an online visual-object tracking method that is robust even in an adversarial environment, where various disturbances may occur on the target appearance, etc. The proposed method is based on a delayed-Hedge algorithm for aggregating multiple arbitrary online trackers with adaptive weights. The robustness in the tracking performance is guaranteed theoretically in term of \"regret\" by the property of the delayed-Hedge algorithm. Roughly speaking, the proposed method can achieve a similar tracking performance as the best one among all the trackers to be aggregated in an adversarial environment. The experimental study on various tracking tasks shows that the proposed method could achieve state-of-the-art performance by aggregating various online trackers.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Song_Adaptive_Aggregation_of_Arbitrary_Online_Trackers_with_a_Regret_Bound_WACV_2020_paper.pdf",
        "aff": "Kyushu University+RIKEN AIP; Kyushu University+RIKEN AIP; Kyushu University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Song_Adaptive_Aggregation_of_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 7514145,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14825165969259817878&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "human.ait.kyushu-u.ac.jp;ait.kyushu-u.ac.jp;ait.kyushu-u.ac.jp",
        "email": "human.ait.kyushu-u.ac.jp;ait.kyushu-u.ac.jp;ait.kyushu-u.ac.jp",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0",
        "aff_unique_norm": "Kyushu University;RIKEN",
        "aff_unique_dep": ";Advanced Institute for Computational Science",
        "aff_unique_url": "https://www.kyushu-u.ac.jp;https://www.aip.riken.jp",
        "aff_unique_abbr": "Kyushu U;RIKEN AIP",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Adaptive Neural Connections for Sparsity Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Gain_Adaptive_Neural_Connections_for_Sparsity_Learning_WACV_2020_paper.html",
        "author": "Alex Gain;  Prakhar Kaushik;  Hava Siegelmann",
        "abstract": "Sparsity learning aims to decrease the computational and memory costs of large deep neural networks (DNNs) via pruning neural connections while simulaneously retaining high accuracy. A large body of work has developed sparsity learning approaches, with recent large-scale experiments showing that two main methods, magnitude pruning and Variational Dropout (VD), achieve similar state-of-the-art results for classification tasks. We propose Adaptive Neural Connections (ANC), a method for explicitly parameterizing fine-grained neuron-to-neuron connections via adjacency matrices at each layer that are learned through backpropagation. Explicitly parameterizing neuron-to-neuron connections confers two primary advantages: 1. Sparsity can be explicitly optimized for via norm-based regularization on the adjacency matrices; and 2. When combined with VD (which we term, ANC-VD), the adjacencies can be interpreted as learned weight importance parameters, which we hypothesize leads to improved convergence for VD. Experiments with ResNet18 show that architectures augmented with ANC outperform their vanilla counterparts.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Gain_Adaptive_Neural_Connections_for_Sparsity_Learning_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science, Johns Hopkins University; Department of Computer Science, Johns Hopkins University; School of Computer Science, University of Massachussetts Amherst",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 963261,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16714412817848282018&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "jhu.edu;jhu.edu;cs.umass.edu",
        "email": "jhu.edu;jhu.edu;cs.umass.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Johns Hopkins University;University of Massachusetts Amherst",
        "aff_unique_dep": "Department of Computer Science;School of Computer Science",
        "aff_unique_url": "https://www.jhu.edu;https://www.umass.edu",
        "aff_unique_abbr": "JHU;UMass Amherst",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Amherst",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adversarial Defense based on Structure-to-Signal Autoencoders",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Palacio_Adversarial_Defense_based_on_Structure-to-Signal_Autoencoders_WACV_2020_paper.html",
        "author": "Sebastian Palacio;  Joachim Folz;  Jorn Hees;  Andreas Dengel",
        "abstract": "Adversarial attacks have exposed the intricacies of the complex loss surfaces approximated by neural networks. In this paper, we present a defense strategy against gradient-based attacks, on the premise that input gradients need to expose information about the semantic manifold for attacks to be successful. We propose an architecture based on compressive autoencoders (AEs) with a two-stage training scheme, creating not only an architectural bottleneck but also a representational bottleneck. We show that the proposed mechanism yields robust results against a collection of gradient-based attacks under challenging white-box conditions. This defense is attack-agnostic and can, therefore, be used for arbitrary pre-trained models, while not compromising the original performance. These claims are supported by experiments conducted with state-of-the-art image classifiers (ResNet50 and Inception v3), on the full ImageNet validation set. Experiments, including counterfactual analysis, empirically show that the robustness stems from a shift in the distribution of input gradients, which mitigates the effect of tested adversarial attack methods. Gradients propagated through the proposed AEs represent less semantic information and instead point to low-level structural features.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Palacio_Adversarial_Defense_based_on_Structure-to-Signal_Autoencoders_WACV_2020_paper.pdf",
        "aff": "German Research Center for Artificial Intelligence (DFKI) + TU Kaiserslautern; German Research Center for Artificial Intelligence (DFKI) + TU Kaiserslautern; German Research Center for Artificial Intelligence (DFKI) + TU Kaiserslautern; German Research Center for Artificial Intelligence (DFKI) + TU Kaiserslautern",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 497398,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11818770330295039061&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "dfki.de;dfki.de;dfki.de;dfki.de",
        "email": "dfki.de;dfki.de;dfki.de;dfki.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "German Research Center for Artificial Intelligence;Technische Universit\u00e4t Kaiserslautern",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.dFKI.de;https://www.tu-kl.de",
        "aff_unique_abbr": "DFKI;TU Kaiserslautern",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Adversarial Discriminative Attention for Robust Anomaly Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kimura_Adversarial_Discriminative_Attention_for_Robust_Anomaly_Detection_WACV_2020_paper.html",
        "author": "Daiki Kimura;  Subhajit Chaudhury;  Minori Narita;  Asim Munawar;  Ryuki Tachibana",
        "abstract": "Existing methods for visual anomaly detection predominantly rely on global level pixel comparisons for anomaly score computation without emphasizing on unique local features. However, images from real-world applications are susceptible to unwanted noise and distractions, that might jeopardize the robustness of such anomaly score. To alleviate this problem, we propose a self-supervised masking method that specifically focuses on discriminative parts of images to enable robust anomaly detection. Our experiments reveal that discriminator's class activation map in adversarial training evolves in three stages and finally fixates on the foreground location in the images. Using this property of the activation map, we construct a mask that suppresses spurious signals from the background thus enabling robust anomaly detection by focusing on local discriminative attributes. Additionally, our method can further improve the accuracy by learning a semi-supervised discriminative classifier in cases where a few samples from anomaly classes are available during the training. Experimental evaluations on four different types of datasets demonstrate that our method outperforms previous state-of-the-art methods for each condition and in all domains.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kimura_Adversarial_Discriminative_Attention_for_Robust_Anomaly_Detection_WACV_2020_paper.pdf",
        "aff": "IBM Research AI; IBM Research AI; University of Massachusetts, Amherst; IBM Research AI; IBM Research AI",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Kimura_Adversarial_Discriminative_Attention_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2120913,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4936882366354820783&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "jp.ibm.com;jp.ibm.com;umass.edu;jp.ibm.com;jp.ibm.com",
        "email": "jp.ibm.com;jp.ibm.com;umass.edu;jp.ibm.com;jp.ibm.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "IBM Research;University of Massachusetts Amherst",
        "aff_unique_dep": "AI;",
        "aff_unique_url": "https://www.ibm.com/research;https://www.umass.edu",
        "aff_unique_abbr": "IBM;UMass Amherst",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Amherst",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adversarial Examples for Edge Detection: They Exist, and they Transfer",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Cosgrove_Adversarial_Examples_for_Edge_Detection_They_Exist_and_they_Transfer_WACV_2020_paper.html",
        "author": "Christian Cosgrove;  Alan Yuille",
        "abstract": "Convolutional neural networks have recently advanced the state of the art in many tasks including edge and object boundary detection. However, in this paper, we demonstrate that these edge detectors inherit a troubling property of neural networks: they can be fooled by adversarial examples. We show that adding small perturbations to an image causes HED, a CNN-based edge detection model, to fail to locate edges, to detect nonexistent edges, and even to hallucinate arbitrary configurations of edges. More importantly, we find that these adversarial examples blindly transfer to other CNN-based vision models. In particular, attacks on edge detection result in significant drops in accuracy in models trained to perform unrelated, high-level tasks like image classification and semantic segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Cosgrove_Adversarial_Examples_for_Edge_Detection_They_Exist_and_they_Transfer_WACV_2020_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Cosgrove_Adversarial_Examples_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 612038,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2218816903829123571&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Adversarial Sampling for Active Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mayer_Adversarial_Sampling_for_Active_Learning_WACV_2020_paper.html",
        "author": "Christoph Mayer;  Radu Timofte",
        "abstract": "This paper proposes ASAL, a new GAN based active learning method that generates high entropy samples. Instead of directly annotating the synthetic samples, ASAL searches similar samples from the pool and includes them for training. Hence, the quality of new samples is high and annotations are reliable. To the best of our knowledge, ASAL is the first GAN based AL method applicable to multi-class problems that outperforms random sample selection. Another benefit of ASAL is its small run-time complexity(sub-linear) compared to traditional uncertainty sampling (linear). We present a comprehensive set of experiments on multiple traditional data sets and show that ASAL outperforms similar methods and clearly exceeds the established baseline (random sampling). In the discussion section we analyze in which situations ASAL performs best and why it is sometimes hard to outperform random sample selection.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mayer_Adversarial_Sampling_for_Active_Learning_WACV_2020_paper.pdf",
        "aff": "Computer Vision Lab, ETH Z\u00fcrich, Switzerland; Computer Vision Lab, ETH Z\u00fcrich, Switzerland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Mayer_Adversarial_Sampling_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 504748,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=838529958121868178&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch;vision.ee.ethz.ch",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Z\u00fcrich",
        "aff_unique_dep": "Computer Vision Lab",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Z\u00fcrich",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "AlignNet: A Unifying Approach to Audio-Visual Alignment",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wang_AlignNet_A_Unifying_Approach_to_Audio-Visual_Alignment_WACV_2020_paper.html",
        "author": "Jianren Wang;  Zhaoyuan Fang;  Hang Zhao",
        "abstract": "We present AlignNet, a model that synchronizes videos with reference audios under non-uniform and irregular misalignments. AlignNet learns the end-to-end dense correspondence between each frame of a video and an audio. Our method is designed according to simple and well-established principles: attention, pyramidal processing, warping, and affinity function. Together with the model, we release a dancing dataset Dance50 for training and evaluation. Qualitative, quantitative and subjective evaluation results on dance-music alignment and speech-lip alignment demonstrate that our method far outperforms the state-of-the-art methods. Code, dataset and sample videos are available at our project page.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_AlignNet_A_Unifying_Approach_to_Audio-Visual_Alignment_WACV_2020_paper.pdf",
        "aff": "CMU; University of Notre Dame; MIT",
        "project": "https://jianrenw.github.io/AlignNet/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1298572,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16371291152239167813&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "andrew.cmu.edu;nd.edu;csail.mit.edu",
        "email": "andrew.cmu.edu;nd.edu;csail.mit.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Carnegie Mellon University;University of Notre Dame;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.nd.edu;https://web.mit.edu",
        "aff_unique_abbr": "CMU;Notre Dame;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "An Adversarial Domain Adaptation Network for Cross-Domain Fine-Grained Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wang_An_Adversarial_Domain_Adaptation_Network_for_Cross-Domain_Fine-Grained_Recognition_WACV_2020_paper.html",
        "author": "Yimu Wang;  Renjie Song;  Xiu-Shen Wei;  Lijun Zhang",
        "abstract": "In this paper, we tackle a valuable yet very challenging visual recognition task, where the instances are within a subordinate category, and the target domain undergoes a shift with the source domain. This task, termed as cross-domain fine-grained recognition, relates closely to many real-life scenarios, e.g., recognizing retail products in storage racks by models trained with images collected in controlled environments. To deal with this problem, we design a new algorithm and propose a corresponding fine-grained domain adaptation dataset. Firstly, we propose a novel end-to-end CNN architecture that integrates two specialized modules: an adversarial module for domain alignment and a self-attention module for fine-grained recognition. The adversarial module is used to handle domain shift by gradually aligning the different domains with domain-level and class-level alignments, and strive to help the classifier learn with domain-invariant features generated by nets.  The self-attention module is designed to capture discriminative image regions which are crucial for fine-grained visual recognition. Secondly, we collect a large-scale fine-grained domain adaptation dataset of retail products, which contains 52,011 images of 263 classes from 3 domains. Thirdly, we validate the effectiveness of our method on three datasets, showing that the proposed method can yield significant improvements over baseline methods on fine-grained datasets. Besides, we also evaluate the effectiveness of the self-attention module by performing visualization, which can capture the discriminative image regions in both source and target domains.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_An_Adversarial_Domain_Adaptation_Network_for_Cross-Domain_Fine-Grained_Recognition_WACV_2020_paper.pdf",
        "aff": "Nanjing University; Megvii Technology; Megvii Technology; Nanjing University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2698183,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17955538889609092842&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "lamda.nju.edu.cn;megvii.com;megvii.com;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;megvii.com;megvii.com;lamda.nju.edu.cn",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Nanjing University;Megvii Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nju.edu.cn;https://www.megvii.com",
        "aff_unique_abbr": "Nanjing U;Megvii",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "An Extended Exposure Fusion and its Application to Single Image Contrast Enhancement",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hessel_An_Extended_Exposure_Fusion_and_its_Application_to_Single_Image_WACV_2020_paper.html",
        "author": "Charles Hessel;  Jean-Michel  Morel",
        "abstract": "Exposure Fusion is a high dynamic range imaging technique fusing a bracketed exposure sequence into a high quality image. In this paper, we provide a refined version resolving its out-of-range artifact and its low-frequency halo. It improves on the original Exposure Fusion by augmenting contrast in all image parts. Furthermore, we extend this algorithm to single exposure images, thereby turning it into a competitive contrast enhancement operator. To do so, bracketed images are first simulated from a single input image and then fused by the new version of Exposure Fusion. The resulting algorithm competes with state of the art image enhancement methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hessel_An_Extended_Exposure_Fusion_and_its_Application_to_Single_Image_WACV_2020_paper.pdf",
        "aff": "Universit \u00b4e Paris-Saclay, ENS Paris-Saclay, CNRS, Centre de math \u00b4ematiques et de leurs applications, 94235 Cachan, France; Universit \u00b4e Paris-Saclay, ENS Paris-Saclay, CNRS, Centre de math \u00b4ematiques et de leurs applications, 94235 Cachan, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9682334,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15551959153592251935&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ens-paris-saclay.fr;ens-paris-saclay.fr",
        "email": "ens-paris-saclay.fr;ens-paris-saclay.fr",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 Paris-Saclay",
        "aff_unique_dep": "Centre de math\u00e9matiques et de leurs applications",
        "aff_unique_url": "https://www.universite-paris-saclay.fr",
        "aff_unique_abbr": "UPS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cachan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Analysis and a Solution of Momentarily Missed Detection for Anchor-based Object Detectors",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hosoya_Analysis_and_a_Solution_of_Momentarily_Missed_Detection_for_Anchor-based_WACV_2020_paper.html",
        "author": "Yusuke Hosoya;  Masanori Suganuma;  Takayuki Okatani",
        "abstract": "The employment of convolutional neural networks has led to significant performance improvement on the task of object detection. However, when applying existing detectors to continuous frames in a video, we often encounter momentary miss-detection of objects, that is, objects are undetected exceptionally at a few frames, although they are correctly detected at all other frames. In this paper, we analyze the mechanism of how such miss-detection occurs. For the most popular class of detectors that are based on anchor boxes, we show the followings: i) besides apparent causes such as motion blur, occlusions, background clutters, etc., the majority of remaining miss-detection can be explained by an improper behavior of the detectors at boundaries of the anchor boxes; and ii) this can be rectified by improving the way of choosing positive samples from candidate anchor boxes when training the detectors.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hosoya_Analysis_and_a_Solution_of_Momentarily_Missed_Detection_for_Anchor-based_WACV_2020_paper.pdf",
        "aff": "Graduate School of Information Sciences, Tohoku University+RIKEN Center for AIP; Graduate School of Information Sciences, Tohoku University+RIKEN Center for AIP; Graduate School of Information Sciences, Tohoku University+RIKEN Center for AIP",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5384583,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1058462735254922964&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "vision.is.tohoku.ac.jp;vision.is.tohoku.ac.jp;vision.is.tohoku.ac.jp",
        "email": "vision.is.tohoku.ac.jp;vision.is.tohoku.ac.jp;vision.is.tohoku.ac.jp",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Tohoku University;RIKEN",
        "aff_unique_dep": "Graduate School of Information Sciences;Center for AIP",
        "aff_unique_url": "https://www.tohoku.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "Tohoku U;RIKEN",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Anchor Box Optimization for Object Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhong_Anchor_Box_Optimization_for_Object_Detection_WACV_2020_paper.html",
        "author": "Yuanyi Zhong;  Jianfeng Wang;  Jian Peng;  Lei Zhang",
        "abstract": "In this paper, we propose a general approach to optimize anchor boxes for object detection. Nowadays, anchor boxes are widely adopted in state-of-the-art detection frameworks. However, these frameworks usually pre-define anchor box shapes in heuristic ways and fix the sizes during training. To improve the accuracy and reduce the effort of designing anchor boxes, we propose to dynamically learn the anchor shapes, which allows the anchors to automatically adapt to the data distribution and the network learning capability. The learning approach can be easily implemented with stochastic gradient descent and can be plugged into any anchor box-based detection framework. The extra training cost is almost negligible and it has no impact on the inference time or memory cost.  Exhaustive experiments demonstrate that the proposed anchor optimization method consistently achieves significant improvement (>1% mAP absolute gain) over the baseline methods on several benchmark datasets including Pascal VOC 07+12, MS COCO and Brainwash. Meanwhile, the robustness is also verified towards different anchor initialization methods and the number of anchor shapes, which greatly simplifies the problem of anchor box design.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhong_Anchor_Box_Optimization_for_Object_Detection_WACV_2020_paper.pdf",
        "aff": "University of Illinois at Urbana-Champaign; Microsoft; University of Illinois at Urbana-Champaign; Microsoft",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 663904,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9046494337083773380&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "illinois.edu;microsoft.com;illinois.edu;microsoft.com",
        "email": "illinois.edu;microsoft.com;illinois.edu;microsoft.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Microsoft Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.microsoft.com",
        "aff_unique_abbr": "UIUC;Microsoft",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Animal Detection in Man-made Environments",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Singh_Animal_Detection_in_Man-made_Environments_WACV_2020_paper.html",
        "author": "Abhineet Singh;  Marcin Pietrasik;  Gabriell Natha;  Nehla Ghouaiel;  Ken Brizel;  Nilanjan Ray",
        "abstract": "Automatic detection of animals that have strayed into human inhabited areas has important security and road safety applications. This paper attempts to solve this problem using deep learning techniques from a variety of computer vision fields including object detection, tracking, segmentation and edge detection. Several interesting insights into transfer learning are elicited while adapting models trained on benchmark datasets for real world deployment. Empirical evidence is presented to demonstrate the inability of detectors to generalize from training images of animals in their natural habitats to deployment scenarios of man-made environments. A solution is also proposed using semi-automated synthetic data generation for domain specific training. Code and data used in the experiments are made available to facilitate further work in this domain.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Singh_Animal_Detection_in_Man-made_Environments_WACV_2020_paper.pdf",
        "aff": "Department of Computing Science, University of Alberta; Alberta Centre for Advanced MNT Products (ACAMP); Alberta Centre for Advanced MNT Products (ACAMP); Alberta Centre for Advanced MNT Products (ACAMP); Alberta Centre for Advanced MNT Products (ACAMP); Department of Computing Science, University of Alberta",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Singh_Animal_Detection_in_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3045483,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18121978164358626895&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "University of Alberta;Alberta Centre for Advanced MNT Products",
        "aff_unique_dep": "Department of Computing Science;",
        "aff_unique_url": "https://www.ualberta.ca;",
        "aff_unique_abbr": "UAlberta;ACAMP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Animating Face using Disentangled Audio Representations",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mittal_Animating_Face_using_Disentangled_Audio_Representations_WACV_2020_paper.html",
        "author": "Gaurav Mittal;  Baoyuan Wang",
        "abstract": "Previous methods for audio-driven talking head generation assume the input audio to be clean with a neutral tone. As we show empirically, one can easily break these systems by simply adding certain background noise to the utterance or changing its emotional tone (to for example, sad). To make talking head generation robust to such variations, we propose an explicit audio representation learning framework that disentangles audio sequences into various factors such as phonetic content, emotional tone, background noise and others. We conduct experiments to validate that when conditioned on disentangled content representation, the generated mouth movement by our model is significantly more accurate than previous approaches (without disentangled learning) in the presence of noise and emotional variations. We further demonstrate that our framework is compatible with current state-of-the-art approaches by replacing their original component to learn audio based representation with ours. To the best of our knowledge, this is the first work which improves the performance of talking head generation through a disentangled audio representation perspective, which is important for many real-world applications.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mittal_Animating_Face_using_Disentangled_Audio_Representations_WACV_2020_paper.pdf",
        "aff": "Microsoft; Microsoft",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Mittal_Animating_Face_using_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1001862,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8319429690790628545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Answering Questions about Data Visualizations using Efficient Bimodal Fusion",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kafle_Answering_Questions_about_Data_Visualizations_using_Efficient_Bimodal_Fusion_WACV_2020_paper.html",
        "author": "Kushal Kafle;  Robik Shrestha;  Scott Cohen;  Brian Price;  Christopher Kanan",
        "abstract": "Chart question answering (CQA) is a newly proposed visual question answering (VQA) task where an algorithm must answer questions about data visualizations, e.g. bar charts, pie charts, and line graphs. CQA requires capabilities that natural-image VQA algorithms lack: fine-grained measurements, optical character recognition, and handling out-of-vocabulary words in both questions and answers. Without modifications, state-of-the-art VQA algorithms perform poorly on this task. Here, we propose a novel CQA algorithm called parallel recurrent fusion of image and language (PReFIL). PReFIL first learns bimodal embeddings by fusing question and image features and then intelligently aggregates these learned embeddings to answer the given question. Despite its simplicity, PReFIL greatly surpasses state-of-the art systems and human baselines on both the FigureQA and DVQA datasets. Additionally, we demonstrate that PReFIL can be used to reconstruct tables by asking a series of questions about a chart.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kafle_Answering_Questions_about_Data_Visualizations_using_Efficient_Bimodal_Fusion_WACV_2020_paper.pdf",
        "aff": "Rochester Institute of Technology; Rochester Institute of Technology; Adobe Research; Adobe Research; Rochester Institute of Technology+Paige+Cornell Tech",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Kafle_Answering_Questions_about_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 914950,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6445762653156216980&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "rit.edu;rit.edu;adobe.com;adobe.com;rit.edu",
        "email": "rit.edu;rit.edu;adobe.com;adobe.com;rit.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;0+2+3",
        "aff_unique_norm": "Rochester Institute of Technology;Adobe;Paige;Cornell University",
        "aff_unique_dep": ";Adobe Research;;",
        "aff_unique_url": "https://www.rit.edu;https://research.adobe.com;;https://tech.cornell.edu",
        "aff_unique_abbr": "RIT;Adobe;;Cornell Tech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York City",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Appearance and Shape from Water Reflection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kawahara_Appearance_and_Shape_from_Water_Reflection_WACV_2020_paper.html",
        "author": "Ryo Kawahara;  Meng-Yu Kuo;  Shohei Nobuhara;  Ko Nishino",
        "abstract": "This paper introduces single-image geometric and appearance reconstruction from water reflection photography, i.e., images capturing direct and water-reflected real-world scenes. Water reflection offers an additional viewpoint to the direct sight, collectively forming a stereo pair. The water-reflected scene, however, includes internally scattered and reflected environmental illumination in addition to the scene radiance, which precludes direct stereo matching. We derive a principled iterative method that disentangles this scene radiometry and geometry for reconstructing 3D scene structure as well as its high-dynamic range appearance. In the presence of waves, we simultaneously recover the wave geometry as surface normal perturbations of the water surface. Most important, we show that the water reflection enables calibration of the camera. In other words, for the first time, we show that capturing a direct and water-reflected scene in a single exposure forms a self-calibrating HDR catadioptric stereo camera. We demonstrate our method on a number of images taken in the wild. The results demonstrate a new means for leveraging this accidental catadioptric camera.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kawahara_Appearance_and_Shape_from_Water_Reflection_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Kawahara_Appearance_and_Shape_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1946893,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5776443366974605856&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Architecture Search of Dynamic Cells for Semantic Video Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Nekrasov_Architecture_Search_of_Dynamic_Cells_for_Semantic_Video_Segmentation_WACV_2020_paper.html",
        "author": "Vladimir Nekrasov;  Hao Chen;  Chunhua Shen;  Ian Reid",
        "abstract": "In semantic video segmentation the goal is to acquire consistent dense semantic labelling across image frames. To this end, recent approaches have been reliant on manually arranged operations applied on top of static semantic segmentation networks -- with the most prominent building block being the optical flow able to provide information about scene dynamics. Related to that is the line of research concerned with speeding up static networks by approximating expensive parts of them with cheaper alternatives, while propagating information from previous frames. In this work we attempt to come up with generalisation of those methods, and instead of manually designing contextual blocks that connect per-frame outputs, we propose a neural architecture search solution, where the choice of operations together with their sequential arrangement are being predicted by a separate neural network. We showcase that such generalisation leads to stable and accurate results across common benchmarks, such as CityScapes and CamVid datasets. Importantly, the proposed methodology takes only 2 GPU-days, finds high-performing cells and does not rely on the expensive optical flow computation.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Nekrasov_Architecture_Search_of_Dynamic_Cells_for_Semantic_Video_Segmentation_WACV_2020_paper.pdf",
        "aff": "The University of Adelaide, Australia; The University of Adelaide, Australia; The University of Adelaide, Australia; The University of Adelaide, Australia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Nekrasov_Architecture_Search_of_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1309683,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9294119069527260137&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The University of Adelaide",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Attention Flow: End-to-End Joint Attention Estimation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sumer_Attention_Flow_End-to-End_Joint_Attention_Estimation_WACV_2020_paper.html",
        "author": "Omer Sumer;  Peter Gerjets;  Ulrich Trautwein;  Enkelejda Kasneci",
        "abstract": "This paper addresses the problem of understanding joint attention in third-person social scene videos. Joint attention is the shared gaze behaviour of two or more individuals on an object or an area of interest and has a wide range of applications such as human-computer interaction, educational assessment, treatment of patients with attention disorders, and many more. Our method, Attention Flow, learns joint attention in an end-to-end fashion by using saliency-augmented attention maps and two novel convolutional attention mechanisms that determine to select relevant features and improve joint attention localization. We compare the effect of saliency maps and attention mechanisms and report quantitative and qualitative results on the detection and localization of joint attention in the VideoCoAtt dataset, which contains complex social scenes.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sumer_Attention_Flow_End-to-End_Joint_Attention_Estimation_WACV_2020_paper.pdf",
        "aff": "University of T\u00fcbingen; Leibniz-Institut f\u00fcr Wissensmedien; University of T\u00fcbingen; University of T\u00fcbingen",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2033341,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6303917160460498899&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "uni-tuebingen.de;iwm-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de",
        "email": "uni-tuebingen.de;iwm-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of T\u00fcbingen;Leibniz-Institut f\u00fcr Wissensmedien",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-tuebingen.de/;https://www.iwm-karlsruhe.de",
        "aff_unique_abbr": "Uni T\u00fcbingen;IWM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Attention-based Fusion for Multi-source Human Image Generation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Lathuiliere_Attention-based_Fusion_for_Multi-source_Human_Image_Generation_WACV_2020_paper.html",
        "author": "Stephane Lathuiliere;  Enver Sangineto;  Aliaksandr Siarohin;  Nicu Sebe",
        "abstract": "We present a generalization of the person-image generation task, in which a human image is generated conditioned on a target pose and a set X of source appearance images. In this way, we can exploit multiple, possibly complementary images of the same person which are usually available at training and at testing time. The solution we propose is mainly based on a local attention mechanism which selects relevant information from different source image regions, avoiding the necessity to build specific generators for each specific cardinality of X. The empirical evaluation of our method shows the practical interest of addressing the person-image generation problem in a multi-source setting.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Lathuiliere_Attention-based_Fusion_for_Multi-source_Human_Image_Generation_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Lathuiliere_Attention-based_Fusion_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 813221,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13870204596435783955&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Audio-Visual Model Distillation Using Acoustic Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Perez_Audio-Visual_Model_Distillation_Using_Acoustic_Images_WACV_2020_paper.html",
        "author": "Andres Perez;  Valentina Sanguineti;  Pietro Morerio;  Vittorio Murino",
        "abstract": "In this paper, we investigate how to learn rich and robust feature representations for audio classification from visual data and acoustic images, a novel audio data modality. Former models learn audio representations from raw signals or spectral data acquired by a single microphone, with remarkable results in classification and retrieval. However, such representations are not so robust towards variable environmental sound conditions. We tackle this drawback by exploiting a new multimodal labeled action recognition dataset acquired by a hybrid audio-visual sensor that provides RGB video, raw audio signals, and spatialized acoustic data, also known as acoustic images, where the visual and acoustic images are aligned in space and synchronized in time. Using this richer information, we train audio deep learning models in a teacher-student fashion. In particular, we  distill knowledge into audio networks from both visual and acoustic image teachers. Our experiments suggest that the learned representations are more powerful and have better generalization capabilities than the features learned from models trained using just single-microphone audio data.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Perez_Audio-Visual_Model_Distillation_Using_Acoustic_Images_WACV_2020_paper.pdf",
        "aff": "Pattern Analysis & Computer Vision - Istituto Italiano di Tecnologia; Pattern Analysis & Computer Vision - Istituto Italiano di Tecnologia + Universit\u00e0 degli Studi di Genova, Italy; Pattern Analysis & Computer Vision - Istituto Italiano di Tecnologia; Pattern Analysis & Computer Vision - Istituto Italiano di Tecnologia + Computer Science Department - Universit\u00e0 di Verona, Italy + Huawei Technologies Ltd., Ireland Research Center",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Perez_Audio-Visual_Model_Distillation_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 590603,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14141352677713056516&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mail.polimi.it;iit.it;iit.it;iit.it",
        "email": "mail.polimi.it;iit.it;iit.it;iit.it",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0+2+3",
        "aff_unique_norm": "Istituto Italiano di Tecnologia;Universit\u00e0 degli Studi di Genova;Universit\u00e0 di Verona;Huawei Technologies",
        "aff_unique_dep": "Pattern Analysis & Computer Vision;;Computer Science Department;Ireland Research Center",
        "aff_unique_url": "https://www.iit.it;https://www.unige.it;https://www.univr.it;https://www.huawei.com",
        "aff_unique_abbr": "IIT;UniGe;UniVR;Huawei",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0+0+1",
        "aff_country_unique": "Italy;Ireland"
    },
    {
        "title": "AutoToon: Automatic Geometric Warping for Face Cartoon Generation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Gong_AutoToon_Automatic_Geometric_Warping_for_Face_Cartoon_Generation_WACV_2020_paper.html",
        "author": "Julia Gong;  Yannick Hold-Geoffroy;  Jingwan Lu",
        "abstract": "Caricature, a type of exaggerated artistic portrait, amplifies the distinctive, yet nuanced traits of human faces. This task is typically left to artists, as it has proven difficult to capture subjects' unique characteristics well using automated methods. Recent development of deep end-to-end methods has achieved promising results in capturing style and higher-level exaggerations. However, a key part of caricatures, face warping, has remained challenging for these systems. In this work, we propose AutoToon, the first supervised deep learning method that yields high-quality warps for the warping component of caricatures. Completely disentangled from style, it can be paired with any stylization method to create diverse caricatures. In contrast to prior art, we leverage an SENet and spatial transformer module and train directly on artist warping fields, applying losses both prior to and after warping. As shown by our user studies, we achieve appealing exaggerations that amplify distinguishing features of the face while preserving facial detail.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Gong_AutoToon_Automatic_Geometric_Warping_for_Face_Cartoon_Generation_WACV_2020_paper.pdf",
        "aff": "Stanford University; Adobe Research; Adobe Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Gong_AutoToon_Automatic_Geometric_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5943560,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7237493407075521562&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "stanford.edu;adobe.com;adobe.com",
        "email": "stanford.edu;adobe.com;adobe.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Stanford University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.stanford.edu;https://research.adobe.com",
        "aff_unique_abbr": "Stanford;Adobe",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "BERT representations for Video Question Answering",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Yang_BERT_representations_for_Video_Question_Answering_WACV_2020_paper.html",
        "author": "Zekun Yang;  Noa Garcia;  Chenhui Chu;  Mayu Otani;  Yuta Nakashima;  Haruo Takemura",
        "abstract": "Visual question answering (VQA) aims at answering questions about the visual content of an image or a video. Currently, most work on VQA is focused on image-based question answering, and less attention has been paid into answering questions about videos. However, VQA in video presents some unique challenges that are worth studying: it not only requires to model a sequence of visual features over time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a sequential modelling technique based on Transformers, to encode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pre-trained language-based Transformer. In our experiments, we exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two well-known video VQA datasets: TVQA and Pororo.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Yang_BERT_representations_for_Video_Question_Answering_WACV_2020_paper.pdf",
        "aff": "Osaka University, Japan; Osaka University, Japan; Osaka University, Japan; CyberAgent, Inc., Japan; Osaka University, Japan; Osaka University, Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 907185,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18379155538440773996&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "lab.ime.cmc.osaka-u.ac.jp;ids.osaka-u.ac.jp;ids.osaka-u.ac.jp;cyberagent.co.jp;ids.osaka-u.ac.jp;ime.cmc.osaka-u.ac.jp",
        "email": "lab.ime.cmc.osaka-u.ac.jp;ids.osaka-u.ac.jp;ids.osaka-u.ac.jp;cyberagent.co.jp;ids.osaka-u.ac.jp;ime.cmc.osaka-u.ac.jp",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "Osaka University;CyberAgent, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.osaka-u.ac.jp;https://www.cyberagent.co.jp",
        "aff_unique_abbr": "Osaka U;CyberAgent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "BIRDSAI: A Dataset for Detection and Tracking in Aerial Thermal Infrared Videos",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Bondi_BIRDSAI_A_Dataset_for_Detection_and_Tracking_in_Aerial_Thermal_WACV_2020_paper.html",
        "author": "Elizabeth Bondi;  Raghav Jain;  Palash Aggrawal;  Saket Anand;  Robert Hannaford;  Ashish Kapoor;  Jim Piavis;  Shital Shah;  Lucas Joppa;  Bistra Dilkina;  Milind Tambe",
        "abstract": "Monitoring of protected areas to curb illegal activities like poaching and animal trafficking is a monumental task. To augment existing manual patrolling efforts, unmanned aerial surveillance using visible and thermal infrared (TIR) cameras is increasingly being adopted. Automated data acquisition has become easier with advances in unmanned aerial vehicles (UAVs) and sensors like TIR cameras, which allow surveillance at night when poaching typically occurs. However, it is still a challenge to accurately and quickly process large amounts of the resulting TIR data. In this paper, we present the first large dataset collected using a TIR camera mounted on a fixed-wing UAV in multiple African protected areas. This dataset includes TIR videos of humans and animals with several challenging scenarios like scale variations, background clutter due to thermal reflections, large camera rotations, and motion blur. Additionally, we provide another dataset with videos synthetically generated with the publicly available Microsoft AirSim simulation platform using a 3D model of an African savanna and a TIR camera model. Through our benchmarking experiments on state-of-the-art detectors, we demonstrate that leveraging the synthetic data in a domain adaptive setting can significantly improve detection performance. We also evaluate various recent approaches for single and multi-object tracking. With the increasing popularity of aerial imagery for monitoring and surveillance purposes, we anticipate this unique dataset to be used to develop and evaluate techniques for object detection, tracking, and domain adaptation for aerial, TIR videos.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Bondi_BIRDSAI_A_Dataset_for_Detection_and_Tracking_in_Aerial_Thermal_WACV_2020_paper.pdf",
        "aff": ";;;;;;;;;;",
        "project": "sites.google.com/view/elizabethbondi/dataset",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Bondi_BIRDSAI_A_Dataset_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1744776,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13026190878026444390&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "author_num": 11,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "BRDF-Reconstruction in Photogrammetry Studio Setups",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Innmann_BRDF-Reconstruction_in_Photogrammetry_Studio_Setups_WACV_2020_paper.html",
        "author": "Matthias Innmann;  Jochen Sussmuth;  Marc Stamminger",
        "abstract": "Photogrammetry Studios are a common setup to acquire high-quality 3D geometry from different kinds of real-world objects, humans, etc. In a photo studio like setup, 50 - 200 DSLR cameras are used with object-specific illumination to simultaneously capture images that are processed by algorithms that automatically estimate the camera parameters and detailed geometry. These steps are automated in established pipelines to a large extent and do not require much user input. However, the post-processing typically involves a manual estimation of surface reflectance parameters by an artist, who paints textures to allow for photorealistic rendering. While professional light stages facilitate this process in an automated way, these setups are very expensive and require accurately calibrated light sources and cameras. In our work, we present a new formulation along with a practical solution to reduce these constraints to photo studio like setups by jointly reconstructing the geometric configuration of the lights along with spatially varying surface reflectance properties and its diffuse albedo. In the presented synthetic as well as real-world experiments, we analyze the effect of different optimization objectives and show that our method is able to provide photorealistic reconstruction results with an RMSE of  1 - 3 % on real data.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Innmann_BRDF-Reconstruction_in_Photogrammetry_Studio_Setups_WACV_2020_paper.pdf",
        "aff": "Friedrich-Alexander University Erlangen-N \u00fcrnberg; adidas AG; Friedrich-Alexander University Erlangen-N \u00fcrnberg",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Innmann_BRDF-Reconstruction_in_Photogrammetry_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2234004,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2438125350333159632&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "; ; ",
        "email": "; ; ",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Friedrich-Alexander University Erlangen-N\u00fcrnberg;adidas",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www fau.de;https://www.adidas.com",
        "aff_unique_abbr": "FAU;adidas",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "BSUV-Net: A Fully-Convolutional Neural Network for Background Subtraction of Unseen Videos",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Tezcan_BSUV-Net_A_Fully-Convolutional_Neural_Network_for_Background_Subtraction_of_Unseen_WACV_2020_paper.html",
        "author": "Ozan Tezcan;  Prakash Ishwar;  Janusz Konrad",
        "abstract": "Background subtraction is a basic task in computer vision and video processing often applied as a pre-processing step for object tracking, people recognition, etc. Recently, a number of successful background-subtraction algorithms have been proposed, however nearly all of the top-performing ones are supervised. Crucially, their success relies upon the availability of some annotated frames of the test video during training. Consequently, their performance on completely \"unseen\" videos is undocumented in the literature. In this work, we propose a new, supervised, background-subtraction algorithm for unseen videos (BSUV-Net) based on a fully-convolutional neural network. The input to our network consists of the current frame and two background frames captured at different time scales along with their semantic segmentation maps. In order to reduce the chance of overfitting, we also introduce a new data-augmentation technique which mitigates the impact of illumination difference between the background frames and the current frame. On the CDNet-2014 dataset, BSUV-Net outperforms state-of-the-art algorithms evaluated on unseen videos in terms of several metrics including F-measure, recall and precision.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Tezcan_BSUV-Net_A_Fully-Convolutional_Neural_Network_for_Background_Subtraction_of_Unseen_WACV_2020_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Tezcan_BSUV-Net_A_Fully-Convolutional_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1117388,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16634477234536473793&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Best Frame Selection in a Short Video",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ren_Best_Frame_Selection_in_a_Short_Video_WACV_2020_paper.html",
        "author": "Jian Ren;  Xiaohui Shen;  Zhe Lin;  Radomir Mech",
        "abstract": "People usually take short videos to record meaningful moments in their lives. However, selecting the most representative frame, which not only has high image visual quality but also captures video content, from a short video to share or keep is a time-consuming process for one may need to manually go through all the frames in a video to make a decision. In this paper, we introduce the problem of the best frame selection in a short video and aim to solve it automatically. Towards this end, we collect and will release a diverse large-scale short video dataset that includes 11, 000 videos shoot in our daily life. All videos are assumed to be short (e.g., a few seconds) and each video has human-annotated of the best frame. Then we introduce a deep convolutional neural network (CNN) based approach with ranking objective to automatically pick the best frame from frame sequences extracted via short videos. Additionally, we propose new evaluation metrics, especially for the best frame selection. In experiments, we show our approach outperforms various other methods significantly.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ren_Best_Frame_Selection_in_a_Short_Video_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1957236,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12730280241036267906&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Blended Convolution and Synthesis for Efficient Discrimination of 3D Shapes",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ramasinghe_Blended_Convolution_and_Synthesis_for_Efficient_Discrimination_of_3D_Shapes_WACV_2020_paper.html",
        "author": "Sameera Ramasinghe;  Salman Khan;  Nick Barnes;  Stephen Gould",
        "abstract": "Existing models for shape analysis directly learn feature representations on 3D point clouds. We argue that 3D point clouds are highly redundant and hold irregular (permutation-invariant) structure, which makes it difficult to achieve inter-class discrimination efficiently. In this paper, we propose a two-pronged solution to this problem that is seamlessly integrated in a single blended convolution and synthesis layer. This fully differentiable layer performs two critical tasks in succession. In the first step, it projects the input 3D point clouds into a latent 3D space to synthesize a highly compact and inter-class discriminative point cloud representation. Since, 3D point clouds do not follow a Euclidean topology, standard 2/3D convolutional neural networks offer limited representation capability. Therefore, in the second step, we propose a novel 3D convolution operator functioning inside the unit ball to extract useful volumetric features. We derive formulae to achieve both translation and rotation of our novel convolution kernels. Finally, using the proposed techniques we present an extremely light-weight, end-to-end architecture that achieves compelling results on 3D shape recognition and retrieval.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ramasinghe_Blended_Convolution_and_Synthesis_for_Efficient_Discrimination_of_3D_Shapes_WACV_2020_paper.pdf",
        "aff": "Australian National University; Australian National University; Australian National University; Australian National University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Ramasinghe_Blended_Convolution_and_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 713826,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12000410359930682028&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Body Pose Sonification for a View-Independent Auditory Aid to Blind Rock Climbers",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ramsay_Body_Pose_Sonification_for_a_View-Independent_Auditory_Aid_to_Blind_WACV_2020_paper.html",
        "author": "Joseph Ramsay;  Hyung Jin Chang",
        "abstract": "Rock climbing is a sport in which blind people have traditionally found it extremely difficult to excel due to the high degree of visual problem solving required, and also the requirement to climb with a sighted assistant. We present a system which automates the role of the sighted assistant in order to provide blind people with the freedom to climb and train on their own. We address climbing-specific limitations of a state-of-the-art skeleton tracking system, and discuss the ways in which we mitigated these limitations using post-processing techniques tuned specially for a climbing scenario. We also describe the auditory feedback system used to instruct the blind climber, and demonstrate that a user can learn to follow it in a relatively short time by showing a significant improvement in performance over just a few trials with the system.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ramsay_Body_Pose_Sonification_for_a_View-Independent_Auditory_Aid_to_Blind_WACV_2020_paper.pdf",
        "aff": "University of Birmingham; University of Birmingham",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 783702,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8468995946243136759&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;bham.ac.uk",
        "email": "gmail.com;bham.ac.uk",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Birmingham",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.birmingham.ac.uk",
        "aff_unique_abbr": "Birmingham",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Boosting Deep Face Recognition via Disentangling Appearance and Geometry",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Dabouei_Boosting_Deep_Face_Recognition_via_Disentangling_Appearance_and_Geometry_WACV_2020_paper.html",
        "author": "Ali Dabouei;  Fariborz Taherkhani;  Sobhan Soleymani;  Jeremy Dawson;  Nasser Nasrabadi",
        "abstract": "In this paper, we propose a framework for disentangling the appearance and geometry representations in the face recognition task. To provide supervision for this aim, we generate geometrically identical faces by incorporating spatial transformations. We demonstrate that the proposed approach enhances the performance of deep face recognition models by assisting the training process in two ways. First, it enforces the early and intermediate convolutional layers to learn more representative features that satisfy the properties of disentangled embeddings. Second, it augments the training set by altering faces geometrically. Through extensive experiments, we demonstrate that integrating the proposed approach into state-of-the-art face recognition methods effectively improves their performance on challenging datasets, such as LFW, YTF, and MegaFace. Both theoretical and practical aspects of the method are analyzed rigorously by concerning ablation studies and knowledge transfer tasks. Furthermore, we show that the knowledge leaned by the proposed method can favor other face-related tasks, such as attribute prediction.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Dabouei_Boosting_Deep_Face_Recognition_via_Disentangling_Appearance_and_Geometry_WACV_2020_paper.pdf",
        "aff": "West Virginia University; West Virginia University; West Virginia University; West Virginia University; West Virginia University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1674253,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16904111224376528731&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mix.wvu.edu;gmail.com;mix.wvu.edu;mail.wvu.edu;mail.wvu.edu",
        "email": "mix.wvu.edu;gmail.com;mix.wvu.edu;mail.wvu.edu;mail.wvu.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "West Virginia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wvu.edu",
        "aff_unique_abbr": "WVU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Boosting Standard Classification Architectures Through a Ranking Regularizer",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Taha_Boosting_Standard_Classification_Architectures_Through_a_Ranking_Regularizer_WACV_2020_paper.html",
        "author": "Ahmed Taha;  Yi-Ting Chen;  Teruhisa Misu;  Abhinav Shrivastava;  Larry Davis",
        "abstract": "We employ triplet loss as a feature embedding regularizer to boost classification performance. Standard architectures, like ResNet and Inception, are extended to support both losses with minimal hyper-parameter tuning. This promotes generality while fine-tuning pretrained networks. Triplet loss is a powerful surrogate for recently proposed embedding regularizers. Yet, it is avoided due to large batch-size requirement and high computational cost. Through our experiments, we re-assess these assumptions.  During inference, our network supports both classification and embedding tasks without any computational overhead. Quantitative evaluation highlights a steady improvement on five fine-grained recognition datasets.  Further evaluation on an imbalanced video dataset achieves significant improvement. Triplet loss brings feature embedding capabilities like nearest neighbor to classification models. Code available at http://bit.ly/2LNYEqL",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Taha_Boosting_Standard_Classification_Architectures_Through_a_Ranking_Regularizer_WACV_2020_paper.pdf",
        "aff": "University of Maryland, College Park; Honda Research Institute, USA; Honda Research Institute, USA; University of Maryland, College Park; University of Maryland, College Park",
        "project": "http://bit.ly/2LNYEqL",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Taha_Boosting_Standard_Classification_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1845661,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=277908464570157098&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "University of Maryland;Honda Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://honda-ri.com",
        "aff_unique_abbr": "UMD;HRI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bridged Variational Autoencoders for Joint Modeling of Images and Attributes",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Yadav_Bridged_Variational_Autoencoders_for_Joint_Modeling_of_Images_and_Attributes_WACV_2020_paper.html",
        "author": "Ravindra Yadav;  Ashish Sardana;  Vinay Namboodiri;  Rajesh M Hegde",
        "abstract": "Generative models have recently shown the ability to realistically generate data and model the distribution accurately. However, joint modeling of an image with the attribute that it is labeled with requires learning a cross modal correspondence between images and the attribute data. Though the information present in the images and attributes possess completely different statistical properties altogether, there exists an inherent correspondence that is challenging to capture. Various models have aimed at capturing this correspondence either through joint modeling of a variational autoencoder or through separate encoder networks that are then concatenated. We present an alternative by proposing a bridged variational autoencoder that allows for learning cross-modal correspondence by incorporating cross-modal hallucination losses in the latent space. In comparison to the existing methods, we have found that by incorporating this information into the network we not only obtain better generation results, but also obtain very distinctive latent embeddings thereby increasing the accuracy of cross-modal generated results. We validate the proposed method through comparison with state of the art methods and benchmarking on standard datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Yadav_Bridged_Variational_Autoencoders_for_Joint_Modeling_of_Images_and_Attributes_WACV_2020_paper.pdf",
        "aff": "IIT Kanpur; NVIDIA; IIT Kanpur; IIT Kanpur",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Yadav_Bridged_Variational_Autoencoders_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1141502,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11059362817314890932&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iitk.ac.in;nvidia.com;iitk.ac.in;iitk.ac.in",
        "email": "iitk.ac.in;nvidia.com;iitk.ac.in;iitk.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;NVIDIA Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitk.ac.in;https://www.nvidia.com",
        "aff_unique_abbr": "IITK;NVIDIA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kanpur;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "CANZSL: Cycle-Consistent Adversarial Networks for Zero-Shot Learning from Natural Language",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_CANZSL_Cycle-Consistent_Adversarial_Networks_for_Zero-Shot_Learning_from_Natural_Language_WACV_2020_paper.html",
        "author": "Zhi Chen;  Jingjing Li;  Yadan Luo;  Zi Huang;  Yang Yang",
        "abstract": "Existing methods using generative adversarial approaches for Zero-Shot Learning (ZSL) aim to generate realistic visual features from class semantics by a single generative alignment, which is highly under-constrained. As a result, the previous methods cannot guarantee that the generated visual features can truthfully reflect the corresponding semantics. To address this issue, we propose a novel method named Cycle-consistent Adversarial Networks for Zero-Shot Learning (CANZSL). It encourages a visual feature generator to synthesize realistic visual features from semantics, and then inversely translate back the synthesized visual features to the corresponding semantic space by a semantic feature generator. Furthermore, in this paper a more challenging and practical ZSL problem is considered where the original semantics are from natural language with irrelevant words instead of clean semantics, which are widely used in previous work. Specifically, a multi-modal consistent bidirectional generative adversarial model is trained to handle unseen instances by suppressing noise in the natural language. A forward one-to-many mapping from the class level descriptions to the visual features is coupled with an inverse many-to-one mapping from the visual space to the semantic space. Thus, a multi-modal cycle-consistency loss between the synthesized semantic representations and the ground truth can be learned and leveraged to enforce the generated semantic features to approximate to the real distribution in semantic space. Extensive experiments are conducted to demonstrate that our method consistently outperforms state-of-the-art approaches on natural language-based zero-shot learning tasks.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_CANZSL_Cycle-Consistent_Adversarial_Networks_for_Zero-Shot_Learning_from_Natural_Language_WACV_2020_paper.pdf",
        "aff": "School of ITEE, University of Queensland; University of Electronic Science and Technology of China; School of ITEE, University of Queensland; School of ITEE, University of Queensland; University of Electronic Science and Technology of China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4416978,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18061281310437178150&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;yeah.net;gmail.com;itee.uq.edu.au;gmail.com",
        "email": "gmail.com;yeah.net;gmail.com;itee.uq.edu.au;gmail.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;1",
        "aff_unique_norm": "University of Queensland;University of Electronic Science and Technology of China",
        "aff_unique_dep": "School of Information Technology and Electrical Engineering;",
        "aff_unique_url": "https://www.uq.edu.au;https://www.uestc.edu.cn",
        "aff_unique_abbr": "UQ;UESTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "Australia;China"
    },
    {
        "title": "Calibrated Domain-Invariant Learning for Highly Generalizable Large Scale Re-Identification",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Yuan_Calibrated_Domain-Invariant_Learning_for_Highly_Generalizable_Large_Scale_Re-Identification_WACV_2020_paper.html",
        "author": "Ye Yuan;  Wuyang Chen;  Tianlong Chen;  Yang Yang;  Zhou Ren;  Zhangyang Wang;  Gang Hua",
        "abstract": "Many real-world applications, such as city scale traffic monitoring and control, requires large scale re-identification. However, previous ReID methods often failed to address two limitations in existing ReID benchmarks, i.e. low spatiotemporal coverage and sample imbalance. Notwithstanding their demonstrated success in every single benchmark, they have difficulties in generalizing to unseen environments. As a result, these methods are less applicable in a large scale setting due to poor generalization. In seek for a highly generalizable large-scale ReID method, we present an adversarial domain-invariant feature learning framework (ADIN) that explicitly learns to separate identity-related features from challenging variations, where for the first time \"free\" annotations in ReID data such as video timestamp and camera index are utilized. We take advantage of the nuisance labels that can be obtained \"for free\" in ReID data, such as video timestamp and camera index annotations. Furthermore, we find that the imbalance of nuisance classes jeopardizes the adversarial training, and for mitigation we propose a calibrated adversarial loss that is attentive to nuisance distribution. Experiments on existing large-scale person/vehicle ReID datasets demonstrate that ADIN learns more robust and generalizable representations, as evidenced by its outstanding direct transfer performance across datasets, which is a criterion that can better measure the generalizability of large scale Re-ID methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Yuan_Calibrated_Domain-Invariant_Learning_for_Highly_Generalizable_Large_Scale_Re-Identification_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Texas A&M University; Department of Computer Science and Engineering, Texas A&M University; Department of Computer Science and Engineering, Texas A&M University; Walmart Technology; Wormpex AI Research; Department of Computer Science and Engineering, Texas A&M University; Wormpex AI Research",
        "project": "",
        "github": "https://github.com/TAMU-VITA/ADIN",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Yuan_Calibrated_Domain-Invariant_Learning_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 925073,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3740778924980444796&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "tamu.edu;tamu.edu;tamu.edu;walmart.com;gmail.com;tamu.edu;gmail.com",
        "email": "tamu.edu;tamu.edu;tamu.edu;walmart.com;gmail.com;tamu.edu;gmail.com",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2;0;2",
        "aff_unique_norm": "Texas A&M University;Walmart Inc.;Wormpex AI Research",
        "aff_unique_dep": "Department of Computer Science and Engineering;Technology;AI Research",
        "aff_unique_url": "https://www.tamu.edu;https://www.walmart.com;",
        "aff_unique_abbr": "TAMU;Walmart;Wormpex AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Can I teach a robot to replicate a line art",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/B.V._Can_I_teach_a_robot_to_replicate_a_line_art_WACV_2020_paper.html",
        "author": "Raghav B.V.;  Subham Kumar;  Vinay Namboodiri",
        "abstract": "Line art is arguably one of the fundamental and versatile modes of expression. We propose a pipeline for a robot to look at a grayscale line art and redraw it. The key novel elements of our pipeline are: a) we propose a novel task of mimicking line drawings, b) to solve the pipeline we modify the Quick-draw dataset and obtain supervised training for converting a line drawing into a series of strokes c) we propose a multi-stage segmentation and graph interpretation pipeline for solving the problem. The resultant method has also been deployed on a CNC plotter as well as a robotic arm. We have trained several variations of the proposed methods and evaluate these on a dataset obtained from Quick-draw. Through the best methods we observe an accuracy of around 98% for this task, which is a significant improvement over the baseline architecture we adapted from. This therefore allows for deployment of the method on robots for replicating line art in a reliable manner. We also show that while the rule-based vectorization methods do suffice for simple drawings, it fails for more complicated sketches, unlike our method which generalizes well to more complicated distributions.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/B.V._Can_I_teach_a_robot_to_replicate_a_line_art_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Technology Kanpur; Indian Institute of Technology Kanpur; Indian Institute of Technology Kanpur",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1229960,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15705556353641697825&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "email": "iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Kanpur",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kanpur",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Can a CNN Automatically Learn the Significance of Minutiae Points for Fingerprint Matching?",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chowdhury_Can_a_CNN_Automatically_Learn_the_Significance_of_Minutiae_Points_WACV_2020_paper.html",
        "author": "Anurag Chowdhury;  Simon Kirchgasser;  Andreas Uhl;  Arun Ross",
        "abstract": "Most automated fingerprint recognition systems use minutiae points for comparing fingerprints. In the parlance of Computer Vision, minutiae can be viewed as handcrafted features, i.e., features that have been proposed by human experts for the task of fingerprint recognition. In this work, we raise the following question: Can a machine learning system automatically determine the significance of minutiae points for fingerprint matching? To this effect, a patch-based Siamese Convolutional Neural Network (CNN), which does not explicitly rely on the extraction of minutiae points, is designed and trained from scratch. The purpose of this network is to learn the most effective features for matching fingerprint images. The features learned by this network are analyzed using Gradient-weighted Class Activation Mapping (Grad-CAM) to determine if they correlate with the locations of minutiae points. Our experiments suggest that the proposed network automatically learns to focus on minutiae points, when available, for fingerprint matching. Thus, an automated learner without any explicit domain knowledge establishes the significance of minutiae points for fingerprint matching.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chowdhury_Can_a_CNN_Automatically_Learn_the_Significance_of_Minutiae_Points_WACV_2020_paper.pdf",
        "aff": "Michigan State University, USA; University of Salzburg, Austria; University of Salzburg, Austria; Michigan State University, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1327692,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16514579242402728902&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cse.msu.edu;cs.sbg.ac.at;cs.sbg.ac.at;cse.msu.edu",
        "email": "cse.msu.edu;cs.sbg.ac.at;cs.sbg.ac.at;cse.msu.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Michigan State University;University of Salzburg",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.msu.edu;https://www.uni-salzburg.at",
        "aff_unique_abbr": "MSU;USAL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;Austria"
    },
    {
        "title": "Casting Geometric Constraints in Semantic Segmentation as Semi-Supervised Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Stekovic_Casting_Geometric_Constraints_in_Semantic_Segmentation_as_Semi-Supervised_Learning_WACV_2020_paper.html",
        "author": "Sinisa Stekovic;  Friedrich Fraundorfer;  Vincent Lepetit",
        "abstract": "We propose a simple yet effective method to learn to segment new indoor scenes from video frames: State-of-the-art methods trained on one dataset, even as large as the SUNRGB-D dataset, can perform poorly when applied to images that are not part of the dataset, because of the dataset bias, a common phenomenon in computer vision. To make semantic segmentation more useful in practice, one can exploit geometric constraints. Our main contribution is to show that these  constraints can be cast conveniently as semi-supervised terms, which enforce the fact  that the same class should be predicted for the projections of the same 3D location in different images. This is interesting as we can exploit general existing techniques developed for semi-supervised learning to efficiently incorporate the constraints. We show that this approach can efficiently and accurately learn to segment target sequences of ScanNet and our own target sequences using only annotations from SUNRGB-D, and geometric relations between the video frames of target sequences.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Stekovic_Casting_Geometric_Constraints_in_Semantic_Segmentation_as_Semi-Supervised_Learning_WACV_2020_paper.pdf",
        "aff": "Institute for Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Universit \u00b4e Paris-Est, \u00b4Ecole des Ponts ParisTech, Paris, France+Institute for Computer Graphics and Vision, Graz University of Technology, Graz, Austria",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Stekovic_Casting_Geometric_Constraints_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 811776,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6611245872122669674&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "Graz University of Technology;Universit\u00e9 Paris-Est",
        "aff_unique_dep": "Institute for Computer Graphics and Vision;\u00c9cole des Ponts ParisTech",
        "aff_unique_url": "https://www.tugraz.at;https://www.ponts.org",
        "aff_unique_abbr": "TU Graz;UPE",
        "aff_campus_unique_index": "0;0;1+0",
        "aff_campus_unique": "Graz;Paris",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "Austria;France"
    },
    {
        "title": "Characteristic Regularisation for Super-Resolving Face Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Cheng_Characteristic_Regularisation_for_Super-Resolving_Face_Images_WACV_2020_paper.html",
        "author": "Zhiyi Cheng;  Xiatian Zhu;  Shaogang Gong",
        "abstract": "Existing facial image super-resolution (SR) methods focus mostly on improving \"artificially down-sampled\" low-resolution (LR) imagery. Such SR models, although strong at handling artificial LR images, often suffer from significant performance drop on genuine LR test data. Previous unsupervised domain adaptation (UDA) methods address this issue by training a model using unpaired genuine LR and HR data as well as cycle consistency loss formulation. However, this renders the model overstretched with two tasks: consistifying the visual characteristics and enhancing the image resolution. Importantly, this makes the end-to-end model training ineffective due to the difficulty of back-propagating gradients through two concatenated CNNs. To solve this problem, we formulate a method that joins the advantages of conventional SR and UDA models. Specifically, we separate and control the optimisations for characteristics consistifying and image super-resolving by introducing Characteristic Regularisation (CR) between them. This task split makes the model training more effective and computationally tractable. Extensive evaluations demonstrate the performance superiority of our method over state-of-the-art SR and UDA models on both genuine and artificial LR facial imagery data.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Cheng_Characteristic_Regularisation_for_Super-Resolving_Face_Images_WACV_2020_paper.pdf",
        "aff": "Queen Mary University of London; Vision Semantics Limited, London, UK + Queen Mary University of London; Queen Mary University of London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 883607,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3142473312613040225&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff_domain": "qmul.ac.uk;gmail.com;qmul.ac.uk",
        "email": "qmul.ac.uk;gmail.com;qmul.ac.uk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Queen Mary University of London;Vision Semantics Limited",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.qmul.ac.uk;",
        "aff_unique_abbr": "QMUL;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Charting the Right Manifold: Manifold Mixup for Few-shot Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mangla_Charting_the_Right_Manifold_Manifold_Mixup_for_Few-shot_Learning_WACV_2020_paper.html",
        "author": "Puneet Mangla;  Nupur Kumari;  Abhishek Sinha;  Mayank Singh;  Balaji Krishnamurthy;  Vineeth N Balasubramanian",
        "abstract": "Few-shot learning algorithms aim to learn model parameters capable of adapting to unseen classes with the help of only a few labeled examples. A recent regularization technique - Manifold Mixup focuses on learning a general-purpose representation, robust to small changes in the data distribution. Since the goal of few-shot learning is closely linked to robust representation learning, we study Manifold Mixup in this problem setting. Self-supervised learning is another technique that learns semantically meaningful features, using only the inherent structure of the data. This work investigates the role of learning relevant feature manifold for few-shot tasks using self-supervision and regularization techniques. We observe that regularizing the feature manifold, enriched via self-supervised techniques, with Manifold Mixup significantly improves few-shot learning performance.  We show that our proposed method S2M2 beats the current state-of-the-art accuracy on standard few-shot learning datasets like CIFAR-FS, CUB, mini-ImageNet and tiered-ImageNet by 3-8 %. Through extensive experimentation, we show that the features learned using our approach generalize to complex few-shot evaluation tasks, cross-domain scenarios and are robust against slight changes to data distribution.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mangla_Charting_the_Right_Manifold_Manifold_Mixup_for_Few-shot_Learning_WACV_2020_paper.pdf",
        "aff": "IIT Hyderabad, India+Media and Data Science Research lab, Adobe; Media and Data Science Research lab, Adobe; Media and Data Science Research lab, Adobe; Media and Data Science Research lab, Adobe; IIT Hyderabad, India; Media and Data Science Research lab, Adobe",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Mangla_Charting_the_Right_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1738008,
        "gs_citation": 457,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2285224614205770120&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "iith.ac.in;adobe.com;adobe.com;adobe.com;iith.ac.in;adobe.com",
        "email": "iith.ac.in;adobe.com;adobe.com;adobe.com;iith.ac.in;adobe.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;1;0;1",
        "aff_unique_norm": "Indian Institute of Technology Hyderabad;Adobe",
        "aff_unique_dep": ";Media and Data Science Research lab",
        "aff_unique_url": "https://www.iith.ac.in;https://www.adobe.com",
        "aff_unique_abbr": "IIT Hyderabad;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hyderabad;",
        "aff_country_unique_index": "0+1;1;1;1;0;1",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "ChromaGAN: Adversarial Picture Colorization with Semantic Class Distribution",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Vitoria_ChromaGAN_Adversarial_Picture_Colorization_with_Semantic_Class_Distribution_WACV_2020_paper.html",
        "author": "Patricia Vitoria;  Lara Raad;  Coloma Ballester",
        "abstract": "The colorization of grayscale images is an ill-posed problem, with multiple correct solutions. In this paper, we propose an adversarial learning colorization approach coupled with semantic information. A generative network is used to infer the chromaticity of a given grayscale image conditioned to semantic clues. This network is framed in an adversarial model that learns to colorize by incorporating perceptual and semantic understanding of color and class distributions. The model is trained via a fully self-supervised strategy. Qualitative and quantitative results show the capacity of the proposed method to colorize images in a realistic way achieving state-of-the-art results.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Vitoria_ChromaGAN_Adversarial_Picture_Colorization_with_Semantic_Class_Distribution_WACV_2020_paper.pdf",
        "aff": "Department of Information and Communication Technologies, University Pompeu Fabra, Barcelona, Spain; Department of Information and Communication Technologies, University Pompeu Fabra, Barcelona, Spain; Department of Information and Communication Technologies, University Pompeu Fabra, Barcelona, Spain",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2296645,
        "gs_citation": 240,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8453672123675290087&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "upf.edu;upf.edu;upf.edu",
        "email": "upf.edu;upf.edu;upf.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University Pompeu Fabra",
        "aff_unique_dep": "Department of Information and Communication Technologies",
        "aff_unique_url": "https://www.upf.edu",
        "aff_unique_abbr": "UPF",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Barcelona",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "title": "City-Scale Road Extraction from Satellite Imagery v2: Road Speeds and Travel Times",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Van_Etten_City-Scale_Road_Extraction_from_Satellite_Imagery_v2_Road_Speeds_and_WACV_2020_paper.html",
        "author": "Adam Van Etten",
        "abstract": "Automated road network extraction from remote sensing imagery remains a significant challenge despite its importance in a broad array of applications. To this end, we explore road network extraction at scale with inference of semantic features of the graph, identifying speed limits and route travel times for each roadway. We call this approach City-Scale Road Extraction from Satellite Imagery v2 (CRESIv2), Including estimates for travel time permits true optimal routing (rather than just the shortest geographic distance), which is not possible with existing remote sensing imagery based methods. We evaluate our method using two sources of labels (OpenStreetMap, and those from the SpaceNet dataset), and find that models both trained and tested on SpaceNet labels outperform OpenStreetMap labels by greater than 60%. We quantify the performance of our algorithm with the Average Path Length Similarity (APLS) and map topology (TOPO) graph-theoretic metrics over a diverse test area covering four cities in the SpaceNet dataset. For a traditional edge weight of geometric distance, we find an aggregate of 5% improvement over existing methods for SpaceNet data. We also test our algorithm on Google satellite imagery with OpenStreetMap labels, and find a 23% improvement over previous work. Metric scores decrease by only 4% on large graphs when using travel time rather than geometric distance for edge weights, indicating that optimizing routing for travel time is feasible with this approach.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Van_Etten_City-Scale_Road_Extraction_from_Satellite_Imagery_v2_Road_Speeds_and_WACV_2020_paper.pdf",
        "aff": "In-Q-Tel CosmiQ Works",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Van_Etten_City-Scale_Road_Extraction_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 9814104,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6934143594565072924&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iqt.org",
        "email": "iqt.org",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "In-Q-Tel",
        "aff_unique_dep": "CosmiQ Works",
        "aff_unique_url": "https://www.in-q-tel.org/",
        "aff_unique_abbr": "In-Q-Tel",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Class-Discriminative Feature Embedding For Meta-Learning based Few-Shot Classification",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Rahimpour_Class-Discriminative_Feature_Embedding_For_Meta-Learning_based_Few-Shot_Classification_WACV_2020_paper.html",
        "author": "Alireza Rahimpour;  Hairong Qi",
        "abstract": "Although deep learning-based approaches have been very effective in solving problems with plenty of labeled data, they suffer in tackling problems for which labeled data are scarce. In few-shot classification, the objective is to train a classifier from only a handful of labeled examples in a support set.   In this paper, we propose a few-shot learning framework based on structured margin loss which takes into account the global structure of the support set in order to generate a highly discriminative feature space where the features from distinct classes are well separated in clusters. Moreover, in our meta-learning-based framework, we propose a context-aware query embedding encoder for incorporating support set context into query embedding and generating more discriminative and task-dependent query embeddings. The task-dependent features help the meta-learner to learn a distribution over tasks more effectively. Extensive experiments based on few-shot, zero-shot and semi-supervised learning on three benchmarks show the advantages of the proposed model compared to the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Rahimpour_Class-Discriminative_Feature_Embedding_For_Meta-Learning_based_Few-Shot_Classification_WACV_2020_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 435117,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8627549363240525173&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Class-incremental Learning via Deep Model Consolidation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhang_Class-incremental_Learning_via_Deep_Model_Consolidation_WACV_2020_paper.html",
        "author": "Junting Zhang;  Jie Zhang;  Shalini Ghosh;  Dawei Li;  Serafettin Tasci;  Larry Heck;  Heming Zhang;  C.-C. Jay Kuo",
        "abstract": "Deep neural networks (DNNs) often suffer from \"catastrophic forgetting\" during incremental learning (IL) --- an abrupt degradation of performance on the original set of classes when the training objective is adapted to a newly added set of classes. Existing IL approaches tend to produce a model that is biased towards either the old classes or new classes, unless with the help of exemplars of the old data. To address this issue, we propose a class-incremental learning paradigm called Deep Model Consolidation (DMC), which works well even when the original training data is not available. The idea is to first train a separate model only for the new classes, and then combine the two individual models trained on data of two distinct set of classes (old classes and new classes) via a novel double distillation training objective. The two existing models are consolidated by exploiting publicly available unlabeled auxiliary data. This overcomes the potential difficulties due to unavailability of original training data. Compared to the state-of-the-art techniques, DMC demonstrates significantly better performance in image classification (CIFAR-100 and CUB-200) and object detection (PASCAL VOC 2007) in the single-headed IL setting.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Class-incremental_Learning_via_Deep_Model_Consolidation_WACV_2020_paper.pdf",
        "aff": ";;;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Zhang_Class-incremental_Learning_via_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3947195,
        "gs_citation": 463,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13949609631359900180&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "author_num": 8,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Classifying All Interacting Pairs in a Single Shot",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chafik_Classifying_All_Interacting_Pairs_in_a_Single_Shot_WACV_2020_paper.html",
        "author": "Sanaa Chafik;  Astrid Orcesi;  Romaric Audigier;  Bertrand Luvison",
        "abstract": "In this paper, we introduce a novel human interaction detection approach, based on CALIPSO (Classifying ALl Interacting Pairs in a Single shOt), a classifier of human-object interactions. This new single-shot interaction classifier estimates interactions simultaneously for all human-object pairs, regardless of their number and class. State-of-the-art approaches adopt a multi-shot strategy based on a pairwise estimate of interactions for a set of human-object candidate pairs, which leads to a complexity depending, at least, on the number of interactions or, at most, on the number of candidate pairs. In contrast, the proposed method estimates the interactions on the whole image. Indeed, it simultaneously estimates all interactions between all human subjects and object targets by performing a single forward pass throughout the image. Consequently, it leads to a constant complexity and computation time independent of the number of subjects, objects or interactions in the image. In detail, interaction classification is achieved on a dense grid of anchors thanks to a joint multi-task network that learns three complementary tasks simultaneously: (i) prediction of the types of interaction, (ii) estimation of the presence of a target and (iii) learning of an embedding which maps interacting subject and target to a same representation, by using a metric learning strategy. In addition, we introduce an object-centric passive-voice verb estimation which significantly improves results. Evaluations on the two well-known Human-Object Interaction image datasets, V-COCO and HICO-DET, demonstrate the competitiveness of the proposed method (2nd place) compared to the state-of-the-art while having constant computation time regardless of the number of objects and interactions in the image.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chafik_Classifying_All_Interacting_Pairs_in_a_Single_Shot_WACV_2020_paper.pdf",
        "aff": "CEA, LIST, Vision and Learning Lab for Scene Analysis, PC 184, F-91191 Gif-sur-Yvette, France+Vision Lab, ThereSIS, Thales SIX GTS, Campus Polytechnique, Palaiseau, France; CEA, LIST, Vision and Learning Lab for Scene Analysis, PC 184, F-91191 Gif-sur-Yvette, France+Vision Lab, ThereSIS, Thales SIX GTS, Campus Polytechnique, Palaiseau, France; CEA, LIST, Vision and Learning Lab for Scene Analysis, PC 184, F-91191 Gif-sur-Yvette, France+Vision Lab, ThereSIS, Thales SIX GTS, Campus Polytechnique, Palaiseau, France; CEA, LIST, Vision and Learning Lab for Scene Analysis, PC 184, F-91191 Gif-sur-Yvette, France+Vision Lab, ThereSIS, Thales SIX GTS, Campus Polytechnique, Palaiseau, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 676973,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13843842952400345768&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cea.fr;cea.fr;cea.fr;cea.fr",
        "email": "cea.fr;cea.fr;cea.fr;cea.fr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "CEA;Thales SIX GTS",
        "aff_unique_dep": "Vision and Learning Lab for Scene Analysis;Vision Lab",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Campus Polytechnique",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "France"
    },
    {
        "title": "Cloud Removal from Satellite Images using Spatiotemporal Generator Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sarukkai_Cloud_Removal_from_Satellite_Images_using_Spatiotemporal_Generator_Networks_WACV_2020_paper.html",
        "author": "Vishnu Sarukkai;  Anirudh Jain;  Burak Uzkent;  Stefano  Ermon",
        "abstract": "Satellite images hold great promise for continuous environmental monitoring and earth observation. Occlusions cast by clouds, however, can severely limit coverage, making ground information extraction more difficult. Existing pipelines typically perform cloud removal with simple temporal composites and hand-crafted filters. In contrast, we cast the problem of cloud removal as a conditional image synthesis challenge, and we propose a trainable spatiotemporal generator network (STGAN) to remove clouds. We train our model on a new large-scale spatiotemporal dataset that we construct, containing 97640 image pairs covering all continents. We demonstrate experimentally that the proposed STGAN model outperforms standard models and can generate realistic cloud-free images with high PSNR and SSIM values across a variety of atmospheric conditions, leading to improved performance in downstream tasks such as land cover classification.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sarukkai_Cloud_Removal_from_Satellite_Images_using_Spatiotemporal_Generator_Networks_WACV_2020_paper.pdf",
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Sarukkai_Cloud_Removal_from_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2230179,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15671272946288369784&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff_domain": "stanford.edu;stanford.edu; ; ",
        "email": "stanford.edu;stanford.edu; ; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "CoachGAN",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Brodie_CoachGAN_WACV_2020_paper.html",
        "author": "Mike Brodie",
        "abstract": "CoachGAN provides an inference time method to improve outputs from GAN generator models. Similar to creating adversarial examples to fool neural network classifiers, CoachGAN exploits gradient information, in this case from a pretrained discriminator model. Unlike the process of generating adversarial examples, which uses gradient descent to alter outputs directly, CoachGAN alters the inputs of generator models. This allows for output enhancements at test time without any additional model training. CoachGAN adapts easily to existing algorithms and does not depend on specific model architectures. In addition to qualitative samples, we quantitatively demonstrate the ability of CoachGAN to improve IS and FID scores across a variety of GAN architectures and tasks.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Brodie_CoachGAN_WACV_2020_paper.pdf",
        "aff": "",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Brodie_CoachGAN_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1010348,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:aqanACzHzDUJ:scholar.google.com/&scioq=CoachGAN&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff_domain": "",
        "email": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Color Composition Similarity and Its Application in Fine-grained Similarity",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ha_Color_Composition_Similarity_and_Its_Application_in_Fine-grained_Similarity_WACV_2020_paper.html",
        "author": "Mai Lan Ha;  Vlad Hosu;  Volker Blanz",
        "abstract": "Assessing visual similarity in-the-wild, a core ability of the human visual system, is a challenging problem for computer vision methods because of its subjective nature and limited annotated datasets. We make a stride forward, showing that visual similarity can be better studied by isolating its components. We identify color composition similarity as an important aspect and study its interaction with category-level similarity. Color composition similarity considers the distribution of colors and their layout in images. We create predictive models accounting for the global similarity that is beyond pixel-based and patch-based, or histogram level information. Using an active learning approach, we build a large-scale color composition similarity dataset with subjective ratings via crowd-sourcing, the first of its kind. We train a Siamese network using the dataset to create a color similarity metric and descriptors which outperform existing color descriptors. We also provide a benchmark for global color descriptors for perceptual color similarity. Finally, we combine color similarity and category level features for fine-grained visual similarity. Our proposed model surpasses the state-of-the-art performance while using three orders of magnitude less training data. The results suggest that our proposal to study visual similarity by isolating its components, modeling and combining them is a promising paradigm for further development.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ha_Color_Composition_Similarity_and_Its_Application_in_Fine-grained_Similarity_WACV_2020_paper.pdf",
        "aff": "University of Siegen; University of Konstanz; University of Siegen",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Ha_Color_Composition_Similarity_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1508526,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6605502902789172841&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "informatik.uni-siegen.de;uni-konstanz.de;informatik.uni-siegen.de",
        "email": "informatik.uni-siegen.de;uni-konstanz.de;informatik.uni-siegen.de",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Siegen;University of Konstanz",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-siegen.de;https://www.uni-konstanz.de",
        "aff_unique_abbr": "Uni Siegen;Uni Konstanz",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Combinational Class Activation Maps for Weakly Supervised Object Localization",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Yang_Combinational_Class_Activation_Maps_for_Weakly_Supervised_Object_Localization_WACV_2020_paper.html",
        "author": "Seunghan Yang;  Yoonhyung Kim;  Youngeun Kim;  Changick Kim",
        "abstract": "Weakly supervised object localization has recently attracted attention since it aims to identify both class labels and locations of objects by using image-level labels. Most previous methods utilize the activation map corresponding to the highest activation source. Exploiting only one activation map of the highest probability class is often biased into limited regions or sometimes even highlights background regions. To resolve these limitations, we propose to use activation maps, named combinational class activation maps (CCAM), which are linear combinations of activation maps from the highest to the lowest probability class. By using CCAM for localization, we suppress background regions to help highlighting foreground objects more accurately. In addition, we design the network architecture to consider spatial relationships for localizing relevant object regions. Specifically, we integrate non-local modules into an existing base network at both low- and high-level layers. Our final model, named non-local combinational class activation maps (NL-CCAM), obtains superior performance compared to previous methods on representative object localization benchmarks including ILSVRC 2016 and CUB-200-2011. Furthermore, we show that the proposed method has a great capability of generalization by visualizing other datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Yang_Combinational_Class_Activation_Maps_for_Weakly_Supervised_Object_Localization_WACV_2020_paper.pdf",
        "aff": "Korea Advanced Institute of Science Technology (KAIST), Daejeon, Korea; Korea Advanced Institute of Science Technology (KAIST), Daejeon, Korea; Korea Advanced Institute of Science Technology (KAIST), Daejeon, Korea; Korea Advanced Institute of Science Technology (KAIST), Daejeon, Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Yang_Combinational_Class_Activation_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 937492,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8111881431812861947&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Daejeon",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Combining Compositional Models and Deep Networks For Robust Object Classification under Occlusion",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kortylewski_Combining_Compositional_Models_and_Deep_Networks_For_Robust_Object_Classification_WACV_2020_paper.html",
        "author": "Adam Kortylewski;  Qing Liu;  Huiyu Wang;  Zhishuai Zhang;  Alan Yuille",
        "abstract": "Deep convolutional neural networks (DCNNs) are powerful models that yield impressive results at object classification. However, recent work has shown that they do not generalize well to partially occluded objects and to mask attacks. In contrast to DCNNs, compositional models are robust to partial occlusion, however, they are not as discriminative as deep models. In this work, we combine DCNNs and compositional object models to retain the best of both approaches: a discriminative model that is robust to partial occlusion and mask attacks. Our model is learned in two steps. First, a standard DCNN is trained for image classification. Subsequently, we cluster the DCNN features into dictionaries. We show that the dictionary components resemble object part detectors and learn the spatial distribution of parts for each object class. We propose mixtures of compositional models to account for large changes in the spatial activation patterns (e.g. due to changes in the 3D pose of an object). At runtime, an image is first classified by the DCNN in a feedforward manner.  The prediction uncertainty is used to detect partially occluded objects, which in turn are classified by the compositional model. Our experimental results demonstrate that combining compositional models and DCNNs resolves a fundamental problem of current deep learning approaches to computer vision: The combined model recognizes occluded objects, even when it has not been exposed to occluded objects during training, while at the same time maintaining high discriminative performance for non-occluded objects.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kortylewski_Combining_Compositional_Models_and_Deep_Networks_For_Robust_Object_Classification_WACV_2020_paper.pdf",
        "aff": "Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5583872,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5964150662780411645&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "email": "jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Component Attention Guided Face Super-Resolution Network: CAGFace",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kalarot_Component_Attention_Guided_Face_Super-Resolution_Network_CAGFace_WACV_2020_paper.html",
        "author": "Ratheesh Kalarot;  Tao Li;  Fatih Porikli",
        "abstract": "To make the best use of the underlying structure of faces, the collective information through face datasets and the intermediate estimates during the upsampling process, here we introduce a fully convolutional multi-stage neural network for 4x super-resolution for face images. We implicitly impose facial component-wise attention maps using a segmentation network to allow our network to focus on face-inherent patterns. Each stage of our network is composed of a stem layer, a residual backbone, and spatial upsampling layers. We recurrently apply stages to reconstruct an intermediate image, and then reuse its space-to-depth converted versions to bootstrap and enhance image quality progressively. Our experiments show that our face super-resolution method achieves quantitatively superior and perceptually pleasing results in comparison to state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kalarot_Component_Attention_Guided_Face_Super-Resolution_Network_CAGFace_WACV_2020_paper.pdf",
        "aff": "The University of Auckland; Purdue University; The Australian National University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2291144,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3164853581530901436&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "aucklanduni.ac.nz;purdue.edu;anu.edu.au",
        "email": "aucklanduni.ac.nz;purdue.edu;anu.edu.au",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "The University of Auckland;Purdue University;Australian National University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.auckland.ac.nz;https://www.purdue.edu;https://www.anu.edu.au",
        "aff_unique_abbr": "UoA;Purdue;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "New Zealand;United States;Australia"
    },
    {
        "title": "Composition-Aware Image Aesthetics Assessment",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Liu_Composition-Aware_Image_Aesthetics_Assessment_WACV_2020_paper.html",
        "author": "Dong Liu;  Rohit Puri;  Nagendra Kamath;  Subhabrata Bhattacharya",
        "abstract": "Automatic image aesthetics assessment is important for a wide variety of applications such as on-line photo suggestion, photo album management and image retrieval. Previous methods have focused on mapping the holistic image content to a high or low aesthetics rating. However, the composition information of an image characterizes the harmony of its visual elements according to the principles of art, and provides richer information for learning aesthetics. In this work, we propose to model the image composition information as the mutual dependency of its local regions, and design a novel architecture to leverage such information to boost the performance of aesthetics assessment. To achieve this, we densely partition an image into local regions and compute aesthetics-preserving features over the regions to characterize the aesthetics properties of image content. With the feature representation of local regions, we build a region composition graph in which each node denotes one region and any two nodes are connected by an edge weighted by the similarity of the region features. We perform reasoning on this graph via graph convolution, in which the activation of each node is determined by its highly correlated neighbors. Our method naturally uncovers the mutual dependency of local regions in the network training procedure, and achieves the state-of-the-art performance on the benchmark visual aesthetics datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Liu_Composition-Aware_Image_Aesthetics_Assessment_WACV_2020_paper.pdf",
        "aff": "Net\ufb02ix Inc., Los Gatos, CA, USA; Net\ufb02ix Inc., Los Gatos, CA, USA; Net\ufb02ix Inc., Los Gatos, CA, USA; Net\ufb02ix Inc., Los Gatos, CA, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 723270,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12643680897880824160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "netflix.com;netflix.com;netflix.com;netflix.com",
        "email": "netflix.com;netflix.com;netflix.com;netflix.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Netflix Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.netflix.com",
        "aff_unique_abbr": "Netflix",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Gatos",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "CompressNet: Generative Compression at Extremely Low Bitrates",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Raman_CompressNet_Generative_Compression_at_Extremely_Low_Bitrates_WACV_2020_paper.html",
        "author": "Suraj Kiran  Raman;  Aditya Ramesh;  Vijayakrishna Naganoor;  Shubham Dash;  Giridharan Kumaravelu;  Honglak Lee",
        "abstract": "Compressing images at extremely low bitrates (< 0.1 bpp) has always been a challenging task as the quality of reconstruction significantly reduces due to the strongly imposing constraint on the number of bits allocated for the compressed data. With the increasing need to transfer large amounts of images with limited bandwidth, compressing images to very low sizes is a crucial task. However, the existing methods are not effective at extremely low bitrates. To address this need we propose a novel network called CompressNet which augments a Stacked Autoencoder with a Switch Prediction Network (SAE-SPN). This helps in the reconstruction of visually pleasing images at these low bitrates (< 0.1 bpp). We benchmark the performance of our proposed method on the Cityscapes dataset, evaluating over different metrics at very low bitrates showing that our method outperforms the other state-of-the-art. In particular, at a bitrate of 0.07, CompressNet achieves 22% lower Perceptual Loss and 55% lower Frechet Inception Distance (FID) compared to the deep learning SOTA methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Raman_CompressNet_Generative_Compression_at_Extremely_Low_Bitrates_WACV_2020_paper.pdf",
        "aff": "University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 901943,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3626369811720686055&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "CookGAN: Meal Image Synthesis from Ingredients",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Han_CookGAN_Meal_Image_Synthesis_from_Ingredients_WACV_2020_paper.html",
        "author": "Fangda Han;  Ricardo Guerrero;  Vladimir Pavlovic",
        "abstract": "In this work we propose a new computational framework, based on generative deep models, for synthesis of photo-realistic food meal images from textual list of its ingredients. Previous works on synthesis of images from text typically rely on pre-trained text models to extract text features, followed by generative neural networks (GAN) aimed to generate realistic images conditioned on the text features. These works mainly focus on generating spatially compact and well-defined categories of objects, such as birds or flowers, but meal images are significantly more complex, consisting of multiple ingredients whose appearance and spatial qualities are further modified by cooking methods. To generate real-like meal images from ingredients, we propose Cook Generative Adversarial Networks (CookGAN), CookGAN first builds an attention-based ingredients-image association model, which is then used to condition a generative neural network tasked with synthesizing meal images. Furthermore, a cycle-consistent constraint is added to further improve image quality and control appearance. Experiments show our model is able to generate meal images corresponding to the ingredients.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Han_CookGAN_Meal_Image_Synthesis_from_Ingredients_WACV_2020_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 820176,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17703228618205673828&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Cooperative Initialization based Deep Neural Network Training",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Singh_Cooperative_Initialization_based_Deep_Neural_Network_Training_WACV_2020_paper.html",
        "author": "Pravendra Singh;  Munender Varshney;  Vinay Namboodiri",
        "abstract": "Researchers have proposed various activation functions. These activation functions help the deep network to learn non-linear behavior with a significant effect on training dynamics and task performance. The performance of these activations also depends on the initial state of the weight parameters, i.e., different initial state leads to a difference in the performance of a network. In this paper, we have proposed a cooperative initialization for training the deep network using ReLU activation function to improve the network performance. Our approach uses multiple activation functions in the initial few epochs for the update of all sets of weight parameters while training the network. These activation functions cooperate to overcome their drawbacks in the update of weight parameters, which in effect learn better \"feature representation\" and boost the network performance later. Cooperative initialization based training also helps in reducing the overfitting problem and does not increase the number of parameters, inference (test) time in the final model while improving the performance. Experiments show that our approach outperforms various baselines and, at the same time, performs well over various tasks such as classification and detection. The Top-1 classification accuracy of the model trained using our approach improves by 2.8% for VGG-16 and 2.1% for ResNet-56 on CIFAR-100 dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Singh_Cooperative_Initialization_based_Deep_Neural_Network_Training_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science and Engineering, IIT Kanpur, India; Department of Computer Science and Engineering, IIT Kanpur, India; Department of Computer Science and Engineering, IIT Kanpur, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 624448,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10183927347035901640&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "email": "iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Kanpur",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kanpur",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Coordinated Joint Multimodal Embeddings for Generalized Audio-Visual Zero-shot Classification and Retrieval of Videos",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Parida_Coordinated_Joint_Multimodal_Embeddings_for_Generalized_Audio-Visual_Zero-shot_Classification_and_WACV_2020_paper.html",
        "author": "Kranti  Parida;  Neeraj  Matiyali;  Tanaya Guha;  Gaurav Sharma",
        "abstract": "We present an audio-visual multimodal approach for the task of zero-shot learning (ZSL) for classification and retrieval of videos. ZSL has been studied extensively in the recent past but has primarily been limited to visual modality and to images. We demonstrate that both audio and visual modalities are important for ZSL for videos. Since a dataset to study the task is currently not available, we also construct an appropriate multimodal dataset with 33 classes containing 156, 416 videos, from an existing large scale audio event dataset. We empirically show that the performance improves by adding audio modality for both tasks of zero-shot classification and retrieval, when using multi-modal extensions of embedding learning methods. We also propose a novel method to predict the 'dominant' modality using a jointly learned modality attention network. We learn the attention in a semi-supervised setting and thus do not require any additional explicit labelling for the modalities. We provide qualitative validation of the modality specific attention, which also successfully generalizes to unseen test classes.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Parida_Coordinated_Joint_Multimodal_Embeddings_for_Generalized_Audio-Visual_Zero-shot_Classification_and_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Parida_Coordinated_Joint_Multimodal_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1796558,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15300578422604478258&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Cross-Conditioned Recurrent Networks for Long-Term Synthesis of Inter-Person Human Motion Interactions",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kundu_Cross-Conditioned_Recurrent_Networks_for_Long-Term_Synthesis_of_Inter-Person_Human_Motion_WACV_2020_paper.html",
        "author": "Jogendra Nath Kundu;  Himanshu Buckchash;  Priyanka Mandikal;  Rahul M V;  Anirudh Jamkhandi;  Venkatesh Babu RADHAKRISHNAN",
        "abstract": "Modeling dynamics of human motion is one of the most challenging sequence modeling problem, with diverse applications in animation industry, human-robot interaction, motion-based surveillance, etc. Available attempts to use auto-regressive techniques for long-term single-person motion generation usually fails, resulting in stagnated motion or divergence to unrealistic pose patterns. In this paper, we propose a novel cross-conditioned recurrent framework targeting long-term synthesis of inter-person interactions beyond several minutes. We carefully integrate positive implications of both auto-regressive and encoder-decoder recurrent architecture, by interchangeably utilizing two separate fixed-length cross person motion prediction models for long-term generation in a novel hierarchical fashion. As opposed to prior approaches, we guarantee structural plausibility of 3D pose by training the recurrent model to regress latent representation of a separately trained generative pose embedding network. Different variants of the proposed frameworks are evaluated through extensive experiments on SBU-interaction, CMU-MoCAP and an inhouse collection of duet-dance dataset. Qualitative and quantitative evaluation on several tasks, such as Short-term motion prediction, Long-term motion synthesis and Interaction-based motion retrieval against prior state-of-the-art approaches clearly highlight superiority of the proposed framework.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kundu_Cross-Conditioned_Recurrent_Networks_for_Long-Term_Synthesis_of_Inter-Person_Human_Motion_WACV_2020_paper.pdf",
        "aff": "Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India; Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India; Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India; Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India; Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India; Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4047733,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10109499731903562081&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "iisc.ac.in;cs.iitr.ac.in;gmail.com;andrew.cmu.edu;gmail.com;iisc.ac.in",
        "email": "iisc.ac.in;cs.iitr.ac.in;gmail.com;andrew.cmu.edu;gmail.com;iisc.ac.in",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Computational Data Systems",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Cross-Domain Face Synthesis using a Controllable GAN",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mokhayeri_Cross-Domain_Face_Synthesis_using_a_Controllable_GAN_WACV_2020_paper.html",
        "author": "Fania Mokhayeri;  Kaveh Kamali;  Eric Granger",
        "abstract": "The performance of face recognition (FR) systems for video surveillance has been shown to improve when the design data is augmented through synthetic face generation. This is true, for instance, with pair-wise matchers (e.g., deep Siamese networks) that rely on a reference gallery, typically with one still image per individual. However, generating synthetic images based on stills may not improve performance during operations due to the domain shift w.r.t. the target domain. Moreover, despite the emergence of Generative Adversarial Networks (GANs) for realistic synthetic generation, it is often difficult to control the conditions under which synthetic faces are generated. In this paper, a cross-domain face synthesis approach is proposed that integrates a new Controllable GAN (C-GAN). It employs an off-the-shelf 3D face model as a simulator to generate facial images under various poses. The simulated images and noise are input to the C-GAN for realism refinement. It relies on an additional adversarial game as a third player to preserve the identity and specific facial attributes of the refined images. This allows generating realistic synthetic face images that reflect capture conditions in the target domain, while controlling the GAN output such that faces may be generated under desired pose conditions. Experiments were performed using videos from the Chokepoint and COX-S2V datasets, and a deep Siamese network for FR with a single reference still per person. Results indicate that the proposed approach can provide a higher level of accuracy compared to state-of-the-art approaches for synthetic data augmentation.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mokhayeri_Cross-Domain_Face_Synthesis_using_a_Controllable_GAN_WACV_2020_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3255768,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17919067969708205569&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Cross-Time and Orientation-Invariant Overhead Image Geolocalization Using Deep Local Features",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Tian_Cross-Time_and_Orientation-Invariant_Overhead_Image_Geolocalization_Using_Deep_Local_Features_WACV_2020_paper.html",
        "author": "Yuxin Tian;  Xueqing Deng;  Yi Zhu;  Shawn Newsam",
        "abstract": "Overhead image geolocalization is becoming increasingly important due to the growing collection of drone imagery without location information. In this paper, we perform large-scale overhead image geolocalization by matching a query image to wide-area reference imagery with known location. We use deep local features so that the query image need not align with but only overlap the tiled reference imagery. We further address two key challenges. For when the query and reference imagery are from different dates, we perform cross-time geolocalization using time invariant features learned using a Siamese network. For when the query and reference imagery are oriented differently, we introduce an orientation normalization network. We demonstrate our contributions on two new high-resolution overhead image datasets. Our method significantly outperforms strong baselines on cross-time geolocalization and is shown to exhibit promising orientation invariance.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Tian_Cross-Time_and_Orientation-Invariant_Overhead_Image_Geolocalization_Using_Deep_Local_Features_WACV_2020_paper.pdf",
        "aff": "Electrical Engineering and Computer Science, University of California, Merced; Electrical Engineering and Computer Science, University of California, Merced; Electrical Engineering and Computer Science, University of California, Merced; Electrical Engineering and Computer Science, University of California, Merced",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1446706,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2611159934387117023&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ucmerced.edu;ucmerced.edu;ucmerced.edu;ucmerced.edu",
        "email": "ucmerced.edu;ucmerced.edu;ucmerced.edu;ucmerced.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Merced",
        "aff_unique_dep": "Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.ucmerced.edu",
        "aff_unique_abbr": "UC Merced",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Merced",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Cross-View Contextual Relation Transferred Network for Unsupervised Vehicle Tracking in Drone Videos",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Song_Cross-View_Contextual_Relation_Transferred_Network_for_Unsupervised_Vehicle_Tracking_in_WACV_2020_paper.html",
        "author": "Wenfeng Song;  Shuai Li;  Tao Chang;  Aimin Hao;  Qinping Zhao;  Hong Qin",
        "abstract": "Recently CNN-centric object tracking methods have been gaining tremendous success in ground-view videos, however, it remains hard to cope with vehicle tracking in unmanned aerial vehicle (UAV) videos. The key difficulties mainly stem from lacking large-scale well-labeled training datasets and view-invariant appearance model for fast-moving drone-view vehicles. We enhance the vehicle's cross-view feature by exploring relations between the pivotal context and the target to facilitate unsupervised vehicle tracking.  The relation is modeled as the relevance of the target and its contextual regions in the tracking task.   Specifically, we propose a contextual relation actor-critic (CRAC) framework integrates an actor-critic agent with a dual GAN learning mechanism, which aims to dynamically search the related contextual regions and transfer the relations from ground-view to drone-view videos while retaining the discriminative features.   We demonstrate that CRAC could be applied to several state-of-the-art trackers by extensive experiments and ablation studies on four public benchmarks. All the experiments confirm that, our CRAC can improve the performance of state-of-the-art methods in terms of accuracy, robustness, and versatility.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Song_Cross-View_Contextual_Relation_Transferred_Network_for_Unsupervised_Vehicle_Tracking_in_WACV_2020_paper.pdf",
        "aff": "State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; Beihang University Qingdao Research Institute+State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; Stony Brook University, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Song_Cross-View_Contextual_Relation_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2248034,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9743437649128401296&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;cs.stonybrook.edu",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;cs.stonybrook.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+0;0;0;0;1",
        "aff_unique_norm": "Beihang University;Stony Brook University",
        "aff_unique_dep": "State Key Laboratory of Virtual Reality Technology and Systems;",
        "aff_unique_url": "http://www.buaa.edu.cn;https://www.stonybrook.edu",
        "aff_unique_abbr": "Beihang;SBU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Qingdao",
        "aff_country_unique_index": "0;0+0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wang_Cross-modal_Scene_Graph_Matching_for_Relationship-aware_Image-Text_Retrieval_WACV_2020_paper.html",
        "author": "Sijin Wang;  Ruiping Wang;  Ziwei Yao;  Shiguang Shan;  Xilin Chen",
        "abstract": "Image-text retrieval of natural scenes has been a popular research topic. Since image and text are heterogeneous cross-modal data, one of the key challenges is how to learn comprehensive yet unified representations to express the multi-modal data. A natural scene image mainly involves two kinds of visual concepts, objects and their relationships, which are equally essential to image-text retrieval. Therefore, a good representation should account for both of them. In the light of recent success of scene graph in many CV and NLP tasks for describing complex natural scenes, we propose to represent image and text with two kinds of scene graphs: visual scene graph (VSG) and textual scene graph (TSG), each of which is exploited to jointly characterize objects and relationships in the corresponding modality. The image-text retrieval task is then naturally formulated as cross-modal scene graph matching. Specifically, we design two particular scene graph encoders in our model for VSG and TSG, which can refine the representation of each node on the graph by aggregating neighborhood information. As a result, both object-level and relationship-level cross-modal features can be obtained, which favorably enables us to evaluate the similarity of image and text in the two levels in a more plausible way. We achieve state-of-the-art results on Flickr30k and MS COCO, which verifies the advantages of our graph matching based approach for image-text retrieval.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_Cross-modal_Scene_Graph_Matching_for_Relationship-aware_Image-Text_Retrieval_WACV_2020_paper.pdf",
        "aff": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Wang_Cross-modal_Scene_Graph_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6928022,
        "gs_citation": 286,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16745970977504886082&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "vipl.ict.ac.cn;ict.ac.cn;vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "vipl.ict.ac.cn;ict.ac.cn;vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "CrossNet: Latent Cross-Consistency for Unpaired Image Translation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sendik_CrossNet_Latent_Cross-Consistency_for_Unpaired_Image_Translation_WACV_2020_paper.html",
        "author": "Omry Sendik;  Danny Cohen-Or;  Dani Lischinski",
        "abstract": "Recent GAN-based architectures have been able to deliver impressive performance on the general task of image-to-image translation. In particular, it was shown that a wide variety of image translation operators may be learned from two image sets, containing images from two different domains, without establishing an explicit pairing between the images. This was made possible by introducing clever regularizers to overcome the under-constrained nature of the unpaired translation problem. In this work, we introduce a novel architecture for unpaired image translation, and explore several new regularizers enabled by it. Specifically, our architecture comprises a pair of GANs, as well as a pair of translators between their respective latent spaces. These cross-translators enable us to impose several regularizing constraints on the learnt image translation operator, collectively referred to as latent cross-consistency. Our results show that our proposed architecture and latent cross-consistency constraints are able to outperform the existing state-of-the-art on a variety of image translation tasks.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sendik_CrossNet_Latent_Cross-Consistency_for_Unpaired_Image_Translation_WACV_2020_paper.pdf",
        "aff": "Tel Aviv University; The Hebrew University of Jerusalem; Tel Aviv University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Sendik_CrossNet_Latent_Cross-Consistency_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3611818,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1060371300363822189&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com; ; ",
        "email": "gmail.com; ; ",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Tel Aviv University;The Hebrew University of Jerusalem",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tau.ac.il;https://www.huji.ac.il",
        "aff_unique_abbr": "TAU;HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Crowded Human Detection via an Anchor-pair Network",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhu__Crowded_Human_Detection_via_an_Anchor-pair_Network_WACV_2020_paper.html",
        "author": "Jinguo Zhu;  Zejian Yuan;  Chong Zhang;  Wanchao Chi;  Yonggen Ling;  shenghao zhang",
        "abstract": "This paper presents an anchor-pair network for crowded human detection, which can overcome and solve the difficulties caused by occlusion in crowded scenes. Specifically, we use a function-aware network structure to extract more distinctive and discriminative features for head and full-body respectively, and then a CNN module is also exploited to fuse the features by learning the correlations between head and full-body to reduce crowd errors. Meanwhile, a novel paired form for anchors, denoted as anchor-pair, is proposed to estimate the head regions and full-body regions simultaneously. Furthermore, a new ingenious JointNMS is introduced to perform on the detected head and full-body box pairs, which produces significant performance improvement in heavily occluded scenarios at tiny computational cost. Our anchor-pair network achieves a state-of-the-art result on the CrowdHuman dataset which reduces the MR 2 to 55.43%, achieving 11.59% relative improvement over our dataset baseline.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhu__Crowded_Human_Detection_via_an_Anchor-pair_Network_WACV_2020_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2348260,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15346644138622613984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "D3D: Distilled 3D Networks for Video Action Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Stroud_D3D_Distilled_3D_Networks_for_Video_Action_Recognition_WACV_2020_paper.html",
        "author": "Jonathan Stroud;  David Ross;  Chen Sun;  Jia Deng;  Rahul Sukthankar",
        "abstract": "State-of-the-art methods for action recognition commonly use two networks: the spatial stream, which takes RGB frames as input, and the temporal stream, which takes optical flow as input. In recent work, both streams are 3D Convolutional Neural Networks, which extract features using spatiotemporal filters. These filters can respond to motion, and therefore should allow the network to learn motion representations, removing the need for optical flow. However, we still see significant benefits in performance by feeding optical flow into the temporal stream, indicating that the spatial stream is \"missing\" some of the signal that the temporal stream captures. In this work, we first investigate whether motion representations are indeed missing in the spatial stream, and show that there is significant room for improvement. Second, we demonstrate that these motion representations can be improved using distillation, that is, by tuning the spatial stream to mimic the temporal stream, effectively combining both models into a single stream. Finally, we show that our Distilled 3D Network (D3D) achieves performance on par with the two-stream approach, with no need to compute optical flow during inference.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Stroud_D3D_Distilled_3D_Networks_for_Video_Action_Recognition_WACV_2020_paper.pdf",
        "aff": "Google Research\u2020University of Michigan; Google Research; Google Research; Google Research\u2021Princeton University; Google Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Stroud_D3D_Distilled_3D_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 385154,
        "gs_citation": 239,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6186294476048384065&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "umich.edu;google.com;google.com;cs.princeton.edu;google.com",
        "email": "umich.edu;google.com;google.com;cs.princeton.edu;google.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "Google Research;Google;Princeton University",
        "aff_unique_dep": "Google Research;Google Research;",
        "aff_unique_url": "https://research.google;https://research.google;https://www.princeton.edu",
        "aff_unique_abbr": "Google Research;Google Research;Princeton",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "DATNet: Dense Auxiliary Tasks for Object Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Levinshtein_DATNet_Dense_Auxiliary_Tasks_for_Object_Detection_WACV_2020_paper.html",
        "author": "Alex Levinshtein;  Alborz Rezazadeh Sereshkeh;  Konstantinos Derpanis",
        "abstract": "Beginning with R-CNN, there has been a rapid advancement in two-stage object detection approaches. While two-stage approaches remain the state-of-the-art in object detection, anchor-free single-stage methods have been gaining momentum. We believe that the strength of the former is in their region of interest (ROI) pooling stage, while the latter simplifies the learning problem by converting object detection into dense per-pixel prediction tasks. In this paper, we propose to combine the strengths of each approach in a new architecture. In particular, we first define several auxiliary tasks related to object detection and generate dense per-pixel predictions using a shared feature extraction backbone. As a consequence of this architecture, the shared backbone is trained using both the standard object detection losses and these per-pixel ones. Moreover, by combining the features from dense predictions with those from the backbone, we realize a more discriminative representation for subsequent downstream processing. In addition, we feed the fused features into a novel multi-scale ROI pooling layer, followed by per-ROI predictions. We refer to our architecture as the Dense Auxiliary Tasks Network (DATNet). We present an extensive set of evaluations of our method on the Pascal VOC and COCO datasets and show considerable accuracy improvements over comparable baselines.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Levinshtein_DATNet_Dense_Auxiliary_Tasks_for_Object_Detection_WACV_2020_paper.pdf",
        "aff": "Samsung AI Centre Toronto; Samsung AI Centre Toronto; Samsung AI Centre Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1976567,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16077964542666083660&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "samsung.com;samsung.com;samsung.com",
        "email": "samsung.com;samsung.com;samsung.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Samsung AI Centre",
        "aff_unique_dep": "AI Centre",
        "aff_unique_url": "https://www.samsung.com/global/innovation/ai-research-centers/",
        "aff_unique_abbr": "Samsung AI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "DAVID: Dual-Attentional Video Deblurring",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wu_DAVID_Dual-Attentional_Video_Deblurring_WACV_2020_paper.html",
        "author": "Junru Wu;  Xiang Yu;  Ding Liu;  Manmohan Chandraker;  Zhangyang Wang",
        "abstract": "Blind video deblurring restores sharp frames from a blurry sequence without any prior. It is a challenging task because the blur due to camera shake, object movement and defocusing is heterogeneous in both temporal and spatial dimensions. Traditional methods train on datasets synthesized with a single level of blur, and thus do not generalize well across levels of blurriness. To address this challenge, we propose a dual attention mechanism to dynamically aggregate temporal cues for deblurring with an end-to-end trainable network structure. Specifically, an internal attention module adaptively selects the optimal temporal scales for restoring the sharp center frame. An external attention module adaptively aggregates and refines multiple sharp frame estimates, from several internal attention modules designed for different blur levels. To train and evaluate on more diverse blur severity levels, we propose a Challenging DVD dataset generated from the raw DVD video set by pooling frames with different temporal windows. Our framework achieves consistently better performance on this more challenging dataset while obtaining strongly competitive results on the original DVD benchmark. Extensive ablative studies and qualitative visualizations further demonstrate the advantage of our method in handling real video blur.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wu_DAVID_Dual-Attentional_Video_Deblurring_WACV_2020_paper.pdf",
        "aff": "Texas A&M University; NEC Laboratories America; ByteDance AI Lab + University of California, San Diego; NEC Laboratories America; Texas A&M University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1269710,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8864651595827094151&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "tamu.edu;tamu.edu;nec-labs.com;nec-labs.com;illinois.edu",
        "email": "tamu.edu;tamu.edu;nec-labs.com;nec-labs.com;illinois.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3;1;0",
        "aff_unique_norm": "Texas A&M University;NEC Laboratories America;ByteDance;University of California, San Diego",
        "aff_unique_dep": ";;AI Lab;",
        "aff_unique_url": "https://www.tamu.edu;https://www.nec-labs.com;https://www.bytedance.com;https://www.ucsd.edu",
        "aff_unique_abbr": "TAMU;NEC Labs America;ByteDance;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;1+0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "DCIL: Deep Contextual Internal Learning for Image Restoration and Image Retargeting",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mastan_DCIL_Deep_Contextual_Internal_Learning_for_Image_Restoration_and_Image_WACV_2020_paper.html",
        "author": "Indra Deep Mastan;  Shanmuganathan Raman",
        "abstract": "Recently, there is a vast interest in developing methods which are independent of the training samples such as deep image prior, zero-shot learning, and internal learning. The methods above are based on the common goal of maximizing image features learning from a single image despite inherent technical diversity. In this work, we bridge the gap between the various unsupervised approaches above and propose a general framework for image restoration and image retargeting. We use contextual feature learning and internal learning to improvise the structure similarity between the source and the target images. We perform image resize application in the following setups: classical image resize using super-resolution, a challenging image resize where the low-resolution image contains noise, and content-aware image resize using image retargeting. We also provide comparisons to the relevant state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mastan_DCIL_Deep_Contextual_Internal_Learning_for_Image_Restoration_and_Image_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Technology Gandhinagar; Indian Institute of Technology Gandhinagar",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2793857,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11136322680765889341&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iitgn.ac.in;iitgn.ac.in",
        "email": "iitgn.ac.in;iitgn.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Gandhinagar",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitgn.ac.in",
        "aff_unique_abbr": "IITGN",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Gandhinagar",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "DGGAN: Depth-image Guided Generative Adversarial Networks for Disentangling RGB and Depth Images in 3D Hand Pose Estimation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_DGGAN_Depth-image_Guided_Generative_Adversarial_Networks_forDisentangling_RGB_and_Depth_WACV_2020_paper.html",
        "author": "Liangjian Chen;  Shih-Yao Lin;  Yusheng Xie;  Yen-Yu Lin;  Wei Fan;  Xiaohui Xie",
        "abstract": "Estimating3D hand poses from RGB images is essentialto a wide range of potential applications, but is challengingowing to substantial ambiguity in the inference of depth in-formation from RGB images. State-of-the-art estimators ad-dress this problem by regularizing3D hand pose estimationmodels during training to enforce the consistency betweenthe predicted3D poses and the ground truth depth maps.However, these estimators rely on the availability of bothRGB images and paired depth maps during training. In thisstudy, we propose a conditional generative adversarial net-work model, called Depth-image Guided GAN (DGGAN),to generate realistic depth maps conditioned on the inputRGB image, and use the synthesized depth maps to regular-ize the3D hand pose estimation model, therefore eliminat-ing the need for ground truth depth maps. Experimental re-sults on multiple benchmark datasets show that the synthe-sized depth maps produced by DGGAN are quite effective inregularizing the pose estimation model, yielding new state-of-the-art results in estimation accuracy, notably reducingthe mean3D end-point errors (EPE) by4.7%,16.5%, and6.8%on the RHD, STB and MHP datasets, respectively.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_DGGAN_Depth-image_Guided_Generative_Adversarial_Networks_forDisentangling_RGB_and_Depth_WACV_2020_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2109381,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4275425795169166970&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "DIPNet: Dynamic Identity Propagation Network for Video Object Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hu_DIPNet_Dynamic_Identity_Propagation_Network_for_Video_Object_Segmentation_WACV_2020_paper.html",
        "author": "Ping Hu;  Jun Liu;  Gang Wang;  Vitaly Ablavsky;  Kate Saenko;  Stan Sclaroff",
        "abstract": "Many recent methods for semi-supervised Video Object Segmentation (VOS) have achieved good performance by exploiting the annotated first frame via one-shot fine-tuning or mask propagation. However, heavily relying on the first frame may weaken the robustness for VOS, since video objects can show large variations through time. In this work, we propose a Dynamic Identity Propagation Network (DIPNet) that adaptively propagates and accurately segments the video objects over time. To achieve this, DIPNet disentangles the VOS task at each time step into a dynamic propagation phase and a spatial segmentation phase. The former utilizes a novel identity representation to adaptively propagate objects' reference information over time, which enhances the robustness to video objects' temporal variations. The latter uses the propagated information to tackle the object segmentation as an easier static image problem that can be optimized via slight fine-tuning on the first frame, thus reducing the computational cost.  As a result, by optimizing these two components to complement each other, we can achieve a robust system for VOS. Evaluations on four benchmark datasets show that DIPNet provides state-of-the-art performance with time efficiency.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hu_DIPNet_Dynamic_Identity_Propagation_Network_for_Video_Object_Segmentation_WACV_2020_paper.pdf",
        "aff": "Boston University; Singapore University of Technology and Design; Alibaba Group; Boston University; Boston University; Boston University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1600733,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17581288263905172834&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "bu.edu;sutd.edu.sg;gmail.com;bu.edu;bu.edu;bu.edu",
        "email": "bu.edu;sutd.edu.sg;gmail.com;bu.edu;bu.edu;bu.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0;0",
        "aff_unique_norm": "Boston University;Singapore University of Technology and Design;Alibaba Group",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.bu.edu;https://www.sutd.edu.sg;https://www.alibaba.com",
        "aff_unique_abbr": "BU;SUTD;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0;0;0",
        "aff_country_unique": "United States;Singapore;China"
    },
    {
        "title": "DeFraudNet:End2End Fingerprint Spoof Detection using Patch Level Attention",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/B_DeFraudNetEnd2End_Fingerprint_Spoof_Detection_using_Patch_Level_Attention_WACV_2020_paper.html",
        "author": "Anusha B;  Sayan Banerjee;  Subhasis Chaudhuri",
        "abstract": "In recent years, fingerprint recognition systems have made remarkable advancements in the field of biometric security as it plays an important role in personal, national and global security. In spite of all these notable advancements, the fingerprint recognition technology is still susceptible to spoof attacks which can significantly jeopardize the user security. The cross sensor and cross material spoof detection still pose a challenge with a myriad of spoof materials emerging every day, compromising sensor interoperability and robustness. This paper proposes a novel method for fingerprint spoof detection using both global and local fingerprint feature descriptors. These descriptors are extracted using DenseNet which significantly improves cross-sensor, cross-material and cross-dataset performance. A novel patch attention network is used for finding the most discriminative patches and also for network fusion. We evaluate our method on four publicly available datasets: LivDet 2011, 2013, 2015 and 2017. A set of comprehensive experiments are carried out to evaluate cross-sensor, cross-material and cross-dataset performance over these datasets. The proposed approach achieves an average accuracy of 99.52%, 99.16% and 99.72% on LivDet 2017, 2015 and 2011 respectively outperforming the current state-of-the-art results by 3% and 4% for LivDet 2015 and 2011 respectively.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/B_DeFraudNetEnd2End_Fingerprint_Spoof_Detection_using_Patch_Level_Attention_WACV_2020_paper.pdf",
        "aff": "IIT Bombay; IIT Bombay; IIT Bombay",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 625823,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10022035981736744460&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;gmail.com;ee.iitb.ac.in",
        "email": "gmail.com;gmail.com;ee.iitb.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitb.ac.in",
        "aff_unique_abbr": "IITB",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mumbai",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "DeOccNet: Learning to See Through Foreground Occlusions in Light Fields",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wang_DeOccNet_Learning_to_See_Through_Foreground_Occlusions_in_Light_Fields_WACV_2020_paper.html",
        "author": "Yingqian Wang;  Tianhao Wu;  Jungang Yang;  Longguang Wang;  Wei An;  Yulan Guo",
        "abstract": "Background objects occluded in some views of a light field (LF) camera can be seen by other views. Consequently, occluded surfaces are possible to be reconstructed from LF images.  In this paper, we handle the LF de-occlusion (LF-DeOcc) problem using a deep encoder-decoder network (namely, DeOccNet). In our method, sub-aperture images (SAIs) are first given to the encoder to incorporate both spatial and angular information. The encoded representations are then used by the decoder to render an occlusion-free center-view SAI. To the best of our knowledge, DeOccNet is the first deep learning-based LF-DeOcc method. To handle the insufficiency of training data, we propose an LF synthesis approach to embed selected occlusion masks into existing LF images. Besides, several synthetic and real-world LFs are developed for performance evaluation. Experimental results show that, after training on the generated data, our DeOccNet can effectively remove foreground occlusions and achieves superior performance as compared to other state-of-the-art methods. Source codes are available at: https://github.com/YingqianWang/DeOccNet.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_DeOccNet_Learning_to_See_Through_Foreground_Occlusions_in_Light_Fields_WACV_2020_paper.pdf",
        "aff": "College of Electronic Science and Technology, National University of Defense Technology, China; College of Electronic Science and Technology, National University of Defense Technology, China; College of Electronic Science and Technology, National University of Defense Technology, China; College of Electronic Science and Technology, National University of Defense Technology, China; College of Electronic Science and Technology, National University of Defense Technology, China; College of Electronic Science and Technology, National University of Defense Technology, China+School of Electronics and Communication Engineering, Sun Yat-sen University, China",
        "project": "",
        "github": "https://github.com/YingqianWang/DeOccNet",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3494327,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2353870074908921644&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "nudt.edu.cn; ; ; ; ;nudt.edu.cn",
        "email": "nudt.edu.cn; ; ; ; ;nudt.edu.cn",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0+1",
        "aff_unique_norm": "National University of Defense Technology;Sun Yat-sen University",
        "aff_unique_dep": "College of Electronic Science and Technology;School of Electronics and Communication Engineering",
        "aff_unique_url": ";http://www.sysu.edu.cn",
        "aff_unique_abbr": ";SYSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Adaptive Wavelet Network",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Rodriguez_Deep_Adaptive_Wavelet_Network_WACV_2020_paper.html",
        "author": "Maria Ximena Bastidas Rodriguez;  Adrien Gruson;  Luisa Polania;  Shin Fujieda;  Flavio Prieto;  Kohei Takayama;  Toshiya Hachisuka",
        "abstract": "Even though convolutional neural networks have become the method of choice in many fields of computer vision, they still lack interpretability and are usually designed manually in a cumbersome trial-and-error process. This paper aims at overcoming those limitations by proposing a deep neural network, which is designed in a systematic fashion and is interpretable, by integrating multiresolution analysis at the core of the deep neural network design. By using the lifting scheme, it is possible to generate a wavelet representation and design a network capable of learning wavelet coefficients in an end-to-end form. Compared to state-of-the-art architectures, the proposed model requires less hyper-parameter tuning and achieves competitive accuracy in image classification tasks. The Code implemented for this research is available at https://github.com/mxbastidasr/DAWN_WACV2020",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Rodriguez_Deep_Adaptive_Wavelet_Network_WACV_2020_paper.pdf",
        "aff": "Universidad Nacional de Colombia; The University of Tokyo + McGill University; Target Corporation; AMD Japan Ltd; Universidad Nacional de Colombia; Digital Frontier Inc.; The University of Tokyo",
        "project": "",
        "github": "https://github.com/mxbastidasr/DAWN",
        "supp": "",
        "arxiv": "",
        "pdf_size": 628554,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18368940625008067013&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "unal.edu.co;gmail.com;udel.edu;amd.com;unal.edu.co;dfx.co.jp;ci.i.u-tokyo.ac.jp",
        "email": "unal.edu.co;gmail.com;udel.edu;amd.com;unal.edu.co;dfx.co.jp;ci.i.u-tokyo.ac.jp",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;3;4;0;5;1",
        "aff_unique_norm": "Universidad Nacional de Colombia;University of Tokyo;McGill University;Target Corporation;AMD Japan Ltd;Digital Frontier Inc.",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.unal.edu.co;https://www.u-tokyo.ac.jp;https://www.mcgill.ca;https://www.target.com;https://www.amd.com/ja-jp;https://www.digitalfrontier.com",
        "aff_unique_abbr": "UNAL;UTokyo;McGill;Target;AMD Japan;DFI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+2;3;1;0;1;1",
        "aff_country_unique": "Colombia;Japan;Canada;United States"
    },
    {
        "title": "Deep Bayesian Network for Visual Question Generation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Patro_Deep_Bayesian_Network_for_Visual_Question_Generation_WACV_2020_paper.html",
        "author": "Badri Patro;  Vinod  Kurmi;  Sandeep Kumar;  Vinay Namboodiri",
        "abstract": "Generating natural questions from an image is a semantic task that requires using vision and language modalities to learn multimodal representations. Images can have multiple visual and language cues such as places, captions, and tags. In this paper, we propose a principled deep Bayesian learning framework that combines these cues to produce natural questions. We observe that with the addition of more cues and by minimizing uncertainty in the among cues, the Bayesian network becomes more confident. We propose a Minimizing Uncertainty of Mixture of Cues (MUMC), that minimizes uncertainty present in a mixture of cues experts for generating probabilistic questions. This is a Bayesian framework and the results show a remarkable similarity to natural questions as validated by a human study. We observe that with the addition of more cues and by minimizing uncertainty among the cues, the Bayesian framework becomes more confident. Ablation studies of our model indicate that a subset of cues is inferior at this task and hence the principled fusion of cues is preferred. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU-n, METEOR, ROUGE, and CIDEr). Here we provide project link for Deep Bayesian VQG https://delta-lab-iitk.github.io/BVQG/.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Patro_Deep_Bayesian_Network_for_Visual_Question_Generation_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Technology, Kanpur; Indian Institute of Technology, Kanpur; Indian Institute of Technology, Kanpur; Indian Institute of Technology, Kanpur",
        "project": "https://delta-lab-iitk.github.io/BVQG/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 19380398,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4224112572164844752&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "iitk.ac.in;iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "email": "iitk.ac.in;iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Kanpur",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Kanpur",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Deep Image Blending",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhang_Deep_Image_Blending_WACV_2020_paper.html",
        "author": "Lingzhi Zhang;  Tarmily Wen;  Jianbo Shi",
        "abstract": "Image composition is an important operation to create visual content. Among image composition tasks, image blending aims to seamlessly blend an object from a source image onto a target image with lightly mask adjustment. A popular approach is Poisson image blending, which enforces the gradient domain smoothness in the composite image. However, this approach only considers the boundary pixels of target image, and thus can not adapt to texture of target image. In addition, the colors of the target image often seep through the original source object too much causing a significant loss of content of the source object. We propose a Poisson blending loss that achieves the same purpose of Poisson image blending. In addition, we jointly optimize the proposed Poisson blending loss as well as the style and content loss computed from a deep network, and reconstruct the blending region by iteratively updating the pixels using the L-BFGS solver. In the blending image, we not only smooth out gradient domain of the blending boundary but also add consistent texture into the blending region. User studies show that our method can outperform strong baselines as well as state-of-the-art approaches when placing objects onto both paintings and real-world images.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Deep_Image_Blending_WACV_2020_paper.pdf",
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "project": "",
        "github": "https://github.com/owenzlz/DeepImageBlending",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6494924,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3319238530034530318&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "seas.upenn.edu;sas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;sas.upenn.edu;seas.upenn.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Learning on Small Datasets without Pre-Training using Cosine Loss",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Barz_Deep_Learning_on_Small_Datasets_without_Pre-Training_using_Cosine_Loss_WACV_2020_paper.html",
        "author": "Bjorn Barz;  Joachim Denzler",
        "abstract": "Two things seem to be indisputable in the contemporary deep learning discourse: 1. The categorical cross-entropy loss after softmax activation is the method of choice for classification. 2. Training a CNN classifier from scratch on small datasets does not work well. In contrast to this, we show that the cosine loss function provides substantially better performance than cross-entropy on datasets with only a handful of samples per class. For example, the accuracy achieved on the CUB-200-2011 dataset without pre-training is by 30% higher than with the cross-entropy loss. Further experiments on other popular datasets confirm our findings. Moreover, we demonstrate that integrating prior knowledge in the form of class hierarchies is straightforward with the cosine loss and improves classification performance further.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Barz_Deep_Learning_on_Small_Datasets_without_Pre-Training_using_Cosine_Loss_WACV_2020_paper.pdf",
        "aff": "Friedrich Schiller University Jena, Germany; Friedrich Schiller University Jena, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 460556,
        "gs_citation": 231,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2961800920690469803&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "uni-jena.de; ",
        "email": "uni-jena.de; ",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Friedrich Schiller University Jena",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-jena.de/",
        "aff_unique_abbr": "FSU Jena",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Deep Position-Aware Hashing for Semantic Continuous Image Retrieval",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wang_Deep_Position-Aware_Hashing_for_Semantic_Continuous_Image_Retrieval_WACV_2020_paper.html",
        "author": "Ruikui Wang;  Ruiping Wang;  Shishi Qiao;  Shiguang Shan;  Xilin Chen",
        "abstract": "Preserving the semantic similarity is one of the most important goals of hashing. Most existing deep hashing methods employ pairs or triplets of samples in training stage, which only consider the semantic similarity within a mini-batch and depict the local positional relationship in Hamming space, leading to intermittent semantic similarity preservation. In this paper, we propose Deep Position-Aware Hashing (DPAH) to ensure continuous semantic similarity in Hamming space by modeling global positional relationship. Specifically, we introduce a set of learnable class centers as the global proxies to represent the global information and generate discriminative binary codes by constraining the distance between data points and class centers. In addition, in order to reduce the information loss caused by relaxing the binary codes to real-values in optimization, we propose kurtosis loss (KT loss) to handle the distribution of real-valued features before thresholding to be double-peak, and then enable the real-valued features to be more binary-like. Comprehensive experiments on three datasets show that our DPAH outperforms state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_Deep_Position-Aware_Hashing_for_Semantic_Continuous_Image_Retrieval_WACV_2020_paper.pdf",
        "aff": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Wang_Deep_Position-Aware_Hashing_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3212531,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3108555233135094164&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "vipl.ict.ac.cn;vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "vipl.ict.ac.cn;vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Remote Sensing Methods for Methane Detection in Overhead Hyperspectral Imagery",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kumar_Deep_Remote_Sensing_Methods_for_Methane_Detection_in_Overhead_Hyperspectral_WACV_2020_paper.html",
        "author": "Satish Kumar;  Carlos Torres;  Oytun Ulutan;  Alana Ayasse;  Dar Roberts;  B. S. Manjunath",
        "abstract": "Effective analysis of hyperspectral imagery is essential for gathering fast and actionable information of large areas affected by atmospheric and green house gases. Existing methods, which process hyperspectral data to detect amorphous gases such as CH4 require manual inspection from domain experts and annotation of massive datasets. These methods do not scale well and are prone to human errors due to the plumes' small pixel-footprint signature. The proposed Hyperspectral Mask-RCNN (H-mrcnn) uses principled statistics, signal processing, and deep neural networks to address these limitations. H-mrcnn introduces fast algorithms to analyze large-area hyper-spectral information and methods to autonomously represent and detect CH4 plumes. H-mrcnn processes information by match-filtering sliding windows of hyperspectral data across the spectral bands. This process produces information-rich features that are both effective plume representations and gas concentration analogs. The optimized matched-filtering stage processes spectral data, which is spatially sampled to train an ensemble of gas detectors. The ensemble outputs are fused to estimate a natural and accurate plume mask. Thorough evaluation demonstrates that H-mrcnn matches the manual and experience-dependent annotation process of experts by 85% (IOU). H-mrcnn scales to larger datasets, reduces the manual data processing and labeling time (12 times), and produces rapid actionable information about gas plumes.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kumar_Deep_Remote_Sensing_Methods_for_Methane_Detection_in_Overhead_Hyperspectral_WACV_2020_paper.pdf",
        "aff": "University of California Santa Barbara\u2020ECE Department\u2020; University of California Santa Barbara\u2020ECE Department\u2020; University of California Santa Barbara\u2020ECE Department\u2020; University of California Santa Barbara\u2021Geography Department\u2021; University of California Santa Barbara\u2021Geography Department\u2021; University of California Santa Barbara\u2020ECE Department\u2020",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Kumar_Deep_Remote_Sensing_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1085738,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7984401613037069627&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ucsb.edu;ece.ucsb.edu;ucsb.edu;geog.ucsb.edu;geog.ucsb.edu;ucsb.edu",
        "email": "ucsb.edu;ece.ucsb.edu;ucsb.edu;geog.ucsb.edu;geog.ucsb.edu;ucsb.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Santa Barbara",
        "aff_unique_dep": "ECE Department",
        "aff_unique_url": "https://www.ucsb.edu",
        "aff_unique_abbr": "UCSB",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Santa Barbara",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "DeepErase: Weakly Supervised Ink Artifact Removal in Document Text Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Qi_DeepErase_Weakly_Supervised_Ink_Artifact_Removal_in_Document_Text_Images_WACV_2020_paper.html",
        "author": "Yike Qi;  W. Ronny Huang;  Qianqian Li;  Jonathan Degange",
        "abstract": "Paper-intensive industries like insurance, law, and government have long leveraged optical character recognition (OCR) to automatically transcribe hordes of scanned documents into text strings for downstream processing. Even today, there are still many scanned documents and mail that come into businesses in non-digital format. Text to be extracted from real world documents is often nestled inside rich formatting, such as tabular structures or forms with fill-in-the-blank boxes or underlines whose ink often touches or even strikes through the ink of the text itself. Further, the text region could have random ink smudges or spurious strokes. Such ink artifacts can severely interfere with the performance of recognition algorithms or other downstream processing tasks. In this work, we propose DeepErase, a neural-based preprocessor to erase ink artifacts from text images. We devise a method to programmatically assemble real text images and real artifacts into realistic-looking \"dirty\" text images, and use them to train an artifact segmentation network in a weakly supervised manner, since pixel-level annotations are automatically obtained during the assembly process. In addition to high segmentation accuracy, we show that our cleansed images achieve a significant boost in recognition accuracy by popular OCR software such as Tesseract 4.0. Finally, we test DeepErase on out-of-distribution datasets (NIST SDB) of scanned IRS tax return forms and achieve double-digit improvements in accuracy. All experiments are performed on both printed and handwritten text.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Qi_DeepErase_Weakly_Supervised_Ink_Artifact_Removal_in_Document_Text_Images_WACV_2020_paper.pdf",
        "aff": "Ernst & Young LLP; Ernst & Young LLP; Ernst & Young LLP; Ernst & Young LLP",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 650562,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16660973212602344831&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "ey.com;ey.com;ey.com;ey.com",
        "email": "ey.com;ey.com;ey.com;ey.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Ernst & Young",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ey.com",
        "aff_unique_abbr": "EY",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "DeepFuse: An IMU-Aware Network for Real-Time 3D Human Pose Estimation from Multi-View Image",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Huang_DeepFuse_An_IMU-Aware_Network_for_Real-Time_3D_Human_Pose_Estimation_WACV_2020_paper.html",
        "author": "Fuyang Huang;  Ailing Zeng;  Minhao Liu;  Qiuxia Lai;  Qiang Xu",
        "abstract": "In this paper, we propose a two-stage fully 3D network, namely DeepFuse, to estimate human pose in 3D space by fusing body-worn Inertial Measurement Unit (IMU) data and multi-view images deeply. The first stage is designed for pure vision estimation. To preserve data primitiveness of multi-view inputs, the vision stage uses multi-channel volume as data representation and 3D soft-argmax as activation layer. The second one is the IMU refinement stage which introduces an IMU-bone layer to fuse the IMU and vision data earlier at data level. without requiring a given skeleton model a priori, we can achieve a mean joint error of 28.9mm on TotalCapture dataset and 13.4mm on Human3.6M dataset under protocol 1, improving the SOTA result by a large margin. Finally, we discuss the effectiveness of a fully 3D network for 3D pose estimation experimentally which may benefit future research.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Huang_DeepFuse_An_IMU-Aware_Network_for_Real-Time_3D_Human_Pose_Estimation_WACV_2020_paper.pdf",
        "aff": "The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 998671,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13822411588416011094&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "DeepPTZ: Deep Self-Calibration for PTZ Cameras",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhang_DeepPTZ_Deep_Self-Calibration_for_PTZ_Cameras_WACV_2020_paper.html",
        "author": "Chaoning Zhang;  Francois Rameau;  Junsik Kim;  Dawit Mureja Argaw;  Jean-Charles Bazin;  In So Kweon",
        "abstract": "Rotating and zooming cameras, also called PTZ (Pan-Tilt-Zoom) cameras, are widely used in modern surveillance systems. While their zooming ability allows acquiring detailed images of the scene, it also makes their calibration more challenging since any zooming action results in a modification of their intrinsic parameters. Therefore, such camera calibration has to be computed online; this process is called self-calibration. In this paper, given an image pair captured by a PTZ camera, we propose a deep learning based approach to automatically estimate the focal length and distortion parameters of both images as well as the rotation angles between them. The proposed approach relies on a dual-Siamese structure, imposing bidirectional constraints. The proposed network is trained on a large-scale dataset automatically generated from a set of panoramas. Empirically, we demonstrate that our proposed approach achieves competitive performance with respect to both deep learning-based and traditional state-of-the art methods. Our code and model will be publicly available at https://github.com/ChaoningZhang/DeepPTZ.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_DeepPTZ_Deep_Self-Calibration_for_PTZ_Cameras_WACV_2020_paper.pdf",
        "aff": "KAIST, South Korea; KAIST, South Korea; KAIST, South Korea; KAIST, South Korea; KAIST, South Korea; KAIST, South Korea",
        "project": "",
        "github": "https://github.com/ChaoningZhang/DeepPTZ",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3149713,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=907320061052538054&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;gmail.com;gmail.com;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "gmail.com;gmail.com;gmail.com;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Dense Extreme Inception Network: Towards a Robust CNN Model for Edge Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Poma_Dense_Extreme_Inception_Network_Towards_a_Robust_CNN_Model_for_WACV_2020_paper.html",
        "author": "Xavier Soria Poma;  Edgar Riba;  Angel Sappa",
        "abstract": "This paper proposes a Deep Learning based edge detector, which is inspired on both HED (Holistically-Nested Edge Detection) and Xception networks. The proposed approach generates thin edge-maps that are plausible for human eyes; it can be used in any edge detection task without previous training or fine tuning process. As a second contribution, a large dataset with carefully annotated edges, has been generated. This dataset has been used for training the proposed approach as well the state-of-the-art algorithms for comparisons. Quantitative and qualitative evaluations have been performed on different benchmarks showing improvements with the proposed method when F-measure of ODS and OIS are considered.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Poma_Dense_Extreme_Inception_Network_Towards_a_Robust_CNN_Model_for_WACV_2020_paper.pdf",
        "aff": "Computer Vision Center - Universitat Autonoma de Barcelona; Computer Vision Center - Universitat Autonoma de Barcelona; Computer Vision Center - Universitat Autonoma de Barcelona + Escuela Superior Polit \u00b4ecnica del Litoral",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8702704,
        "gs_citation": 454,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4574149425579417261&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cvc.uab.es;cvc.uab.es;cvc.uab.es",
        "email": "cvc.uab.es;cvc.uab.es;cvc.uab.es",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Universitat Autonoma de Barcelona;Escuela Superior Polit\u00e9cnica del Litoral",
        "aff_unique_dep": "Computer Vision Center;",
        "aff_unique_url": "https://www.uab.cat;https://www.espae.edu.ec",
        "aff_unique_abbr": "UAB;ESPOL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "Spain;Ecuador"
    },
    {
        "title": "Depth Completion via Deep Basis Fitting",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Qu_Depth_Completion_via_Deep_Basis_Fitting_WACV_2020_paper.html",
        "author": "Chao Qu;  Ty  Nguyen;  Camillo Taylor",
        "abstract": "In this paper we consider the task of image-guided depth completion where our system must infer the depth at every pixel of an input image based on the image content and a sparse set of depth measurements. We propose a novel approach that builds upon the strengths of modern deep learning techniques and classical fitting algorithms and significantly improves performance. The proposed method replaces the final 1-by-1 convolutional layer employed in most depth completion networks with a least squares fitting module which computes weights by fitting the implicit depth bases to the given sparse depth measurements. In addition, we show how our proposed method can be naturally extended to a multi-scale formulation for improved self-supervised training. We demonstrate through extensive experiments on various datasets that our approach achieves consistent improvements over a state-of-the-art baseline method with minimal computational overhead.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Qu_Depth_Completion_via_Deep_Basis_Fitting_WACV_2020_paper.pdf",
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 772300,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16834795844173925392&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Detecting Face2Face Facial Reenactment in Videos",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kumar_Detecting_Face2Face_Facial_Reenactment_in_Videos_WACV_2020_paper.html",
        "author": "Prabhat Kumar;  Mayank Vatsa;  Richa Singh",
        "abstract": "Visual content has become the primary source of information, as evident in the billions of images and videos, shared and uploaded every single day. This has led to an increase in alterations in images and videos to make them more informative and eye-catching for the viewers worldwide. Some of these alterations are simple, like copy-move, and are easily detectable, while other sophisticated alterations like reenactment are hard to detect. Reenactment alterations allow the source to change the target expressions and create photo-realistic images and videos where such modifications are hard to detect. Significant work has been done towards creating such images and videos. However, the detection of such alterations still requires research. This research proposes a learning-based algorithm for detecting reenactment based alterations. The proposed algorithm uses a multi-stream network that learns regional artifacts and provides a robust performance at various compression levels. We also propose a loss function for the balanced learning of the streams for the proposed network. The performance is evaluated on the publicly available FaceForensics dataset. The results show state-of-the-art classification accuracy of 99.96%, 99.10%, and 91.20% for no, easy, and hard compression factors, respectively.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kumar_Detecting_Face2Face_Facial_Reenactment_in_Videos_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Science Bangalore; Indian Institute of Technology Jodhpur; Indian Institute of Technology Jodhpur",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 703554,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17488145576931585397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iisc.ac.in;iitj.ac.in;iitj.ac.in",
        "email": "iisc.ac.in;iitj.ac.in;iitj.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Indian Institute of Science;Indian Institute of Technology Jodhpur",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.iitj.ac.in",
        "aff_unique_abbr": "IISc;IIT Jodhpur",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Bangalore;Jodhpur",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Detecting Morphed Face Attacks Using Residual Noise from Deep Multi-Scale Context Aggregation Network",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Venkatesh_Detecting_Morphed_Face_Attacks_Using_Residual_Noise_from_Deep_Multi-Scale_WACV_2020_paper.html",
        "author": "Sushma Venkatesh;  Raghavendra Ramachandra;  Kiran Raja;  Luuk Spreeuwers;  Raymond Veldhuis;  Christoph Busch",
        "abstract": "The evolving deployment of face recognition system has raised concerns regarding the vulnerability of those systems to various attacks. The morphed face attack involves two different face images via morphing to obtain an attack image face image similar to both original images. The obtained morphed image can easily be verified against both subjects visually and by Face Recognition Systems (FRS). The face morphing attack thus raises a severe concern to various security applications like border control and e-passport unless such attacks are detected and mitigated.  In this work, we propose a novel method to reliably detect the morphed face attacks using a new denoising framework. Considering the time complexity and parameterization efforts, we realize the proposed method using a new deep Multi-scale Context Aggregation Network (MS-CAN).  Extensive experiments are carried out on three different morphed face image datasets. The Morphed Attack Detection (MAD) performance of the proposed method is also benchmarked against 13 different state-of-the-art techniques using the ISO IEC 30107-3 evaluation metrics. Based on the obtained quantitative results reported, the proposed method has indicated the best performance.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Venkatesh_Detecting_Morphed_Face_Attacks_Using_Residual_Noise_from_Deep_Multi-Scale_WACV_2020_paper.pdf",
        "aff": "Norwegian Biometrics Laboratory + IDI, NTNU, Norway; Norwegian Biometrics Laboratory + IDI, NTNU, Norway; IDI, NTNU, Norway; University of Twente, Enschede, Netherlands; University of Twente, Enschede, Netherlands; Norwegian Biometrics Laboratory + IDI, NTNU, Norway",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 786095,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4208027550169409813&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ntnu.no;ntnu.no;ntnu.no;utwente.nl;utwente.nl;ntnu.no",
        "email": "ntnu.no;ntnu.no;ntnu.no;utwente.nl;utwente.nl;ntnu.no",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0;0;1;1;0+0",
        "aff_unique_norm": "Norwegian University of Science and Technology;University of Twente",
        "aff_unique_dep": "Norwegian Biometrics Laboratory;",
        "aff_unique_url": "https://www.ntnu.edu;https://www.utwente.nl",
        "aff_unique_abbr": "NTNU;UT",
        "aff_campus_unique_index": ";;1;1;",
        "aff_campus_unique": ";Enschede",
        "aff_country_unique_index": "0+0;0+0;0;1;1;0+0",
        "aff_country_unique": "Norway;Netherlands"
    },
    {
        "title": "Detecting the Starting Frame of Actions in Video",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kwak_Detecting_the_Starting_Frame_of_Actions_in_Video_WACV_2020_paper.html",
        "author": "Iljung Kwak;  Jian-Zhong Guo;  Adam Hantman;  David Kriegman;  Kristin Branson",
        "abstract": "In this work, we address the problem of precisely localizing key frames of an action, for example, the precise time that a pitcher releases a baseball, or the precise time that a crowd begins to applaud. Key frame localization is a largely overlooked and important action-recognition problem, for example in the field of neuroscience, in which we would like to understand the neural activity that produces the start of a bout of an action. To address this problem, we introduce a novel structured loss function that properly weights the types of errors that matter in such applications: it more heavily penalizes extra and missed action start detections over small misalignments. Our structured loss is based on the best matching between predicted and labeled action starts. We train recurrent neural networks (RNNs) to minimize differentiable approximations of this loss. To evaluate these methods, we introduce the Mouse Reach Dataset, a large, annotated video dataset of mice performing a sequence of actions. The dataset was collected and labeled by experts for the purpose of neuroscience research. On this dataset, we demonstrate that our method outperforms related approaches and baseline methods using an unstructured loss.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kwak_Detecting_the_Starting_Frame_of_Actions_in_Video_WACV_2020_paper.pdf",
        "aff": "UC, San Diego + Janelia Research Campus; Janelia Research Campus; Janelia Research Campus; UC, San Diego; Janelia Research Campus",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Kwak_Detecting_the_Starting_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 873449,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5681193163279263045&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "janelia.hhmi.org;janelia.hhmi.org;janelia.hhmi.org;cs.ucsd.edu;janelia.hhmi.org",
        "email": "janelia.hhmi.org;janelia.hhmi.org;janelia.hhmi.org;cs.ucsd.edu;janelia.hhmi.org",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;0;1",
        "aff_unique_norm": "University of California, San Diego;HHMI Janelia Research Campus",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://ucsd.edu;https://www.janelia.org",
        "aff_unique_abbr": "UCSD;Janelia",
        "aff_campus_unique_index": "0+1;1;1;0;1",
        "aff_campus_unique": "San Diego;Ashburn",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Devon: Deformable Volume Network for Learning Optical Flow",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Lu_Devon_Deformable_Volume_Network_for_Learning_Optical_Flow_WACV_2020_paper.html",
        "author": "Yao Lu;  Jack Valmadre;  Heng Wang;  Juho  Kannala;  Mehrtash Harandi;  Philip Torr",
        "abstract": "State-of-the-art neural network models estimate large displacement optical flow in multi-resolution and use warping to propagate the estimation between two resolutions. Despite their impressive results, it is known that there are two problems with the approach. First, the multi-resolution estimation of optical flow fails in situations where small objects move fast. Second, warping creates artifacts when occlusion or dis-occlusion happens. In this paper, we propose a new neural network module, Deformable Cost Volume, which alleviates the two problems. Based on this module, we designed the Deformable Volume Network (Devon) which can estimate multi-scale optical flow in a single high resolution. Experiments show Devon is more suitable in handling small objects moving fast and achieves comparable results to the state-of-the-art methods in public benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Lu_Devon_Deformable_Volume_Network_for_Learning_Optical_Flow_WACV_2020_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1847958,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3273781498255947723&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Differentiable Scene Graphs",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Raboh_Differentiable_Scene_Graphs_WACV_2020_paper.html",
        "author": "Moshiko Raboh;  Roei Herzig;  Jonathan Berant;  Gal Chechik;  Amir Globerson",
        "abstract": "Reasoning about complex visual scenes involves perception of entities and their relations. Scene Graphs (SGs) provide a natural representation for reasoning tasks, by assigning labels to both entities (nodes) and relations (edges). Reasoning systems based on SGs are typically trained in a two-step procedure: first, a model is trained to predict SGs from images, and next a separate model is trained to reason based on the predicted SGs. However, it would seem preferable to train such systems in an end-to-end manner. The challenge, which we address here is that scene-graph representations are non-differentiable and therefore it isn't clear how to use them as intermediate components. Here we propose Differentiable Scene Graphs (DSGs), an image representation that is amenable to differentiable end-to-end optimization, and requires supervision only from the downstream tasks. DSGs provide a dense representation for all regions and pairs of regions, and do not spend modelling capacity on regions of the image that do not contain objects or relations of interest. We evaluate our model on the challenging task of identifying referring relationships (RR) in three benchmark datasets: Visual Genome, VRD and CLEVR. Using DSGs as an intermediate representation leads to new state-of-the-art performance. The full code is available at https://github.com/shikorab/DSG.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Raboh_Differentiable_Scene_Graphs_WACV_2020_paper.pdf",
        "aff": "Tel Aviv University; Tel Aviv University; Tel Aviv University + AI2; Bar-Ilan University + NVIDIA Research; Tel Aviv University",
        "project": "",
        "github": "https://github.com/shikorab/DSG",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Raboh_Differentiable_Scene_Graphs_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3896393,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13570442966100407638&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;2+3;0",
        "aff_unique_norm": "Tel Aviv University;AI2;Bar-Ilan University;NVIDIA Corporation",
        "aff_unique_dep": ";;;NVIDIA Research",
        "aff_unique_url": "https://www.tau.ac.il;https://www.ai2.edu;https://www.biu.ac.il;https://www.nvidia.com/research",
        "aff_unique_abbr": "TAU;AI2;BIU;NVIDIA",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0+1;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "title": "Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy Supervision",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mangalam_Disentangling_Human_Dynamics_for_Pedestrian_Locomotion_Forecasting_with_Noisy_Supervision_WACV_2020_paper.html",
        "author": "Karttikeya Mangalam;  Ehsan Adeli;  Kuan-Hui Lee;  Adrien Gaidon;  Juan Carlos Niebles",
        "abstract": "We tackle the problem of Human Locomotion Forecasting, a task for jointly predicting the spatial positions of several keypoints on human body in the near future under an egocentric setting. In contrast to the previous work that aims to solve either the task of pose prediction or trajectory forecasting in isolation, we propose a framework to unify these two problems and address the practically useful task of pedestrian locomotion prediction in the wild.  Among the major challenges in solving this task is the scarcity of annotated egocentric video datasets with dense annotations for pose, depth, or egomotion.   To surmount this difficulty,  we use state-of-the-art models to generate (noisy) annotations and propose robust forecasting models that can learn from this noisy supervision. We present a method to disentangle the overall pedestrian motion into easier to learn subparts by utilizing a pose completion and a decomposition module. The completion module fills in the missing key-point annotations and the decomposition module breaks the cleaned locomotion down to global (trajectory) and local (pose keypoint movements). Further, with Quasi RNN as our backbone, we propose a novel hierarchical trajectory forecasting network that utilizes low-level vision domain specific signals like egomotion and depth to predict the global trajectory.  Our method leads to state-of-the-art results for the prediction of human locomotion in the egocentric view",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mangalam_Disentangling_Human_Dynamics_for_Pedestrian_Locomotion_Forecasting_with_Noisy_Supervision_WACV_2020_paper.pdf",
        "aff": "Stanford University+University of California, Berkeley; Stanford University; Toyota Research Institute; Toyota Research Institute; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1497893,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8084504583476983307&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cs.berkeley.edu;cs.stanford.edu;tri.global;tri.global;cs.stanford.edu",
        "email": "cs.berkeley.edu;cs.stanford.edu;tri.global;tri.global;cs.stanford.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;2;2;0",
        "aff_unique_norm": "Stanford University;University of California, Berkeley;Toyota Research Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.berkeley.edu;https://www.tri.global",
        "aff_unique_abbr": "Stanford;UC Berkeley;TRI",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Stanford;Berkeley;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Distributed Iterative Gating Networks for Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Karim_Distributed_Iterative_Gating_Networks_for_Semantic_Segmentation_WACV_2020_paper.html",
        "author": "Rezaul Karim;  Md Amirul Islam;  Neil D. B. Bruce",
        "abstract": "In this paper, we present a canonical structure for controlling information flow in neural networks with an efficient feedback routing mechanism based on a strategy of Distributed Iterative Gating (DIGNet). The structure of this mechanism derives from a strong conceptual foundation, and presents a light-weight mechanism for adaptive control of computation similar to recurrent convolutional neural networks by integrating feedback signals with a feed forward architecture. In contrast to other RNN formulations, DIGNet generates feedback signals in a cascaded manner that implicitly carries information from all the layers above. This cascaded feedback propagation by means of the propagator gates is found to be more effective compared to other feedback mechanisms that use feedback from output of either the corresponding stage or from the previous stage. Experiments reveal the high degree of capability that this recurrent approach with cascaded feedback presents over feed-forward baselines and other recurrent models for pixel-wise labeling problems on three challenging datasets, PASCAL VOC 2012, COCO-Stuff, and ADE20K.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Karim_Distributed_Iterative_Gating_Networks_for_Semantic_Segmentation_WACV_2020_paper.pdf",
        "aff": "York University; Ryerson University+Vector Institute for Artificial Intelligence; Ryerson University+Vector Institute for Artificial Intelligence",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1208252,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10938918217575890659&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "yorku.ca;scs.ryerson.ca;ryerson.ca",
        "email": "yorku.ca;scs.ryerson.ca;ryerson.ca",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1+2",
        "aff_unique_norm": "York University;Ryerson University;Vector Institute for Artificial Intelligence",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.yorku.ca;https://www.ryerson.ca;https://vectorinstitute.ai/",
        "aff_unique_abbr": "York U;Ryerson;Vector Institute",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Do As I Do: Transferring Human Motion and Appearance between Monocular Videos with Spatial and Temporal Constraints",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Gomes_Do_As_I_Do_Transferring_Human_Motion_and_Appearance_between_WACV_2020_paper.html",
        "author": "Thiago Gomes;  Renato Martins;  Joao Ferreira;  Erickson Nascimento",
        "abstract": "Creating plausible virtual actors from images of real actors remains one of the key challenges in computer vision and computer graphics. Marker-less human motion estimation and shape modeling from images in the wild bring this challenge to the fore. Although the recent advances on view synthesis and image-to-image translation, currently available formulations are limited to transfer solely style and do not take into account the character's motion and shape, which are by nature intermingled to produce plausible human forms. In this paper, we propose a unifying formulation for transferring appearance and retargeting human motion from monocular videos that regards all these aspects. Our method synthesizes new videos of people in a different context where they were initially recorded. Differently from recent appearance transferring methods, our approach takes into account body shape, appearance, and motion constraints. The evaluation is performed with several experiments using publicly available real videos containing hard conditions. Our method is able to transfer both human motion and appearance outperforming state-of-the-art methods, while preserving specific features of the motion that must be maintained (e.g., feet touching the floor, hands touching a particular object) and holding the best visual quality and appearance metrics such as Structural Similarity (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS)",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Gomes_Do_As_I_Do_Transferring_Human_Motion_and_Appearance_between_WACV_2020_paper.pdf",
        "aff": "Universidade Federal de Minas Gerais (UFMG), Brazil; Universidade Federal de Minas Gerais (UFMG), Brazil+INRIA, France; Universidade Federal de Minas Gerais (UFMG), Brazil; Universidade Federal de Minas Gerais (UFMG), Brazil",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Gomes_Do_As_I_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2652558,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5338723692206260310&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "dcc.ufmg.br;dcc.ufmg.br;dcc.ufmg.br;dcc.ufmg.br",
        "email": "dcc.ufmg.br;dcc.ufmg.br;dcc.ufmg.br;dcc.ufmg.br",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Universidade Federal de Minas Gerais;INRIA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ufmg.br;https://www.inria.fr",
        "aff_unique_abbr": "UFMG;INRIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "Brazil;France"
    },
    {
        "title": "Does Face Recognition Accuracy Get Better With Age? Deep Face Matchers Say No",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Albiero_Does_Face_Recognition_Accuracy_Get_Better_With_Age_Deep_Face_WACV_2020_paper.html",
        "author": "Vitor Albiero;  Kevin Bowyer;  Kushal Vangara;  Michael King",
        "abstract": "Previous studies generally agree that face recognition accuracy is higher for older persons than for younger persons. But most previous studies were before the wave of deep learning matchers, and most considered accuracy only in terms of the verification rate for genuine pairs. This paper investigates accuracy for age groups 16-29, 30-49 and 50-70, using three modern deep CNN matchers, and considers differences in the impostor and genuine distributions as well as verification rates and ROC curves. We find thataccuracy is lower for older persons and higher for younger persons. In contrast, a pre deep learning matcher on the same dataset shows the traditional result of higher accuracy for older persons, although its overall accuracy is much lower than that of the deep learning matchers. Comparing the impostor and genuine distributions, we conclude that impostor scores have a larger effect than genuine scores in causing lower accuracy for the older age group. We also investigate the effects of training data across the age groups. Our results show that fine-tuning the deep CNN models on additional images of older persons actually lowers accuracy for the older age group. Also, we fine-tune and train from scratch two models using age-balanced training datasets, and these results also show lower accuracy for older age group. These results argue that the lower accuracy for the older age group is not due to imbalance in the original training data.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Albiero_Does_Face_Recognition_Accuracy_Get_Better_With_Age_Deep_Face_WACV_2020_paper.pdf",
        "aff": "University of Notre Dame; University of Notre Dame; Florida Institute of Technology; Florida Institute of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Albiero_Does_Face_Recognition_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2774537,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10948406705168010131&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nd.edu;nd.edu;my.fit.edu;fit.edu",
        "email": "nd.edu;nd.edu;my.fit.edu;fit.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "University of Notre Dame;Florida Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nd.edu;https://www.fit.edu",
        "aff_unique_abbr": "Notre Dame;FIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Domain Bridge for Unpaired Image-to-Image Translation and Unsupervised Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Pizzati_Domain_Bridge_for_Unpaired_Image-to-Image_Translation_and_Unsupervised_Domain_Adaptation_WACV_2020_paper.html",
        "author": "Fabio Pizzati;  Raoul de Charette;  Michela Zaccaria;  Pietro Cerri",
        "abstract": "Image-to-image translation architectures may have limited effectiveness in some circumstances. For example, while generating rainy scenarios, they may fail to model typical traits of rain as water drops, and this ultimately impacts the synthetic images realism. With our method, called domain bridge, web-crawled data are exploited to reduce the domain gap, leading to the inclusion of previously ignored elements in the generated images. We make use of a network for clear to rain translation trained with the domain bridge to extend our work to Unsupervised Domain Adaptation (UDA). In that context, we introduce an online multimodal style-sampling strategy, where image translation multimodality is exploited at training time to improve performances. Finally, a novel approach for self-supervised learning is presented, and used to further align the domains. With our contributions, we simultaneously increase the realism of the generated images, while reaching on par performances with respect to the UDA state-of-the-art, with a simpler approach.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Pizzati_Domain_Bridge_for_Unpaired_Image-to-Image_Translation_and_Unsupervised_Domain_Adaptation_WACV_2020_paper.pdf",
        "aff": "VisLab+Inria; Inria; University of Parma; VisLab",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Pizzati_Domain_Bridge_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2073567,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12673332007918319039&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "inria.fr;inria.fr;studenti.unipr.it;ambarella.com",
        "email": "inria.fr;inria.fr;studenti.unipr.it;ambarella.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;2;0",
        "aff_unique_norm": "VisLab;Inria;University of Parma",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.inria.fr;https://www.unipr.it",
        "aff_unique_abbr": ";Inria;Unipr",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;2",
        "aff_country_unique": ";France;Italy"
    },
    {
        "title": "Domain-Specific Semantics Guided Approach to Video Captioning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/M_Domain-Specific_Semantics_Guided_Approach_to_Video_Captioning_WACV_2020_paper.html",
        "author": "Hemalatha M;  C Chandra Shekhar",
        "abstract": "In video captioning, the description of a video usually relies on the domain to which the video belongs. Typically, the videos belong to wide range domains such as sports, music, news, cooking, etc. In many cases, a video can be associated with more than one domain. In this paper, we propose an approach to video captioning that uses domain-specific decoders. We build a domain classifier to obtain the estimates of probabilities of a video belonging to different domains. For each video, we identify the top-k domains based on the estimated probabilities. Each video in the training data set is shared in training the domain-specific decoders of top-k labels obtained from the domain classifier. The domain-specific decoders use the domain-specific semantic tags for generating captions. The proposed approach uses the Temporal VLAD for preprocessing the features extracted from 2D-CNN and 3D-CNN features. The preprocessed features provide better feature representation of the videos. The effectiveness of the proposed approach is demonstrated through the results of experimental studies on Microsoft Video Description (MSVD) corpus and MSR-VTT dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/M_Domain-Specific_Semantics_Guided_Approach_to_Video_Captioning_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Technology Madras; Indian Institute of Technology Madras",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1313129,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17071711494471040914&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;cse.iitm.ac.in",
        "email": "gmail.com;cse.iitm.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Madras",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitm.ac.in",
        "aff_unique_abbr": "IIT Madras",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madras",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Dual-Mode Training with Style Control and Quality Enhancement for Road Image Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Venator_Dual-Mode_Training_with_Style_Control_and_Quality_Enhancement_for_Road_WACV_2020_paper.html",
        "author": "Moritz Venator;  Fengyi Shen;  Selcuk Aklanoglu;  Erich Bruns;  Klaus Diepold;  Andreas Maier",
        "abstract": "Dealing properly with different viewing conditions remains a key challenge for computer vision in autonomous driving. Domain adaptation has opened new possibilities for data augmentation, translating arbitrary road scene images into different environmental conditions. Although multimodal concepts have demonstrated the capability to separate content and style, we find that existing methods fail to reproduce scenes in the exact appearance given by a reference image. In this paper, we address the aforementioned problem by introducing a style alignment loss between output and reference image. We integrate this concept into a multimodal unsupervised image-to-image translation model with a novel dual-mode training process and additional adversarial losses. Focusing on road scene images, we evaluate our model in various aspects including visual quality and feature matching. Our experiments reveal that we are able to significantly improve both style alignment and image quality in different viewing conditions. Adapting concepts from neural style transfer, our new training approach allows to control the output of multimodal domain adaptation, making it possible to generate arbitrary scenes and viewing conditions for data augmentation.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Venator_Dual-Mode_Training_with_Style_Control_and_Quality_Enhancement_for_Road_WACV_2020_paper.pdf",
        "aff": "AUDI AG+ Friedrich-Alexander University Erlangen-N\u00fcrnberg; Technical University of Munich; AUDI AG+ Friedrich-Alexander University Erlangen-N\u00fcrnberg; AUDI AG+ Friedrich-Alexander University Erlangen-N\u00fcrnberg; Technical University of Munich; Friedrich-Alexander University Erlangen-N\u00fcrnberg",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Venator_Dual-Mode_Training_with_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1908404,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7204922821419421542&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "audi.de;tum.de;audi.de;audi.de;tum.de;fau.de",
        "email": "audi.de;tum.de;audi.de;audi.de;tum.de;fau.de",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+1;0+1;2;1",
        "aff_unique_norm": "AUDI AG;Friedrich-Alexander University Erlangen-N\u00fcrnberg;Technical University of Munich",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.audi.com;https://www fau.de;https://www.tum.de",
        "aff_unique_abbr": "AUDI;FAU;TUM",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0+0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Dynamic Motion Representation for Human Action Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Asghari-Esfeden_Dynamic_Motion_Representation_for_Human_Action_Recognition_WACV_2020_paper.html",
        "author": "Sadjad Asghari-Esfeden;  Mario Sznaier;  Octavia Camps",
        "abstract": "Despite the advances in Human Activity Recognition, the ability to exploit the dynamics of human body motion in videos has yet to be achieved. In numerous recent works, researchers have used appearance and motion as independent inputs to infer the action that is taking place in a specific video. In this paper, we highlight that while using a novel representation of human body motion, we can benefit from appearance and motion simultaneously. As a result, better performance of action recognition can be achieved. We start with a pose estimator to extract the location and heat-map of body joints in each frame. We use a dynamic encoder to generate a fixed size representation from these body joint heat-maps. Our experimental results show that training a convolutional neural network with the dynamic motion representation outperforms state-of-the-art action recognition models. By modeling distinguishable activities as distinct dynamical systems and with the help of two stream networks, we obtain the best performance on HMDB, JHMDB, UCF-101, and AVA datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Asghari-Esfeden_Dynamic_Motion_Representation_for_Human_Action_Recognition_WACV_2020_paper.pdf",
        "aff": "Smartvid.io + Northeastern University; Northeastern University; Northeastern University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 907117,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13624944918239381250&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "ece.neu.edu;coe.neu.edu;coe.neu.edu",
        "email": "ece.neu.edu;coe.neu.edu;coe.neu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Smartvid.io;Northeastern University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.smartvid.io;https://www.northeastern.edu",
        "aff_unique_abbr": ";NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "EDGE20: A Cross Spectral Evaluation Dataset for Multiple Surveillance Problems",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Le_EDGE20_A_Cross_Spectral_Evaluation_Dataset_for_Multiple_Surveillance_Problems_WACV_2020_paper.html",
        "author": "Ha Le;  Christos Smailis;  Lei Shi;  Ioannis Kakadiaris",
        "abstract": "Surveillance-related datasets that have been released in recent years focus only on one specific problem at a time (e.g., pedestrian detection, face detection, or face recognition), while most of them were collected using visible spectrum (VIS) cameras. Even though some cross-spectral datasets were presented in the past, they were acquired in a constrained setup, which limited the performance of methods for the aforementioned problems under a cross-spectral setting. This work introduces a new dataset, named EDGE20, that can be used in addressing the problems of pedestrian detection, face detection, and face recognition in images captured using trail cameras under the VIS and NIR spectra. Data acquisition was performed in an outdoor environment, during both day and night, under unconstrained acquisition conditions. The collection of images is accompanied by a rich set of annotations, consisting of person and facial bounding boxes, unique subject identifiers, and labels that characterize facial images as frontal, profile, or back faces. Moreover, the performance of several state-of-the-art methods was evaluated for each of the scenarios covered by our dataset. The baseline results we obtained highlight the difficulty of current methods in the tasks of cross-spectral pedestrian detection, face detection, and face recognition due to unconstrained conditions, including low resolution, pose variation, illumination variation, occlusions, and motion blur.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Le_EDGE20_A_Cross_Spectral_Evaluation_Dataset_for_Multiple_Surveillance_Problems_WACV_2020_paper.pdf",
        "aff": "Computational Biomedicine Lab; Computational Biomedicine Lab; Computational Biomedicine Lab; Computational Biomedicine Lab",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6762126,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=217047462976426350&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "central.uh.edu;central.uh.edu;central.uh.edu;central.uh.edu",
        "email": "central.uh.edu;central.uh.edu;central.uh.edu;central.uh.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Computational Biomedicine Lab",
        "aff_unique_dep": "Computational Biomedicine",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "ELoPE: Fine-Grained Visual Classification with Efficient Localization, Pooling and Embedding",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hanselmann_ELoPE_Fine-Grained_Visual_Classification_with_Efficient_Localization_Pooling_and_Embedding_WACV_2020_paper.html",
        "author": "Harald Hanselmann;  Hermann Ney",
        "abstract": "The task of fine-grained visual classification (FGVC) deals with classification problems that display a small inter-class variance such as distinguishing between different bird species or car models. State-of-the-art approaches typically tackle this problem by integrating an elaborate attention mechanism or (part-) localization method into a standard convolutional neural network (CNN). Also in this work the aim is to enhance the performance of a backbone CNN such as ResNet by including three efficient and lightweight components specifically designed for FGVC. This is achieved by using global k-max pooling, a discriminative embedding layer trained by optimizing class means and an efficient localization module that estimates bounding boxes using only class labels for training. The resulting model achieves state-of-the-art recognition accuracies on multiple FGVC benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hanselmann_ELoPE_Fine-Grained_Visual_Classification_with_Efficient_Localization_Pooling_and_Embedding_WACV_2020_paper.pdf",
        "aff": "Human Language Technology and Pattern Recognition Group, Computer Science Department, RWTH Aachen University, 52062 Aachen, Germany + AppTek GmbH, 52062 Aachen, Germany; Human Language Technology and Pattern Recognition Group, Computer Science Department, RWTH Aachen University, 52062 Aachen, Germany + AppTek GmbH, 52062 Aachen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 818469,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=999169496724958848&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cs.rwth-aachen.de;cs.rwth-aachen.de",
        "email": "cs.rwth-aachen.de;cs.rwth-aachen.de",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "RWTH Aachen University;AppTek GmbH",
        "aff_unique_dep": "Computer Science Department;",
        "aff_unique_url": "https://www.rwth-aachen.de;",
        "aff_unique_abbr": "RWTH;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Aachen;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Efficient Object Detection in Large Images Using Deep Reinforcement Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Uzkent_Efficient_Object_Detection_in_Large_Images_Using_Deep_Reinforcement_Learning_WACV_2020_paper.html",
        "author": "Burak Uzkent;  Christopher Yeh;  Stefano  Ermon",
        "abstract": "Traditionally, an object detector is applied to every part of the scene of interest, and its accuracy and computational cost increases with higher resolution images. However, in some application domains such as remote sensing, purchasing high spatial resolution images is expensive. To reduce the large computational and monetary cost associated with using high spatial resolution images, we propose a conditional reinforcement learning agent that adaptively selects the spatial resolution of each image that is provided to the detector. In particular, we train the agent in a dual reward setting to choose low spatial resolution images to be run through a coarse level detector when the image is dominated by large objects, and high spatial resolution image to be run through a fine level detector when it is dominated by small objects. This reduces the dependency on high spatial resolution images for building a robust detector and increases run-time efficiency. We perform experiments on the xView dataset, consisting of large images, where we increase run-time efficiency by 60% and use high resolution images only 30% of the time while maintaining similar accuracy as a detector that uses only high resolution images.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Uzkent_Efficient_Object_Detection_in_Large_Images_Using_Deep_Reinforcement_Learning_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1165851,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7895537598851652453&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.stanford.edu;stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;stanford.edu;cs.stanford.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Efficient Video Semantic Segmentation with Labels Propagation and Refinement",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Paul_Efficient_Video_Semantic_Segmentation_with_Labels_Propagation_and_Refinement_WACV_2020_paper.html",
        "author": "Matthieu Paul;  Christoph Mayer;  Luc Van Gool;  Radu Timofte",
        "abstract": "This paper tackles the problem of real-time semantic segmentation of high definition videos using a hybrid GPU / CPU approach. We propose an Efficient Video Segmentation (EVS) pipeline that combines:  (i) On the CPU, a very fast optical flow method, that is used to exploit the temporal aspect of the video and propagate semantic information from one frame to the next. It runs in parallel with the GPU.  (ii) On the GPU, two Convolutional Neural Networks: A main segmentation network that is used to predict dense semantic labels from scratch, and a Refiner that is designed to improve predictions from previous frames with the help of a fast Inconsistencies Attention Module (IAM). The latter can identify regions that cannot be propagated accurately.  We suggest several operating points depending on the desired frame rate and accuracy. Our pipeline achieves accuracy levels competitive to the existing real-time methods for semantic image segmentation(mIoU above 60%), while achieving much higher frame rates. On the popular Cityscapes dataset with high resolution frames (2048 x 1024), the proposed operating points range from 80 to 1000 Hz on a single GPU and CPU.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Paul_Efficient_Video_Semantic_Segmentation_with_Labels_Propagation_and_Refinement_WACV_2020_paper.pdf",
        "aff": "Computer Vision Lab, ETH Z\u00fcrich, Switzerland; Computer Vision Lab, ETH Z\u00fcrich, Switzerland; Computer Vision Lab, ETH Z\u00fcrich, Switzerland; Computer Vision Lab, ETH Z\u00fcrich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1977302,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4551034920229689054&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Z\u00fcrich",
        "aff_unique_dep": "Computer Vision Lab",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Z\u00fcrich",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "End to End Lip Synchronization with a Temporal AutoEncoder",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Shalev_End_to_End_Lip_Synchronization_with_a_Temporal_AutoEncoder_WACV_2020_paper.html",
        "author": "Yoav Shalev;  Lior Wolf",
        "abstract": "We study the problem of syncing the lip movement in a video with the audio stream. Our solution finds an optimal alignment using a dual-domain recurrent neural network that is trained on synthetic data we generate by dropping and duplicating video frames. Once the alignment is found, we modify the video in order to sync the two sources. Our method is shown to greatly outperform the literature methods on a variety of existing and new benchmarks. As an application, we demonstrate our ability to robustly align text-to-speech generated audio with an existing video stream. Our code is attached as supplementary.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Shalev_End_to_End_Lip_Synchronization_with_a_Temporal_AutoEncoder_WACV_2020_paper.pdf",
        "aff": "School of Computer Science; School of Computer Science",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Shalev_End_to_End_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2246564,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13298054940866553624&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "; ",
        "email": "; ",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Computer Science",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "End-To-End Trainable Video Super-Resolution Based on a New Mechanism for Implicit Motion Estimation and Compensation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Liu_End-To-End_Trainable_Video_Super-Resolution_Based_on_a_New_Mechanism_for_WACV_2020_paper.html",
        "author": "Xiaohong Liu;  Lingshi Kong;  Yang Zhou;  Jiying Zhao;  Jun Chen",
        "abstract": "Video super-resolution aims at generating a high-resolution video from its low-resolution counterpart. With the rapid rise of deep learning, many recently proposed video super-resolution methods use convolutional neural networks in conjunction with explicit motion compensation to capitalize on statistical dependencies within and across low-resolution frames. Two common issues of such methods are noteworthy. Firstly, the quality of the final reconstructed HR video is often very sensitive to the accuracy of motion estimation. Secondly, the warp grid needed for motion compensation, which is specified by the two flow maps delineating pixel displacements in horizontal and vertical directions, tends to introduce additional errors and jeopardize the temporal consistency across video frames. To address these issues, we propose a novel dynamic local filter network to perform implicit motion estimation and compensation by employing, via locally connected layers, sample-specific and position-specific dynamic local filters that are tailored to the target pixels. We also propose a global refinement network based on ResBlock and autoencoder structures to exploit non-local correlations and enhance the spatial consistency of super-resolved frames. The experimental results demonstrate that the proposed method outperforms the state-of-the-art, and validate its strength in terms of local transformation handling, temporal consistency as well as edge sharpness.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Liu_End-To-End_Trainable_Video_Super-Resolution_Based_on_a_New_Mechanism_for_WACV_2020_paper.pdf",
        "aff": "McMaster University; McMaster University; University of Ottawa; University of Ottawa; McMaster University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6549463,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=104193709165837508&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mcmaster.ca;mcmaster.ca;uottawa.ca;uottawa.ca;mcmaster.ca",
        "email": "mcmaster.ca;mcmaster.ca;uottawa.ca;uottawa.ca;mcmaster.ca",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "McMaster University;University of Ottawa",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mcmaster.ca;https://www.uottawa.ca",
        "aff_unique_abbr": "McMaster;U Ottawa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Enhanced generative adversarial network for 3D brain MRI super-resolution",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wang_Enhanced_generative_adversarial_network_for_3D_brain_MRI_super-resolution_WACV_2020_paper.html",
        "author": "Jiancong  Wang;  Yuhua Chen;  Yifan Wu;  Jianbo Shi;  James Gee",
        "abstract": "Single image super-resolution (SISR) reconstruction for magnetic resonance imaging (MRI) has generated significant interest because of its potential to not only speed up imaging but to improve quantitative processing and analysis of available image data.  Generative Adversarial Networks (GAN) have proven to perform well in image recovery tasks. In this work, we followed the GAN framework and developed a generator coupled with discriminator to tackle the task of 3D SISR on T1 brain MRI images. We developed a novel 3D memory-efficient residual-dense block generator (MRDG) that achieves state-of-the-art performance in terms of PSNR (Peak Signal to Noise Ratio), SSIM (Structural Similarity) and NRMSE (Normalized Root Mean Squared Error) metrics. Paired with MRDG, we also designed a pyramid pooling discriminator (PPD) to recover details on different size scales simultaneously. Finally, we introduced model blending, a simple and computational efficient method to balance between image and texture quality in the final output, to the task of SISR on 3D images.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_Enhanced_generative_adversarial_network_for_3D_brain_MRI_super-resolution_WACV_2020_paper.pdf",
        "aff": "Penn Image Computing and Science Laboratory, University of Pennsylvania, Philadelphia, PA 19104, USA; Department of Bioengineering, University of California, Los Angeles, CA 90095, USA; Penn Image Computing and Science Laboratory, University of Pennsylvania, Philadelphia, PA 19104, USA; Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, USA; Penn Image Computing and Science Laboratory, University of Pennsylvania, Philadelphia, PA 19104, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2817249,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16324118673905289650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "pennmedicine.upenn.edu;ucla.edu;seas.upenn.edu;seas.upenn.edu;upenn.edu",
        "email": "pennmedicine.upenn.edu;ucla.edu;seas.upenn.edu;seas.upenn.edu;upenn.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "University of Pennsylvania;University of California, Los Angeles",
        "aff_unique_dep": "Penn Image Computing and Science Laboratory;Department of Bioengineering",
        "aff_unique_url": "https://www.upenn.edu;https://www.ucla.edu",
        "aff_unique_abbr": "UPenn;UCLA",
        "aff_campus_unique_index": "0;1;0;0;0",
        "aff_campus_unique": "Philadelphia;Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "EpO-Net: Exploiting Geometric Constraints on Dense Trajectories for Motion Saliency",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Akhter_EpO-Net_Exploiting_Geometric_Constraints_on_Dense_Trajectories_for_Motion_Saliency_WACV_2020_paper.html",
        "author": "Ijaz Akhter;  Mohsen Ali;  Muhammad Faisal;  RICHARD HARTLEY",
        "abstract": "The existing approaches for salient motion segmentation are unable to explicitly learn geometric cues and often give false detections on prominent static objects. We exploit multiview geometric constraints to avoid such shortcomings. To handle the nonrigid background like a sea, we also propose a robust fusion mechanism between motion and appearance-based features. We find dense trajectories, covering every pixel in the video, and propose trajectory-based epipolar distances to distinguish between background and foreground regions. Trajectory epipolar distances are data-independent and can be readily computed given a few features' correspondences between the images. We show that by combining epipolar distances with optical flow, a powerful motion network can be learned. Enabling the network to leverage both of these features, we propose a simple mechanism, we call input-dropout. Comparing the motion-only networks, we outperform the previous state of the art on DAVIS-2016 dataset by 5.2% in the mean IoU score. By robustly fusing our motion network with an appearance network using the input-dropout mechanism, we also outperform the previous methods on DAVIS-2016, 2017 and Segtrackv2 dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Akhter_EpO-Net_Exploiting_Geometric_Constraints_on_Dense_Trajectories_for_Motion_Saliency_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Akhter_EpO-Net_Exploiting_Geometric_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3358623,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2285846837773401248&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Erasing Scene Text with Weak Supervision",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zdenek_Erasing_Scene_Text_with_Weak_Supervision_WACV_2020_paper.html",
        "author": "Jan Zdenek;  Hideki Nakayama",
        "abstract": "Scene text erasing is a task of removing text from natural scene images, which has been gaining attention in recent years. The main motivation is to conceal private information such as license plate numbers, and house nameplates that can appear in images. In this work, we propose a method for scene text erasing that approaches the problem as a general inpainting task. In contrast to previous methods, which require pairs of original images containing text and images from which the text has been removed, our method does not need corresponding image pairs for training. We use a separately trained scene text detector and an inpainting network. The scene text detector predicts segmentation maps of text instances which are then used as masks for the inpainting network. The network for inpainting, trained on a large-scale image dataset, fills in masked out regions in an input image and generates a final image in which the original text is no longer present. The results show that our method is able to successfully remove text and fill in the created holes to produce natural-looking images.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zdenek_Erasing_Scene_Text_with_Weak_Supervision_WACV_2020_paper.pdf",
        "aff": "The University of Tokyo; The University of Tokyo",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1600053,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7878862416900363547&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "nlab.ci.i.u-tokyo.ac.jp;nlab.ci.i.u-tokyo.ac.jp",
        "email": "nlab.ci.i.u-tokyo.ac.jp;nlab.ci.i.u-tokyo.ac.jp",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Estimate 3D Camera Pose from 2D Pedestrian Trajectories",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xu_Estimate_3D_Camera_Pose_from_2D_Pedestrian_Trajectories_WACV_2020_paper.html",
        "author": "Yan Xu;  Vivek Roy;  Kris Kitani",
        "abstract": "We consider the task of re-calibrating the 3D pose of a static surveillance camera, whose pose may change due to external forces, such as birds, wind, falling objects or earthquakes.  Conventionally, camera pose estimation can be solved with a PnP (Perspective-n-Point) method using 2D-to-3D feature correspondences, when 3D points are known.  However, 3D point annotations are not always available or practical to obtain in real-world applications.  We propose an alternative strategy for extracting 3D information to solve for camera pose by using pedestrian trajectories. We observe that 2D pedestrian trajectories indirectly contain useful 3D information that can be used for inferring camera pose.  To leverage this information, we propose a data-driven approach by training a neural network (NN) regressor to model a direct mapping from 2D pedestrian trajectories projected on the image plane to 3D camera pose. We demonstrate that our regressor trained only on synthetic data can be directly applied to real data, thus eliminating the need to label any real data. We evaluate our method across six different scenes from the Town Centre Street and DUKEMTMC datasets.  Our method achieves an improvement of around 50% on both position and orientation prediction accuracy when compared to other SOTA methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xu_Estimate_3D_Camera_Pose_from_2D_Pedestrian_Trajectories_WACV_2020_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1328941,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=243282799102038294&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;cs.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;cs.cmu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Evaluation of Image Inpainting for Classification and Retrieval",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Black_Evaluation_of_Image_Inpainting_for_Classification_and_Retrieval_WACV_2020_paper.html",
        "author": "Samuel Black;  Somayeh Keshavarz;  Richard Souvenir",
        "abstract": "A common approach to censoring digital image content is masking the region(s) of interest with a solid color or pattern. In the case where the masked image will be used as input for classification or matching, the mask itself may impact the results. Recent work in image inpainting provides an alternative to masking by replacing the foreground with predicted background. In this paper, we perform an extensive evaluation of inpainting approaches to understand how well inpainted images can serve as proxies for the original in classification and retrieval. Results indicate that the metrics typically used to  evaluate inpainting performance (e.g., reconstruction accuracy) do not necessarily correspond to improved classification or retrieval, especially in the case of person-shaped masked regions.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Black_Evaluation_of_Image_Inpainting_for_Classification_and_Retrieval_WACV_2020_paper.pdf",
        "aff": "Department of Computer and Information Sciences, Temple University; Department of Computer and Information Sciences, Temple University; Department of Computer and Information Sciences, Temple University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3447830,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13762987574033854496&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "temple.edu;temple.edu;temple.edu",
        "email": "temple.edu;temple.edu;temple.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Temple University",
        "aff_unique_dep": "Department of Computer and Information Sciences",
        "aff_unique_url": "https://www.temple.edu",
        "aff_unique_abbr": "Temple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Event-based Star Tracking via Multiresolution Progressive Hough Transforms",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Bagchi_Event-based_Star_Tracking_via_Multiresolution_Progressive_Hough_Transforms_WACV_2020_paper.html",
        "author": "Samya Bagchi;  Tat-Jun Chin",
        "abstract": "Star trackers are state-of-the-art attitude estimation devices which function by recognising and tracking star patterns. Most commercial star trackers use conventional optical sensors. A recent alternative is to use event sensors, which could enable more energy-efficient and faster star trackers. However, this demands new algorithms that can efficiently cope with high-speed asynchronous data, and are feasible on resource-constrained computing platforms. To this end, we propose an event-based processing approach for star tracking. Our technique operates on the event stream from a star field, by using multiresolution Hough Transforms to time-progressively integrate event data and produce accurate relative rotations. Optimisation via rotation averaging is then used to fuse the relative rotations and jointly refine the absolute orientations. Our technique is designed to be feasible for asynchronous operation on standard hardware. Moreover, compared to state-of-the-art event-based motion estimation schemes, our technique is much more efficient and accurate.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Bagchi_Event-based_Star_Tracking_via_Multiresolution_Progressive_Hough_Transforms_WACV_2020_paper.pdf",
        "aff": "University of Adelaide; University of Adelaide",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Bagchi_Event-based_Star_Tracking_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 7904731,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2688988134789141880&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Exploring 3 R's of Long-term Tracking: Redetection, Recovery and Reliability",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Karthik_Exploring_3_Rs_of_Long-term_Tracking_Redetection_Recovery_and_Reliability_WACV_2020_paper.html",
        "author": "Shyamgopal Karthik;  Abhinav Moudgil;  Vineet Gandhi",
        "abstract": "Recent works have proposed several long term tracking benchmarks and highlight the importance of moving towards long-duration tracking to bridge the gap with application requirements. The current evaluation methodologies, however, do not focus on several aspects that are crucial in a long term perspective like Re-detection, Recovery, and Reliability.  In this paper, we propose novel evaluation strategies for a more in-depth analysis of trackers from a long-term perspective. More specifically, (a) we test re-detection capability of the trackers in the wild by simulating virtual cuts, (b) we investigate the role of chance in the recovery of tracker after failure and (c) we propose a novel metric allowing visual inference on the ability of a tracker to track contiguously (without any failure) at a given accuracy. We present several original insights derived from an extensive set of quantitative and qualitative experiments.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Karthik_Exploring_3_Rs_of_Long-term_Tracking_Redetection_Recovery_and_Reliability_WACV_2020_paper.pdf",
        "aff": "Center for Visual Information Technology, Kohli Center on Intelligent Systems, IIIT-Hyderabad, India; Center for Visual Information Technology, Kohli Center on Intelligent Systems, IIIT-Hyderabad, India; Center for Visual Information Technology, Kohli Center on Intelligent Systems, IIIT-Hyderabad, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2431118,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18246777803807075359&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "IIIT-Hyderabad",
        "aff_unique_dep": "Center for Visual Information Technology",
        "aff_unique_url": "https://www.iiit.ac.in",
        "aff_unique_abbr": "IIIT-H",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Exploring Hate Speech Detection in Multimodal Publications",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Gomez_Exploring_Hate_Speech_Detection_in_Multimodal_Publications_WACV_2020_paper.html",
        "author": "Raul Gomez;  Jaume Gibert;  Lluis Gomez;  Dimosthenis Karatzas",
        "abstract": "In this work we target the problem of hate speech detection in multimodal publications formed by a text and an image. We gather and annotate a large scale dataset from Twitter, MMHS150K, and propose different models that jointly analyze textual and visual information for hate speech detection, comparing them with unimodal detection. We provide quantitative and qualitative results and analyze the challenges of the proposed task. We find that, even though images are useful for the hate speech detection task, current multimodal models cannot outperform models analyzing only text. We discuss why and open the field and the dataset for further research.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Gomez_Exploring_Hate_Speech_Detection_in_Multimodal_Publications_WACV_2020_paper.pdf",
        "aff": "Eurecat, Centre Tecnol`ogic de Catalunya, Unitat de Tecnologies Audiovisuals, Barcelona, Spain+Computer Vision Center, Universitat Aut`onoma de Barcelona, Barcelona, Spain; Eurecat, Centre Tecnol`ogic de Catalunya, Unitat de Tecnologies Audiovisuals, Barcelona, Spain; Computer Vision Center, Universitat Aut`onoma de Barcelona, Barcelona, Spain; Computer Vision Center, Universitat Aut`onoma de Barcelona, Barcelona, Spain",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1458361,
        "gs_citation": 307,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2325878047305498011&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "eurecat.org;eurecat.org;cvc.uab.es;cvc.uab.es",
        "email": "eurecat.org;eurecat.org;cvc.uab.es;cvc.uab.es",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;1;1",
        "aff_unique_norm": "Eurecat;Universitat Autonoma de Barcelona",
        "aff_unique_dep": "Unitat de Tecnologies Audiovisuals;Computer Vision Center",
        "aff_unique_url": "https://www.eurecat.org;https://www.uab.cat",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Barcelona",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "title": "Extracting identifying contours for African elephants and humpback whales using a learned appearance model",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Weideman_Extracting_identifying_contours_for_African_elephants_and_humpback_whales_using_WACV_2020_paper.html",
        "author": "Hendrik Weideman;  Chuck Stewart;  Jason Parham;  Jason Holmberg;  Kiirsten Flynn;  John Calambokidis;  D. Barry Paul;  Anka Bedetti;  Michelle Henley;  Frank Pope;  Jerenimo Lepirei",
        "abstract": "This paper addresses the problem of identifying individual animals in images based on extracting and matching contours, focusing in particular on the trailing edges of humpback whale flukes and the outline of the ears of African savanna elephants. A coarse-grained FCNN is learned to isolate the contour in an image, and a fine-grained FCNN is learned to provide more precise boundary information. The latter is trained by generating synthetic boundaries from coarse, easily-extracted training data, avoiding tedious manual effort. An A* algorithm extracts the final contour, which is converted to set of digital curvature descriptors and matched against a database of descriptors using local-naive Bayes nearest neighbors. We show that using the learned fine-grained FCNN produces more accurate contours than using image gradients for fine localization, especially for elephant ears where the boundaries are primarily texture. Matching using contours extracted using the fine-grained FCNN improves top-1 accuracy from 80% to 85% for flukes and 78% to 84% for ears.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Weideman_Extracting_identifying_contours_for_African_elephants_and_humpback_whales_using_WACV_2020_paper.pdf",
        "aff": "Bloomberg L.P.; Rensselaer Polytechnic Institute; Wild Me; Wild Me; Cascadia Research Collective; Cascadia Research Collective; Elephants Alive; Elephants Alive; Elephants Alive; Save the Elephants; Save the Elephants",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3683940,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4542172680576197019&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "bloomberg.net;cs.rpi.edu;wildme.org;wildme.org;cascadiaresearch.org;cascadiaresearch.org;elephantsalive.org;elephantsalive.org;elephantsalive.org;savetheelephants.org;savetheelephants.org",
        "email": "bloomberg.net;cs.rpi.edu;wildme.org;wildme.org;cascadiaresearch.org;cascadiaresearch.org;elephantsalive.org;elephantsalive.org;elephantsalive.org;savetheelephants.org;savetheelephants.org",
        "author_num": 11,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;3;3;4;4;4;5;5",
        "aff_unique_norm": "Bloomberg;Rensselaer Polytechnic Institute;Wild Me;Cascadia Research Collective;Elephants Alive;Save the Elephants",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.bloomberg.com;https://www.rpi.edu;https://www.wildme.org;https://www.cascadiaresearch.org;https://www.elephantsalive.org;",
        "aff_unique_abbr": "Bloomberg;RPI;;;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;1;1;1",
        "aff_country_unique": "United States;South Africa;"
    },
    {
        "title": "Eye Contact Correction using Deep Neural Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Isikdogan_Eye_Contact_Correction_using_Deep_Neural_Networks_WACV_2020_paper.html",
        "author": "Furkan Isikdogan;  Timo Gerasimow;  Gilad Michael",
        "abstract": "In a typical video conferencing setup, it is hard to maintain eye contact during a call since it requires looking into the camera rather than the display. We propose an eye contact correction model that restores the eye contact regardless of the relative position of the camera and display. Unlike previous solutions, our model redirects the gaze from an arbitrary direction to the center without requiring a redirection angle or camera/display/user geometry as inputs. We use a deep convolutional neural network that inputs a monocular image and produces a vector field and a brightness map to correct the gaze. We train this model in a bi-directional way on a large set of synthetically generated photorealistic images with perfect labels. The learned model is a robust eye contact corrector which also predicts the input gaze implicitly at no additional cost.      Our system is primarily designed to improve the quality of video conferencing experience. Therefore, we use a set of control mechanisms to prevent creepy results and to ensure a smooth and natural video conferencing experience. The entire eye contact correction system runs end-to-end in real-time on a commodity CPU and does not require any dedicated hardware, making our solution feasible for a variety of devices.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Isikdogan_Eye_Contact_Correction_using_Deep_Neural_Networks_WACV_2020_paper.pdf",
        "aff": "Intel Corporation, Santa Clara, CA; Intel Corporation, Santa Clara, CA; Intel Corporation, Santa Clara, CA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1047346,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12707610079330292473&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "intel.com;intel.com;intel.com",
        "email": "intel.com;intel.com;intel.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Intel Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.intel.com",
        "aff_unique_abbr": "Intel",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Santa Clara",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "EyeGAN: Gaze-Preserving, Mask-Mediated Eye Image Synthesis",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kaur_EyeGAN_Gaze-Preserving_Mask-Mediated_Eye_Image_Synthesis_WACV_2020_paper.html",
        "author": "Harsimran Kaur;  Roberto Manduchi",
        "abstract": "Automatic synthesis of realistic eye images with prescribed gaze direction is important for multiple application domains. We introduce EyeGAN, an algorithm to generate eye images in the style of a desired target domain, that inherit annotations available in images from a source domain. EyeGAN takes in input ternary masks, which are used as domain-independent proxies for gaze direction. We evaluate EyeGAN against competing eye image synthesis algorithms by measuring a specific gaze consistency index. In addition, we present results from multiple experiments (involving eye region segmentation, pupil localization, and gaze direction estimation) showing that the use of EyeGAN generated images with inherited annotations for network training leads to superior performances compared to other domain transfer algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kaur_EyeGAN_Gaze-Preserving_Mask-Mediated_Eye_Image_Synthesis_WACV_2020_paper.pdf",
        "aff": "University of California, Santa Cruz; University of California, Santa Cruz",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 608385,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15547434536727014798&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ucsc.edu; ",
        "email": "ucsc.edu; ",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Santa Cruz",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsc.edu",
        "aff_unique_abbr": "UCSC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Santa Cruz",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "FX-GAN: Self-Supervised GAN Learning via Feature Exchange",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Huang_FX-GAN_Self-Supervised_GAN_Learning_via_Feature_Exchange_WACV_2020_paper.html",
        "author": "Rui Huang;  Wenju Xu;  Teng-Yok Lee;  Anoop Cherian;  Ye Wang;  Tim Marks",
        "abstract": "We propose a self-supervised approach to improve the training of Generative Adversarial Networks (GANs) via inducing the discriminator to examine the structural consistency of images. Although natural image samples provide ideal examples of both valid structure and valid texture, learning to reproduce both together remains an open challenge. In our approach, we augment the training set of natural images with modified examples that have degraded structural consistency. These degraded examples are automatically created by randomly exchanging pairs of patches in an image's convolutional feature map. We call this approach feature exchange. With this setup, we propose a novel GAN formulation, termed Feature eXchange GAN (FX-GAN), in which the discriminator is trained not only to distinguish real versus generated images, but also to perform the auxiliary task of distinguishing between real images and structurally corrupted (feature-exchanged) real images. This auxiliary task causes the discriminator to learn the proper feature structure of natural images, which in turn guides the generator to produce images with more realistic structure. Compared with strong GAN baselines, our proposed self-supervision approach improves generated image quality, diversity, and training stability for both the unconditional and class-conditional settings.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Huang_FX-GAN_Self-Supervised_GAN_Learning_via_Feature_Exchange_WACV_2020_paper.pdf",
        "aff": "Carnegie Mellon University; University of Kansas; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Huang_FX-GAN_Self-Supervised_GAN_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3367329,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13707753801581622571&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "alumni.cmu.edu;ku.edu;merl.com;merl.com;merl.com;merl.com",
        "email": "alumni.cmu.edu;ku.edu;merl.com;merl.com;merl.com;merl.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;2;2",
        "aff_unique_norm": "Carnegie Mellon University;University of Kansas;Mitsubishi Electric Research Laboratories",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.ku.edu;https://www.merl.com",
        "aff_unique_abbr": "CMU;KU;MERL",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast Deep Stereo with 2D Convolutional Processing of Cost Signatures",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Yee_Fast_Deep_Stereo_with_2D_Convolutional_Processing_of_Cost_Signatures_WACV_2020_paper.html",
        "author": "Kyle Yee;  Ayan Chakrabarti",
        "abstract": "Modern neural network-based algorithms are able to produce highly accurate depth estimates from stereo image pairs, nearly matching the reliability of measurements from more expensive depth sensors. However, this accuracy comes with a higher computational cost since these methods use network architectures designed to compute and process matching scores across all candidate matches at all locations, with floating point computations repeated across a match volume with dimensions corresponding to both space and disparity. This leads to longer running times to process each image pair, making them impractical for real-time use in robots and autonomous vehicles. We propose a new stereo algorithm that employs a significantly more efficient network architecture. Our method builds an initial match cost volume using traditional matching costs that are fast to compute, and trains a network to estimate disparity from this volume. Crucially, our network only employs per-pixel and two-dimensional convolution operations: to summarize the local match information at each location as a low-dimensional feature vector, and to spatially process these \"cost-signature\" features to produce a dense disparity map. Experimental results on KITTI show that our method delivers competitive accuracy at significantly higher speeds---running at 48 frames per second on a modern GPU.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Yee_Fast_Deep_Stereo_with_2D_Convolutional_Processing_of_Cost_Signatures_WACV_2020_paper.pdf",
        "aff": "Amazon; Washington University in St. Louis",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6037500,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=177507579341068621&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;wustl.edu",
        "email": "gmail.com;wustl.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Amazon.com, Inc.;Washington University in St. Louis",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.amazon.com;https://wustl.edu",
        "aff_unique_abbr": "Amazon;WashU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";St. Louis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast Image Reconstruction with an Event Camera",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Scheerlinck_Fast_Image_Reconstruction_with_an_Event_Camera_WACV_2020_paper.html",
        "author": "Cedric Scheerlinck;  Henri Rebecq;  Daniel Gehrig;  Nick Barnes;  Robert  Mahony;  Davide Scaramuzza",
        "abstract": "Event cameras are powerful new sensors able to capture high dynamic range with microsecond temporal resolution and no motion blur. Their strength is detecting brightness changes (called events) rather than capturing direct brightness images; however, algorithms can be used to convert events into usable image representations for applications such as classification. Previous works rely on hand-crafted spatial and temporal smoothing techniques to reconstruct images from events. State-of-the-art video reconstruction has recently been achieved using neural networks that are large (10M parameters) and computationally expensive, requiring 30ms for a forward-pass at 640 x 480 resolution on a modern GPU. We propose a novel neural network architecture for video reconstruction from events that is smaller (38k vs. 10M parameters) and faster (10ms vs. 30ms) than state-of-the-art with minimal impact to performance. Videos and Datasets: https://cedric-scheerlinck.github.io/firenet",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Scheerlinck_Fast_Image_Reconstruction_with_an_Event_Camera_WACV_2020_paper.pdf",
        "aff": "Australian National University; University of Zurich; University of Zurich; Australian National University; Australian National University; University of Zurich",
        "project": "https://cedric-scheerlinck.github.io/\ufb01renet",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 734921,
        "gs_citation": 256,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6727745131264909246&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "anu.edu.au;ifi.uzh.ch;ifi.uzh.ch;anu.edu.au;anu.edu.au;ifi.uzh.ch",
        "email": "anu.edu.au;ifi.uzh.ch;ifi.uzh.ch;anu.edu.au;anu.edu.au;ifi.uzh.ch",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;0;1",
        "aff_unique_norm": "Australian National University;University of Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.anu.edu.au;https://www.uzh.ch",
        "aff_unique_abbr": "ANU;UZH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;0;1",
        "aff_country_unique": "Australia;Switzerland"
    },
    {
        "title": "Fast Postprocessing for Difficult Discrete Energy Minimization Problems",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Akhter_Fast_Postprocessing_for_Difficult_Discrete_Energy_Minimization_Problems_WACV_2020_paper.html",
        "author": "Ijaz Akhter;  Loong Fah Cheong;  RICHARD HARTLEY",
        "abstract": "Despite the rapid progress in discrete energy minimization, certain problems involving high connectivity and a high number of labels are considered very hard but are still very relevant in computer vision. We propose a post-processing technique to improve the sub-optimal results of the existing methods on such problems. Our core contribution is a mapping between the binary min-cut problem and finding the shortest path in a directed acyclic graph. Using this mapping, we present an algorithm to find an approximate solution for the min-cut problem. We also extend the same idea for multi-label factor-graphs in the form of an iterative move-making algorithm. The proposed algorithm is extremely fast, yet outperforms the existing techniques in terms of accuracy as well as the computational time. We demonstrate competitive or better results on problems where already high-quality work is done.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Akhter_Fast_Postprocessing_for_Difficult_Discrete_Energy_Minimization_Problems_WACV_2020_paper.pdf",
        "aff": "KeepTruckin, Inc; National University of Singapore; Australian National University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 678755,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:4mDTdBz6ooIJ:scholar.google.com/&scioq=Fast+Postprocessing+for+Difficult+Discrete+Energy+Minimization+Problems&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff_domain": "keeptruckin.com;nus.edu.sg;anu.edu.au",
        "email": "keeptruckin.com;nus.edu.sg;anu.edu.au",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "KeepTruckin;National University of Singapore;Australian National University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.keeptruckin.com;https://www.nus.edu.sg;https://www.anu.edu.au",
        "aff_unique_abbr": ";NUS;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "United States;Singapore;Australia"
    },
    {
        "title": "Fast Video Multi-Style Transfer",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Gao_Fast_Video_Multi-Style_Transfer_WACV_2020_paper.html",
        "author": "Wei Gao;  Yijun Li;  Yihang Yin;  Ming-Hsuan Yang",
        "abstract": "Recent progresses in video style transfer have shown promising results which contain less flickering effects. However, existing algorithms mainly trade off generality for efficiency, i.e., constructing one network per style example, and often work well for short video clips only. Specifically, we design a multi-instance normalization block (MIN-Block) to learn different style examples and a ConvLSTM module to encourage the temporal consistency. The proposed algorithm is demonstrated to be able to generate temporally-consistent video transfer results in different styles while keeping each stylized frame visually pleasing. Extensive experimental results show that the proposed method performs favorably again single-style models and some post-processing techniques that alleviate the flickering issue. We achieve as many as 120 stylization effects in a single model and show results on long-term videos that consist of thousands of frames.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Gao_Fast_Video_Multi-Style_Transfer_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5808519,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14728542306892808002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Few-Shot Learning of Video Action Recognition Only Based on Video Contents",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Bo_Few-Shot_Learning_of_Video_Action_Recognition_Only_Based_on_Video_WACV_2020_paper.html",
        "author": "Yang Bo;  Yangdi Lu;  Wenbo  He",
        "abstract": "The success of video action recognition based on Deep Neural Networks (DNNs) is highly dependent on a large number of manually labeled videos. In this paper, we introduce a supervised learning approach to recognize video actions with very few training videos. Specifically, we propose Temporal Attention Vectors (TAVs) which adapt various length videos to preserve the temporal information of the entire video. We evaluate the TAVs on UCF101 and HMDB51. Without training any deep 3D or 2D frame feature extractors on video datasets (only pre-trained on ImageNet), the TAVs only introduce 2.1M parameters but outperforms the state-of-the-art video action recognition benchmarks with very few labeled training videos (e.g. 92% on UCF101 and 59% on HMDB51, with 10 and 8 training videos per class, respectively). Furthermore, our approach can still achieve competitive results on full datasets (97.1% on UCF101 and 77% on HMDB51).",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Bo_Few-Shot_Learning_of_Video_Action_Recognition_Only_Based_on_Video_WACV_2020_paper.pdf",
        "aff": "McMaster University; McMaster University; McMaster University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Bo_Few-Shot_Learning_of_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 927986,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4910329289988372131&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "mcmaster.ca; ; ",
        "email": "mcmaster.ca; ; ",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "McMaster University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mcmaster.ca",
        "aff_unique_abbr": "McMaster",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Few-Shot Scene Adaptive Crowd Counting Using Meta-Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Reddy_Few-Shot_Scene_Adaptive_Crowd_Counting_Using_Meta-Learning_WACV_2020_paper.html",
        "author": "Mahesh Kumar Krishna Reddy;  Mohammad Hossain;  Mrigank Rochan;  Yang Wang",
        "abstract": "We consider the problem of few-shot scene adaptive crowd counting. Given a target camera scene, our goal is to adapt a model to this specific scene with only a few labeled images of that scene. The solution to this problem has potential applications in numerous real-world scenarios, where we ideally like to deploy a crowd counting model specially adapted to a target camera. We accomplish this challenge by taking inspiration from the recently introduced learning-to-learn paradigm in the context of few-shot regime. In training, our method learns the model parameters in a way that facilitates the fast adaptation to the target scene. At test time, given a target scene with a small number of labeled data, our method quickly adapts to that scene with a few gradient updates to the learned parameters. Our extensive experimental results show that the proposed approach outperforms other alternatives in few-shot scene adaptive crowd counting.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Reddy_Few-Shot_Scene_Adaptive_Crowd_Counting_Using_Meta-Learning_WACV_2020_paper.pdf",
        "aff": "University of Manitoba; Huawei Technologies Co., Ltd. + University of Manitoba; University of Manitoba; University of Manitoba",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Reddy_Few-Shot_Scene_Adaptive_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2962124,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8512389087745734071&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.umanitoba.ca;gmail.com;cs.umanitoba.ca;cs.umanitoba.ca",
        "email": "cs.umanitoba.ca;gmail.com;cs.umanitoba.ca;cs.umanitoba.ca",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "University of Manitoba;Huawei Technologies",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://umanitoba.ca;https://www.huawei.com",
        "aff_unique_abbr": "U of M;Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0;0",
        "aff_country_unique": "Canada;China"
    },
    {
        "title": "Figure Captioning with Relation Maps for Reasoning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_Figure_Captioning_with_Relation_Maps_for_Reasoning_WACV_2020_paper.html",
        "author": "Charles Chen;  Ruiyi Zhang;  Eunyee Koh;  Sungchul Kim;  Scott Cohen;  Ryan Rossi",
        "abstract": "Figures, such as line plots, pie charts, bar charts, are widely used to convey important information in a concise format. In this work, we investigate the problem of figure caption generation where the goal is to automatically generate a natural language description  for a given figure. While natural image captioning has been studied extensively, figure captioning has received relatively little attention and remains a challenging problem. A successful solution to this task has many potential applications, such as: 1) adding captions to the output of a visualization tool; 2) summarizing documents with a number of figures with or without proper captions;  3) improving user experience by allowing figure content to be accessible to those with visual impairment. To solve this problem, we collect a dataset FigCAP for testing the capability of generating captions, and propose a captioning framework with novel attention models. In order to solve the exposure bias issue, we further train the captioning model with sequence-level policy based on reinforcement learning, which directly optimizes evaluation  metrics. Extensive experiments show that our proposed models outperform strong image captioning baselines, thus demonstrating a significant potential for automatic generating captions for figures.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Figure_Captioning_with_Relation_Maps_for_Reasoning_WACV_2020_paper.pdf",
        "aff": "Ohio University; Duke University; Adobe Research; Adobe Research; Adobe Research; Adobe Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 850003,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11474679125925103390&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "ohio.odu;duke.edu;adobe.com;adobe.com;adobe.com;adobe.com",
        "email": "ohio.odu;duke.edu;adobe.com;adobe.com;adobe.com;adobe.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;2;2",
        "aff_unique_norm": "Ohio University;Duke University;Adobe",
        "aff_unique_dep": ";;Adobe Research",
        "aff_unique_url": "https://www.ohio.edu;https://www.duke.edu;https://research.adobe.com",
        "aff_unique_abbr": "OU;Duke;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Filter Distillation for Network Compression",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Cuadros_Filter_Distillation_for_Network_Compression_WACV_2020_paper.html",
        "author": "Xavier Suau Cuadros;  Luca Zappella;  Nicholas Apostoloff",
        "abstract": "In this paper we introduce Principal Filter Analysis (PFA), an easy to use and effective method for neural network compression. PFA exploits the correlation between filter responses within network layers to recommend a smaller network that maintain as much as possible the accuracy of the full model. We propose two algorithms: the first allows users to target compression to specific network property, such as number of trainable variable (footprint), and produces a compressed model that satisfies the requested property while preserving the maximum amount of spectral energy in the responses of each layer, while the second is a parameter-free heuristic that selects the compression used at each layer by trying to mimic an ideal set of uncorrelated responses. Since PFA compresses networks based on the correlation of their responses we show in our experiments that it gains the additional flexibility of adapting each architecture to a specific domain while compressing. PFA is evaluated against several architectures and datasets, and shows considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. Our tests show that PFA is competitive with state-of-the-art approaches while removing adoption barriers thanks to its practical implementation, intuitive philosophy and ease of use.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Cuadros_Filter_Distillation_for_Network_Compression_WACV_2020_paper.pdf",
        "aff": "Apple; Apple; Apple",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Cuadros_Filter_Distillation_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1696692,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7957494734893776281&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "apple.com;apple.com;apple.com",
        "email": "apple.com;apple.com;apple.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Apple Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.apple.com",
        "aff_unique_abbr": "Apple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fine-Grained Motion Representation For Template-Free Visual Tracking",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Shuang_Fine-Grained_Motion_Representation_For_Template-Free_Visual_Tracking_WACV_2020_paper.html",
        "author": "Kai Shuang;  Yuheng Huang;  Yue Sun;  Zhun Cai;  Hao Guo",
        "abstract": "The object tracking task requires tracking the arbitrary target in consecutive video frames. Recently, several attempts have been made to develop the template-free models to attain generality. However, the current template-free paradigm only estimates the displacement to approximate the motion of the object. The displacement is insufficient to represent complex bounding box transformation, including scaling and rotation. We argue that the coarse-grained representation of object motion limits the performance of current template-free approaches. In this paper, we explore the finer-grained motion estimation to improve the accuracy of the template-free model. In respect of the image space, our method estimates the transformation for each pixel in the image. Concern on the motion representation, we represent the motion by the transformation parameterized by displacement, scaling, and rotation. By applying the differential vector operators on the optical flow, our approach estimates both displacement, scaling, and rotation for each pixel in a unified theory. To the best of our knowledge, we are the first work to model the displacement, scaling, and rotation in a unified theory with the optical flow. To further improve the localization accuracy, we develop the appearance branch to introduce the appearance information into our model. Furthermore, to suppress optical flow estimation failure samples during training, we propose a novel loss function Limited L1. The experiment shows our model FGTrack achieves state-of-the-art performance on both NFS and VOT2017 datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Shuang_Fine-Grained_Motion_Representation_For_Template-Free_Visual_Tracking_WACV_2020_paper.pdf",
        "aff": "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China+Science and Technology on Communication Networks Laboratory, Shijiazhuang, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Trusfort Technology Co., Ltd.; Beijing Trusfort Technology Co., Ltd.; Beijing Trusfort Technology Co., Ltd.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1496219,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1784964703906577910&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;trusfort.com;trusfort.com;trusfort.com",
        "email": "bupt.edu.cn;bupt.edu.cn;trusfort.com;trusfort.com;trusfort.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;2;2;2",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Science and Technology on Communication Networks Laboratory;Beijing Trusfort Technology Co., Ltd.",
        "aff_unique_dep": "State Key Laboratory of Networking and Switching Technology;;",
        "aff_unique_url": "http://www.bupt.edu.cn/;;",
        "aff_unique_abbr": "BUPT;;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mafla_Fine-grained_Image_Classification_and_Retrieval_by_Combining_Visual_and_Locally_WACV_2020_paper.html",
        "author": "Andres Mafla;  Sounak Dey;  Ali Furkan Biten;  Lluis Gomez;  Dimosthenis Karatzas",
        "abstract": "Text contained in an image carries high-level semantics that can be exploited to achieve richer image understanding. In particular, the mere presence of text provides strong guiding content that should be employed to tackle a diversity of computer vision tasks such as image retrieval, fine-grained classification, and visual question answering. In this paper, we address the problem of fine-grained classification and image retrieval by leveraging textual information along with visual cues to comprehend the existing intrinsic relation between the two modalities. The novelty of the proposed model consists of the usage of a PHOC descriptor to construct a bag of textual words along with a Fisher Vector Encoding that captures the morphology of text. This approach provides a stronger multimodal representation for this task and as our experiments demonstrate, it achieves state-of-the-art results on two different tasks, fine-grained classification and image retrieval.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mafla_Fine-grained_Image_Classification_and_Retrieval_by_Combining_Visual_and_Locally_WACV_2020_paper.pdf",
        "aff": "Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain",
        "project": "",
        "github": "http://www.github.com/DreadPiratePsyopus/Fine Grained Clf",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Mafla_Fine-grained_Image_Classification_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2323267,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18376384381427716662&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cvc.uab.es;cvc.uab.es;cvc.uab.es;cvc.uab.es;cvc.uab.es",
        "email": "cvc.uab.es;cvc.uab.es;cvc.uab.es;cvc.uab.es;cvc.uab.es",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Universitat Aut\u00f2noma de Barcelona",
        "aff_unique_dep": "Computer Vision Center",
        "aff_unique_url": "https://www.uab.cat",
        "aff_unique_abbr": "UAB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "title": "FlowNet3D++: Geometric Losses For Deep Scene Flow Estimation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wang_FlowNet3D_Geometric_Losses_For_Deep_Scene_Flow_Estimation_WACV_2020_paper.html",
        "author": "Zirui Wang;  Shuda Li;  Henry Howard-Jenkins;  Victor Prisacariu;  Min Chen",
        "abstract": "We present FlowNet3D++, a deep scene flow estimation network. Inspired by classical methods, FlowNet3D++ incorporates geometric constraints in the form of point-toplane distance and angular alignment between individual vectors in the flow field, into FlowNet3D. We demonstrate that the addition of these geometric loss terms improves the previous state-of-art FlowNet3D accuracy from 57.85% to 63.43%. To further demonstrate the effectiveness of our geometric constraints, we propose a benchmark for flow estimation on the task of dynamic 3D reconstruction, thus providing a more holistic and practical measure of performance than the breakdown of individual metrics previously used to evaluate scene flow. This is made possible through the contribution of a novel pipeline to integrate point-based scene flow predictions into a global dense volume. FlowNet3D++ achieves up to a 15.0% reduction in reconstruction error over FlowNet3D, and up to a 35.2% improvement over KillingFusion alone. We will release our scene flow estimation code later.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_FlowNet3D_Geometric_Losses_For_Deep_Scene_Flow_Estimation_WACV_2020_paper.pdf",
        "aff": "University of Oxford; University of Oxford; University of Oxford; University of Oxford; University of Oxford",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Wang_FlowNet3D_Geometric_Losses_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 626299,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4380975100014809793&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk;oerc.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk;oerc.ox.ac.uk",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Focusing Visual Relation Detection on Relevant Relations with Prior Potentials",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/PLESSE_Focusing_Visual_Relation_Detection_on_Relevant_Relations_with_Prior_Potentials_WACV_2020_paper.html",
        "author": "Francois PLESSE;  Alexandru Ginsca;  Bertrand DELEZOIDE;  Francoise PRETEUX",
        "abstract": "Understanding images relies on the understanding of how visible objects are linked to each other. Current approaches of Visual Relation Detection (VRD) are hindered by the high frequency of some relations: when an important focus is put on them, more meaningful ones are overlooked. We address this challenge by learning the relative relevance of relations, and integrating this term into a novel scene graph extraction scheme. We show that this allows our model to predict relations on fewer and more relevant object pairs. It outperforms MotifNet, a state of the art model, on the Visual Genome dataset. It increases the Class Macro recall, the metric we propose to use, from 38.1% to 44.4%. In addition, we propose a new split of Visual Genome, with a more balanced relation distribution, emphasizing on the detection of uncommon relations and validates the use of the previous metric. On this set, our model outperforms MotifNet on all metrics, e.g. from 39.6% to 44.0% at 10 predictions per image on the relation classification task.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/PLESSE_Focusing_Visual_Relation_Detection_on_Relevant_Relations_with_Prior_Potentials_WACV_2020_paper.pdf",
        "aff": "DRT LIST DIASI LASTI; DRT LIST DIASI LASTI; DRT LIST DIASI LASTI; Ecole des Ponts - Cermics",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1103799,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6795141905523529643&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cea.fr;cea.fr;cea.fr;enpc.fr",
        "email": "cea.fr;cea.fr;cea.fr;enpc.fr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "1",
        "aff_unique_norm": ";Ecole des Ponts ParisTech",
        "aff_unique_dep": ";Cermics",
        "aff_unique_url": ";https://www.ponts.org",
        "aff_unique_abbr": ";ENPC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";France"
    },
    {
        "title": "Fourier Based Pre-Processing For Seeing Through Water",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/James_Fourier_Based_Pre-Processing_For_Seeing_Through_Water_WACV_2020_paper.html",
        "author": "Jerin Geo James;  Ajit Rajwade",
        "abstract": "Consider a scene submerged underneath a fluctuating water surface. Images of such a scene, when acquired from a camera in the air, exhibit significant spatial distortions. In this paper, we present a novel, computationally efficient pre-processing algorithm to correct a significant amount (~ 50%)  of apparent distortion present in video sequences of such a scene. We demonstrate that when the partially restored video output from this stage is given as input to other methods, it significantly improves their performance. This algorithm involves (i) tracking a small number N of salient feature points across the T frames to yield point-trajectories \\ \\boldsymbol q_i  \\triangleq \\ (x_ it ,y_ it )\\ _ t=1 ^T\\ _ i=1 ^N, and (ii) using the point-trajectories to infer the deformations at other non-tracked points in every frame. A Fourier decomposition of the N trajectories, followed by a novel Fourier phase-interpolation step, is used to infer deformations at all other points. Our method exploits the inherent spatio-temporal characteristics of the fluctuating water surface to correct non-rigid deformations to a very large extent. The source code, datasets and supplemental material can be accessed at [1], [2].",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/James_Fourier_Based_Pre-Processing_For_Seeing_Through_Water_WACV_2020_paper.pdf",
        "aff": "IIT Bombay; IIT Bombay",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/James_Fourier_Based_Pre-Processing_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1095756,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12643658161257531639&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "cse.iitb.ac.in;cse.iitb.ac.in",
        "email": "cse.iitb.ac.in;cse.iitb.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitb.ac.in",
        "aff_unique_abbr": "IITB",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mumbai",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "From Image to Video Face Inpainting: Spatial-Temporal Nested GAN (STN-GAN) for Usability Recovery",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wu_From_Image_to_Video_Face_Inpainting_Spatial-Temporal_Nested_GAN_STN-GAN_WACV_2020_paper.html",
        "author": "Yifan Wu;  Vivek Singh;  Ankur Kapoor",
        "abstract": "In this paper, we propose to use constrained inpainting methods to recover usability of corrupted images. Here we focus on the example of face images that are masked for privacy protection but complete images are required for further algorithm development. The task is tackled in a progressive manner: 1) the generated images should look realistic; 2) the generated images must satisfy spatial constraints, if available; 3) when applied to video data, temporal consistency should be retained. We first present a spatial inpainting framework to synthesize face images which can incorporate spatial constraints, provided as positions of facial markers and show that it outperforms state-of-the-art methods. Next, we propose Spatial-Temporal Nested GAN (STN-GAN) to adapt image inpainting framework, trained on 200k images, to video data by incorporating temporal information using residual blocks. Experiments on multiple public datasets show STN-GAN attains spatio-temporal consistency effectively and efficiently. Furthermore, we show that spatial constraints can be perturbed to obtain different inpainted results from a single source.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wu_From_Image_to_Video_Face_Inpainting_Spatial-Temporal_Nested_GAN_STN-GAN_WACV_2020_paper.pdf",
        "aff": "Siemens Healthineers, Digital Services, Digital Technology & Innovation, Princeton, NJ, USA; Siemens Healthineers, Digital Services, Digital Technology & Innovation, Princeton, NJ, USA; Siemens Healthineers, Digital Services, Digital Technology & Innovation, Princeton, NJ, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 957067,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=566147915057391544&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff_domain": "siemens-healthineers.com;siemens-healthineers.com;siemens-healthineers.com",
        "email": "siemens-healthineers.com;siemens-healthineers.com;siemens-healthineers.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Siemens Healthineers",
        "aff_unique_dep": "Digital Services, Digital Technology & Innovation",
        "aff_unique_url": "https://www.siemens-healthineers.com",
        "aff_unique_abbr": "Siemens Healthineers",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Frustum VoxNet for 3D object detection from RGB-D or Depth images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Shen_Frustum_VoxNet_for_3D_object_detection_from_RGB-D_or_Depth_WACV_2020_paper.html",
        "author": "Xiaoke Shen;  Ioannis Stamos",
        "abstract": "Recently, there have been a plethora of classification and detection systems from RGB as well as 3D images. In this work, we describe a new 3D object detection system from an RGB-D or depth-only point cloud. Our system first detects objects in 2D (either RGB, or pseudo-RGB constructed from depth). The next step is to detect 3D objects within the 3D frustums these 2D detections define. This is achieved by voxelizing parts of the frustums (since frustums can be really large), instead of using the whole frustums as done in earlier work. The main novelty of our system has to do with determining which parts (3D proposals) of the frustums to voxelize, thus allowing us to provide high resolution representations around the objects of interest. It also allows our system to have reduced memory requirements. These 3D proposals are fed to an efficient ResNet-based 3D Fully Convolutional Network (FCN). Our 3D detection system is fast, and can be integrated into a robotics platform. With respect to systems that do not perform voxelization (such as PointNet), our methods can operate without the requirement of subsampling of the datasets. We have also introduced a pipelining approach that further improves the efficiency of our system. Results on SUN RGB-D dataset show that our system, which is based on a small network, can process 20 frames per second with comparable detection results to the state-of-the-art [16], achieving a 2x speedup.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Shen_Frustum_VoxNet_for_3D_object_detection_from_RGB-D_or_Depth_WACV_2020_paper.pdf",
        "aff": "The Graduate Center, CUNY; Hunter College & The Graduate Center, CUNY",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Shen_Frustum_VoxNet_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1368346,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7602216226117813786&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gradcenter.cuny.edu;hunter.cuny.edu",
        "email": "gradcenter.cuny.edu;hunter.cuny.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "City University of New York;Hunter College",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gc.cuny.edu;https://www.hunter.cuny.edu",
        "aff_unique_abbr": "CUNY;Hunter College",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "The Graduate Center;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fungi Recognition: A Practical Use Case",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sulc_Fungi_Recognition_A_Practical_Use_Case_WACV_2020_paper.html",
        "author": "Milan Sulc;  Lukas Picek;  Jiri Matas;  Thomas Jeppesen;  Jacob Heilmann-Clausen",
        "abstract": "The paper presents a system for visual recognition of 1394 fungi species based on deep convolutional neural networks and its deployment in a citizen-science project. The system allows users to automatically identify observed specimens, while providing valuable data to biologists and computer vision researchers. The underlying classification method scored first in the FGVCx Fungi Classification Kaggle competition organized in connection with the Fine-Grained Visual Categorization (FGVC) workshop at CVPR 2018. We describe our winning submission and evaluate all technicalities that increased the recognition scores, and discuss the issues related to deployment of the system via the web- and mobile- interfaces.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sulc_Fungi_Recognition_A_Practical_Use_Case_WACV_2020_paper.pdf",
        "aff": "CTU in Prague; University of West Bohemia; CTU in Prague; Global Biodiversity Information Facility; University of Copenhagen",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1787434,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3832232065045538195&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cmp.felk.cvut.cz;kky.zcu.cz;cmp.felk.cvut.cz;gbif.org;snm.ku.dk",
        "email": "cmp.felk.cvut.cz;kky.zcu.cz;cmp.felk.cvut.cz;gbif.org;snm.ku.dk",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2;3",
        "aff_unique_norm": "Czech Technical University;University of West Bohemia;Global Biodiversity Information Facility;University of Copenhagen",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ctu.cz;https://www.zcu.cz;https://www.gbif.org;https://www.ku.dk",
        "aff_unique_abbr": "CTU;ZCU;GBIF;UCPH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Prague;",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "Czech Republic;Denmark"
    },
    {
        "title": "FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal Data",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Krispel_FuseSeg_LiDAR_Point_Cloud_Segmentation_Fusing_Multi-Modal_Data_WACV_2020_paper.html",
        "author": "Georg Krispel;  Michael Opitz;  Georg Waltner;  Horst Possegger;  Horst Bischof",
        "abstract": "We introduce a simple yet effective fusion method of LiDAR and RGB data to segment LiDAR point clouds. Utilizing the dense native range representation of a LiDAR sensor and the setup calibration, we establish point correspondences between the two input modalities. Subsequently, we are able to warp and fuse the features from one domain into the other. Therefore, we can jointly exploit information from both data sources within one single network.  To show the merit of our method, we extend SqueezeSeg, a point cloud segmentation network, with an RGB feature branch and fuse it into the original structure. Our extension called FuseSeg leads to an improvement of up to 18% IoU on the KITTI benchmark. In addition to the improved accuracy, we also achieve real-time performance at 50 fps, five times as fast as the KITTI LiDAR data recording speed.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Krispel_FuseSeg_LiDAR_Point_Cloud_Segmentation_Fusing_Multi-Modal_Data_WACV_2020_paper.pdf",
        "aff": "Institute of Computer Graphics and Vision; Institute of Computer Graphics and Vision; Institute of Computer Graphics and Vision; Institute of Computer Graphics and Vision; Institute of Computer Graphics and Vision",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 910845,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11896091043160247348&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Institute of Computer Graphics and Vision",
        "aff_unique_dep": "Computer Graphics and Vision",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Fusing Semantics and Motion State Detection for Robust Visual SLAM",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Singh_Fusing_Semantics_and_Motion_State_Detection_for_Robust_Visual_SLAM_WACV_2020_paper.html",
        "author": "Gaurav Singh;  Meiqing Wu;  Siew-Kei Lam",
        "abstract": "Achieving robust pose tracking and mapping in highly dynamic environments is a major challenge faced by existing visual SLAM (vSLAM) systems. In this paper, we increase the robustness of existing vSLAM by accurately removing moving objects from the scene so that they will not contribute to pose estimation and mapping. Specifically, semantic information is fused with motion states of the scene via a probability framework to enable accurate and robust moving object extraction in order to retain the useful features for pose estimation and mapping. Our work highlights the importance of distinguishing between motion states of potential moving objects for vSLAM in highly dynamic environments. The proposed method can be integrated into existing vSLAM systems to increase their robustness in dynamic environments without incurring much computation cost. We provide extensive experimental results on three well-known datasets to show that the proposed technique outperforms existing vSLAM methods in indoor and outdoor environments, under various scenarios such as crowded scenes.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Singh_Fusing_Semantics_and_Motion_State_Detection_for_Robust_Visual_SLAM_WACV_2020_paper.pdf",
        "aff": "School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 679734,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11465335382004129309&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "e.ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "email": "e.ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "GAR: Graph Assisted Reasoning for Object Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Li_GAR_Graph_Assisted_Reasoning_for_Object_Detection_WACV_2020_paper.html",
        "author": "Zheng Li;  Xiaocong Du;  Yu Cao",
        "abstract": "It is well believed that object-object relations and object-scene relations inherently improve the accuracy of object detection. However, the way to efficiently model relations remains a problem. Graph Convolutional Network (GCN), an effective method to handle structured data with relations, inspires us to leverage graphs in modeling relations for objection detection tasks. In this work, we propose a novel approach, Graph Assisted Reasoning (GAR), to utilize a heterogeneous graph in modeling object-object relations and object-scene relations. GAR fuses the features from neighboring object nodes as well as scene nodes and produces better recognition than that produced from individual object nodes. Moreover, compared to previous approaches using Recurrent Neural Network (RNN), the light-weight and low-coupling architecture of GAR further facilitates its integration into the object detection module. Comprehensive experiments on PASCAL VOC and MS COCO datasets demonstrate the efficacy of GAR.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Li_GAR_Graph_Assisted_Reasoning_for_Object_Detection_WACV_2020_paper.pdf",
        "aff": "Arizona State University; Arizona State University; Arizona State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 787817,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14925727602583809016&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Gaze Estimation for Assisted Living Environments",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Dias_Gaze_Estimation_for_Assisted_Living_Environments_WACV_2020_paper.html",
        "author": "Philipe Ambrozio Dias;  Damiano Malafronte;  Henry Medeiros;  Francesca Odone",
        "abstract": "Effective assisted living environments must be able to perform inferences on how their occupants interact with one another as well as with surrounding objects. To accomplish this goal using a vision-based automated approach, multiple tasks such as pose estimation, object segmentation and gaze estimation must be addressed. Gaze direction provides some of the strongest indications of how a person interacts with the environment. In this paper, we propose a simple neural network regressor that estimates the gaze direction of individuals in a multi-camera assisted living scenario, relying only on the relative positions of facial keypoints collected from a single pose estimation model. To handle cases of keypoint occlusion, our model exploits a novel confidence gated unit in its input layer. In addition to the gaze direction, our model also outputs an estimation of its own prediction uncertainty. Experimental results on a public benchmark demonstrate that our approach performs on par with a complex, dataset-specific baseline, while its uncertainty predictions are highly correlated to the actual angular error of corresponding estimations. Finally, experiments on images from a real assisted living environment demonstrate that our model has a higher suitability for its final application.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Dias_Gaze_Estimation_for_Assisted_Living_Environments_WACV_2020_paper.pdf",
        "aff": "Marquette University (EECE), USA; Italian Institute of Technology; Marquette University (EECE), USA; University of Genova (MaLGa-DIBRIS), Italy",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Dias_Gaze_Estimation_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2366891,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18315002465731582479&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "marquette.edu;iit.it;marquette.edu;unige.it",
        "email": "marquette.edu;iit.it;marquette.edu;unige.it",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Marquette University;Italian Institute of Technology;University of Genova",
        "aff_unique_dep": "Electrical and Computer Engineering;;MaLGa-DIBRIS",
        "aff_unique_url": "https://www.marquette.edu;https://www.iit.it;https://www.unige.it",
        "aff_unique_abbr": "MU;IIT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "United States;Italy"
    },
    {
        "title": "Generating Positive Bounding Boxes for Balanced Training of Object Detectors",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Oksuz_Generating_Positive_Bounding_Boxes_for_Balanced_Training_of_Object_Detectors_WACV_2020_paper.html",
        "author": "Kemal Oksuz;  Baris Can Cam;  Emre Akbas;  Sinan Kalkan",
        "abstract": "Two-stage deep object detectors generate a set of regions-of-interest (RoIs) in the first stage, then, in the second stage, identify objects among the proposed RoIs that sufficiently overlap with a ground truth (GT) box. The second stage is known to suffer from a bias towards RoIs that have low intersection-over-union (IoU) with the associated GT boxes. To address this issue, we first propose a sampling method to generate bounding boxes (BB) that overlap with a given reference box more than a given IoU threshold. Then, we use this BB generation method to develop a positive RoI (pRoI) generator that, for the second stage, produces RoIs following any desired spatial or IoU distribution. We show that our pRoI generator is able to simulate other sampling methods for positive examples such as hard example mining and prime sampling. Using our generator as an analysis tool, we show that (i) IoU imbalance has an adverse effect on performance, (ii) hard positive example mining improves the performance only for certain input IoU distributions, and (iii) the imbalance among the foreground classes has an adverse effect on performance and that it can be alleviated at the batch level. Finally, we train Faster R-CNN using our pRoI generator and, compared to conventional training, obtain better or on-par performance for low IoUs and significant improvements when trained for higher IoUs for Pascal VOC and MS COCO datasets. The code is available at: https://github.com/kemaloksuz/BoundingBoxGenerator",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Oksuz_Generating_Positive_Bounding_Boxes_for_Balanced_Training_of_Object_Detectors_WACV_2020_paper.pdf",
        "aff": "Department of Computer Engineering, Middle East Technical University, Ankara, Turkey; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey",
        "project": "",
        "github": "https://github.com/kemaloksuz/BoundingBoxGenerator",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Oksuz_Generating_Positive_Bounding_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2631008,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8385159364669545488&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "metu.edu.tr;metu.edu.tr;metu.edu.tr;metu.edu.tr",
        "email": "metu.edu.tr;metu.edu.tr;metu.edu.tr;metu.edu.tr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Middle East Technical University",
        "aff_unique_dep": "Department of Computer Engineering",
        "aff_unique_url": "https://www.metu.edu.tr",
        "aff_unique_abbr": "METU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Ankara",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Turkey"
    },
    {
        "title": "Generative Model with Semantic Embedding and Integrated Classifier for Generalized Zero-Shot Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Pambala_Generative_Model_with_Semantic_Embedding_and_Integrated_Classifier_for_Generalized_WACV_2020_paper.html",
        "author": "Ayyappa Pambala;  Titir Dutta;  Soma  Biswas",
        "abstract": "Generative models have achieved impressive performance for the generalized zero-shot learning task by learning the mapping from attributes to feature space. In this work, we propose to derive semantic inferences from images and use them for the generation, which enables us to capture the bidirectional information i.e., visual to semantic and semantic to visual spaces. Specifically, we propose a Semantic Embedding module which not only gives image specific semantic information to the generative model for generation of better features, but also makes sure that the generated features can be mapped to the correct semantic space. We also propose an Integrated Classifier, which is trained along with the generator. This module not only eliminates the requirement of additional classifier for new object categories which is required by the existing generative approaches, but also facilitates the generation of more discriminative and useful features. This approach can be used seamlessly for the task of few-shot learning. Extensive experiments on four benchmark datasets, namely, CUB, SUN, AWA1, AWA2 for both zero-shot learning and few-shot setting show the effectiveness of the proposed approach.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Pambala_Generative_Model_with_Semantic_Embedding_and_Integrated_Classifier_for_Generalized_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Science; Indian Institute of Science; Indian Institute of Science",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 839085,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13967225631231944749&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "email": "iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Generative Pseudo-label Refinement for Unsupervised Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Morerio_Generative_Pseudo-label_Refinement_for_Unsupervised_Domain_Adaptation_WACV_2020_paper.html",
        "author": "Pietro Morerio;  Riccardo  Volpi;  Ruggero Ragonesi;  Vittorio Murino",
        "abstract": "We investigate and characterize the inherent resilience of conditional Generative Adversarial Networks (cGANs) against noise in their conditioning labels, and exploit this fact in the context of Unsupervised Domain Adaptation (UDA). In UDA, a classifier trained on the labelled source set can be used to infer pseudo-labels on the unlabelled target set. However, this will result in a significant amount of misclassified examples (due to the well-known domain shift issue), which can be interpreted as noise injection in the ground-truth labels for the target set. We show that cGANs are, to some extent, robust against such \"shift noise\". Indeed, cGANs trained with noisy pseudo-labels, are able to filter such noise and generate cleaner target samples. We exploit this finding in an iterative procedure where a generative model and a classifier are jointly trained: in turn, the generator allows to sample cleaner data from the target distribution, and the classifier allows to associate better labels to target samples, progressively refining target pseudo-labels. Results on common benchmarks show that our method performs better or comparably with the unsupervised domain adaptation state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Morerio_Generative_Pseudo-label_Refinement_for_Unsupervised_Domain_Adaptation_WACV_2020_paper.pdf",
        "aff": "Pattern Analysis & Computer Vision - Istituto Italiano di Tecnologia; Pattern Analysis & Computer Vision - Istituto Italiano di Tecnologia; Pattern Analysis & Computer Vision - Istituto Italiano di Tecnologia + Universit `a degli Studi di Genova, Italy; Pattern Analysis & Computer Vision - Istituto Italiano di Tecnologia + Computer Science Department - Universit `a di Verona, Italy + Huawei Technologies Ltd., Ireland Research Center",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Morerio_Generative_Pseudo-label_Refinement_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 825441,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4003135053166172144&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "iit.it;iit.it;iit.it;iit.it",
        "email": "iit.it;iit.it;iit.it;iit.it",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;0+2+3",
        "aff_unique_norm": "Istituto Italiano di Tecnologia;University of Genoa;Universit\u00e0 di Verona;Huawei Technologies",
        "aff_unique_dep": "Pattern Analysis & Computer Vision;;Computer Science Department;Ireland Research Center",
        "aff_unique_url": "https://www.iit.it;https://www.unige.it;https://www.univr.it;https://www.huawei.com",
        "aff_unique_abbr": "IIT;UniGe;UniVR;Huawei",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0+0+1",
        "aff_country_unique": "Italy;Ireland"
    },
    {
        "title": "Geometric Image Correspondence Verification by Dense Pixel Matching",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Laskar_Geometric_Image_Correspondence_Verification_by_Dense_Pixel_Matching_WACV_2020_paper.html",
        "author": "Zakaria Laskar;  Iaroslav Melekhov;  Hamed Rezazadegan Tavakoli;  Juha Ylioinas;  Juho  Kannala",
        "abstract": "This paper addresses the problem of determining dense pixel correspondences between two images and its application to geometric correspondence verification in image retrieval. The main contribution is a geometric correspondence verification approach for re-ranking a shortlist of retrieved database images based on their dense pair-wise matching with the query image at a pixel level. We determine a set of cyclically consistent dense pixel matches between the pair of images and evaluate local similarity of matched pixels using neural network based image descriptors. Final re-ranking is based on a novel similarity function, which fuses the local similarity metric with a global similarity metric and a geometric consistency measure computed for the matched pixels. For dense matching our approach utilizes a modified version of a recently proposed dense geometric correspondence network (DGC-Net), which we also improve by optimizing the architecture. The proposed model and similarity metric compare favourably to the state-of-the-art image retrieval methods. In addition, we apply our method to the problem of long-term visual localization demonstrating promising results and generalization across datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Laskar_Geometric_Image_Correspondence_Verification_by_Dense_Pixel_Matching_WACV_2020_paper.pdf",
        "aff": "Aalto University, Espoo, Finland; Aalto University, Espoo, Finland; Nokia Technologies, Espoo, Finland; Aalto University, Espoo, Finland; Aalto University, Espoo, Finland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Laskar_Geometric_Image_Correspondence_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1306309,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8007038144944984454&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "aalto.fi;aalto.fi;nokia.com;aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi;nokia.com;aalto.fi;aalto.fi",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Aalto University;Nokia Technologies",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.aalto.fi;https://www.nokia.com/technologies/",
        "aff_unique_abbr": "Aalto;Nokia Tech",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Espoo",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "title": "Global Co-occurrence Feature Learning and Active Coordinate System Conversion for Skeleton-based Action Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Li_Global_Co-occurrence_Feature_Learning_and_Active_Coordinate_System_Conversion_for_WACV_2020_paper.html",
        "author": "Sheng Li;  Tingting Jiang;  Tiejun Huang;  Yonghong Tian",
        "abstract": "Skeleton-based action recognition has attracted more and more attention in recent years. Besides, the rapid development of deep learning has greatly improved the performance.However, the current exploration of action cooccurrence is still not comprehensive enough. Most existing works only mine co-occurrence features from the temporal or spatial domain seperately, and it's common to combine them in the end. Different from previous works, our approach is able to learn temporal and spatial co-occurrence features integratedly and globally, which is called spatio-temporal-unit feature enhancement (STUFE). In order to better align the skeleton data, we introduce a novel method for skeleton data preprocessing called active coordinate system conversion (ACSC). A coordinate system can be learned automatically to transform skeleton samples for alignment. By the way, the proposed methods are compatible with current two types of mainstream models, the CNN-based and GCN-based models. Finally, on the two benchmarks of NTU-RGB+D and SBU Kinect Interaction, we validated our methods based on two mainstream models.The results show that our methods achieve the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Li_Global_Co-occurrence_Feature_Learning_and_Active_Coordinate_System_Conversion_for_WACV_2020_paper.pdf",
        "aff": "NELVT, Department of Computer Science, Peking University, China; NELVT, Department of Computer Science, Peking University, China; NELVT, Department of Computer Science, Peking University, China; NELVT, Department of Computer Science, Peking University, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1707284,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15503394702163337044&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Global Context Reasoning for Semantic Segmentation of 3D Point Clouds",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ma_Global_Context_Reasoning_for_Semantic_Segmentation_of_3D_Point_Clouds_WACV_2020_paper.html",
        "author": "Yanni Ma;  Yulan Guo;  Hao Liu;  Yinjie  Lei;  Gongjian Wen",
        "abstract": "Global contextual dependency is important for semantic segmentation of 3D point clouds. However, most existing approaches stack feature extraction layers to enlarge the receptive field to aggregate more contextual information of points along the spatial dimension. In this paper, we propose a Point Global Context Reasoning (PointGCR) module to capture global contextual information along the channel dimension. In PointGCR, an undirected graph representation (namely, ChannelGraph) is used to learn channel independencies. Specifically, channel maps are first represented as graph nodes and the independencies between nodes are then represented as graph edges. PointGCR is a plug-andplay and end-to-end trainable module. It can easily be integrated into an existing segmentation network and achieves a significant performance improvement. We conduct extensive experiments to evaluate the proposed PointGCR module on both indoor and outdoor datasets. Experimental results show that our PointGCR module efficiently captures global contextual dependencies and significantly improve the segmentation performance of several existing networks.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ma_Global_Context_Reasoning_for_Semantic_Segmentation_of_3D_Point_Clouds_WACV_2020_paper.pdf",
        "aff": "Sun Yat-sen University; Sun Yat-sen University+National University of Defense Technology; Sun Yat-sen University; Sichuan University; National University of Defense Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 408042,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4597928590888597828&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "mail2.sysu.edu.cn;nudt.edu.cn;mail2.sysu.edu.cn;scu.edu.cn;sina.com",
        "email": "mail2.sysu.edu.cn;nudt.edu.cn;mail2.sysu.edu.cn;scu.edu.cn;sina.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;2;1",
        "aff_unique_norm": "Sun Yat-sen University;National University of Defense Technology;Sichuan University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.sysu.edu.cn/;http://www.nudt.edu.cn/;https://www.scu.edu.cn",
        "aff_unique_abbr": "SYSU;NUDT;SCU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Going Beyond the Regression Paradigm with Accurate Dot Prediction for Dense Crowds",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sam_Going_Beyond_the_Regression_Paradigm_with_Accurate_Dot_Prediction_for_WACV_2020_paper.html",
        "author": "deepak babu sam;  Skand Peri;  Mukuntha Narayanan Sundararaman;  Venkatesh Babu RADHAKRISHNAN",
        "abstract": "We present an alternative to the paradigm of density regression widely being employed for tackling crowd counting. In the prevalent regression approach, a model is trained for mapping images to its crowd density rather than counting by detecting every person. This framework is motivated from the difficulty to discriminate humans in highly dense crowds where unfavorable perspective, occlusion and clutter are prevalent. Though regression methods estimate overall crowd counts pretty well, localization of individual persons suffers and varies considerably across the entire density spectrum. Moreover, individual detection of people aids more explainable practical systems than predicting blind crowd count or density map. Hence, we move away from density regression and reformulate the task as localized dot prediction in dense crowds. Our dot detection model, DD-CNN, is trained for pixel-wise binary classification to detect people instead of regressing local crowd density. In order to handle severe scale variation and detect people of all scales with accurate dots, we use a novel multi-scale architecture which does not require any ground truth scale information. This training regime, which incorporates top-down feedback, helps our model to localize people in sparse as well as dense crowds. Our model delivers superior counting performance on major crowd datasets. We also evaluate on some additional metrics and evidence superior localization of the dot detection formulation.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sam_Going_Beyond_the_Regression_Paradigm_with_Accurate_Dot_Prediction_for_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4598431,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4057872446361665967&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Going Much Wider with Deep Networks for Image Super-Resolution",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Singh_Going_Much_Wider_with_Deep_Networks_for_Image_Super-Resolution_WACV_2020_paper.html",
        "author": "Vikram Singh;  Keerthan Ramnath;  Subrahmanyam Arunachalam;  Anurag Mittal",
        "abstract": "Divide and Conquer is a well-established approach in the literature that has efficiently solved a variety of problems. However, it is yet to be explored in full in solving image super-resolution. To predict a sharp up-sampled image, this work proposes a divide and conquer approach based wide and deep network (WDN) that divides the 4x up-sampling problem into 32 disjoint subproblems that can be solved simultaneously and independently of each other. Half of these subproblems deal with predicting the overall features of the high-resolution image, while the remaining are exclusively for predicting the finer details. Additionally, a technique that is found to be more effective in calibrating the pixel intensities has been proposed. Results obtained on multiple datasets demonstrate the improved performance of the proposed wide and deep network over state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Singh_Going_Much_Wider_with_Deep_Networks_for_Image_Super-Resolution_WACV_2020_paper.pdf",
        "aff": "Computer Vision Lab, Indian Institute of Technology-Madras; Computer Vision Lab, Indian Institute of Technology-Madras + Chemical Engg., IIT-Madras; Computer Vision Lab, Indian Institute of Technology-Madras + I & C Engg., NIT-Tiruchirappalli; Computer Vision Lab, Indian Institute of Technology-Madras",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 746953,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11704774891838843799&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "cse.iitm.ac.in;smail.iitm.ac.in;cse.iitm.ac.in;cse.iitm.ac.in",
        "email": "cse.iitm.ac.in;smail.iitm.ac.in;cse.iitm.ac.in;cse.iitm.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+0;0+1;0",
        "aff_unique_norm": "Indian Institute of Technology Madras;National Institute of Technology, Tiruchirappalli",
        "aff_unique_dep": "Computer Vision Lab;Department of Industrial and Chemical Engineering",
        "aff_unique_url": "https://www.iitm.ac.in;https://www.nitt.edu",
        "aff_unique_abbr": "IIT-M;NIT-T",
        "aff_campus_unique_index": "0;0+0;0+1;0",
        "aff_campus_unique": "Madras;Tiruchirappalli",
        "aff_country_unique_index": "0;0+0;0+0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "GradMix: Multi-source Transfer across Domains and Tasks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Li_GradMix_Multi-source_Transfer_across_Domains_and_Tasks_WACV_2020_paper.html",
        "author": "Junnan Li;  Ziwei Xu;  Wong Yongkang;  Qi Zhao;  Mohan Kankanhalli",
        "abstract": "The computer vision community is witnessing an unprecedented rate of new tasks being proposed and addressed, thanks to the deep convolutional networks' capability to find complex mappings from X to Y. The advent of each task often accompanies the release of a large-scale annotated dataset, for supervised training of deep network. However, it is expensive and time-consuming to manually label sufficient amount of training data. Therefore, it is important to develop algorithms that can leverage off-the-shelf labeled dataset to learn useful knowledge for the target task. While previous works mostly focus on transfer learning from a single source, we study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting. We propose GradMix, a model-agnostic method applicable to any model trained with gradient-based learning rule, to transfer knowledge via gradient descent by weighting and mixing the gradients from all sources during training. GradMix follows a meta-learning objective, which assigns layer-wise weights to the source gradients, such that the combined gradient follows the direction that minimize the loss for a small set of samples from the target dataset. In addition, we propose to adaptively adjust the learning rate for each mini-batch based on its importance to the target task, and a pseudo-labeling method to leverage the unlabeled samples in the target domain. We conduct MS-DTT experiments on two tasks: digit recognition and action recognition, and demonstrate the advantageous performance of the proposed method against multiple baselines.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Li_GradMix_Multi-source_Transfer_across_Domains_and_Tasks_WACV_2020_paper.pdf",
        "aff": "School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore; Department of Computer Science and Engineering, University of Minnesota; School of Computing, National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 660229,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6400111356187024019&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "u.nus.edu;u.nus.edu;nus.edu.sg;cs.umn.edu;comp.nus.edu.sg",
        "email": "u.nus.edu;u.nus.edu;nus.edu.sg;cs.umn.edu;comp.nus.edu.sg",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "National University of Singapore;University of Minnesota",
        "aff_unique_dep": "School of Computing;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.umn.edu",
        "aff_unique_abbr": "NUS;UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "title": "Graph Networks for Multiple Object Tracking",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Li_Graph_Networks_for_Multiple_Object_Tracking_WACV_2020_paper.html",
        "author": "Jiahe Li;  Xu Gao;  Tingting Jiang",
        "abstract": "Multiple object tracking (MOT) task requires reasoning the states of all targets and associating these targets in a global way. However, existing MOT methods mostly focus on the local relationship among objects and ignore the global relationship. Some methods formulate the MOT problem as a graph optimization problem. However, these methods are based on static graphs, which are seldom updated. To solve these problems, we design a new near-online MOT method with an end-to-end graph network. Specifically, we design an appearance graph network and a motion graph network to capture the appearance and the motion similarity separately. The updating mechanism is carefully designed in our graph network, which means that nodes, edges and the global variable in the graph can be updated. The global variable can capture the global relationship to help tracking. Finally, a strategy to handle missing detections is proposed to remedy the defect of the detectors. Our method is evaluated on both the MOT16 and the MOT17 benchmarks, and experimental results show the encouraging performance of our method.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Li_Graph_Networks_for_Multiple_Object_Tracking_WACV_2020_paper.pdf",
        "aff": "NELVT, Department of Computer Science, Peking University, China; NELVT, Department of Computer Science, Peking University, China + Baidu; NELVT, Department of Computer Science, Peking University, China",
        "project": "",
        "github": "https://github.com/yinizhizhu/GNMOT",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Li_Graph_Networks_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2800352,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17237827118600633255&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Peking University;Baidu, Inc.",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.baidu.com",
        "aff_unique_abbr": "PKU;Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Graph Neural Networks for Image Understanding Based on Multiple Cues: Group Emotion Recognition and Event Recognition as Use Cases",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Guo_Graph_Neural_Networks_for_Image_Understanding_Based_on_Multiple_Cues_WACV_2020_paper.html",
        "author": "Xin Guo;  Luisa Polania;  Bin Zhu;  Charles Boncelet;  Kenneth Barner",
        "abstract": "A graph neural network (GNN) for image understanding based on multiple cues is proposed in this paper. Compared to traditional feature and decision fusion approaches that neglect the fact that features can interact and exchange information, the proposed GNN is able to pass information among features extracted from different models. Two image understanding tasks, namely group-level emotion recognition (GER) and event recognition, which are highly semantic and require the interaction of several deep models to synthesize multiple cues, were selected to validate the performance of the proposed method. It is shown through experiments that the proposed method achieves state-of-the-art performance on the selected image understanding tasks. In addition, a new group-level emotion recognition database is introduced and shared in this paper.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Guo_Graph_Neural_Networks_for_Image_Understanding_Based_on_Multiple_Cues_WACV_2020_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1795058,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13352906677341596539&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Hand-Priming in Object Localization for Assistive Egocentric Vision",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Lee_Hand-Priming_in_Object_Localization_for_Assistive_Egocentric_Vision_WACV_2020_paper.html",
        "author": "Kyungjun Lee;  Abhinav Shrivastava;  Hernisa Kacorri",
        "abstract": "Egocentric vision holds great promises for increasing access to visual information and improving the quality of life for people with visual impairments, with object recognition being one of the daily challenges for this population. While we strive to improve recognition performance, it remains difficult to identify which object is of interest to the user; the object may not even be included in the frame due to challenges in camera aiming without visual feedback. Also, gaze information, commonly used to infer the area of interest in egocentric vision, is often not dependable. However, blind users often tend to include their hand either interacting with the object that they wish to recognize or simply placing it in proximity for better camera aiming. We propose localization models that leverage the presence of the hand as the contextual information for priming the center area of the object of interest. In our approach, hand segmentation is fed to either the entire localization network or its last convolutional layers. Using egocentric datasets from sighted and blind individuals, we show that the hand-priming achieves higher precision than other approaches, such as fine-tuning, multi-class, and multi-task learning, which also encode hand-object interactions in localization.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Lee_Hand-Priming_in_Object_Localization_for_Assistive_Egocentric_Vision_WACV_2020_paper.pdf",
        "aff": "University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1090171,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2446349748201021228&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cs.umd.edu;cs.umd.edu;umd.edu",
        "email": "cs.umd.edu;cs.umd.edu;umd.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "High Accuracy Face Geometry Capture using a Smartphone Video",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Agrawal_High_Accuracy_Face_Geometry_Capture_using_a_Smartphone_Video_WACV_2020_paper.html",
        "author": "Shubham Agrawal;  Anuj Pahuja;  Simon Lucey",
        "abstract": "What's the most accurate 3D model of your face you can obtain while sitting at your desk? We attempt to answer this question in our work. High fidelity face reconstructions have so far been limited to either studio settings or through expensive 3D scanners. On the other hand, unconstrained reconstruction methods are typically limited by low-capacity models. Our method reconstructs accurate face geometry of a subject using a video shot from a smartphone in an unconstrained environment. Our approach takes advantage of recent advances in visual SLAM, keypoint detection, and object detection to improve accuracy and robustness. By not being constrained to a model subspace, our reconstructed meshes capture important details while being robust to noise and being topologically consistent. Our evaluations show that our method outperforms current single and multi-view baselines by a significant margin, both in terms of geometric accuracy and in capturing person-specific details important for making realistic looking models.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Agrawal_High_Accuracy_Face_Geometry_Capture_using_a_Smartphone_Video_WACV_2020_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Agrawal_High_Accuracy_Face_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 961638,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3601198819553289238&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "alumni.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "alumni.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "High-Frequency Refinement for Sharper Video Super-Resolution",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Singh_High-Frequency_Refinement_for_Sharper_Video_Super-Resolution_WACV_2020_paper.html",
        "author": "Vikram Singh;  Akshay Sharma;  Sudharshann Devanathan;  Anurag Mittal",
        "abstract": "A video super-resolution technique is expected to generate a `sharp' upsampled video. The sharpness in the generated video comes from the precise prediction of the high-frequency details (e.g. object edges). Thus high-frequency prediction becomes a vital sub-problem of the super-resolution task. To generate a sharp-upsampled video, this paper proposes an upsampling network architecture `HFR-Net' that works on the principle of `explicit refinement and fusion of high-frequency details'. To implement this principle and to train HFR-Net, a novel technique named 2-phase progressive-retrogressive training is being proposed. Additionally, a method called dual motion warping is also being introduced to preprocess the videos that have varying motion intensities (slow and fast). Results on multiple video datasets demonstrate the improved performance of our approach over the current state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Singh_High-Frequency_Refinement_for_Sharper_Video_Super-Resolution_WACV_2020_paper.pdf",
        "aff": "Computer Vision Lab, Indian Institute of Technology-Madras; Computer Vision Lab, Indian Institute of Technology-Madras + Carnegie Mellon University; Computer Vision Lab, Indian Institute of Technology-Madras + New York University; Computer Vision Lab, Indian Institute of Technology-Madras",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1408291,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18436546334854080247&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff_domain": "cse.iitm.ac.in;andrew.cmu.edu;nyu.edu;cse.iitm.ac.in",
        "email": "cse.iitm.ac.in;andrew.cmu.edu;nyu.edu;cse.iitm.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0+2;0",
        "aff_unique_norm": "Indian Institute of Technology Madras;Carnegie Mellon University;New York University",
        "aff_unique_dep": "Computer Vision Lab;;",
        "aff_unique_url": "https://www.iitm.ac.in;https://www.cmu.edu;https://www.nyu.edu",
        "aff_unique_abbr": "IIT-M;CMU;NYU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Madras;",
        "aff_country_unique_index": "0;0+1;0+1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "HistoNet: Predicting size histograms of object instances",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sharma_HistoNet_Predicting_size_histograms_of_object_instances_WACV_2020_paper.html",
        "author": "Kishan Sharma;  Moritz Gold;  Christian  Zurbruegg;  Laura Leal-Taixe;  Jan Dirk Wegner",
        "abstract": "We propose to predict histograms of object sizes in crowded scenes directly without any explicit object instance segmentation. What makes this task challenging is the high density of objects (of the same category), which makes instance identification hard.  Instead of explicitly segmenting object instances, we show that directly learning histograms of object sizes improves accuracy while using drastically less parameters. This is very useful for application scenarios where explicit, pixel-accurate instance segmentation is not needed, but their lies interest in the overall distribution of instance sizes. Our core applications are in biology, where we estimate the size distribution of soldier fly larvae, and medicine, where we estimate the size distribution of cancer cells as an intermediate step to calculate tumor cellularity score. Given an image with hundreds of small object instances, we output the total count and the size histogram.  We also provide a new data set for this task, the FlyLarvae data set, which consists of 11,000 larvae instances labeled pixel-wise. Our method results in an overall improvement in the count and size distribution prediction as compared to state-of-the-art instance segmentation method Mask R-CNN.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sharma_HistoNet_Predicting_size_histograms_of_object_instances_WACV_2020_paper.pdf",
        "aff": "TU Munich; ETH Zurich + EAWAG; EAWAG; TU Munich; ETH Zurich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1135164,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14012335448884929927&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tum.de;hest.ethz.ch;eawag.ch;tum.de;ethz.ch",
        "email": "tum.de;hest.ethz.ch;eawag.ch;tum.de;ethz.ch",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;2;0;1",
        "aff_unique_norm": "Technical University of Munich;ETH Zurich;EAWAG",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tum.de;https://www.ethz.ch;https://www.eawag.ch",
        "aff_unique_abbr": "TUM;ETHZ;EAWAG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+1;1;0;1",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "title": "How Much Deep Learning does Neural Style Transfer Really Need? An Ablation Study",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Du_How_Much_Deep_Learning_does_Neural_Style_Transfer_Really_Need_WACV_2020_paper.html",
        "author": "Len Du",
        "abstract": "Neural style transfer has been a \"killer app\" for deep learning, drawing attention from and advertising the effectiveness to both the academic and the general public. However, we have found by ablative experiments that optimizing an image in the way neural style transfer does, while the objective functions (or more precisely, the functions to transform raw images to corresponding feature maps being compared) are constructed without pretrained weights or biases, worked al- most as well. We can even factor out the deepness (multiple layers of alternating linear and nonlinear transformations) alltogether and have neural style transfer work to a certain extent. This raises the question how much of the the current success of deep learning in computer vision should be attributed to training, structure or simply spatially aggregating the image.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Du_How_Much_Deep_Learning_does_Neural_Style_Transfer_Really_Need_WACV_2020_paper.pdf",
        "aff": "",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Du_How_Much_Deep_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9961656299063894266&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "",
        "email": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "I-MOVE: Independent Moving Objects for Velocity Estimation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Schwan_I-MOVE_Independent_Moving_Objects_for_Velocity_Estimation_WACV_2020_paper.html",
        "author": "Jonathan Schwan;  Akshay Dhamija;  Terrance Boult",
        "abstract": "We introduce I-MOVE, the first publicly available RGB-D/stereo dataset for estimating velocities of independently moving objects. Velocity estimation given RGB-D data is an unsolved problem. The I-MOVE dataset provides an opportunity for generalizable velocity estimation models to be created and have their performance be accurately and fairly measured. The dataset features various outdoor and indoor scenes of single and multiple moving objects. Compared to other datasets, I-MOVE is unique because the RGB-D data and speed for each object are supplied for a variety of different settings/environments, objects, and motions. The dataset includes training and test sequences captured from four different depth camera views and three 4K-stereo setups. The data are also time-synchronized with three Doppler radars to provide the magnitude of velocity ground truth. The I-MOVE dataset includes complex scenes from moving pedestrians via walking and biking to multiple rolling objects, all captured with the seven cameras, providing over 500 Depth/Stereo videos.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Schwan_I-MOVE_Independent_Moving_Objects_for_Velocity_Estimation_WACV_2020_paper.pdf",
        "aff": "University of Colorado, Colorado Springs; University of Colorado, Colorado Springs; University of Colorado, Colorado Springs",
        "project": "www.vast.uccs.edu/imove",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Schwan_I-MOVE_Independent_Moving_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1030136,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16505630465534969396&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff_domain": "vast.uccs.edu;vast.uccs.edu;vast.uccs.edu",
        "email": "vast.uccs.edu;vast.uccs.edu;vast.uccs.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Colorado",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uccs.edu",
        "aff_unique_abbr": "UCO",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Colorado Springs",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ICface: Interpretable and Controllable Face Reenactment Using GANs",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Tripathy_ICface_Interpretable_and_Controllable_Face_Reenactment_Using_GANs_WACV_2020_paper.html",
        "author": "Soumya Tripathy;  Juho  Kannala;  Esa Rahtu",
        "abstract": "This paper presents a generic face animator that can control the pose and expressions of a given face image. The animation is driven by human interpretable control signals consisting of head pose angles and the Action Unit (AU) values. The control information can be obtained from multiple sources including external driving videos and manual controls. Due to the interpretable nature of the driving signal, one can easily mix the information between multiple sources (e.g. pose from one image and expression from another) and apply selective post- production editing. The proposed face animator is implemented as a two-stage neural network model that is learned in a self-supervised manner using a large video collection. The proposed Interpretable and Controllable face reenactment network (ICface) is compared to the state-of-the-art neural network-based face animation techniques in multiple tasks. The results indicate that ICface produces better visual quality while being more versatile than most of the comparison methods. The introduced model could provide a lightweight and easy to use tool for a multitude of advanced image and video editing tasks. The program code will be publicly available upon the acceptance of the paper.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Tripathy_ICface_Interpretable_and_Controllable_Face_Reenactment_Using_GANs_WACV_2020_paper.pdf",
        "aff": "Tampere University; Aalto University of Technology; Tampere University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5022750,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14038426770859114775&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "tuni.fi;aalto.fi;tuni.fi",
        "email": "tuni.fi;aalto.fi;tuni.fi",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Tampere University;Aalto University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tuni.fi;https://www.aalto.fi",
        "aff_unique_abbr": "Tuni;Aalto",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "title": "Identifying Recurring Patterns with Deep Neural Networks for Natural Image Denoising",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xia_Identifying_Recurring_Patterns_with_Deep_Neural_Networks_for_Natural_Image_WACV_2020_paper.html",
        "author": "Zhihao Xia;  Ayan Chakrabarti",
        "abstract": "Image denoising methods must effectively model, implicitly or explicitly, the vast diversity of patterns and textures that occur in natural images. This is challenging, even for modern methods that leverage deep neural networks trained to regress to clean images from noisy inputs. One recourse is to rely on \"internal\" image statistics, by searching for similar patterns within the input image itself.  In this work, we propose a new method for natural image denoising that trains a deep neural network to determine whether patches in a noisy image input share common underlying patterns. Given a pair of noisy patches, our network predicts  whether different sub-band coefficients of the original noise-free patches are similar. The denoising algorithm then aggregates matched coefficients to obtain an initial estimate of the clean image. Finally, this estimate is provided as input, along with the original noisy image, to a standard regression-based denoising network. Experiments show that our method achieves state-of-the-art color image denoising performance, including with a blind version that trains a common model for a range of noise levels, and does not require knowledge of level of noise in an input image. Our approach also has a distinct advantage when training with limited amounts of training data.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xia_Identifying_Recurring_Patterns_with_Deep_Neural_Networks_for_Natural_Image_WACV_2020_paper.pdf",
        "aff": "Washington University in St. Louis; Washington University in St. Louis",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Xia_Identifying_Recurring_Patterns_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3939422,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1892488652929983196&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "wustl.edu;wustl.edu",
        "email": "wustl.edu;wustl.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WashU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ImaGINator: Conditional Spatio-Temporal GAN for Video Generation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/WANG_ImaGINator_Conditional_Spatio-Temporal_GAN_for_Video_Generation_WACV_2020_paper.html",
        "author": "Yaohui WANG;  Piotr Bilinski;  Francois Bremond;  Antitza  Dantcheva",
        "abstract": "Generating human videos based on single images entails the challenging simultaneous generation of realistic and visual appealing appearance and motion. In this context, we propose a novel conditional GAN architecture, namely ImaGINator, which given a single image, a condition (label of a facial expression or action) and noise, decomposes appearance and motion in both latent and high level feature spaces, generating realistic videos. This is achieved by (i)a novel spatio-temporal fusion scheme, which generates dynamic motion, while retaining appearance throughout the full video sequence by transmitting appearance (originating from the single image) through all layers of the network. In addition, we propose (ii) a novel transposed (1+2)D convolution, factorizing the transposed 3D convolutional filters into separate transposed temporal and spatial components, which yields significantly gains in video quality and speed. We extensively evaluate our approach on the facial expression datasets MUG and UvA-NEMO, as well as on the action datasets NATOPS and Weizmann. We show that our approach achieves significantly better quantitative and qualitative results than the state-of-the-art. The source code and models are available under https://github.com/wyhsirius/ImaGINator.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/WANG_ImaGINator_Conditional_Spatio-Temporal_GAN_for_Video_Generation_WACV_2020_paper.pdf",
        "aff": "Inria+Universit \u00b4e C\u02c6ote d\u2019Azur; University of Warsaw; Inria+Universit \u00b4e C\u02c6ote d\u2019Azur; Inria+Universit \u00b4e C\u02c6ote d\u2019Azur",
        "project": "",
        "github": "https://github.com/wyhsirius/ImaGINator",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/WANG_ImaGINator_Conditional_Spatio-Temporal_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2521859,
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3259097797777756191&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "inria.fr;mimuw.edu.pl;inria.fr;inria.fr",
        "email": "inria.fr;mimuw.edu.pl;inria.fr;inria.fr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+1;0+1",
        "aff_unique_norm": "Inria;Universit\u00e9 C\u00f4te d\u2019Azur;University of Warsaw",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.inria.fr;https://www.univ-cotedazur.fr;https://www.uw.edu.pl",
        "aff_unique_abbr": "Inria;UCA;UW",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;0+0;0+0",
        "aff_country_unique": "France;Poland"
    },
    {
        "title": "Image Difficulty Curriculum for Generative Adversarial Networks (CuGAN)",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Soviany_Image_Difficulty_Curriculum_for_Generative_Adversarial_Networks_CuGAN_WACV_2020_paper.html",
        "author": "Petru Soviany;  Claudiu Ardei;  Radu Tudor Ionescu;  Marius Leordeanu",
        "abstract": "Despite the significant advances in recent years, Generative Adversarial Networks (GANs) are still notoriously hard to train. In this paper, we propose three novel curriculum learning strategies for training GANs. All strategies are first based on ranking the training images by their difficulty scores, which are estimated by a state-of-the-art image difficulty predictor. Our first strategy is to divide images into gradually more difficult batches. Our second strategy introduces a novel curriculum loss function for the discriminator that takes into account the difficulty scores of the real images. Our third strategy is based on sampling from an evolving distribution, which favors the easier images during the initial training stages and gradually converges to a uniform distribution, in which samples are equally likely, regardless of difficulty. We compare our curriculum learning strategies with the classic training procedure on two tasks: image generation and image translation. Our experiments indicate that all strategies provide faster convergence and superior results. For example, our best curriculum learning strategy applied on spectrally normalized GANs (SNGANs) fooled human annotators in thinking that generated CIFAR-like images are real in 25.0% of the presented cases, while the SNGANs trained using the classic procedure fooled the annotators in only 18.4% cases. Similarly, in image translation, the human annotators preferred the images produced by the Cycle-consistent GAN (CycleGAN) trained using curriculum learning in 40.5% cases and those produced by CycleGAN based on classic training in only 19.8% cases, 39.7% cases being labeled as ties.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Soviany_Image_Difficulty_Curriculum_for_Generative_Adversarial_Networks_CuGAN_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1785801,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6868373509176545987&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Image Hashing via Linear Discriminant Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hong_Image_Hashing_via_Linear_Discriminant_Learning_WACV_2020_paper.html",
        "author": "Weixiang Hong;  Yu-Ting Chang;  Haifang Qin;  Wei-Chih Hung;  Yi-Hsuan Tsai;  Ming-Hsuan Yang",
        "abstract": "Hashing has attracted attention in recent years due to the rapid growth of image and video data on the web. Benefiting from recent advances in deep learning, deep supervised hashing has achieved promising results for image retrieval. However, existing methods are either less efficient in data usage or incapable of learning linearly discriminative binary codes. In this paper, we revisit linear discriminative analysis and propose a linear discriminative hashing (LDH) objective that is efficient in training and achieves better accuracy in retrieval. With the joint supervision of a classification loss, we design a robust deep network to obtain binary codes that are inter-class separable and intra-class compact, which provides better representations for image retrieval. We conduct extensive experiments on three benchmark datasets, and our LDH algorithm performs favorably against existing state-of-the-art deep supervised hashing methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hong_Image_Hashing_via_Linear_Discriminant_Learning_WACV_2020_paper.pdf",
        "aff": "National University of Singapore; UC Merced; Peking University; UC Merced; NEC Labs America; UC Merced/Google",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Hong_Image_Hashing_via_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4558302,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3840664995999449772&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "outlook.com;ucmerced.edu;pku.edu.cn;ucmerced.edu;gmail.com;ucmerced.edu",
        "email": "outlook.com;ucmerced.edu;pku.edu.cn;ucmerced.edu;gmail.com;ucmerced.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1;3;1",
        "aff_unique_norm": "National University of Singapore;University of California, Merced;Peking University;NEC Labs America",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.ucmerced.edu;http://www.pku.edu.cn;https://www.nec-labs.com",
        "aff_unique_abbr": "NUS;UCM;Peking U;NEC LA",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Merced",
        "aff_country_unique_index": "0;1;2;1;1;1",
        "aff_country_unique": "Singapore;United States;China"
    },
    {
        "title": "Image denoising via K-SVD with primal-dual active set algorithm",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xiao_Image_denoising_via_K-SVD_with_primal-dual_active_set_algorithm_WACV_2020_paper.html",
        "author": "Quan Xiao;  Canhong Wen;  Zirui Yan",
        "abstract": "K-SVD algorithm has been successfully applied to image denoising tasks dozens of years but the big bottleneck in speed and accuracy still needs attention to break. For the sparse coding stage in K-SVD, which involves l0 constraint, prevailing methods usually seek approximate solutions greedily but are less effective once the noise level is high. The alternative l1 optimization is proved to be powerful than l0, however, the time consumption prevents it from the implementation. In this paper, we propose a new K-SVD framework called K-SVDp by applying the Primal-dual active set (PDAS) algorithm to it. Different from the greedy algorithms based K-SVD, the K-SVDp algorithm develops a selection strategy motivated by KKT (Karush-Kuhn-Tucker) condition and yields to an efficient update in the sparse coding stage. Since the K-SVDp algorithm seeks for an equivalent solution to the dual problem iteratively with simple explicit expression in this denoising problem, speed and quality of denoising can be reached simultaneously. Experiments are carried out and demonstrate the comparable denoising performance of our K-SVDp with state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xiao_Image_denoising_via_K-SVD_with_primal-dual_active_set_algorithm_WACV_2020_paper.pdf",
        "aff": "Department of Statistics and Finance; Department of Statistics and Finance; Department of Statistics and Finance",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4753726,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15816443029595208477&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mail.ustc.edu.cn;ustc.edu.cn;mail.ustc.edu.cn",
        "email": "mail.ustc.edu.cn;ustc.edu.cn;mail.ustc.edu.cn",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Economics and Finance",
        "aff_unique_dep": "Department of Statistics and Finance",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Image identification of Protea species with attributes and subgenus scaling",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Thompson_Image_identification_of_Protea_species_with_attributes_and_subgenus_scaling_WACV_2020_paper.html",
        "author": "Peter Thompson;  Willie Brink",
        "abstract": "The flowering plant genus Protea is a dominant representative for the biodiversity of the Cape Floristic Region in South Africa, and from a conservation point of view important to monitor. The recent surge in popularity of crowd-sourced wildlife monitoring platforms presents challenges and opportunities for automatic image based species identification. We consider the problem of identifying the Protea species in a given image with additional (but optional) attributes linked to the observation, such as location and date. We collect training and test data from a crowd-sourced platform, and find that the Protea identification problem is exacerbated by considerable inter-class similarity, data scarcity, class imbalance, as well as large variations in image quality, composition and background. Our proposed solution consists of three parts. The first part incorporates a variant of multi-region attention into a pretrained convolutional neural network, to focus on the flowerhead in the image. The second part performs coarser-grained classification on subgenera (superclasses) and then rescales the output of the first part. The third part conditions a probabilistic model on the additional attributes associated with the observation. We perform an ablation study on the proposed model and its constituents, and find that all three components together outperform our baselines and all other variants quite significantly.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Thompson_Image_identification_of_Protea_species_with_attributes_and_subgenus_scaling_WACV_2020_paper.pdf",
        "aff": "Stellenbosch University; Stellenbosch University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3774317,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3887853884304791848&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "hoekwil.com;sun.ac.za",
        "email": "hoekwil.com;sun.ac.za",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stellenbosch University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sun.ac.za",
        "aff_unique_abbr": "SU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Africa"
    },
    {
        "title": "Image to Video Domain Adaptation Using Web Supervision",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kae_Image_to_Video_Domain_Adaptation_Using_Web_Supervision_WACV_2020_paper.html",
        "author": "Andrew Kae;  Yale Song",
        "abstract": "Training deep neural networks typically requires large amounts of labeled data which may be scarce or expensive to obtain for a particular target domain. As an alternative, we can leverage webly-supervised data (i.e. results from a public search engine) which are relatively plentiful but may contain noisy results. In this work, we propose a novel two-stage approach to learn a video classifier using webly-supervised data. We argue that learning appearance features and temporal features sequentially, rather than jointly, is an easier optimization for this task. We show this by first learning an image model from web images, which is used to initialize and train a video model. Our model applies domain adaptation to account for potential domain shift present between the source domain (webly-supervised data) and target domain, and also accounts for noise by adding a novel attention component. We report results competitive with state-of-the-art for webly-supervised approaches (while simplifying the training process) on UCF-101 and also evaluate on Kinetics for comparison.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kae_Image_to_Video_Domain_Adaptation_Using_Web_Supervision_WACV_2020_paper.pdf",
        "aff": "Yahoo Research; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3997227,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=307051328120490218&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "verizonmedia.com;microsoft.com",
        "email": "verizonmedia.com;microsoft.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Yahoo;Microsoft Corporation",
        "aff_unique_dep": "Yahoo Research;Microsoft Research",
        "aff_unique_url": "https://research.yahoo.com;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Yahoo Research;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Improved Embeddings with Easy Positive Triplet Mining",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xuan_Improved_Embeddings_with_Easy_Positive_Triplet_Mining_WACV_2020_paper.html",
        "author": "Hong Xuan;  Abby Stylianou;  Robert Pless",
        "abstract": "Deep metric learning seeks to define an embedding where semantically similar images are embedded to nearby locations, and semantically dissimilar images are embedded to distant locations. Substantial work has focused on loss functions and strategies to learn these embeddings by pushing images from the same class as close together in the embedding space as possible. In this paper, we propose an alternative, loosened embedding strategy that requires the embedding function only map each training image to the most similar examples from the same class, an approach we call \"Easy Positive\" mining. We provide a collection of experiments and visualizations that highlight that this Easy Positive mining leads to embeddings that are more flexible and generalize better to new unseen data. This simple mining strategy yields recall performance that exceeds state of the art approaches (including those with complicated loss functions and ensemble methods) on image retrieval datasets including CUB, Stanford Online Products, In-Shop Clothes and Hotels-50K.ositive mining leads to embeddings that are more flexibly and generalize better to new data.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xuan_Improved_Embeddings_with_Easy_Positive_Triplet_Mining_WACV_2020_paper.pdf",
        "aff": "George Washington University; George Washington University; George Washington University",
        "project": "",
        "github": "https://github.com/littleredxh/EasyPositiveHardNegative",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6374980,
        "gs_citation": 188,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14975900966940097581&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gwu.edu;gwu.edu;gwu.edu",
        "email": "gwu.edu;gwu.edu;gwu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "George Washington University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gwu.edu",
        "aff_unique_abbr": "GWU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Improving Object Detection with Inverted Attention",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Huang_Improving_Object_Detection_with_Inverted_Attention_WACV_2020_paper.html",
        "author": "zeyi huang;  Wei Ke;  Dong Huang",
        "abstract": "Improving object detectors against occlusion, blur and noise is a critical step to deploy detectors in real applications. Since it is not possible to exhaust all image defects and occlusions through data collection, many researchers seek to generate occluded samples. The generated hard samples are either images or feature maps with coarse patches dropped out in the spatial dimensions. Significant overheads are required in generating hard samples and/or estimating drop-out patches using extra network branches. In this paper, we improve object detectors using a highly efficient and fine-grain mechanism called Inverted Attention (IA). Different from the original detector network that only focuses on the dominant part of objects, the detector network with IA iteratively inverts attention on feature maps which push the detector to discover new discriminative clues and puts more attention on complementary object parts, feature channels and even context. Our approach (1) operates along both the spatial and channels dimensions of the feature maps; (2) requires no extra training on hard samples, no extra network parameters for attention estimation, and no testing overheads. Experiments show that our approach consistently improved state-of-the-art detectors on benchmark databases.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Huang_Improving_Object_Detection_with_Inverted_Attention_WACV_2020_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1590143,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5912306395437813846&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;cmu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Improving Style Transfer with Calibrated Metrics",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Yeh_Improving_Style_Transfer_with_Calibrated_Metrics_WACV_2020_paper.html",
        "author": "Mao-Chuang Yeh;  Shuai Tang;  Anand Bhattad;  Chuhang Zou;  David Forsyth",
        "abstract": "Style transfer methods produce a transferred image which is a rendering of a content image in the manner of a style image. We seek to understand how to improve style transfer. To do so requires quantitative evaluation procedures, but current evaluation is qualitative, mostly involving user studies. We describe a novel quantitative evaluation procedure. Our procedure relies on two statistics: the Effectiveness (E) statistic measures the extent that a given style has been transferred to the target, and the Coherence (C) statistic measures the extent to which the original image's content is preserved. Our statistics are calibrated to human preference: targets with larger values of E (resp C) will reliably be preferred by human subjects in comparisons of style (resp. content). We use these statistics to investigate relative performance of a number of style transfer methods, revealing a number of intriguing properties. Admissible methods lie on a Pareto frontier (i.e. improving E reduces C, or vice versa). Three methods are admissible: Universal style transfer produces very good C but weak E; modifying the optimization used for Gatys' loss produces a method with strong E and strong C; and a modified cross-layer method has slightly better E at strong cost in C.  While the histogram loss improves the E statistics of Gatys' method, it does not make the method admissible.  Surprisingly, style weights have relatively little effect in improving EC scores, and most variability in transfer is explained by the style itself (meaning experimenters can be misguided by selecting styles).",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Yeh_Improving_Style_Transfer_with_Calibrated_Metrics_WACV_2020_paper.pdf",
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "https://github.com/stringtron/quantative style",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Yeh_Improving_Style_Transfer_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3320684,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5305826469789231813&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Inferring Super-Resolution Depth from a Moving Light-Source Enhanced RGB-D Sensor: A Variational Approach",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sang_Inferring_Super-Resolution_Depth_from_a_Moving_Light-Source_Enhanced_RGB-D_Sensor_WACV_2020_paper.html",
        "author": "Lu Sang;  Bjoern Haefner;  Daniel Cremers",
        "abstract": "A novel approach towards depth map super-resolution using multi-view uncalibrated photometric stereo is presented. Practically, an LED light source is attached to a commodity RGB-D sensor and is used to capture objects from multiple viewpoints with unknown motion. This non-static camera-to-object setup is described with a nonconvex variational approach such that no calibration on lighting or camera motion is require due to the formulation of an end-to-end joint optimization problem. Solving the proposed variational model results in high resolution depth, reflectance and camera estimates, as we show on challenging synthetic and real-world datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sang_Inferring_Super-Resolution_Depth_from_a_Moving_Light-Source_Enhanced_RGB-D_Sensor_WACV_2020_paper.pdf",
        "aff": "Technical University of Munich, Germany; Technical University of Munich, Germany; Technical University of Munich, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2476062,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5131231504963470989&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "tum.de;tum.de;tum.de",
        "email": "tum.de;tum.de;tum.de",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technical University of Munich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tum.de",
        "aff_unique_abbr": "TUM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Instance Segmentation for the Quantification of Microplastic Fiber Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wegmayr_Instance_Segmentation_for_the_Quantification_of_Microplastic_Fiber_Images_WACV_2020_paper.html",
        "author": "Viktor Wegmayr;  Aytunc Sahin;  Bjorn Saemundsson;  Joachim  Buhmann",
        "abstract": "Microplastics pollution has been recognized as a serious environmental concern, with serious research efforts underway to determine primary causes. Experiments typically generate bright-field images of microplastic fibers that are filtered from water. Environmental decision making in process engineering critically relies on accurate quantification of microplastic fibers in these images. To satisfy the required standards, images are often analyzed manually, resulting in a highly tedious process, with thousands of fiber instances per image. While the shape of individual fibers is relatively simple, it is difficult to separate them in highly crowded scenes with significant overlap. We propose a fiber instance detection pipeline, which decomposes the fiber detection and segmentation into manageable subproblems. Well separated instances are identified with robust image processing techniques, such as adaptive thresholding, and morphological skeleton analysis, while tangled fibers are separated by an algorithm based on deep pixel embeddings. Moreover, we present a modified Intersection-over- Union metric as a more appropriate similarity metric for elongated shapes. Our approach improves significantly on out-of-sample data, in particular for difficult cases of intersecting fibers.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wegmayr_Instance_Segmentation_for_the_Quantification_of_Microplastic_Fiber_Images_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1668675,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16038384535422620209&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;student.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;student.ethz.ch;inf.ethz.ch",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Instance Segmentation of Benthic Scale Worms at a Hydrothermal Site",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Shashidhara_Instance_Segmentation_of_Benthic_Scale_Worms_at_a_Hydrothermal_Site_WACV_2020_paper.html",
        "author": "Bhuvan Malladihalli Shashidhara;  Mitchell Scott;  Aaron Marburg",
        "abstract": "Subsea hydrothermal vents, typically existing at water depths below natural light penetration, contain diverse and unique macrofaunal environments. Traditionally, long-term ecological observation has been difficult as the extreme depth, temperature and pressure make in situ video surveys challenging. However, the introduction of subsea cabled arrays has allowed for the long time series collection of high definition imagery from these vents. To study the benthic hydrothermal vent environment, we propose an inference pipeline consisting of a U-Net followed by VGG-16 CNN to perform instance segmentation of scale worms, a specific macrofaunal family. The developed pipeline exhibits an average precision (AP) of 0.671 AP@[0.5], despite the difficult camouflaged imagery and low training data inputs. We further explore full pipeline training data requirements, as the dynamic scene in question requires the pipeline to be re-trained on an approximately monthly basis for effective segmentation. We find that the VGG-16 CNN portion of the pipeline is typically more sensitive to training data variation than the U-Net portion.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Shashidhara_Instance_Segmentation_of_Benthic_Scale_Worms_at_a_Hydrothermal_Site_WACV_2020_paper.pdf",
        "aff": "University of Washington Applied Physics Laboratory; University of Washington Applied Physics Laboratory; University of Washington Applied Physics Laboratory",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4225307,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15684168099270961457&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "uw.edu;uw.edu;uw.edu",
        "email": "uw.edu;uw.edu;uw.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Applied Physics Laboratory",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Intelligent Image Collection: Building the Optimal Dataset",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Gwilliam_Intelligent_Image_Collection_Building_the_Optimal_Dataset_WACV_2020_paper.html",
        "author": "Matthew Gwilliam;  Ryan Farrell",
        "abstract": "Key recognition tasks such as fine-grained visual categorization (FGVC) have benefited from increasing attention among computer vision researchers. The development and evaluation of new approaches relies heavily on benchmark datasets; such datasets are generally built primarily with categories that have images readily available, omitting categories with insufficient data. This paper takes a step back and rethinks dataset construction, focusing on intelligent image collection driven by: (i) the inclusion of all desired categories, and, (ii) the recognition performance on those categories. Based on a small, author-provided initial dataset, the proposed system recommends which categories the authors should prioritize collecting additional images for, with the intent of optimizing overall categorization accuracy. We show that mock datasets built using this method outperform datasets built without such a guiding framework. Additional experiments give prospective dataset creators intuition into how, based on their circumstances and goals, a dataset should be constructed.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Gwilliam_Intelligent_Image_Collection_Building_the_Optimal_Dataset_WACV_2020_paper.pdf",
        "aff": "Brigham Young University; Brigham Young University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1540650,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16143902305160051262&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;cs.byu.edu",
        "email": "gmail.com;cs.byu.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Brigham Young University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.byu.edu",
        "aff_unique_abbr": "BYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Internet of Things (IoT) Discovery Using Deep Neural Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Lo_Internet_of_Things_IoT_Discovery_Using_Deep_Neural_Networks_WACV_2020_paper.html",
        "author": "Ephraim Lo;  JoHannah Kohl",
        "abstract": "We present a novel approach to Internet of Things (IoT) discovery using Deep Neural Network (DNN) based object detection. Traditional methods of IoT discovery are based on either manual or automated monitoring of predetermined channel frequencies. Our method takes the spectrogram images that a human analyst visually scans for manual spectrum exploration and applies the state-of-the-art You Only Look Once (YOLO) object detection algorithm to detect and localize signal objects in time and frequency. We focus specifically on the class of signals that employ the Long Range (LoRa) modulation scheme, which uses chirp spread spectrum technology to provide high network efficiency and robustness against both in- and out-of-band interference. Our detection system is designed with scalability for real or near real-time processing capabilities and achieves 81.82% mAP in real-time on a fourth generation mobile Intel CPU without GPU support. Lastly, we present preliminary detection results for other IoT signals including Zigbee, Bluetooth, and Wi-Fi.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Lo_Internet_of_Things_IoT_Discovery_Using_Deep_Neural_Networks_WACV_2020_paper.pdf",
        "aff": "North Point Defense; North Point Defense",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1690492,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3560925338666988127&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "northpointdefense.com;northpointdefense.com",
        "email": "northpointdefense.com;northpointdefense.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "North Point Defense",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Inverse Rectification for Efficient Procam Pattern Correspondence",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Qiu_Inverse_Rectification_for_Efficient_Procam_Pattern_Correspondence_WACV_2020_paper.html",
        "author": "Yubo Qiu;  Jonathon Malcolm;  Sheikh Ziauddin;  Michael Greenspan;  Abhay Vatoo",
        "abstract": "A method called inverse rectification, is proposed which facilitates the establishment of correspondences across a projected pattern and an acquired image. A pattern of features comprising vertical dashes is warped by the inverse of the rectifying homography of the projector-camera pair, prior to projection. This warping imparts upon the system the property that projected features will fall on distinct conjugate epipolar lines of the rectified projector and acquired camera images. This reduces the correspondence search to a trivial constant-time table lookup once a feature is found in the camera image, and leads to robust, accurate, and extremely efficient disparity calculations. A projector-camera range sensor is developed based on this method, and is shown experimentally to be effective, with bandwidth exceeding some existing consumer-level range sensors.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Qiu_Inverse_Rectification_for_Efficient_Procam_Pattern_Correspondence_WACV_2020_paper.pdf",
        "aff": "1Department of Electrical and Computer Engineering + 2Ingenuity Labs + 3School of Computing; 1Department of Electrical and Computer Engineering + 2Ingenuity Labs + 3School of Computing; 1Department of Electrical and Computer Engineering; 1Department of Electrical and Computer Engineering + 2Ingenuity Labs; 1Department of Electrical and Computer Engineering + 2Ingenuity Labs + 3School of Computing",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2435297,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11851286151086610871&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff_domain": "queensu.ca;queensu.ca;queensu.ca;queensu.ca;queensu.ca",
        "email": "queensu.ca;queensu.ca;queensu.ca;queensu.ca;queensu.ca",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;0+1+2;0;0+1;0+1+2",
        "aff_unique_norm": "Department of Electrical and Computer Engineering;Ingenuity Labs;School of Computing",
        "aff_unique_dep": "Electrical and Computer Engineering;;School of Computing",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";;;",
        "aff_country_unique": ""
    },
    {
        "title": "Is Pruning Compression?: Investigating Pruning Via Network Layer Similarity",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Blakeney_Is_Pruning_Compression_Investigating_Pruning_Via_Network_Layer_Similarity_WACV_2020_paper.html",
        "author": "Cody Blakeney;  Yan Yan;  Ziliang Zong",
        "abstract": "Unstructured neural network pruning is an effective technique that can significantly reduce theoretical model size, computation demand and energy consumption of large neural networks without compromising accuracy. However, a number of fundamental questions about pruning are not answered yet. For example, do the pruned neural networks contain the same representations as the original network? Is pruning a compression or evolution process? Does pruning only work on trained neural networks? What is the role and value of the uncovered sparsity structure? In this paper, we strive to answer these questions by analyzing three unstructured pruning methods (magnitude based pruning, post-pruning re-initialization, and random sparse initialization). We conduct extensive experiments using the Singular Vector Canonical Correlation Analysis (SVCCA) tool to study and contrast layer representations of pruned and original ResNet, VGG, and ConvNet models. We have several interesting observations: 1) Pruned neural network models evolve to substantially different representations while still maintaining similar accuracy. 2) Initialized sparse models can achieve reasonably good accuracy compared to well-engineered pruning methods. 3) Sparsity structures discovered by pruning models are not inherently important or useful.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Blakeney_Is_Pruning_Compression_Investigating_Pruning_Via_Network_Layer_Similarity_WACV_2020_paper.pdf",
        "aff": "Texas State University; Texas State University; Texas State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 797465,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11937494229014194570&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "txtstate.edu; ; ",
        "email": "txtstate.edu; ; ",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Texas State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.txstate.edu",
        "aff_unique_abbr": "TXST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "It's All About The Scale - Efficient Text Detection Using Adaptive Scaling",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Richardson_Its_All_About_The_Scale_-_Efficient_Text_Detection_Using_WACV_2020_paper.html",
        "author": "Elad Richardson;  Yaniv Azar;  Or Avioz;  Niv Geron;  Tomer Ronen;  Zach Avraham;  Stav Shapiro",
        "abstract": "\"Text can appear anywhere\". This property requires us to carefully process all the pixels in an image in order to accurately localize all text instances. In particular, for the more difficult task of localizing small text regions, many methods use an enlarged image or even several rescaled ones as their input. This significantly increases the processing time of the entire image and needlessly enlarges background regions. If we were to have a prior telling us the coarse location of text instances in the image and their approximate scale, we could have adaptively chosen which regions to process and how to rescale them, thus significantly reducing the processing time. To estimate this prior we propose a segmentation-based network with an additional \"scale predictor\", an output channel that predicts the scale of each text segment. The network is applied on a scaled down image to efficiently approximate the desired prior, without processing all the pixels of the original image. The approximated prior is then used to create a compact image containing only text regions, resized to a canonical scale, which is fed again to the segmentation network for fine-grained detection. We show that our approach offers a powerful alternative to fixed scaling schemes, achieving an equivalent accuracy to larger input scales while processing far fewer pixels. Qualitative and quantitative results are presented on the ICDAR15 and ICDAR17 MLT benchmarks to validate our approach.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Richardson_Its_All_About_The_Scale_-_Efficient_Text_Detection_Using_WACV_2020_paper.pdf",
        "aff": "Penta-AI; Penta-AI; Penta-AI; Penta-AI; Penta-AI; Penta-AI; Penta-AI",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6462571,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4965718121676729529&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com; ; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ; ",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Penta-AI",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Li_IterNet_Retinal_Image_Segmentation_Utilizing_Structural_Redundancy_in_Vessel_Networks_WACV_2020_paper.html",
        "author": "Liangzhi Li;  Manisha Verma;  Yuta Nakashima;  Hajime Nagahara;  Ryo Kawasaki",
        "abstract": "Retinal vessel segmentation is of great interest for diagnosis of retinal vascular diseases. To further improve the performance of vessel segmentation, we propose IterNet, a new model based on UNet, with the ability to find obscured details of the vessel from the segmented vessel image itself, rather than the raw input image. IterNet consists of multiple iterations of a mini-UNet, which can be 4X deeper than the common UNet. IterNet also adopts the weight-sharing and skip-connection features to facilitate training; therefore, even with such a large architecture, IterNet can still learn from merely 10 20 labeled images, without pre-training or any prior knowledge. IterNet achieves AUCs of 0.9816, 0.9851, and 0.9881 on three mainstream datasets, namely DRIVE, CHASE-DB1, and STARE, respectively, which currently are the best scores in the literature. The source code is available.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Li_IterNet_Retinal_Image_Segmentation_Utilizing_Structural_Redundancy_in_Vessel_Networks_WACV_2020_paper.pdf",
        "aff": "Osaka University; Osaka University; Osaka University; Osaka University; Osaka University",
        "project": "",
        "github": "https://github.com/conscienceli/IterNet",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Li_IterNet_Retinal_Image_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2749547,
        "gs_citation": 398,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11655862582801488233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ids.osaka-u.ac.jp;ids.osaka-u.ac.jp;ids.osaka-u.ac.jp;ids.osaka-u.ac.jp;ophthal.med.osaka-u.ac.jp",
        "email": "ids.osaka-u.ac.jp;ids.osaka-u.ac.jp;ids.osaka-u.ac.jp;ids.osaka-u.ac.jp;ophthal.med.osaka-u.ac.jp",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Osaka University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "Osaka U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Iterative and Adaptive Sampling with Spatial Attention for Black-Box Model Explanations",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Vasu_Iterative_and_Adaptive_Sampling_with_Spatial_Attention_for_Black-Box_Model_WACV_2020_paper.html",
        "author": "Bhavan Vasu;  Chengjiang  Long",
        "abstract": "Deep neural networks have achieved great success in many real-world applications, yet it remains unclear and difficult to explain their decision-making process to an end user. In this paper, we address the explainable AI problem for deep neural networks with our proposed framework, named IASSA that generates an importance map indicating how salient each pixel is for the model's prediction with an iterative and adaptive sampling module. We employ an affinity matrix calculated on multi-level deep learning features to explore long-range pixel-to-pixel correlation, which can shift the saliency values guided by our long-range and parameter-free spatial attention. Extensive experiments on the MS-COCO dataset show that our proposed approach matches or exceeds the performance of state-of-the-art black-box explanation methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Vasu_Iterative_and_Adaptive_Sampling_with_Spatial_Attention_for_Black-Box_Model_WACV_2020_paper.pdf",
        "aff": "Kitware Inc., Clifton Park, NY, USA 12065; Kitware Inc., Clifton Park, NY, USA 12065",
        "project": "",
        "github": "https://github.com/vbhavank/IASSA-Saliency",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1071878,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16563727498543855716&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "kitware.com;kitware.com",
        "email": "kitware.com;kitware.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Kitware Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kitware.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Jointly Trained Image and Video Generation using Residual Vectors",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Dandi_Jointly_Trained_Image_and_Video_Generation_using_Residual_Vectors_WACV_2020_paper.html",
        "author": "Yatin Dandi;  Aniket Das;  Soumye Singhal;  Vinay Namboodiri;  Piyush Rai",
        "abstract": "In this work, we propose a modeling technique for jointly training image and video generation models by simultaneously learning to map latent variables with a fixed prior onto real images and interpolate over images to generate videos. The proposed approach models the variations in representations using residual vectors encoding the change at each time step over a summary vector for the entire video. We utilize the technique to jointly train an image generation model with a fixed prior along with a video generation model lacking constraints such as disentanglement. The joint training enables the image generator to exploit temporal information while the video generation model learns to flexibly share information across frames. Moreover, experimental results verify our approach's compatibility with pre-training on videos or images and training on datasets containing a mixture of both. A comprehensive set of quantitative and qualitative evaluations reveal the improvements in sample quality and diversity over both video generation and image generation baselines.  We further demonstrate the technique's capabilities of exploiting similarity in features across frames by applying it to a model based on decomposing the video into motion and content. The proposed model allows minor variations in content across frames while maintaining the temporal dependence through latent vectors encoding the pose or motion features.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Dandi_Jointly_Trained_Image_and_Video_Generation_using_Residual_Vectors_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Technology, Kanpur; Indian Institute of Technology, Kanpur; Indian Institute of Technology, Kanpur; Indian Institute of Technology, Kanpur; Indian Institute of Technology, Kanpur",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 852661,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7804085596748991913&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "iitk.ac.in;iitk.ac.in;gmail.com;cse.iitk.ac.in;cse.iitk.ac.in",
        "email": "iitk.ac.in;iitk.ac.in;gmail.com;cse.iitk.ac.in;cse.iitk.ac.in",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Kanpur",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Kanpur",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Kornia: an Open Source Differentiable Computer Vision Library for PyTorch",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Riba_Kornia_an_Open_Source_Differentiable_Computer_Vision_Library_for_PyTorch_WACV_2020_paper.html",
        "author": "Edgar Riba;  Dmytro Mishkin;  Daniel Ponsa;  Ethan Rublee;  Gary Bradski",
        "abstract": "This work presents Kornia -- an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. At its core, the package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are also provided including a benchmark comparing to  existing vision libraries.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Riba_Kornia_an_Open_Source_Differentiable_Computer_Vision_Library_for_PyTorch_WACV_2020_paper.pdf",
        "aff": "Computer Vision Center + OSVF-OpenCV .org; VRG, Faculty of Electrical Engineering, Czech Technical University in Prague; Computer Vision Center; Arraiy, Inc.; OSVF-OpenCV .org",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2722123,
        "gs_citation": 437,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1987217708688023508&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "gmail.com;gmail.com;cvc.uab.es;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;cvc.uab.es;gmail.com;gmail.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0;3;1",
        "aff_unique_norm": "Computer Vision Center;Open Source Virtual Security Force;Czech Technical University in Prague;Arraiy, Inc.",
        "aff_unique_dep": ";;Faculty of Electrical Engineering;",
        "aff_unique_url": "https://www.cvc.uab.cat/;https://osvf.org;https://www.fel.cvut.cz;",
        "aff_unique_abbr": "CVC;OSVF;CTU;",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Prague",
        "aff_country_unique_index": "0;2;0;3",
        "aff_country_unique": "Spain;;Czech Republic;United States"
    },
    {
        "title": "L*ReLU: Piece-wise Linear Activation Functions for Deep Fine-grained Visual Categorization",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Basirat_LReLU_Piece-wise_Linear_Activation_Functions_for_Deep_Fine-grained_Visual_Categorization_WACV_2020_paper.html",
        "author": "Mina Basirat;  PETER ROTH",
        "abstract": "Deep neural networks paved the way for significant improvements in image visual categorization during the last years. However, even though the tasks are highly varying, differing in complexity and difficulty, existing solutions mostly build on the same architectural decisions. This also applies to the selection of activation functions (AFs), where most approaches build on Rectified Linear Units (ReLUs). In this paper, however, we show that the choice of a proper AF has a significant impact on the classification accuracy, in particular, if fine, subtle details are of relevance. Therefore, we propose to model the absence and the presence of features via the AF by using piece-wise AFs, which we refer to as L*ReLU. In this way, we can ensure the required properties, while still inheriting the benefits in terms of computational efficiency. We demonstrate our approach for the tasks of Fine-grained Visual Categorization (FGVC), running experiments on seven different benchmark datasets. The results do not only demonstrate superior results but also that for different tasks, having different characteristics, different AFs are selected.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Basirat_LReLU_Piece-wise_Linear_Activation_Functions_for_Deep_Fine-grained_Visual_Categorization_WACV_2020_paper.pdf",
        "aff": "Institute of Computer Graphics and Vision, Graz University of Technology; Institute of Computer Graphics and Vision, Graz University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1037299,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9070551002107376947&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "Institute of Computer Graphics and Vision",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TU Graz",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "LEAF-QA: Locate, Encode & Attend for Figure Question Answering",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chaudhry_LEAF-QA_Locate_Encode__Attend_for_Figure_Question_Answering_WACV_2020_paper.html",
        "author": "Ritwick Chaudhry;  Sumit Shekhar;  Utkarsh Gupta;  Pranav Maneriker;  Prann Bansal;  Ajay Joshi",
        "abstract": "We introduce LEAF-QA, a comprehensive dataset of 250,000 densely annotated  figures/charts, constructed from real-world open data sources, along with 2 million question-answer (QA) pairs querying the structure and semantics of these charts. LEAF-QA highlights the problem of multimodal QA, which is notably different from conventional visual QA (VQA), and has recently gained interest in the community. Furthermore, LEAF-QA is significantly more complex than previous attempts at chart QA, viz. FigureQA and DVQA, which present only limited variations in chart data. LEAF-QA being constructed from real-world sources, requires a novel architecture to enable question answering. To this end, LEAF-Net, a deep architecture involving chart element localization, question and answer encoding in terms of chart elements, and an attention network is proposed. Different experiments are conducted to demonstrate the challenges of QA on LEAF-QA. The proposed architecture, LEAF-Net also considerably advances the current state-of-the-art on FigureQA and DVQA.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chaudhry_LEAF-QA_Locate_Encode__Attend_for_Figure_Question_Answering_WACV_2020_paper.pdf",
        "aff": "Carnegie Mellon University; Adobe Research; IIT Roorkee; Ohio State University; IIT Kanpur; IIT Madras",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1548028,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15156225480547344508&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "andrew.cmu.edu;adobe.com;ec.iitr.ac.in;osu.edu;iitk.ac.in;smail.iitm.ac.in",
        "email": "andrew.cmu.edu;adobe.com;ec.iitr.ac.in;osu.edu;iitk.ac.in;smail.iitm.ac.in",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4;5",
        "aff_unique_norm": "Carnegie Mellon University;Adobe;Indian Institute of Technology Roorkee;Ohio State University;Indian Institute of Technology Kanpur;Indian Institute of Technology Madras",
        "aff_unique_dep": ";Adobe Research;;;;",
        "aff_unique_url": "https://www.cmu.edu;https://research.adobe.com;https://www.iitr.ac.in;https://www.osu.edu;https://www.iitk.ac.in;https://www.iitm.ac.in",
        "aff_unique_abbr": "CMU;Adobe;IITR;OSU;IITK;IITM",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Kanpur;Madras",
        "aff_country_unique_index": "0;0;1;0;1;1",
        "aff_country_unique": "United States;India"
    },
    {
        "title": "Lane detection using lane boundary marker network with road geometry constraints",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Khan_Lane_detection_using_lane_boundary_marker_network_with_road_geometry_WACV_2020_paper.html",
        "author": "Hussam Ullah Khan;  Afsheen Rafaqat Ali;  Ali Hassan;  Ahmed Ali;  Wajahat Kazmi;  Aamer Zaheer",
        "abstract": "Lane detection is of critical importance to both the self-driving cars as well as advanced driver assistance systems. While current methods use a range of features from low-level to deep features extracted from convolutional neural networks, they all suffer from the problem of occlusion and struggle to detect lanes with low or no evidence on the road. In this paper, we use a lane boundary marker network to detect keypoints along the lane boundaries. An inverse perspective mapping is estimated using road geometry which is then applied to the detected markers and lines/curves are fitted jointly on the rectified points. Finally, missing lane boundaries are predicted using lane geometry constraints i.e., equidistant and parallelism. Reciprocal weighted averaging ensures lane boundaries with strong evidence dominate their predicted alternatives. The results show a significant improvement of +7.8%, +6.8% and +1.2% of F1 scores over the state-of-the-art on CULane, Caltech and TuSimple datasets, respectively. This proves our algorithm's robustness against both occluded and missing lanes cases. Furthermore, we also show that our algorithm can be combined with other lane detectors to improve their lane retrieval potential.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Khan_Lane_detection_using_lane_boundary_marker_network_with_road_geometry_WACV_2020_paper.pdf",
        "aff": "KeepTruckin Inc. (RnD); KeepTruckin Inc. (RnD); KeepTruckin Inc. (RnD); KeepTruckin Inc. (RnD); KeepTruckin Inc. (RnD); KeepTruckin Inc. (RnD)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4613173,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7004447900124855480&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "keeptruckin.com;keeptruckin.com;keeptruckin.com;keeptruckin.com;keeptruckin.com;keeptruckin.com",
        "email": "keeptruckin.com;keeptruckin.com;keeptruckin.com;keeptruckin.com;keeptruckin.com;keeptruckin.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "KeepTruckin Inc.",
        "aff_unique_dep": "Research and Development",
        "aff_unique_url": "https://www.keeptruckin.com",
        "aff_unique_abbr": "KeepTruckin",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learn a Global Appearance Semi-Supervisedly for Synthesizing Person Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ge_Learn_a_Global_Appearance_Semi-Supervisedly_for_Synthesizing_Person_Images_WACV_2020_paper.html",
        "author": "Zhipeng Ge;  Fei Chen;  Yu Zhou;  Yao Yu;  Sidan Du",
        "abstract": "We present a novel approach for person images synthesis in this paper, that can generate person images in arbitrary poses, shapes and views. Unlike existing methods just using keypoints' locations in heatmaps format, we propose to render SMPL model to UV maps, which can provide human structural information about poses and shapes. Thus, by varying the parameters of poses, shapes and camera in SMPL model, we can generate different person images with various attributions in a simple way, while in most cases we can only obtain new shapes of people by computer graphics methods. We train an end to end generative adversarial network with unlabeled data. As our SMPL parameters come from a pretrained model, we call our overall network semi-supervised. Our network keeps a global appearance during the fine-tuning stage of the target person, thus we can get a complete appearance of the target person, rather than the inaccurate appearance caused by inferencing without enough information. Experiments on Human3.6M Dataset and a self-collected dataset demonstrate the excellent effectiveness of our approach on person images synthesis for different applications.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ge_Learn_a_Global_Appearance_Semi-Supervisedly_for_Synthesizing_Person_Images_WACV_2020_paper.pdf",
        "aff": "Nanjing University; Nanjing University; Nanjing University; Nanjing University; Nanjing University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 914440,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:1LHevrawllUJ:scholar.google.com/&scioq=Learn+a+Global+Appearance+Semi-Supervisedly+for+Synthesizing+Person+Images&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Discriminative and Generalizable Representations by Spatial-Channel Partition for Person Re-Identification",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_Learning_Discriminative_and_Generalizable_Representations_by_Spatial-Channel_Partition_for_Person_WACV_2020_paper.html",
        "author": "Hao Chen;  Benoit Lagadec;  Francois Bremond",
        "abstract": "In Person Re-Identification (Re-ID) task, combining local and global features is a common strategy to overcome missing key parts and misalignment on models based only on global features. Using this combination, neural networks yield impressive performance in Re-ID task. Previous part-based models mainly focus on spatial partition strategies. Recently, operations on channel information, such as Group Normalization and Channel Attention, have brought significant progress to various visual tasks. However, channel partition has not drawn much attention in Person Re-ID. In this paper, we conduct a study to exploit the potential of channel partition in Re-ID task. Based on this study, we propose an end-to-end Spatial and Channel partition Representation network (SCR) in order to better exploit both spatial and channel information. Experiments conducted on three mainstream image-based evaluation protocols including Market-1501, DukeMTMC-ReID and CUHK03 and one video-based evaluation protocol MARS validate the performance of our model, which outperforms previous state-of-the-art in both single and cross domain Re-ID tasks.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Learning_Discriminative_and_Generalizable_Representations_by_Spatial-Channel_Partition_for_Person_WACV_2020_paper.pdf",
        "aff": "University of C\u00f4te d\u2019Azur, Inria, Stars Project-Team, France+European Systems Integration, France; European Systems Integration, France; University of C\u00f4te d\u2019Azur, Inria, Stars Project-Team, France",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Chen_Learning_Discriminative_and_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 610556,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5671147913261289613&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "inria.fr;esifrance.net;inria.fr",
        "email": "inria.fr;esifrance.net;inria.fr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "University of C\u00f4te d\u2019Azur;European Systems Integration",
        "aff_unique_dep": "Inria, Stars Project-Team;",
        "aff_unique_url": "https://www.univ-cotedazur.fr;",
        "aff_unique_abbr": "UCA;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Learning Multimodal Representations for Unseen Activities",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Piergiovanni_Learning_Multimodal_Representations_for_Unseen_Activities_WACV_2020_paper.html",
        "author": "AJ Piergiovanni;  Michael Ryoo",
        "abstract": "We present a method to learn a joint multimodal representation space that enables recognition of unseen activities in videos. We first compare the effect of placing various constraints on the embedding space using paired text and video data. We also propose a method to improve the joint embedding space using an adversarial formulation, allowing it to benefit from unpaired text and video data.  By using unpaired text data, we show the ability to learn a representation that better captures unseen activities. In addition to testing on publicly available datasets, we introduce a new, large-scale text/video dataset. We experimentally confirm that using paired and unpaired data to learn a shared embedding space benefits three difficult tasks (i) zero-shot activity classification, (ii) unsupervised activity discovery, and (iii) unseen activity captioning, outperforming the state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Piergiovanni_Learning_Multimodal_Representations_for_Unseen_Activities_WACV_2020_paper.pdf",
        "aff": "Indiana University; Indiana University + Stony Brook University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Piergiovanni_Learning_Multimodal_Representations_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3316400,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7352560410976887744&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "indiana.edu;indiana.edu",
        "email": "indiana.edu;indiana.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Indiana University;Stony Brook University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.indiana.edu;https://www.stonybrook.edu",
        "aff_unique_abbr": "IU;SBU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning a distance function with a Siamese network to localize anomalies in videos",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ramachandra_Learning_a_distance_function_with_a_Siamese_network_to_localize_WACV_2020_paper.html",
        "author": "Bharathkumar Ramachandra;  Michael Jones;  Ranga Vatsavai",
        "abstract": "This work introduces a new approach to localize anomalies in surveillance video. The main novelty is the idea of using a Siamese convolutional neural network (CNN) to learn a distance function between a pair of video patches (spatio-temporal regions of video).  The learned distance function, which is not specific to the target video, is used  to  measure the distance between each video patch in the testing video and the video patches found in normal training video.  If a testing video patch is not similar to any normal video patch then it must be anomalous. We compare our approach to previously published algorithms using 4 evaluation measures and 3 challenging target benchmark datasets.  Experiments show that our approach either surpasses or performs comparably to current state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ramachandra_Learning_a_distance_function_with_a_Siamese_network_to_localize_WACV_2020_paper.pdf",
        "aff": "North Carolina State University; Mitsubishi Electric Research Labs (MERL); North Carolina State University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Ramachandra_Learning_a_distance_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 884642,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3315021617497372070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ncsu.edu;merl.com;ncsu.edu",
        "email": "ncsu.edu;merl.com;ncsu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "North Carolina State University;Mitsubishi Electric Research Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ncsu.edu;https://www.merl.com",
        "aff_unique_abbr": "NCSU;MERL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning from Noisy Labels via Discrepant Collaborative Training",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Han_Learning_from_Noisy_Labels_via_Discrepant_Collaborative_Training_WACV_2020_paper.html",
        "author": "Yan Han;  SOUMAVA ROY;  Lars Petersson;  Mehrtash Harandi",
        "abstract": "Noise is ubiquitous in the world around us. Difficulty inestimating the noise within a dataset makes learning fromsuch a dataset a difficult and challenging task.  In this pa-per, we propose a novel and effective learning frameworkin  order  to  alleviate  the  adverse  effects  of  noise  within  adataset. Towards this aim, we modify a collaborative train-ing  framework  to  utilize  discrepancy  constraints  betweenrespective feature extractors enabling the learning of dis-tinct, yet discriminative features, pacifying the adverse ef-fects  of  noise.    Empirical  results  of  our  proposed  algo-rithm,  Discrepant  Collaborative  Training  (DCT),  achievecompetitive results against several current state-of-the-artalgorithms across MNIST, CIFAR10 and CIFAR100, as wellas large fine-grained image classification datasets such asCUBS-200-2011 and CARS196 for different levels of noise.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Han_Learning_from_Noisy_Labels_via_Discrepant_Collaborative_Training_WACV_2020_paper.pdf",
        "aff": "The Australian National University+DATA61-CSIRO; The Australian National University+DATA61-CSIRO; The Australian National University+DATA61-CSIRO; Monash University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Han_Learning_from_Noisy_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 732053,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7305116895162373702&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "anu.edu.au;anu.edu.au;data61.csiro.au;monash.edu",
        "email": "anu.edu.au;anu.edu.au;data61.csiro.au;monash.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;2",
        "aff_unique_norm": "Australian National University;Commonwealth Scientific and Industrial Research Organisation;Monash University",
        "aff_unique_dep": ";DATA61;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.csiro.au;https://www.monash.edu",
        "aff_unique_abbr": "ANU;CSIRO;Monash",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Learning from THEODORE: A Synthetic Omnidirectional Top-View Indoor Dataset for Deep Transfer Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Scheck_Learning_from_THEODORE_A_Synthetic_Omnidirectional_Top-View_Indoor_Dataset_for_WACV_2020_paper.html",
        "author": "Tobias Scheck;  Roman Seidel;  Gangolf Hirtz",
        "abstract": "Recent work about synthetic indoor datasets from perspective views has shown significant improvements of object detection results with Convolutional Neural Networks(CNNs). In this paper, we introduce THEODORE: a novel, large-scale indoor dataset containing 100,000 high- resolution diversified fisheye images with 14 classes. To this end, we create 3D virtual environments of living rooms, different human characters and interior textures. Beside capturing fisheye images from virtual environments we create annotations for semantic segmentation, instance masks and bounding boxes for object detection tasks. We compare our synthetic dataset to state of the art real-world datasets for omnidirectional images. Based on MS COCO weights, we show that our dataset is well suited for fine-tuning CNNs for object detection. Through a high generalization of our models by means of image synthesis and domain randomization we reach an AP up to 0.84 for class person on High-Definition Analytics dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Scheck_Learning_from_THEODORE_A_Synthetic_Omnidirectional_Top-View_Indoor_Dataset_for_WACV_2020_paper.pdf",
        "aff": "Chemnitz University of Technology; Chemnitz University of Technology; Chemnitz University of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Scheck_Learning_from_THEODORE_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1484323,
        "gs_citation": 27866,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14936087872278494548&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff_domain": "etit.tu-chemnitz.de;etit.tu-chemnitz.de;etit.tu-chemnitz.de",
        "email": "etit.tu-chemnitz.de;etit.tu-chemnitz.de;etit.tu-chemnitz.de",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Chemnitz University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tu-chemnitz.de",
        "aff_unique_abbr": "TUC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Learning to Detect Head Movement in Unconstrained Remote Gaze Estimation in the Wild",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wang_Learning_to_Detect_Head_Movement_in_Unconstrained_Remote_Gaze_Estimation_WACV_2020_paper.html",
        "author": "Zhecan Wang;  Jian Zhao;  Cheng Lu;  Fan Yang;  Han Huang;  lianji li;  Yandong Guo",
        "abstract": "Unconstrained remote gaze estimation remains challenging mostly due to its vulnerability to the large variability in head-pose. Prior solutions struggle to maintain reliable accuracy in unconstrained remote gaze tracking. Among them, appearance-based solutions demonstrate tremendous potential in improving gaze accuracy. However, existing works still suffer from head movement and are not robust enough to handle real-world scenarios. Especially most of them study gaze estimation under controlled scenarios where the collected datasets often cover limited ranges of both head-pose and gaze which introduces further bias. In this paper, we propose novel end-to-end appearance-based gaze estimation methods that could more robustly incorporate different levels of head-pose representations into gaze estimation. Our method could generalize to real-world scenarios with low image quality, different lightings and scenarios where direct head-pose information is not available. To better demonstrate the advantage of our methods, we further propose a new benchmark dataset with the most rich distribution of head-gaze combination reflecting real-world scenarios. Extensive evaluations on several public datasets and our own dataset demonstrate that our method consistently outperforms the state-of-the-arts by a significant margin.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_Learning_to_Detect_Head_Movement_in_Unconstrained_Remote_Gaze_Estimation_WACV_2020_paper.pdf",
        "aff": "Columbia University + XPENG Motors; Institute of North Electronic Equipment, Beijing, China + XPENG Motors; XPENG Motors; XPENG Motors; Temple University; XPENG Motors; XPENG Motors",
        "project": "",
        "github": "https://zhaoj9014.github.io/",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10883374,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15767022254089970433&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;u.nus.edu;hotmail.com;xiaopeng.com;temple.edu;xiaopeng.com;live.com",
        "email": "gmail.com;u.nus.edu;hotmail.com;xiaopeng.com;temple.edu;xiaopeng.com;live.com",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1;1;1;3;1;1",
        "aff_unique_norm": "Columbia University;XPENG Motors;Institute of North Electronic Equipment;Temple University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.columbia.edu;https://www.xpengmotor.com;;https://www.temple.edu",
        "aff_unique_abbr": "Columbia;XPENG;;Temple",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1+1;1;1;0;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Leveraging Filter Correlations for Deep Model Compression",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Singh_Leveraging_Filter_Correlations_for_Deep_Model_Compression_WACV_2020_paper.html",
        "author": "Pravendra Singh;  Vinay Kumar Verma;  Piyush Rai;  Vinay Namboodiri",
        "abstract": "We present a filter correlation based model compression approach for deep convolutional neural networks. Our approach iteratively identifies pairs of filters with the largest pairwise correlations and drops one of the filters from each such pair. However, instead of discarding one of the filters from each such pair naively, the model is re-optimized to make the filters in these pairs maximally correlated, so that discarding one of the filters from the pair results in minimal information loss. Moreover, after discarding the filters in each round, we further finetune the model to recover from the potential small loss incurred by the compression. We evaluate our proposed approach using a comprehensive set of experiments and ablation studies. Our compression method yields state-of-the-art FLOPs compression rates on various benchmarks, such as LeNet-5, VGG-16, and ResNet-50,56, while still achieving excellent predictive performance for tasks such as object detection on benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Singh_Leveraging_Filter_Correlations_for_Deep_Model_Compression_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science and Engineering, IIT Kanpur, India; Department of Computer Science and Engineering, IIT Kanpur, India; Department of Computer Science and Engineering, IIT Kanpur, India; Department of Computer Science and Engineering, IIT Kanpur, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 560568,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15926463755411031640&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "iitk.ac.in;iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "email": "iitk.ac.in;iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Kanpur",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Kanpur",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Leveraging Pretrained Image Classifiers for Language-Based Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Golub_Leveraging_Pretrained_Image_Classifiers_for_Language-Based_Segmentation_WACV_2020_paper.html",
        "author": "David Golub;  Roberto Martin-Martin;  Ahmed El-Kishky;  Silvio Savarese",
        "abstract": "Current semantic segmentation models cannot easily generalize to new object classes unseen during train time: they require additional annotated images and retraining. We propose a novel segmentation model that injects visual priors from pretrained image classifiers into semantic segmentation architectures, allowing them to segment out new target labels without retraining. As visual priors, we use the activations of pretrained image classifiers, which provide noisy indications of the spatial location of both the target object and distractor objects in the scene. We leverage language semantics to obtain these activations for a target label unseen by the classifier. Further experiments show that the visual priors obtained via language semantics for both relevant anddistracting objects are key to our performance",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Golub_Leveraging_Pretrained_Image_Classifiers_for_Language-Based_Segmentation_WACV_2020_paper.pdf",
        "aff": "Stanford University; Stanford University; University of Illinois at Urbana-Champaign; Stanford University",
        "project": "https://sites.google.com/stanford.edu/cls-seg",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1951013,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15508494873305215714&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.stanford.edu;stanford.edu;illinois.edu;stanford.org",
        "email": "cs.stanford.edu;stanford.edu;illinois.edu;stanford.org",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Stanford University;University of Illinois at Urbana-Champaign",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://illinois.edu",
        "aff_unique_abbr": "Stanford;UIUC",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Stanford;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Lightweight 3D Human Pose Estimation Network Training Using Teacher-Student Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hwang_Lightweight_3D_Human_Pose_Estimation_Network_Training_Using_Teacher-Student_Learning_WACV_2020_paper.html",
        "author": "Dong-Hyun Hwang;  Suntae Kim;  Nicolas Monet;  Hideki Koike;  Soonmin Bae",
        "abstract": "We present MoVNect, a lightweight deep neural network to capture 3D human pose using a single RGB camera.  To improve the overall performance of the model, we apply the teacher-student learning method based knowledge distillation to 3D human pose estimation. Real-time post-processing makes the CNN output yield temporally stable 3D skeletal information, which can be used in applications directly. We implement a 3D avatar application running on mobile in real-time to demonstrate that our network achieves both high accuracy and fast inference time. Extensive evaluations show the advantages of our lightweight model with the proposed training method over previous 3D pose estimation methods on the Human3.6M dataset and mobile devices.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hwang_Lightweight_3D_Human_Pose_Estimation_Network_Training_Using_Teacher-Student_Learning_WACV_2020_paper.pdf",
        "aff": "School of Computing, Tokyo Institute of Technology+Clova AI Video, NA VER Corp.; Clova AI Video, NA VER Corp.; Computer Vision Group, NA VER LABS Europe; School of Computing, Tokyo Institute of Technology; Clova AI Video, NA VER Corp.",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Hwang_Lightweight_3D_Human_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 927477,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=669402894887816012&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "m.titech.ac.jp;navercorp.com;naverlabs.com;acm.org;navercorp.com",
        "email": "m.titech.ac.jp;navercorp.com;naverlabs.com;acm.org;navercorp.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;2;0;1",
        "aff_unique_norm": "Tokyo Institute of Technology;NA VER Corp.;NA VER LABS Europe",
        "aff_unique_dep": "School of Computing;Clova AI Video;Computer Vision Group",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.navercorp.com;",
        "aff_unique_abbr": "Titech;NAVER;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tokyo;",
        "aff_country_unique_index": "0+1;1;2;0;1",
        "aff_country_unique": "Japan;South Korea;Unknown"
    },
    {
        "title": "Local Binary Pattern Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Lin_Local_Binary_Pattern_Networks_WACV_2020_paper.html",
        "author": "Jeng-Hau Lin;  Justin Lazarow;  Andrew Yang;  Dezhi Hong;  Rajesh Gupta;  Zhuowen Tu",
        "abstract": "Emerging edge devices such as sensor nodes are increasingly being tasked with non-trivial tasks related to sensor data processing and even application-level inferences from this sensor data. These devices are, however, extraordinarily resource-constrained in terms of CPU power (often Cortex M0-3 class CPUs), available memory (in few KB to MBytes), and energy. Under these constraints, we explore a novel approach to character recognition using local binary pattern networks, or LBPNet, that can learn and perform bit-wise operations in an end-to-end fashion. LBPNet has its advantage for characters whose features are composed of structured strokes and distinctive outlines. LBPNet uses local binary comparisons and random projections in place of conventional convolution (or approximation of convolution) operations, providing an important means to improve memory efficiency as well as inference speed. We evaluate LBPNet on a number of character recognition benchmark datasets as well as several object classification datasets and demonstrate its effectiveness and efficiency.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Lin_Local_Binary_Pattern_Networks_WACV_2020_paper.pdf",
        "aff": "Computer Science and Engineering; Computer Science and Engineering; Computer Science and Engineering; Computer Science and Engineering; Computer Science and Engineering; Cognitive Science + Computer Science and Engineering",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2628549,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14163021226376494534&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1+0",
        "aff_unique_norm": "Computer Science and Engineering;Cognitive Science",
        "aff_unique_dep": "Computer Science and Engineering;Cognitive Science Department",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Localizing Grouped Instances for Efficient Detection in Low-Resource Scenarios",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Royer_Localizing_Grouped_Instances_for_Efficient_Detection_in_Low-Resource_Scenarios_WACV_2020_paper.html",
        "author": "Amelie Royer;  Christoph Lampert",
        "abstract": "State-of-the-art detection systems generally focus, and are evaluated on, their ability to exhaustively retrieve objects densely distributed in the image, across a wide variety of appearances and semantic categories. Orthogonal to this, many practical object detection applications, for example in remote sensing, instead require dealing with large images that contain only a few small objects of a single class, scattered heterogeneously across the space. In addition, they are often subject to strict computational constraints, such as limited battery capacity and computing power.  To tackle these more practical scenarios, we propose a novel detection scheme that offers a flexible and efficient framework for detection tasks with variable object sizes and densities: We rely on a sequence of detection stages, each of which has the ability to predict groups of objects as well as individuals. Similar to a detection cascade, this multi-stage architecture spares computational effort by discarding large irrelevant regions of the image early during the detection process. The ability to group objects provides further computational and memory savings, as it allows working with lower image resolutions in early stages, where groups are more easily detected than individuals. We report experimental results on two aerial image datasets, and show that the proposed method is as accurate yet computationally more efficient than standard single-shot detectors, consistently across three different backbone architectures.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Royer_Localizing_Grouped_Instances_for_Efficient_Detection_in_Low-Resource_Scenarios_WACV_2020_paper.pdf",
        "aff": "IST Austria; IST Austria",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Royer_Localizing_Grouped_Instances_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1026225,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2676804465090258694&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ist.ac.at;ist.ac.at",
        "email": "ist.ac.at;ist.ac.at",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Institute of Science and Technology Austria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ist.ac.at",
        "aff_unique_abbr": "IST Austria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "Long-Short Graph Memory Network for Skeleton-based Action Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Huang_Long-Short_Graph_Memory_Network__for_Skeleton-based_Action_Recognition_WACV_2020_paper.html",
        "author": "Junqin Huang;  zhenhuan huang;  Xiang Xiang;  Xuan Gong;  Baochang Zhang",
        "abstract": "Current studies have shown the effectiveness of long short-term memory network (LSTM) for skeleton-based human action recognition in capturing temporal and spatial features of the skeleton sequence. Nevertheless, it still remains challenging for LSTM to extract the latent structural dependency among nodes. In this paper, we introduce a new long-short graph memory network (LSGM) to improve the capability of LSTM to model the skeleton sequence - a type of graph data. Our proposed LSGM can learn high-level temporal-spatial features end-to-end, enabling LSTM to extract the spatial information that is neglected but intrinsic to the skeleton graph data. To improve the discriminative ability of the temporal and spatial module, we use a calibration module termed as graph temporal-spatial calibration (GTSC) to calibrate the learned temporal-spatial features. By integrating the two modules into the same framework, we obtain a stronger generalization capability in processing dynamic graph data and achieve a significant performance improvement on the NTU and SYSU dataset. Experimental results have validated the effectiveness of our proposed LSGM+GTSC model in extracting temporal and spatial information from dynamic graph data.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Huang_Long-Short_Graph_Memory_Network__for_Skeleton-based_Action_Recognition_WACV_2020_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 744665,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13777292900156636670&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Looking Ahead: Anticipating Pedestrians Crossing with Future Frames Prediction",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chaabane_Looking_Ahead_Anticipating_Pedestrians_Crossing_with_Future_Frames_Prediction_WACV_2020_paper.html",
        "author": "Mohamed Chaabane;  Ameni Trabelsi;  Nathaniel Blanchard;  Ross Beveridge",
        "abstract": "In this paper, we present an end-to-end future-prediction model that focuses on pedestrian safety. Specifically, our model uses previous video frames, recorded from the perspective of the vehicle, to predict if a pedestrian will cross in front of the vehicle. The long term goal of this work is to design a fully autonomous system that acts and reacts as a defensive human driver would --- predicting future events and reacting to mitigate risk. We focus on pedestrian-vehicle interactions because of the high risk of harm to the pedestrian if their actions are miss-predicted.  Our end-to-end model consists of two stages: the first stage is an encoder-decoder network that learns to predict future video frames. The second stage is a deep spatio-temporal network that utilizes the predicted frames of the first stage to predict the pedestrian's future action.  Our system achieves state-of-the-art accuracy on pedestrian behavior prediction and future frames prediction on the Joint Attention for Autonomous Driving (JAAD) dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chaabane_Looking_Ahead_Anticipating_Pedestrians_Crossing_with_Future_Frames_Prediction_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science, Colorado State University; Department of Computer Science, Colorado State University; Department of Computer Science, Colorado State University; Department of Computer Science, Colorado State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1074321,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1604803343759504533&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "colostate.edu;colostate.edu;colostate.edu;colostate.edu",
        "email": "colostate.edu;colostate.edu;colostate.edu;colostate.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Colorado State University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.colostate.edu",
        "aff_unique_abbr": "CSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Looking deeper into Time for Activities of Daily Living Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Das_Looking_deeper_into_Time_for_Activities_of_Daily_Living_Recognition_WACV_2020_paper.html",
        "author": "Srijan Das;  Monique Thonnat;  Francois Bremond",
        "abstract": "In this paper, we introduce a new approach for Activities of  Daily Living (ADL) recognition. In order to discriminate between activities with similar appearance and motion, we focus on their temporal structure. Actions with subtle and similar motion are hard to disambiguate since long-range temporal information is hard to encode. So, we propose an end-to-end Temporal Model to incorporate long-range temporal information without losing subtle details. The temporal structure is represented globally by different temporal granularities and locally by temporal segments. We also propose a two-level pose driven attention mechanism to take into account the relative importance of the segments and granularities. We validate our approach on 2 public datasets: a 3D human activity dataset (NTU-RGB+D) and a human-object interaction dataset (Northwestern-UCLA Multiview Action 3D). Our Temporal Model can also be incorporated with any existing 3D CNN (including attention based) as a backbone which reveals its robustness.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Das_Looking_deeper_into_Time_for_Activities_of_Daily_Living_Recognition_WACV_2020_paper.pdf",
        "aff": "INRIA Universit \u00b4e Nice C \u02c6ote d\u2019Azur, France; INRIA Universit \u00b4e Nice C \u02c6ote d\u2019Azur, France; INRIA Universit \u00b4e Nice C \u02c6ote d\u2019Azur, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1062137,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9764538932606442546&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIA Universit\u00e9 Nice C\u00f4te d\u2019Azur",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Low Cost, High Performance Automatic Motorcycle Helmet Violation Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chairat_Low_Cost_High_Performance_Automatic_Motorcycle_Helmet_Violation_Detection_WACV_2020_paper.html",
        "author": "Aphinya Chairat;  Matthew Dailey;  Somphop Limsoonthrakul;  Mongkol Ekpanyapong;  Dharma Raj KC",
        "abstract": "Road fatality rates are very high, especially in developing and middle-income countries. One of the main causes of road fatalities is not using motorcycle helmets. Active law enforcement may help increase compliance, but ubiquitous enforcement requires many police officers and may cause traffic jams and safety issues. In this paper, we demonstrate the effectiveness of computer vision and machine learning methods to increase helmet compliance through automated helmet violation detection. The system detects riders and passengers not wearing helmets and consists of motorcyclist detection, helmet violation classification, and tracking. The architecture of the system comprises a single GPU server and multiple computational clients that cooperate to complete the task, with communication over HTTP. In a real-world test, the system is able to detect 97% of helmet violations with a 15% false alarm rate. The client-server architecture reduces cost by 20-30% compared to a baseline architecture.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chairat_Low_Cost_High_Performance_Automatic_Motorcycle_Helmet_Violation_Detection_WACV_2020_paper.pdf",
        "aff": "AI Center, Asian Institute of Technology; AI Center, Asian Institute of Technology; AI Center, Asian Institute of Technology; AI Center, Asian Institute of Technology; University of Arizona",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1021944,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8817264407706728974&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ait.ac.th;ait.ac.th;ait.ac.th;ait.ac.th;email.arizona.edu",
        "email": "ait.ac.th;ait.ac.th;ait.ac.th;ait.ac.th;email.arizona.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Asian Institute of Technology;University of Arizona",
        "aff_unique_dep": "AI Center;",
        "aff_unique_url": "https://www.ait.asia;https://www.arizona.edu",
        "aff_unique_abbr": "AIT;UA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Thailand;United States"
    },
    {
        "title": "MHSAN: Multi-Head Self-Attention Network for Visual Semantic Embedding",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Park_MHSAN_Multi-Head_Self-Attention_Network_for_Visual_Semantic_Embedding_WACV_2020_paper.html",
        "author": "Geondo Park;  Chihye Han;  Wonjun Yoon;  Daeshik Kim",
        "abstract": "Visual-semantic embedding enables various tasks such as image-text retrieval, image captioning, and visual question answering. The key to successful visual-semantic embedding is to express visual and textual data properly by accounting for their intricate relationship. While previous studies have achieved much advance by encoding the visual and textual data into a joint space where similar concepts are closely located, they often represent data by a single vector ignoring the presence of multiple important components in an image or text. Thus, in addition to the joint embedding space, we propose a novel multi-head self-attention network to capture various components of visual and textual data by attending to important parts in data. Our approach achieves the new state-of-the-art results in image-text retrieval tasks on MS-COCO and Flicker30K datasets. Through the visualization of the attention maps that capture distinct semantic components at multiple positions in the image and the text, we demonstrate that our method achieves an effective and interpretable visual-semantic joint space.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Park_MHSAN_Multi-Head_Self-Attention_Network_for_Visual_Semantic_Embedding_WACV_2020_paper.pdf",
        "aff": "Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Lunit Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1220045,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11595210069441841385&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;lunit.io",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;lunit.io",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Lunit Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.lunit.io",
        "aff_unique_abbr": "KAIST;Lunit",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "MLSL: Multi-Level Self-Supervised Learning for Domain Adaptation with Spatially Independent and Semantically Consistent Labeling",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Iqbal_MLSL_Multi-Level_Self-Supervised_Learning_for_Domain_Adaptation_with_Spatially_Independent_WACV_2020_paper.html",
        "author": "Javed Iqbal;  Mohsen Ali",
        "abstract": "Most of the recent Deep Semantic Segmentation algorithms suffer from large generalization errors, even when powerful hierarchical representation models, based on convolutional neural networks, have been employed. This could be attributed to limited training data and large distribution gap in train and test domain datasets. In this paper, we propose a multi-level self-supervised learning model for domain adaptation of semantic segmentation. Exploiting the idea that an object (and most of stuff given context) should be labeled consistently regardless of its location, we generate spatially independent and semantically consistent (SISC) pseudo-labels by segmenting multiple sub-images using base model and designing an aggregation strategy. Image level pseudo weak-labels, PWL, are computed to guide domain adaptation by capturing global context similarity in source and target domain at latent space level. Thus helping latent space learn the representation even when there are very few pixels belonging to the domain category (small object for example) compared to rest of the image. Our multi-level Self-supervised learning (MLSL) outperforms existing state-of-art (self or adversarial learning) algorithms. Specifically, keeping all setting similar and employing MLSL we obtain a mIoU gain of 5.1% on GTA-V to Cityscapes adaptation and 4.3% on SYNTHIA to Cityscapes adaptation compared to the existing state-of-art method",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Iqbal_MLSL_Multi-Level_Self-Supervised_Learning_for_Domain_Adaptation_with_Spatially_Independent_WACV_2020_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Iqbal_MLSL_Multi-Level_Self-Supervised_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 25405287,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4903316953813037906&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Main-Secondary Network for Defect Segmentation of Textured Surface Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xie_Main-Secondary_Network_for_Defect_Segmentation_of_Textured_Surface_Images_WACV_2020_paper.html",
        "author": "Yu Xie;  Fangrui Zhu;  Yanwei Fu",
        "abstract": "Building  an  intelligent  defect  segmentation  system  for textured images has attracted much increasing attention in both  research  and  industrial  communities,  due  to  its  significance values in the practical applications of industrial inspection  and  quality  control.   Previous  models  learned the classical classifiers for segmentation by designing hand-crafted features.  However, defect segmentation of textured surface images poses challenges such as ambiguous shapes and sizes of defects along with varying textures and patterns in the images.  Thus, hand-crafted features based segmentation methods can only be applied to particular types of textured images.  To this end, it is desirable to learn a general deep learning based representation for the automatic segmentation of defects.   Furthermore,  it is relatively less study in efficiently extracting the deep features in the frequency domain, which, nevertheless, should be very important to understand the patterns of textured images.  In this paper,  we  propose  a  novel  defect  segmentation  deep  net-work - Main-Secondary Network (MS-Net). Our MS-Net is trained to model both features from the spatial domain and the frequency domain, where wavelet transform is utilized to extract discriminative information from the frequency do-main.  Extensive experiments show the effectiveness of our MS-Net.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xie_Main-Secondary_Network_for_Defect_Segmentation_of_Textured_Surface_Images_WACV_2020_paper.pdf",
        "aff": "Fudan University; Fudan University; Fudan University+School of Data Science, and MOE Frontiers Center for Brain Science, Shanghai Key Lab of Intelligent Information Processing, Fudan University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3260668,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1045352722150002892&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;gmail.com;fudan.edu.cn",
        "email": "gmail.com;gmail.com;fudan.edu.cn",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "MaskPlus: Improving Mask Generation for Instance Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xu_MaskPlus_Improving_Mask_Generation_for_Instance_Segmentation_WACV_2020_paper.html",
        "author": "Shichao Xu;  Shuyue Lan;  Zhu Qi",
        "abstract": "Instance segmentation is a promising yet challenging topic in computer vision. Recent approaches such as Mask R-CNN typically divide this problem into two parts -- a detection component and a mask generation branch, and mostly focus on the improvement of the detection part.   In this paper, we present an approach that extends Mask R-CNN with five novel techniques for improving the mask generation branch and reducing the conflicts between the mask branch and the detection component in training.   These five techniques are independent to each other and can be flexibly utilized in building various instance segmentation architectures for increasing the overall accuracy. We demonstrate the effectiveness of our approach with tests on the COCO dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xu_MaskPlus_Improving_Mask_Generation_for_Instance_Segmentation_WACV_2020_paper.pdf",
        "aff": "Northwestern University, USA; Northwestern University, USA; Northwestern University, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1712636,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1297765344833658265&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "unorthwestern.edu;unorthwestern.edu;northwestern.edu",
        "email": "unorthwestern.edu;unorthwestern.edu;northwestern.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Northwestern University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.northwestern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Measuring the Utilization of Public Open Spaces by Deep Learning: a Benchmark Study at the Detroit Riverfront",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sun_Measuring_the_Utilization_of_Public_Open_Spaces_by_Deep_Learning_WACV_2020_paper.html",
        "author": "Peng Sun;  Rui Hou;  Jerome Lynch",
        "abstract": "Physical activities and social interactions are essential activities that ensure a healthy lifestyle. Public open spaces (POS), such as parks, plazas and greenways, are key environments that encourage those activities. To evaluate a POS, there is a need to study how humans use the facilities within it. However, traditional approaches to studying use of POS are manual and therefore time and labor intensive. They also may only provide qualitative insights. It is appealing to make use of surveillance cameras and to extract user-related information through computer vision. This paper proposes a proof-of-concept deep learning computer vision framework for measuring human activities quantitatively in POS and demonstrates a case study of the proposed framework using the Detroit Riverfront Conservancy (DRFC) surveillance camera network. A custom image dataset is presented to train the framework; the dataset includes 7826 fully annotated images collected from 18 cameras across the DRFC park space under various illumination conditions. Dataset analysis is also provided as well as a baseline model for one-step user localization and activity recognition. The mAP results are 77.5% for  pedestrian  detection and 81.6% for  cyclist  detection. Behavioral maps are autonomously generated by the framework to locate different POS users and the average error for behavioral localization is within 10 cm.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sun_Measuring_the_Utilization_of_Public_Open_Spaces_by_Deep_Learning_WACV_2020_paper.pdf",
        "aff": "University of Michigan; University of Michigan; University of Michigan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8159206,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=98410541064074727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "MoBiNet: A Mobile Binary Network for Image Classification",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Phan_MoBiNet_A_Mobile_Binary_Network_for_Image_Classification_WACV_2020_paper.html",
        "author": "Hai Phan;  Dang The Huynh;  Yihui He;  Marios Savvides;  Zhiqiang Shen",
        "abstract": "MobileNet and Binary Neural Networks are two among the most widely used techniques to construct deep learning models for performing a variety of tasks on mobile and embedded platforms.In this paper, we present a simple yet efficient scheme to exploit MobileNet binarization at activation function and model weights. However, training a binary network from scratch with separable depth-wise and point-wise convolutions in case of MobileNet is not trivial and prone to divergence. To tackle this training issue, we propose a novel neural network architecture, namely MoBiNet - Mobile Binary Network in which skip connections are manipulated to prevent information loss and vanishing gradient, thus facilitate the training process. More importantly, while existing binary neural networks often make use of cumbersome backbones such as Alex-Net, ResNet, VGG-16 with float-type pre-trained weights initialization, our MoBiNet focuses on binarizing the already-compressed neural networks like MobileNet without the need of a pre-trained model to start with. Therefore, our proposal results in an effectively small model while keeping the accuracy comparable to existing ones. Experiments on ImageNet dataset show the potential of the MoBiNet as it achieves 54.40% top-1 accuracy and dramatically reduces the computational cost with binary operators.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Phan_MoBiNet_A_Mobile_Binary_Network_for_Image_Classification_WACV_2020_paper.pdf",
        "aff": "Carnegie Mellon University; Axon Enterprise; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 511722,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=881358979560616601&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "andrew.cmu.edu;axon.com;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "andrew.cmu.edu;axon.com;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University;Axon Enterprise",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.axon.com",
        "aff_unique_abbr": "CMU;Axon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Model-Agnostic Metric for Zero-Shot Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Shen_Model-Agnostic_Metric_for_Zero-Shot_Learning_WACV_2020_paper.html",
        "author": "Jiayi Shen;  Haochen Wang;  Anran Zhang;  Qiang Qiu;  Xiantong Zhen;  Xianbin Cao",
        "abstract": "Zero-shot Learning (ZSL) aims to learn a classifier to recognize unseen categories without training samples. Most ZSL works based on embedding models handle the visual space and the semantic space through a common metric space and then apply a simple nearest neighbor search which directly leads to the hubness problem, one of the main challenges of ZSL. Contrary to recent works, whose conclusions about hubs are drawn based on Euclidean and specific models like ridge regression, we adopt cosine metric and for the first time prove cosine is model-agnostic to alleviate the hubness problem in ZSL. Assuming that the normalized mapped semantic vectors follow a uniform distribution, we provide theoretical analysis which demonstrates that hubs can be better reduced with a higher-dimensional cosine metric space. Moreover, we introduce a diversity-based regularizer with the cosine metric which underpins the assumption about the uniform distribution and further improves the model's discriminative ability. Extensive experiments on five benchmark datasets and large-scale Imagenet dataset show that our method can consistently improve the performance, surpassing previous embedding methods by large margins.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Shen_Model-Agnostic_Metric_for_Zero-Shot_Learning_WACV_2020_paper.pdf",
        "aff": "School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Duke University, Durham, NC, USA; Inception Institute of Arti\ufb01cial Intelligence, Abu Dhabi, UAE; School of Electronic and Information Engineering, Beihang University, Beijing, China + Key Laboratory of Advanced Technology of Near Space Information System (Beihang University), Ministry of Industry and Information Technology of China, Beijing, China + Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beijing, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Shen_Model-Agnostic_Metric_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 7999867,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13345907607523650110&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "buaa.edu.cn;gmail.com;buaa.edu.cn;duke.edu;gmail.com;buaa.edu.cn",
        "email": "buaa.edu.cn;gmail.com;buaa.edu.cn;duke.edu;gmail.com;buaa.edu.cn",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2;0+0+3",
        "aff_unique_norm": "Beihang University;Duke University;Inception Institute of Artificial Intelligence;Beijing Advanced Innovation Center for Big Data-Based Precision Medicine",
        "aff_unique_dep": "School of Electronic and Information Engineering;;;",
        "aff_unique_url": "http://www.buaa.edu.cn;https://www.duke.edu;;",
        "aff_unique_abbr": "BUAA;Duke;;",
        "aff_campus_unique_index": "0;0;0;1;2;0+0",
        "aff_campus_unique": "Beijing;Durham;Abu Dhabi;",
        "aff_country_unique_index": "0;0;0;1;2;0+0+0",
        "aff_country_unique": "China;United States;United Arab Emirates"
    },
    {
        "title": "MonoLayout: Amodal scene layout from a single image",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mani_MonoLayout_Amodal_scene_layout_from_a_single_image_WACV_2020_paper.html",
        "author": "Kaustubh Mani;  Swapnil Daga;  Shubhika Garg;  Sai Shankar Narasimhan;  Madhava Krishna;  Krishna Murthy Jatavallabhula",
        "abstract": "In this paper, we address the novel, highly challenging problem of estimating the layout of a complex urban driving scenario. Given a single color image captured from a driving platform, we aim to predict the bird's eye view layout of the road and other traffic participants. The estimated layout should reason beyond what is visible in the image, and compensate for the loss of 3D information due to projection. We dub this problem amodal scene layout estimation, which involves hallucinating scene layout for even parts of the world that are occluded in the image. To this end, we present MonoLayout, a deep neural network for real- time amodal scene layout estimation from a single image. We represent scene layout as a multi-channel semantic occupancy grid, and leverage adversarial feature learning to \"hallucinate\" plausible completions for occluded image parts. We extend several state-of-the-art approaches for road-layout estimation and vehicle occupancy estimation in bird's eye view to the amodal setup and thoroughly evaluate against them. By leveraging temporal sensor fusion to generate training labels, we significantly outperform current art (> 10% improvement) over a number of datasets. We also make all our annotations, code, and pretrained models publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mani_MonoLayout_Amodal_scene_layout_from_a_single_image_WACV_2020_paper.pdf",
        "aff": "Robotics Research Center, KCIS, IIIT Hyderabad, India; Robotics Research Center, KCIS, IIIT Hyderabad, India; IIT Kharagpur; Robotics Research Center, KCIS, IIIT Hyderabad, India; Mila - Quebec AI Institute, Montreal, Canada + Universit\u00e9 de Montr\u00e9al; Robotics Research Center, KCIS, IIIT Hyderabad, India",
        "project": "https://hbutsuak95.github.io/monolayout/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Mani_MonoLayout_Amodal_scene_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6261273,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=588818182218524264&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;2+3;0",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad;Indian Institute of Technology Kharagpur;Quebec AI Institute;Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "Robotics Research Center;;AI Institute;",
        "aff_unique_url": "https://iiit Hyderabad.ac.in;https://www.iitkgp.ac.in;https://mila.quebec;https://www.umontreal.ca",
        "aff_unique_abbr": "IIIT Hyderabad;IIT KGP;Mila;UdeM",
        "aff_campus_unique_index": "0;0;1;0;2;0",
        "aff_campus_unique": "Hyderabad;Kharagpur;Montreal;",
        "aff_country_unique_index": "0;0;0;0;1+1;0",
        "aff_country_unique": "India;Canada"
    },
    {
        "title": "MotionRec: A Unified Deep Framework for Moving Object Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mandal_MotionRec_A_Unified_Deep_Framework_for_Moving_Object_Recognition_WACV_2020_paper.html",
        "author": "Murari Mandal;  Lav Kush Kumar;  Mahipal Singh Saran;  Santosh Kumar vipparthi",
        "abstract": "In this paper we present a novel deep learning framework to perform online moving object recognition (MOR) in streaming videos. The existing methods for moving object detection (MOD) only computes class-agnostic pixel-wise binary segmentation of video frames. On the other hand, the object detection techniques do not differentiate between static and moving objects. To the best of our knowledge, this is a first attempt for simultaneous localization and classification of moving objects in a video, i.e. MOR in a single-stage deep learning framework. We achieve this by labelling axis-aligned bounding boxes for moving objects which requires less computational resources than producing pixel-level estimates. In the proposed MotionRec, both temporal and spatial features are learned using past history and current frames respectively. First, the background is estimated with a temporal depth reductionist (TDR) block. Then the estimated background, current frame and temporal median of recent observations are assimilated to encode spatiotemporal motion saliency. Moreover, feature pyramids are generated from these motion saliency maps to perform regression and classification at multiple levels of feature abstractions. MotionRec works online at inference as it requires only few past frames for MOR. Moreover, it doesn't require predefined target initialization from user. We also annotated axis-aligned bounding boxes (42,614 objects (14,814 cars and 27,800 person) in 24,923 video frames of CDnet 2014 dataset) due to lack of available benchmark datasets for MOR. The performance is observed qualitatively and quantitatively in terms of mAP over a defined unseen test set. Experiments show that the proposed MotionRec significantly improves over strong baselines with RetinaNet architectures for MOR.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mandal_MotionRec_A_Unified_Deep_Framework_for_Moving_Object_Recognition_WACV_2020_paper.pdf",
        "aff": "Vision Intelligence Lab, Malaviya National Institute of Technology Jaipur, India; Vision Intelligence Lab, Malaviya National Institute of Technology Jaipur, India; Vision Intelligence Lab, Malaviya National Institute of Technology Jaipur, India; Vision Intelligence Lab, Malaviya National Institute of Technology Jaipur, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Mandal_MotionRec_A_Unified_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1495612,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9352102116056712942&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;gmail.com;gmail.com;mnit.ac.in",
        "email": "gmail.com;gmail.com;gmail.com;mnit.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Malaviya National Institute of Technology Jaipur",
        "aff_unique_dep": "Vision Intelligence Lab",
        "aff_unique_url": "https://www.mnit.ac.in",
        "aff_unique_abbr": "MNIT Jaipur",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Jaipur",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Multi Receptive Field Network for Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Jianlong_Multi_Receptive_Field_Network_for_Semantic_Segmentation_WACV_2020_paper.html",
        "author": "Jianlong Yuan;  Zelu Deng;  Shu Wang;  Zhenbo Luo",
        "abstract": "Semantic segmentation is one of the key tasks in computer vision, which is to assign a category label to each pixel in an image. Despite significant progress achieved recently, most existing methods still suffer from two challenging issues: 1) the size of objects and stuff in an image can be very diverse, demanding for incorporating multi-scale features into the fully convolutional networks (FCNs); 2) the pixels close to or at the boundaries of object/stuff are hard to classify due to the intrinsic weakness of convolutional networks. To address the first issue, we propose a new Multi-Receptive Field Module (MRFM), explicitly taking multi-scale features into account. For the second issue, we design an edge-aware loss which is effective in distinguishing the boundaries of object/stuff. With these two designs,  our Multi Receptive Field Network achieves new state-of-the-art results on  two widely-used semantic segmentation benchmark datasets. Specifically, we achieve a mean IoU of 83.0% on the Cityscapes dataset and 88.4% mean IoU on the pascal VOC2012 dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Jianlong_Multi_Receptive_Field_Network_for_Semantic_Segmentation_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 26025448,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9241563819104589071&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Multi-Label Visual Feature Learning with Attentional Aggregation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Guan_Multi-Label_Visual_Feature_Learning_with_Attentional_Aggregation_WACV_2020_paper.html",
        "author": "Ziqiao  Guan;  Kevin  Yager;  Dantong Yu;  Hong Qin",
        "abstract": "Today convolutional neural networks (CNNs) have reached out to specialized applications in science communities that otherwise would not be adequately tackled. In this paper, we systematically study a multi-label annotation problem of x-ray scattering images in material science. For this application, we tackle an open challenge with training CNNs --- identifying weak scattered patterns with diffuse background interference, which is common in scientific imaging. We articulate an Attentional Aggregation Module (AAM) to enhance feature representations. First, we reweight and highlight important features in the images using data-driven attention maps. We decompose the attention maps into channel and spatial attention components. In the spatial attention component, we design a mechanism to generate multiple spatial attention maps tailored for diversified multi-label learning. Then, we condense the enhanced local features into non-local representations by performing feature aggregation. Both attention and aggregation are designed as network layers with learnable parameters so that CNN training remains fluidly end-to-end, and we apply it in-network a few times so that the feature enhancement is multi-scale. We conduct extensive experiments on CNN training and testing, as well as transfer learning, and empirical studies confirm that our method enhances the discriminative power of visual features of scientific imaging.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Guan_Multi-Label_Visual_Feature_Learning_with_Attentional_Aggregation_WACV_2020_paper.pdf",
        "aff": "Stony Brook University; Brookhaven National Laboratory; New Jersey Institute of Technology; Stony Brook University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 672245,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12751746498202499115&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.stonybrook.edu;bnl.gov;njit.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;bnl.gov;njit.edu;cs.stonybrook.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Stony Brook University;Brookhaven National Laboratory;New Jersey Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stonybrook.edu;https://www.bnl.gov;https://www.njit.edu",
        "aff_unique_abbr": "SBU;BNL;NJIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multi-Level Representation Learning for Deep Subspace Clustering",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kheirandishfard_Multi-Level_Representation_Learning_for_Deep_Subspace_Clustering_WACV_2020_paper.html",
        "author": "Mohsen Kheirandishfard;  Fariba Zohrizadeh;  Farhad Kamangar",
        "abstract": "This paper proposes a novel deep subspace clustering approach which uses convolutional autoencoders to transform input images into new representations lying on a union of linear subspaces. The first contribution of our work is to insert multiple fully-connected linear layers between the encoder layers and their corresponding decoder layers to promote learning more favorable representations for subspace clustering. These connection layers facilitate the feature learning procedure by combining low-level and high-level information for generating multiple sets of self-expressive and informative representations at different levels of the encoder. Moreover, we introduce a novel loss minimization problem which leverages an initial clustering of the samples to effectively fuse the multi-level representations and recover the underlying subspaces more accurately. The loss function is then minimized through an iterative scheme which alternatively updates the network parameters and produces new clusterings of the samples. Experiments on four real-world datasets demonstrate that our approach exhibits superior performance compared to the state-of-the-art methods on most of the subspace clustering problems.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kheirandishfard_Multi-Level_Representation_Learning_for_Deep_Subspace_Clustering_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science and Engineering, University of Texas at Arlington, USA; Department of Computer Science and Engineering, University of Texas at Arlington, USA; Department of Computer Science and Engineering, University of Texas at Arlington, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1082062,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14751190741537283175&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;gmail.com;cse.uta.edu",
        "email": "gmail.com;gmail.com;cse.uta.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Arlington",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.uta.edu",
        "aff_unique_abbr": "UT Arlington",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Arlington",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multi-Modal Association based Grouping for Form Structure Extraction",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Aggarwal_Multi-Modal_Association_based_Grouping_for_Form_Structure_Extraction_WACV_2020_paper.html",
        "author": "Milan Aggarwal;  Mausoom Sarkar;  Hiresh Gupta;  Balaji Krishnamurthy",
        "abstract": "Document structure extraction has been a widely researched area for decades. Recent work in this direction has been deep learning-based, mostly focusing on extracting structure using fully convolution NN through semantic segmentation. In this work, we present a novel multi-modal approach for form structure extraction. Given simple elements such as textruns and widgets, we extract higher-order structures such as TextBlocks, Text Fields, Choice Fields, and Choice Groups, which are essential for information collection in forms. To achieve this, we obtain a local image patch around each low-level element (reference) by identifying candidate elements closest to it. We process textual and spatial representation of candidates sequentially through a BiLSTM to obtain context-aware representations and fuse them with image patch features obtained by processing it through a CNN. Subsequently, the sequential decoder takes this fused feature vector to predict the association type between reference and candidates. These predicted associations are utilized to determine larger structures through connected components analysis. Experimental results show the effectiveness of our approach achieving a recall of 90.29%, 73.80%, 83.12%, and 52.72% for the above structures, respectively, outperforming semantic segmentation baselines significantly. We show the efficacy of our method through ablations, comparing it against using individual modalities. We also introduce our new rich human-annotated Forms Dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Aggarwal_Multi-Modal_Association_based_Grouping_for_Form_Structure_Extraction_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1126121,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2740885127297392132&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Multi-Scale Adversarial Cross-Domain Detection with Robust Discriminative Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Pan_Multi-Scale_Adversarial_Cross-Domain_Detection_with_Robust_Discriminative_Learning_WACV_2020_paper.html",
        "author": "YoungSun Pan;  Andy J Ma;  Yuan Gao;  Jinpeng Wang;  YiQi Lin",
        "abstract": "Domain shift practically exists in almost all computer vision tasks including object detection, caused by which the performance drops evidently. Most existing methods for domain adaptation are specially designed for classification.  For object detection, existing methods separate domain shift into image-level shift and instance-level shift and align image-level feature and instance-level feature respectively. However, we find that there are two problems which remain unsolved yet. First, the scale of objects is not the same even in an image. Second, negative transfer can affect model performance if not handled properly. We improve the performance of cross-domain detection from three perspectives:   1) using multiple dilated convolution kernels with different dilation rate to reduce the image-level domain discrepancy;  2) removing images or instances with low transferability to weaken the influence of negative transfer;  3) diversifying distributions by keeping instances' feature away from each other, and then pull them closer to the center of each category, so that make source samples distribution more dispersed and more robust for cross-domain detection.  We test our model with Cityscapes, Foggy Cityscape and SIM 10K datasets, experimental results show that our method outperforms the state-of-the-art for object detection under the setting of unsupervised domain adaptation (UDA).",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Pan_Multi-Scale_Adversarial_Cross-Domain_Detection_with_Robust_Discriminative_Learning_WACV_2020_paper.pdf",
        "aff": "School of Data and Computer Science, Sun Yat-sen University, China; School of Data and Computer Science, Sun Yat-sen University, China + Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China; School of Data and Computer Science, Sun Yat-sen University, China; School of Electronics and Information Technology, Sun Yat-sen University, China; School of Data and Computer Science, Sun Yat-sen University, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1520887,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11939885767613069801&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff_domain": "mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "email": "mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "Sun Yat-sen University;Key Laboratory of Machine Intelligence and Advanced Computing",
        "aff_unique_dep": "School of Data and Computer Science;Ministry of Education",
        "aff_unique_url": "http://www.sysu.edu.cn/;",
        "aff_unique_abbr": "SYSU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-class Novelty Detection Using Mix-up Technique",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Bhattacharjee_Multi-class_Novelty_Detection_Using_Mix-up_Technique_WACV_2020_paper.html",
        "author": "Supritam Bhattacharjee;  Devraj Mandal;  Soma  Biswas",
        "abstract": "Multi-class novelty detection is increasingly becoming an important area of research due to the continuous increase in the number of object categories. It tries to answer the pertinent question: given a test sample, should we even try to classify it? We propose a novel solution using the concept of mix-up technique for novelty detection, termed as Segregation Network. During training, a pair of examples are selected from the training data and an interpolated data point using their convex combination is constructed. We develop a suitable loss function to train our model to predict its constituent classes. During testing, each input query is combined with the known class prototypes to generate mixed samples which are then passed through the trained network. Our model which is trained to reveal the constituent classes can then be used to determine whether the sample is novel or not. The intuition is that if a query comes from a known class and is mixed with the set of known class prototypes, then the prediction of the trained model for the correct class should be high. In contrast, for a query from a novel class, the predictions for all the known classes should be low. The proposed model is trained using only the available known class data and does not need access to any auxiliary dataset or attributes. Extensive experiments on two benchmark datasets, namely Caltech 256 and Stanford Dogs and comparisons with the state-of-the-art algorithms justifies the usefulness of our approach.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Bhattacharjee_Multi-class_Novelty_Detection_Using_Mix-up_Technique_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Science; Indian Institute of Science; Indian Institute of Science",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1685919,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8329315571578818874&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "email": "iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Multi-timescale Trajectory Prediction for Abnormal Human Activity Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Rodrigues_Multi-timescale_Trajectory_Prediction_for_Abnormal_Human_Activity_Detection_WACV_2020_paper.html",
        "author": "Royston Rodrigues;  Neha Bhargava;  Rajbabu Velmurugan;  Subhasis Chaudhuri",
        "abstract": "A classical approach to abnormal activity detection is to learn a representation for normal activities from the training data and then use this learned representation to detect abnormal activities while testing. Typically, the methods based on this approach operate at a fixed timescale - either a single time-instant (eg frame-based) or a constant time duration (eg video-clip based). But human abnormal activities can take place at different timescales. For example, jumping is a short-term anomaly and loitering is a long-term anomaly in a surveillance scenario. A single and pre-defined timescale is not enough to capture the wide range of anomalies occurring with different time duration. In this paper, we propose a multi-timescale model to capture the temporal dynamics at different timescales. In particular, the proposed model makes future and past predictions at different timescales for a given input pose trajectory. The model is multi-layered where intermediate layers are responsible to generate predictions corresponding to different timescales. These predictions are combined to detect abnormal activities. In addition, we also introduce a single-camera abnormal activity dataset for research use that contains 483,566 annotated frames. Our experiments show that the proposed model can capture the anomalies of different time duration and outperforms existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Rodrigues_Multi-timescale_Trajectory_Prediction_for_Abnormal_Human_Activity_Detection_WACV_2020_paper.pdf",
        "aff": "NEC Corporation, Japan + Indian Institute of Technology Bombay; Oxford Brookes University; Indian Institute of Technology Bombay; Indian Institute of Technology Bombay",
        "project": "",
        "github": "https://rodrigues-royston.github.io/Multi-timescale_Trajectory_Prediction",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3768439,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14963440715435702799&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "nec.com;gmail.com;ee.iitb.ac.in;ee.iitb.ac.in",
        "email": "nec.com;gmail.com;ee.iitb.ac.in;ee.iitb.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;1;1",
        "aff_unique_norm": "NEC Corporation;Indian Institute of Technology Bombay;Oxford Brookes University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nec.com;https://www.iitb.ac.in;https://www.oxfordbrookes.ac.uk",
        "aff_unique_abbr": "NEC;IIT Bombay;OBU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Bombay",
        "aff_country_unique_index": "0+1;2;1;1",
        "aff_country_unique": "Japan;India;United Kingdom"
    },
    {
        "title": "Multi-way Encoding for Robustness",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kim_Multi-way_Encoding_for_Robustness_WACV_2020_paper.html",
        "author": "Donghyun Kim;  Sarah Bargal;  Jianming Zhang;  Stan Sclaroff",
        "abstract": "Deep models are state-of-the-art for many computer vision tasks including image classification and object detection. However, it has been shown that deep models are vulnerable to adversarial examples. We highlight how one-hot encoding directly contributes to this vulnerability and propose breaking away from this widely-used, but highly-vulnerable mapping. We demonstrate that by leveraging a different output encoding, multi-way encoding, we decorrelate source and target models, making target models more secure. Our approach makes it more difficult for adversaries to find useful gradients for generating adversarial attacks. We present robustness for black-box and white-box attacks on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. The strength of our approach is also presented in the form of an attack for model watermarking, raising challenges in detecting stolen models.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kim_Multi-way_Encoding_for_Robustness_WACV_2020_paper.pdf",
        "aff": "Boston University; Boston University; Adobe Research; Boston University",
        "project": "http://cs-people.bu.edu/donhk/research/Multiway_encoding.html",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Kim_Multi-way_Encoding_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 775348,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6043132792724835353&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "bu.edu;bu.edu;adobe.com;bu.edu",
        "email": "bu.edu;bu.edu;adobe.com;bu.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Boston University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.bu.edu;https://research.adobe.com",
        "aff_unique_abbr": "BU;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multimodal Image Outpainting With Regularized Normalized Diversification",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhang_Multimodal_Image_Outpainting_With_Regularized_Normalized_Diversification_WACV_2020_paper.html",
        "author": "Lingzhi Zhang;  Jiancong  Wang;  Jianbo Shi",
        "abstract": "In this paper, we study the problem of generating a set of realistic and diverse backgrounds when given only a small foreground region. We refer to this task as image outpainting. The technical challenge of this task is to synthesize not only plausible but also diverse image outputs. Traditional generative adversarial networks suffer from mode collapse. While recent approaches propose to maximize or preserve the pairwise distance between generated samples with respect to their latent distance, they do not explicitly prevent the diverse samples of different conditional inputs from collapsing. Therefore, we propose a new regularization method to encourage diverse sampling in this conditional synthesis. In addition, we propose a novel feature pyramid discriminator to improve the image quality. Our experimental results show that our model can produce more diverse images without sacrificing visual quality compared to state-of-the-arts approaches in both the CelebA face dataset and the Cityscape scene dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Multimodal_Image_Outpainting_With_Regularized_Normalized_Diversification_WACV_2020_paper.pdf",
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "project": "",
        "github": "https://github.com/owenzlz/DiverseOutpaint",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3390630,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17523570504784641797&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "seas.upenn.edu;pennmedicine.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;pennmedicine.upenn.edu;seas.upenn.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multiparty Visual Co-Occurrences for Estimating Personality Traits in Group Meetings",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhang_Multiparty_Visual_Co-Occurrences_for_Estimating_Personality_Traits_in_Group_Meetings_WACV_2020_paper.html",
        "author": "Lingyu Zhang;  Indrani Bhattacharya;  Mallory Morgan;  Michael Foley;  Christoph Riedl;  Brooke Welles;  Richard Radke",
        "abstract": "Participants' body language during interactions with others in a group meeting can reveal important information about their individual personalities, as well as their contribution to a team.  Here, we focus on the automatic extraction of visual features from each person, including her/his facial activity, body movement, and hand position, and how these features co-occur among team members (e.g., how frequently a person moves her/his arms or makes eye contact when she/he is the focus of attention of the group). We correlate these features with user questionnaires to reveal relationships with the \"Big Five\" personality traits (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism), as well as with team judgements about the leader and dominant contributor in a conversation. We demonstrate that our algorithms achieve state-of-the-art accuracy with an average of 80% for Big-Five personality trait prediction, potentially enabling integration into automatic group meeting understanding systems.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Multiparty_Visual_Co-Occurrences_for_Estimating_Personality_Traits_in_Group_Meetings_WACV_2020_paper.pdf",
        "aff": "Rensselaer Polytechnic Institute; Rensselaer Polytechnic Institute; Rensselaer Polytechnic Institute; Northeastern University; Northeastern University; Northeastern University; Rensselaer Polytechnic Institute",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1364743,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14761064243074762899&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "rpi.edu;rpi.edu;rpi.edu;husky.neu.edu;northeastern.edu;northeastern.edu;ecse.rpi.edu",
        "email": "rpi.edu;rpi.edu;rpi.edu;husky.neu.edu;northeastern.edu;northeastern.edu;ecse.rpi.edu",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1;1;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute;Northeastern University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rpi.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "RPI;NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multiple Object Forecasting: Predicting Future Object Locations in Diverse Environments",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Styles_Multiple_Object_Forecasting_Predicting_Future_Object_Locations_in_Diverse_Environments_WACV_2020_paper.html",
        "author": "Oliver Styles;  Victor Sanchez;  Tanaya Guha",
        "abstract": "This paper introduces the problem of multiple object forecasting (MOF), in which the goal is to predict future bounding boxes of tracked objects. In contrast to existing works on object trajectory forecasting which primarily consider the problem from a birds-eye perspective, we formulate the problem from an object-level perspective and call for the prediction of full object bounding boxes, rather than trajectories alone. Towards solving this task, we introduce the Citywalks dataset, which consists of over 200k high-resolution video frames. Citywalks comprises of footage recorded in 21 cities from 10 European countries in a variety of weather conditions and over 3.5k unique pedestrian trajectories. For evaluation, we adapt existing trajectory forecasting methods for MOF and confirm cross-dataset generalizability on the MOT-17 dataset without fine-tuning.  Finally,  we present STED, a novel encoder-decoder architecture for MOF. STED combines visual and temporal features to model both object-motion and ego-motion, and outperforms existing approaches for MOF. Code & dataset link: https://github.com/olly-styles/Multiple-Object-Forecasting",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Styles_Multiple_Object_Forecasting_Predicting_Future_Object_Locations_in_Diverse_Environments_WACV_2020_paper.pdf",
        "aff": "University of Warwick; University of Warwick; University of Warwick",
        "project": "",
        "github": "https://github.com/olly-styles/Multiple-Object-Forecasting",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2206070,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12797740622584389626&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "warwick.ac.uk;warwick.ac.uk;warwick.ac.uk",
        "email": "warwick.ac.uk;warwick.ac.uk;warwick.ac.uk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Warwick",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.warwick.ac.uk",
        "aff_unique_abbr": "Warwick",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Multiview Co-segmentation for Wide Baseline Images using Cross-view Supervision",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Yao_Multiview_Co-segmentation_for_Wide_Baseline_Images_using_Cross-view_Supervision_WACV_2020_paper.html",
        "author": "Yuan Yao;  Hyun Soo Park",
        "abstract": "This paper presents a method to co-segment an object from wide baseline multiview images using cross-view self-supervision. A key challenge in the wide baseline images lies in the fragility of photometric matching. Inspired by shape-from-silhouette that does not require photometric matching, we formulate a new theory of shape belief transfer---the segmentation belief in one image can be used to predict that of the other image through epipolar geometry. This formulation is differentiable, and therefore, an end-to-end training is possible. We analyze the shape belief transfer to identify the theoretical upper and lower bounds of the unlabeled data segmentation, which characterizes the degenerate cases of co-segmentation. We design a novel triple network that embeds this shape belief transfer, which is agnostic to visual appearance and baseline. The resulting network is validated by recognizing a target object from realworld visual data including non-human species and a subject of interest in social videos where attaining large-scale annotated data is challenging.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Yao_Multiview_Co-segmentation_for_Wide_Baseline_Images_using_Cross-view_Supervision_WACV_2020_paper.pdf",
        "aff": "University of Minnesota; University of Minnesota",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Yao_Multiview_Co-segmentation_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1664827,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2778692645505878428&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "umn.edu;umn.edu",
        "email": "umn.edu;umn.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.minnesota.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multiview Supervision By Registration",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhang_Multiview_Supervision_By_Registration_WACV_2020_paper.html",
        "author": "Yilun Zhang;  Hyun Soo Park",
        "abstract": "This paper presents a semi-supervised learning framework to train a keypoint detector using multiview image streams given the limited labeled data (typically <4%). We leverage the complementary relationship between multiview geometry and visual tracking to provide three types of supervisionary signals to utilize the unlabeled data: (1) keypoint detection in one view can be supervised by other views via the epipolar geometry; (2) a keypoint moves smoothly over time where its optical flow can be used to temporally supervise consecutive image frames to each other; (3) visible keypoint in one view is likely to be visible in the adjacent view. We integrate these three signals in a differentiable fashion to design a new end-to-end neural network composed of three pathways. This design allows us to extensively use the unlabeled data to train the keypoint detector. We show that our approach outperforms existing detectors including DeepLabCut tailored to the keypoint detection of non-human species such as monkeys, dogs, and mice.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Multiview_Supervision_By_Registration_WACV_2020_paper.pdf",
        "aff": "University of Pennsylvania; University of Minnesota",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3003634,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5255809954392953675&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "seas.upenn.edu;umn.edu",
        "email": "seas.upenn.edu;umn.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Pennsylvania;University of Minnesota",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;https://www.minnesota.edu",
        "aff_unique_abbr": "UPenn;UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Munich to Dubai: How far is it for Semantic Segmentation?",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Nandan_Munich_to_Dubai_How_far_is_it_for_Semantic_Segmentation_WACV_2020_paper.html",
        "author": "Shyam Nandan Rai;  Vineeth N Balasubramanian;  Anbumani Subramanian;  C.V. Jawahar",
        "abstract": "Cities having hot weather conditions results in geometrical distortion, thereby adversely affecting the performance of semantic segmentation model. In this work, we study the problem of semantic segmentation model in adapting to such hot climate cities. This issue can be circumvented by collecting and annotating images in such weather conditions and training segmentation models on those images. But the task of semantically annotating images for every environment is painstaking and expensive.  Hence, we propose a framework that improves the performance of semantic segmentation models without explicitly creating an annotated dataset for such adverse weather variations. Our framework consists of two parts, a restoration network to remove the geometrical distortions caused by hot weather and an adaptive segmentation network that is trained on an additional loss to adapt to the statistics of the ground-truth segmentation map. We train our framework on the Cityscapes dataset, which showed a total IoU gain of 12.707 over standard segmentation models. We also observe that the segmentation results obtained by our framework gave a significant improvement for small classes such as poles, person, and rider, which are essential and valuable for autonomous navigation based applications.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Nandan_Munich_to_Dubai_How_far_is_it_for_Semantic_Segmentation_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Nandan_Munich_to_Dubai_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 24688907,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13011122678408418999&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "NRMVS: Non-Rigid Multi-view Stereo",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Innmann_NRMVS_Non-Rigid_Multi-view_Stereo_WACV_2020_paper.html",
        "author": "Matthias Innmann;  Kihwan Kim;  Jinwei Gu;  Matthias Niessner;  Charles Loop;  Marc Stamminger;  Jan Kautz",
        "abstract": "Multi-view Stereo (MVS) is a common solution in photogrammetry applications for the dense reconstruction of a static scene from images. The static scene assumption, however, limits the general applicability of MVS algorithms, as many day-to-day scenes undergo non-rigid motion, e.g., clothes, faces, or human bodies. In this paper, we open up a new challenging direction: Dense 3D reconstruction of scenes with non-rigid changes observed from a small number of images sparsely captured from different views with a single monocular camera, which we call non-rigid multi-view stereo (NRMVS) problem. We formulate this problem as a joint optimization of deformation and depth estimation, using deformation graphs as the underlying representation. We propose a new sparse 3D to 2D matching technique with a dense patch-match evaluation scheme to estimate the most plausible deformation field satisfying depth and photometric consistency. We show that a dense reconstruction of a scene with non-rigid changes from a few images is possible, and demonstrate that our method can be used to interpolate novel deformed scenes from various combinations of deformation estimates derived from the sparse views.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Innmann_NRMVS_Non-Rigid_Multi-view_Stereo_WACV_2020_paper.pdf",
        "aff": "NVIDIA + Friedrich-Alexander University Erlangen-N\u00fcrnberg; NVIDIA; NVIDIA + SenseTime; Technical University of Munich; NVIDIA; Friedrich-Alexander University Erlangen-N\u00fcrnberg; NVIDIA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Innmann_NRMVS_Non-Rigid_Multi-view_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2520792,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3574104493331156618&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "in.tum.de;nvidia.com;nvidia.com;in.tum.de;nvidia.com;informatik.uni-erlangen.de;nvidia.com",
        "email": "in.tum.de;nvidia.com;nvidia.com;in.tum.de;nvidia.com;informatik.uni-erlangen.de;nvidia.com",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0+2;3;0;1;0",
        "aff_unique_norm": "NVIDIA Corporation;Friedrich-Alexander University Erlangen-N\u00fcrnberg;SenseTime;Technical University of Munich",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.nvidia.com;https://www fau.de;https://www.sensetime.com;https://www.tum.de",
        "aff_unique_abbr": "NVIDIA;FAU;SenseTime;TUM",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0+2;1;0;1;0",
        "aff_country_unique": "United States;Germany;China"
    },
    {
        "title": "NeurReg: Neural Registration and Its Application to Image Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhu_NeurReg_Neural_Registration_and_Its_Application_to_Image_Segmentation_WACV_2020_paper.html",
        "author": "Wentao Zhu;  Andriy Myronenko;  Ziyue Xu;  Wenqi Li;  Holger Roth;  Yufang Huang;  Fausto Milletari;  Daguang Xu",
        "abstract": "Registration is a fundamental task in medical image analysis which can be applied to several tasks including image segmentation, intra-operative tracking, multi-modal image alignment, and motion analysis. Popular registration tools such as ANTs and NiftyReg optimize an objective function for each pair of images from scratch which is time-consuming for large images with complicated deformation. Facilitated by the rapid progress of deep learning, learning-based approaches such as VoxelMorph have been emerging for image registration. These approaches can achieve competitive performance in a fraction of a second on advanced GPUs. In this work, we construct a neural registration framework, called NeurReg, with a hybrid loss of displacement fields and data similarity, which substantially improves the current state-of-the-art of registrations. Within the framework, we simulate various transformations by a registration simulator which generates fixed image and displacement field ground truth for training. Furthermore, we design three segmentation frameworks based on the proposed registration framework: 1) atlas-based segmentation, 2) joint learning of both segmentation and registration tasks, and 3) multi-task learning with atlas-based segmentation as an intermediate feature. Extensive experimental results validate the effectiveness of the proposed NeurReg framework based on various metrics: the endpoint error (EPE) of the predicted displacement field, mean square error (MSE), normalized local cross-correlation (NLCC), mutual information (MI), Dice coefficient, uncertainty estimation, and the interpretability of the segmentation. The proposed NeurReg improves registration accuracy with fast inference speed, which can greatly accelerate related medical image analysis tasks.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhu_NeurReg_Neural_Registration_and_Its_Application_to_Image_Segmentation_WACV_2020_paper.pdf",
        "aff": "NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; Cornell University; NVIDIA; NVIDIA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5720371,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8061006942321673462&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com;gmail.com;nvidia.com;nvidia.com",
        "email": "nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com;gmail.com;nvidia.com;nvidia.com",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1;0;0",
        "aff_unique_norm": "NVIDIA Corporation;Cornell University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nvidia.com;https://www.cornell.edu",
        "aff_unique_abbr": "NVIDIA;Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Neural Puppet: Generative Layered Cartoon Characters",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Poursaeed_Neural_Puppet_Generative_Layered_Cartoon_Characters_WACV_2020_paper.html",
        "author": "Omid Poursaeed;  Vladimir Kim;  Eli Shechtman;  Jun Saito;  Serge Belongie",
        "abstract": "We propose a learning based method for generating new animations of a cartoon character given a few example images. Our method is designed to learn from a traditional animation, where each frame is drawn by an artist, and thus the input images lack any common structure, correspondences, or labels. We express pose changes as a deformation of a layered 2.5D template mesh, and devise a novel architecture that learns to predict mesh deformations matching the template to a target image. This enables us to extract a common low-dimensional structure in the diverse set of character poses. We combine recent advances in differentiable rendering as well as mesh-aware models to successfully align common template even if only a few character images are available during training. In addition to coarse poses, character appearance also varies due to shading, out-of-plane motions, and artistic effects. We capture these subtle changes by applying an image translation network to refine the mesh rendering, providing an end-to-end model to generate new animations of a character with high visual quality. We demonstrate that our generative model can be used to synthesize in-between frames and to create data-driven deformation. Our template fitting procedure outperforms state-of-the-art generic techniques for detecting image correspondences.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Poursaeed_Neural_Puppet_Generative_Layered_Cartoon_Characters_WACV_2020_paper.pdf",
        "aff": "Cornell University+Cornell Tech; Adobe Research; Adobe Research; Adobe Research; Cornell University+Cornell Tech",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Poursaeed_Neural_Puppet_Generative_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1463274,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5791566981136050328&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;1;1;1;0+0",
        "aff_unique_norm": "Cornell University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.cornell.edu;https://research.adobe.com",
        "aff_unique_abbr": "Cornell;Adobe",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";New York City",
        "aff_country_unique_index": "0+0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Neural Sign Language Synthesis: Words Are Our Glosses",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zelinka_Neural_Sign_Language_Synthesis_Words_Are_Our_Glosses_WACV_2020_paper.html",
        "author": "Jan Zelinka;  Jakub Kanis",
        "abstract": "This paper deals with a text-to-video sign language synthesis. Instead of direct video production, we focused on skeletal models production. Our main goal in this paper was to design the first fully end-to-end automatic sign language synthesis system trained only on available free data (daily TV broadcasting). Thus, we excluded any manual video annotation. Furthermore, our designed approach even do not rely on any video segmentation. A proposed feed-forward transformer and recurrent transformer were investigated. To improve the performance of our sequence-to-sequence transformer, soft non-monotonic attention was employed in our training process. A benefit of character-level features was compared with word-level features. Besides a novel approach to sign language synthesis, we also present a gradient-descend-based method for the skeletal model estimation improvement. This improvement not only smooths skeletal models and interpolates missing bones but it also creates 3D skeletal models from 2D models. We focused our experiments on a weather forecasting dataset in the Czech Sign Language.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zelinka_Neural_Sign_Language_Synthesis_Words_Are_Our_Glosses_WACV_2020_paper.pdf",
        "aff": "University of West Bohemia, Faculty of Applied Sciences; University of West Bohemia, Faculty of Applied Sciences",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Zelinka_Neural_Sign_Language_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 631274,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=734208188237876224&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ntis.zcu.cz;ntis.zcu.cz",
        "email": "ntis.zcu.cz;ntis.zcu.cz",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of West Bohemia",
        "aff_unique_dep": "Faculty of Applied Sciences",
        "aff_unique_url": "https://www.zcu.cz",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Czech Republic"
    },
    {
        "title": "Non-Rigid Structure from Motion: Prior-Free Factorization Method Revisited",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kumar_Non-Rigid_Structure_from_Motion_Prior-Free_Factorization_Method_Revisited_WACV_2020_paper.html",
        "author": "Suryansh Kumar",
        "abstract": "A simple prior free factorization algorithm [??] is quite often cited work in the field of Non-Rigid Structure from Motion (NRSfM). The benefit of this work lies in its simplicity of implementation, strong theoretical justification to the motion and structure estimation, and its invincible originality.  Despite this, the prevailing view is, that it performs exceedingly inferior to other methods on several benchmark datasets [??]. However, our subtle investigation provides some empirical statistics which made us think against such views. The statistical results we obtained supersedes Dai  \\it et al.  [??] originally reported results on the benchmark datasets by a significant margin under some elementary changes in their core algorithmic idea [??]. Now, these results not only exposes some unrevealed areas for research in NRSfM but also give rise to new mathematical challenges for NRSfM researchers. We argue that by properly utilizing the well-established assumptions about a non-rigidly deforming shape i.e, it deforms smoothly over frames [??] and it spans a low-rank space, the simple prior-free idea can provide results which is comparable to the best available algorithms. In this paper, we explore some of the hidden intricacies missed by Dai  \\it et. al.   work [??] and how some elementary measures and modifications can enhance its performance, as high as approx. 18% on the benchmark dataset. The improved performance is justified and empirically verified by extensive experiments on several datasets. We believe our work has both practical and theoretical importance for the development of better NRSfM algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kumar_Non-Rigid_Structure_from_Motion_Prior-Free_Factorization_Method_Revisited_WACV_2020_paper.pdf",
        "aff": "Computer Vision Lab, ETH Z\u00fcrich, Switzerland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Kumar_Non-Rigid_Structure_from_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 12869990,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11001434704464701607&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "ETH Z\u00fcrich",
        "aff_unique_dep": "Computer Vision Lab",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Z\u00fcrich",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Nonparametric Structure Regularization Machine for 2D Hand Pose Estimation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_Nonparametric_Structure_Regularization_Machine_for_2D_Hand_Pose_Estimation_WACV_2020_paper.html",
        "author": "Yifei Chen;  Haoyu Ma;  Deying Kong;  Xiangyi Yan;  Jianbao Wu;  Wei Fan;  Xiaohui Xie",
        "abstract": "Hand pose estimation is more challenging than body pose estimation due to severe articulation, self-occlusion and high dexterity of the hand. Current approaches often rely on a popular body pose algorithm, such as the Convolutional Pose Machine (CPM), to learn 2D keypoint features. These algorithms cannot adequately address the unique challenges of hand pose estimation, because they are trained solely based on keypoint positions without seeking to explicitly model structural relationship between them. We propose a novel Nonparametric Structure Regularization Machine (NSRM) for 2D hand pose estimation, adopting a cascade multi-task architecture to learn hand structure and keypoint representations jointly. The structure learning is guided by synthetic hand mask representations, which are directly computed from keypoint positions, and is further strengthened by a novel probabilistic representation of hand limbs and an anatomically inspired composition strategy of mask synthesis. We conduct extensive studies on two public datasets - OneHand 10k and CMU Panoptic Hand. Experimental results demonstrate that explicitly enforcing structure learning consistently improves pose estimation accuracy of CPM baseline models, by 1.17% on the first dataset and 4.01% on the second one. The implementation and experiment code is freely available online. Our proposal of incorporating structural learning to hand pose estimation requires no additional training information, and can be a generic add-on module to other pose estimation models.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Nonparametric_Structure_Regularization_Machine_for_2D_Hand_Pose_Estimation_WACV_2020_paper.pdf",
        "aff": "Tencent Hippocrates Research Lab; Department of Computer Science, University of California at Irvine; Department of Computer Science, University of California at Irvine; Department of Computer Science, University of California at Irvine; Tencent Hippocrates Research Lab; Tencent Hippocrates Research Lab; Department of Computer Science, University of California at Irvine",
        "project": "",
        "github": "https://github.com/HowieMa/NSRMhand",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1623448,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5480582195684306489&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tencent.com;uci.edu;uci.edu;uci.edu;tencent.com;tencent.com;uci.edu",
        "email": "tencent.com;uci.edu;uci.edu;uci.edu;tencent.com;tencent.com;uci.edu",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0;0;1",
        "aff_unique_norm": "Tencent;University of California, Irvine",
        "aff_unique_dep": "Hippocrates Research Lab;Department of Computer Science",
        "aff_unique_url": "https://www.tencent.com;https://www.uci.edu",
        "aff_unique_abbr": "Tencent;UCI",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Irvine",
        "aff_country_unique_index": "0;1;1;1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Offset Calibration for Appearance-Based Gaze Estimation via Gaze Decomposition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_Offset_Calibration_for_Appearance-Based_Gaze_Estimation_via_Gaze_Decomposition_WACV_2020_paper.html",
        "author": "Zhaokang Chen;  Bertram Shi",
        "abstract": "Appearance-based gaze estimation provides relatively unconstrained gaze tracking. However, subject-independent models achieve limited accuracy partly due to individual variations. To improve estimation, we propose a gaze decomposition method that enables low complexity calibration, i.e., using calibration data collected when subjects view only one or a few gaze targets and the number of images per gaze target is small. Lowering the complexity of calibration makes it more convenient and less time-consuming for the user, and more widely applicable. Motivated by our finding that the inter-subject squared bias exceeds the intra-subject variance for a subject-independent estimator, we decompose the gaze estimate into the sum of a subject-independent term estimated from the input image by a deep convolutional network and a subject-dependent bias term. During training, both the weights of the deep network and the bias terms are estimated. During testing, if no calibration data is available, we can set the bias term to zero. Otherwise, the bias term can be estimated from images of the subject gazing at known gaze targets. Experimental results on three datasets show that without calibration, our method outperforms state-of-the-art by at least 6.3%. For low complexity calibration sets, our method outperforms other calibration methods. More complex calibration algorithms do not outperform our method until the size of the calibration set is excessively large. Even then, the gains obtained by alternatives are small, e.g., only 0.1 degree lower error for 64 gaze targets. Source code is available at https://github.com/czk32611/Gaze-Decomposition.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Offset_Calibration_for_Appearance-Based_Gaze_Estimation_via_Gaze_Decomposition_WACV_2020_paper.pdf",
        "aff": "The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology",
        "project": "",
        "github": "https://github.com/czk32611/Gaze-Decomposition",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Chen_Offset_Calibration_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3350231,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17460888168922992643&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "connect.ust.hk;ust.hk",
        "email": "connect.ust.hk;ust.hk",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "On Hallucinating Context and Background Pixels from a Face Mask using Multi-scale GANs",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Banerjee_On_Hallucinating_Context_and_Background_Pixels_from_a_Face_Mask_WACV_2020_paper.html",
        "author": "Sandipan Banerjee;  Walter Scheirer;  Kevin Bowyer;  Patrick Flynn",
        "abstract": "We propose a multi-scale GAN model to hallucinate realistic context (forehead, hair, neck, clothes) and background pixels automatically from a single input face mask, without any user supervision. Instead of swapping a face on to an existing picture, our model directly generates realistic context and background pixels based on the features of the provided face mask. Unlike facial inpainting algorithms, it can generate realistic hallucinations even for a large number of missing pixels. Our model is composed of a cascaded network of GAN blocks, each tasked with hallucination of missing pixels at a particular resolution while guiding the synthesis process of the next GAN block. The hallucinated full face image is made photo realistic by using a combination of reconstruction, perceptual, adversarial and identity preserving losses at each block of the network. With a set of extensive experiments, we demonstrate the effectiveness of our model in hallucinating context and background pixels from face masks varying in facial pose, expression and lighting, collected from multiple datasets subject disjoint with our training data. We also compare our method with popular face inpainting and face swapping models in terms of visual quality, realism and identity preservation. Additionally, we analyze our cascaded pipeline and compare it with the progressive growing of GANs, and explore its usage as a data augmentation module for training CNNs.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Banerjee_On_Hallucinating_Context_and_Background_Pixels_from_a_Face_Mask_WACV_2020_paper.pdf",
        "aff": "Affectiva, USA; Department of Computer Science & Engineering, University of Notre Dame, USA; Department of Computer Science & Engineering, University of Notre Dame, USA; Department of Computer Science & Engineering, University of Notre Dame, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Banerjee_On_Hallucinating_Context_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 615715,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2678315792563273448&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "affectiva.com;nd.edu;nd.edu;nd.edu",
        "email": "affectiva.com;nd.edu;nd.edu;nd.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Affectiva;University of Notre Dame",
        "aff_unique_dep": ";Department of Computer Science & Engineering",
        "aff_unique_url": "https://www.affectiva.com;https://www.nd.edu",
        "aff_unique_abbr": ";Notre Dame",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On Scene Flow Computation of Gas Structures with Optical Gas Imaging Cameras",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Rangel_On_Scene_Flow_Computation_of_Gas_Structures__with_Optical_WACV_2020_paper.html",
        "author": "Johannes Rangel;  Robert Schmoll;  Andreas Kroll",
        "abstract": "Gas leak inspection and gas leak quantification are nowadays of high relevance within the oil and gas industry as well as in many other industrial sectors. This has been driven by safety-related issues, economic losses and the considerable climate impact caused by such unwanted gas releases. Due to the latter, the efforts for developing new and more reliable measurement techniques for detecting and quantifying greenhouse gases such as methane have increased in the recent years. In this work, a stereo camera system based on optical gas imaging cameras is used for computing dense 3D velocity information, i.e. scene flow, of escaping gas structures. Here, the optical flow, the disparity and the disparity change in likely gas image regions are computed utilizing classical variational methods. The accuracy of the applied methods and their applicability under real conditions in a biogas plant are characterized and tested. The results show that the recovered 3D gas velocity field per camera frame approaches the average 3D velocity field of the measured gas structure. The accuracy of the used method is affected, among others, when the imaged gas structures exhibit a low contrast.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Rangel_On_Scene_Flow_Computation_of_Gas_Structures__with_Optical_WACV_2020_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Rangel_On_Scene_Flow_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4492630,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5590566203192677145&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "One-to-one Mapping for Unpaired Image-to-image Translation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Shen_One-to-one_Mapping_for_Unpaired_Image-to-image_Translation_WACV_2020_paper.html",
        "author": "Zengming Shen;  S. Kevin Zhou;  Yifan Chen;  Bogdan Georgescu;  Xuqi Liu;  Thomas Huang",
        "abstract": "Recently image-to-image translation has attracted significant interests in the literature, starting from the successful use of the generative adversarial network (GAN), to the introduction of cyclic constraint, to extensions to multiple domains. However, in existing approaches, there is no guarantee that the mapping between two image domains is unique or one-to-one. Here we propose a self-inverse network learning approach for unpaired image-to-image translation. Building on top of CycleGAN, we learn a self-inverse function by simply augmenting the training samples by switching inputs and outputs during training. The outcome of such learning is a proven one-to-one mapping function. Our extensive experiments on a variety of detests, including cross-modal medical image synthesis, object transfiguration, and semantic labeling, consistently demonstrate clear improvement over the CycleGAN method both qualitatively and quantitatively. Especially our proposed method reaches the state-of-the-art result on the label to photo direction of the cityscapes benchmark dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Shen_One-to-one_Mapping_for_Unpaired_Image-to-image_Translation_WACV_2020_paper.pdf",
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Institute of Computing Technology, Chinese Academy of Sciences; Siemens Healthineers; Rutgers University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1148999,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=447894448804496615&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;ict.ac.cn;siemens-healthineers.com;scarletmail.rutgers.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;ict.ac.cn;siemens-healthineers.com;scarletmail.rutgers.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2;3",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Chinese Academy of Sciences;Siemens Healthineers;Rutgers University",
        "aff_unique_dep": ";Institute of Computing Technology;;",
        "aff_unique_url": "https://illinois.edu;http://www.ict.ac.cn;https://www.siemens-healthineers.com;https://www.rutgers.edu",
        "aff_unique_abbr": "UIUC;CAS;Siemens Healthineers;Rutgers",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;1;2;0",
        "aff_country_unique": "United States;China;Germany"
    },
    {
        "title": "Online Lens Motion Smoothing for Video Autofocus",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Abuolaim_Online_Lens_Motion_Smoothing_for_Video_Autofocus_WACV_2020_paper.html",
        "author": "Abdullah Abuolaim;  Michael Brown",
        "abstract": "Autofocus (AF) is the process of moving the camera's lens such that desired scene content is in focus.   AF for single image capture is a well-studied research topic and most modern cameras have hardware support that allows quick lens movements to optimize image sharpness.  How to best perform AF for video is less clear.  Conventional wisdom would suggest that each temporal frame should be as sharp as possible. However, unlike single image capture, the effects of the lens movement is visible in the captured video.  As a result, there are two parameters to consider in AF for video: sharpness and lens movement.  In this paper, we show that users preferred videos with smooth lens movement, even if it results in less overall sharpness.   Based on this observation, we propose two novel AF algorithms for video that strive for both smooth lens movement and sharp scene content.  Specifically, we introduce (1) a bidirectional long short-term memory (BLSTM) module trained on smooth lens trajectories and (2) a simple weighted moving average (WMA) method that factors in prior lens motion. Both of these methods have demonstrated excellent results in terms of reducing lens movements (up to 64% reduction) without greatly affecting the sharpness (less than 5.2% change in sharpness).  Moreover, videos produced using our methods are more preferred by users over conventional AF that aims only for maximizing sharpness.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Abuolaim_Online_Lens_Motion_Smoothing_for_Video_Autofocus_WACV_2020_paper.pdf",
        "aff": "York University, Toronto; York University, Toronto + Samsung AI Center, Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5725555,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4144118178909197701&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "eecs.yorku.ca;eecs.yorku.ca",
        "email": "eecs.yorku.ca;eecs.yorku.ca",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "York University;Samsung AI Center",
        "aff_unique_dep": ";AI Center",
        "aff_unique_url": "https://yorku.ca;https://www.samsung.com/global/innovation/ai-research/",
        "aff_unique_abbr": "York U;SAC",
        "aff_campus_unique_index": "0;0+0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Optimizing Through Learned Errors for Accurate Sports Field Registration",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Jiang_Optimizing_Through_Learned_Errors_for_Accurate_Sports_Field_Registration_WACV_2020_paper.html",
        "author": "Wei Jiang;  Juan Camilo Gamboa Higuera;  Baptiste Angles;  Weiwei Sun;  Mehrsan Javan;  Kwang Moo Yi",
        "abstract": "We propose an optimization-based framework to register sports field templates onto broadcast videos. For accurate registration we go beyond the prevalent feed-forward paradigm. Instead, we propose to train a deep network that regresses the registration error, and then register images by finding the registration parameters that minimize the regressed error. We demonstrate the effectiveness of our method by applying it to real-world sports broadcast videos, outperforming the state of the art. We further apply our method on a synthetic toy example and demonstrate that our method brings significant gains even when the problem is simplified and unlimited training data is available.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Jiang_Optimizing_Through_Learned_Errors_for_Accurate_Sports_Field_Registration_WACV_2020_paper.pdf",
        "aff": "Visual Computing Group, University of Victoria; McGill University; Visual Computing Group, University of Victoria; Visual Computing Group, University of Victoria; SPORTLOGiQ Inc.; Visual Computing Group, University of Victoria",
        "project": "",
        "github": "https://github.com/vcg-uvic/sportsfield_release",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2247470,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8452547834405974182&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "uvic.ca;cim.mcgill.ca;uvic.ca;uvic.ca;sportlogiq.com;uvic.ca",
        "email": "uvic.ca;cim.mcgill.ca;uvic.ca;uvic.ca;sportlogiq.com;uvic.ca",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;2;0",
        "aff_unique_norm": "University of Victoria;McGill University;SPORTLOGiQ",
        "aff_unique_dep": "Visual Computing Group;;",
        "aff_unique_url": "https://www.uvic.ca;https://www.mcgill.ca;https://www.sportlogiq.com",
        "aff_unique_abbr": "UVic;McGill;SPORTLOGiQ",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Victoria;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Overlap Sampler for Region-Based Object Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_Overlap_Sampler_for_Region-Based_Object_Detection_WACV_2020_paper.html",
        "author": "Joya Chen;  Bin Luo;  Qi Wu;  Jia Chen;  Xuezheng Peng",
        "abstract": "The top accuracy of object detection to date is led by region-based approaches, where the per-region stage is responsible for recognizing proposals generated by the region proposal network. In that stage, sampling heuristics (e.g., OHEM, IoU-balanced sampling) is always applied to select a part of examples during training. But nowadays, existing samplers ignore the overlaps among examples, which may result in some low-quality predictions preserved. To mitigate the issue, we propose Overlap Sampler that selects examples according to the overlaps among examples, which enables the training to focus on the important examples. Benefitted from it, the Faster R-CNN could obtain impressively 1.5 points higher Average Precision (AP) on the challenging COCO benchmark, a state-of-the-art result among existing samplers for region-based detectors. Moreover, the proposed sampler also yields considerable improvements for the instance segmentation task. Our code is released at https://github.com/ChenJoya/overlap-sampler.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Overlap_Sampler_for_Region-Based_Object_Detection_WACV_2020_paper.pdf",
        "aff": "Tencent+University of Science and Technology of China; Tencent; Institute of Intelligent Machines, Chinese Academy of Sciences; South China University of Technology; Tencent",
        "project": "",
        "github": "https://github.com/ChenJoya/overlap-sampler",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1416433,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6863179466112582941&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "tencent.com; ; ; ;tencent.com",
        "email": "tencent.com; ; ; ;tencent.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;2;3;0",
        "aff_unique_norm": "Tencent Holdings Limited;University of Science and Technology of China;Chinese Academy of Sciences;South China University of Technology",
        "aff_unique_dep": ";;Institute of Intelligent Machines;",
        "aff_unique_url": "https://www.tencent.com;http://www.ustc.edu.cn;http://www.cas.cn;https://www.scut.edu.cn",
        "aff_unique_abbr": "Tencent;USTC;CAS;SCUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "PSNet: A Style Transfer Network for Point Cloud Stylization on Geometry and Color",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Cao_PSNet_A_Style_Transfer_Network_for_Point_Cloud_Stylization_on_WACV_2020_paper.html",
        "author": "Xu Cao;  Weimin Wang;  Katashi Nagao;  Ryosuke Nakamura",
        "abstract": "We propose a neural style transfer method for colored point clouds which allows stylizing the geometry and/or color property of a point cloud from another. The stylization is achieved by manipulating the content representations and Gram-based style representations extracted from a pre-trained PointNet-based classification network for colored point clouds.  As Gram-based style representation is invariant to the number or the order of points, the style can also be an image in the case of stylizing the color property of a point cloud by merely treating the image as a set of pixels.  Experimental results and analysis demonstrate the capability of the proposed method for stylizing a point cloud either from another point cloud or an image.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Cao_PSNet_A_Style_Transfer_Network_for_Point_Cloud_Stylization_on_WACV_2020_paper.pdf",
        "aff": "Graduate School of Informatics, Nagoya University, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; Graduate School of Informatics, Nagoya University, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1761327,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6432403548116958753&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nagao.nuie.nagoya-u.ac.jp;aist.go.jp;aist.go.jp;nuie.nagoya-u.ac.jp",
        "email": "nagao.nuie.nagoya-u.ac.jp;aist.go.jp;aist.go.jp;nuie.nagoya-u.ac.jp",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Nagoya University;National Institute of Advanced Industrial Science and Technology",
        "aff_unique_dep": "Graduate School of Informatics;",
        "aff_unique_url": "https://www.nagoya-u.ac.jp;https://www.aist.go.jp",
        "aff_unique_abbr": "Nagoya U;AIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Partially Zero-shot Domain Adaptation from Incomplete Target Data with Missing Classes",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ishii_Partially_Zero-shot_Domain_Adaptation_from_Incomplete_Target_Data_with_Missing_WACV_2020_paper.html",
        "author": "Masato Ishii;  Takashi Takenouchi;  Masashi Sugiyama",
        "abstract": "We tackle a domain adaptation problem under partially zero-shot setting. In this setting, a certain subset of classes is missing in the unlabeled target data, while all classes appear in the labeled source data, and the goal is to discriminate all classes at the target domain. To solve this problem, we utilize an adversarial training scheme and adopt instance weighting to estimate the loss related to unavailable target data in the missing classes. The instance weight is computed on the basis of the prediction of deep neural networks, implying which instance would be similar to unseen data and having useful information for the loss estimation. This estimation makes it possible to explicitly consider all classes during the domain adaptation training even in the partially zero-shot setting, which leads to accurate adaptation between domains. Experimental results with several benchmark datasets validate the advantage of our method",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ishii_Partially_Zero-shot_Domain_Adaptation_from_Incomplete_Target_Data_with_Missing_WACV_2020_paper.pdf",
        "aff": "The University of Tokyo + RIKEN + NEC; RIKEN + Future University of Hakodate; The University of Tokyo + RIKEN",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 750739,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18266163818419509272&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;fun.ac.jp;k.u-tokyo.ac.jp",
        "email": "gmail.com;fun.ac.jp;k.u-tokyo.ac.jp",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;1+3;0+1",
        "aff_unique_norm": "University of Tokyo;RIKEN;NEC Corporation;Future University of Hakodate",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.riken.jp;https://www.nec.com;http://www.fu-hakodate.ac.jp/",
        "aff_unique_abbr": "UTokyo;RIKEN;NEC;FUH",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Periphery-Fovea Multi-Resolution Driving Model Guided by Human Attention",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xia_Periphery-Fovea_Multi-Resolution_Driving_Model_Guided_by_Human_Attention_WACV_2020_paper.html",
        "author": "Ye Xia;  Jinkyu Kim;  John Canny;  Karl Zipser;  Teresa Canas-Bajo;  David Whitney",
        "abstract": "Inspired by human vision, we propose a new periphery-fovea multi-resolution driving model that predicts vehicle speed from dash camera videos. The peripheral vision module of the model processes the full video frames in low resolution with large receptive fields. Its foveal vision module selects sub-regions and uses high-resolution input from those regions to improve its driving performance. We train the fovea selection module with supervision from driver gaze. We show that adding high-resolution input from predicted human driver gaze locations significantly improves the driving accuracy of the model. Our periphery-fovea multi-resolution model outperforms a uni-resolution periphery-only model that has the same amount of floating-point operations. More importantly, we demonstrate that our driving model achieves a significantly higher performance gain in pedestrian-involved critical situations than in other non-critical situations.  Our code is publicly available at https://github.com/pascalxia/periphery_fovea_driving.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xia_Periphery-Fovea_Multi-Resolution_Driving_Model_Guided_by_Human_Attention_WACV_2020_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "project": "",
        "github": "https://github.com/pascalxia/periphery_fovea_driving",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Xia_Periphery-Fovea_Multi-Resolution_Driving_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1483664,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8402054222987897882&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 9,
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Personalizing Fast-Forward Videos Based on Visual and Textual Features from Social Network",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ramos_Personalizing_Fast-Forward_Videos_Based_on_Visual_and_Textual_Features_from_WACV_2020_paper.html",
        "author": "Washington Ramos;  Michel Silva;  Edson Araujo;  Alan Neves;  Erickson Nascimento",
        "abstract": "The growth of Social Networks has fueled the habit of people logging their day-to-day activities, and long First-Person Videos (FPVs) are one of the main tools in this new habit. Semantic-aware fast-forward methods are able to decrease the watch time and select meaningful moments, which is key to increase the chances of these videos being watched. However, these methods can not handle semantics in terms of personalization. In this paper, we present a new approach to automatically creating personalized fast-forward videos for FPVs. Our approach explores the availability of text-centric data from the user's social networks such as status updates to infer her/his topics of interest and assigns scores to the input frames according to her/his preferences. Extensive experiments are conducted on three different datasets with simulated and real-world users as input. Our method achieved an average F1 score of up to 12.8 percentage points higher than the best competitors. We also present a user study to demonstrate the effectiveness of our method.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ramos_Personalizing_Fast-Forward_Videos_Based_on_Visual_and_Textual_Features_from_WACV_2020_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Ramos_Personalizing_Fast-Forward_Videos_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2684093,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8257675571776715560&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "PlotQA: Reasoning over Scientific Plots",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Methani_PlotQA_Reasoning_over_Scientific_Plots_WACV_2020_paper.html",
        "author": "Nitesh Methani;  Pritha Ganguly;  Mitesh M. Khapra;  Pratyush Kumar",
        "abstract": "Existing synthetic datasets (FigureQA, DVQA) for reasoning over plots do not contain variability in data labels, real-valued data, or complex reasoning questions.  Consequently, proposed models for these datasets do not fully address the challenge of reasoning over plots. In particular, they assume that the answer comes either from a small fixed size vocabulary or from a bounding box within the image. However, in practice, this is an unrealistic assumption because many questions require reasoning and thus have real-valued answers which appear neither in a small fixed size vocabulary nor in the image. In this work, we aim to bridge this gap between existing datasets and real-world plots.  Specifically, we propose PlotQA with 28.9 million question-answer pairs over 224,377 plots on data from real-world sources and questions based on crowd-sourced question templates.  Further, 80.76% of the out-of-vocabulary (OOV) questions in PlotQA have answers that are not in a fixed vocabulary. Analysis of existing models on PlotQA reveals that they cannot deal with OOV questions:  their overall accuracy on our dataset is in single digits. This is not surprising given that these models were not designed for such questions. As a step towards a more holistic model which can address fixed vocabulary as well as OOV questions, we propose a hybrid approach: Specific questions are answered by choosing the answer from a fixed vocabulary or by extracting it from a predicted bounding box in the plot, while other questions are answered with a table question-answering engine which is fed with a structured table generated by detecting visual elements from the image. On the existing DVQA dataset, our model has an accuracy of 58%, significantly improving on the highest reported accuracy of 46%.  On PlotQA, our model has an accuracy of 22.52%, which is significantly better than state of the art models.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Methani_PlotQA_Reasoning_over_Scientific_Plots_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science and Engineering; Department of Computer Science and Engineering; Department of Computer Science and Engineering; Department of Computer Science and Engineering",
        "project": "bit.ly/PlotQA",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Methani_PlotQA_Reasoning_over_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 536173,
        "gs_citation": 255,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15446045989262535358&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cse.iitm.ac.in;cse.iitm.ac.in;cse.iitm.ac.in;cse.iitm.ac.in",
        "email": "cse.iitm.ac.in;cse.iitm.ac.in;cse.iitm.ac.in;cse.iitm.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://cse.ucsd.edu",
        "aff_unique_abbr": "UCSD CSE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Plug-and-Play Rescaling Based Crowd Counting in Static Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sajid_Plug-and-Play_Rescaling_Based_Crowd_Counting_in_Static_Images_WACV_2020_paper.html",
        "author": "Usman Sajid;  Guanghui Wang",
        "abstract": "Crowd counting is a challenging problem especially in the presence of huge crowd diversity across images and complex cluttered crowd-like background regions, where most previous approaches do not generalize well and consequently produce either huge crowd underestimation or overestimation. To address these challenges, we propose a new image patch rescaling module (PRM) and three independent PRM employed crowd counting methods. The proposed frameworks use the PRM module to rescale the image regions (patches) that require special treatment, whereas the classification process helps in recognizing and discarding any cluttered crowd-like background regions which may result in overestimation. Experiments on three standard benchmarks and cross-dataset evaluation show that our approach outperforms the state-of-the-art models in the RMSE evaluation metric with an improvement up to 10:4%, and possesses superior generalization ability to new datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sajid_Plug-and-Play_Rescaling_Based_Crowd_Counting_in_Static_Images_WACV_2020_paper.pdf",
        "aff": "Electrical Engineering and Computer Science, The University of Kansas, Lawrence, KS, USA 66045; Electrical Engineering and Computer Science, The University of Kansas, Lawrence, KS, USA 66045",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2766446,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=519467496641377289&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ku.edu;ku.edu",
        "email": "ku.edu;ku.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The University of Kansas",
        "aff_unique_dep": "Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.ku.edu",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lawrence",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Plugin Networks for Inference under Partial Evidence",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Koperski_Plugin_Networks_for_Inference_under_Partial_Evidence_WACV_2020_paper.html",
        "author": "Michal Koperski;  Tomasz Konopczynski;  Rafal Nowak;  Piotr Semberecki;  Tomasz Trzcinski",
        "abstract": "In this paper, we propose a novel method to incorporate partial evidence in the inference of deep convolutional neural networks. Contrary to the existing, top-performing methods, which either iteratively modify the input of the network or exploit external label taxonomy to take the partial evidence into account, we add separate network modules (\"Plugin Networks\") to the intermediate layers of a pre-trained convolutional network. The goal of these modules is to incorporate additional signal, i.e. information about known labels, into the inference procedure, and adjust the predicted output accordingly. Since the attached plugins have a simple structure, consisting of only fully connected layers, we drastically reduced the computational cost of training and inference. Also, the proposed architecture allows propagating information about known labels directly to the intermediate layers to improve the final representation. Extensive evaluation of the proposed method confirms that our Plugin Networks outperform the state-of-the-art in a variety of tasks, including scene categorization, multi-label image annotation, and semantic segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Koperski_Plugin_Networks_for_Inference_under_Partial_Evidence_WACV_2020_paper.pdf",
        "aff": "Tooploox; Heidelberg University+Tooploox; Institute of Computer Science, University of Wroclaw+Tooploox; Wroclaw University of Science and Technology+Tooploox; Warsaw University of Technology+Tooploox",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3622012,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11135427172599478546&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tooploox.com; ; ; ; ",
        "email": "tooploox.com; ; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;2+0;3+0;4+0",
        "aff_unique_norm": "Tooploox;Heidelberg University;University of Wroclaw;Wroclaw University of Science and Technology;Warsaw University of Technology",
        "aff_unique_dep": ";;Institute of Computer Science;;",
        "aff_unique_url": "https://www.tooploox.com;https://www.uni-heidelberg.de;https://www.uni.wroc.pl;https://www.pwr.edu.pl;https://www.pw.edu.pl",
        "aff_unique_abbr": ";Uni Heidelberg;;WUST;WUT",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0+0;0+0;0+0",
        "aff_country_unique": "Poland;Germany"
    },
    {
        "title": "PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sun_PointGrow_Autoregressively_Learned_Point_Cloud_Generation_with_Self-Attention_WACV_2020_paper.html",
        "author": "Yongbin Sun;  Yue Wang;  Ziwei Liu;  Joshua Siegel;  Sanjay Sarma",
        "abstract": "Generating 3D point clouds is challenging yet highly desired. This work presents a novel autoregressive model, PointGrow, which can generate diverse and realistic point cloud samples from scratch or conditioned on semantic contexts. This model operates recurrently, with each point sampled according to a conditional distribution given its previously-generated points, allowing inter-point correlations to be well-exploited and 3D shape generative processes to be better interpreted. Since point cloud object shapes are typically encoded by long-range dependencies, we augment our model with dedicated self-attention modules to capture such relations. Extensive evaluations show that PointGrow achieves satisfying performance on both unconditional and conditional point cloud generation tasks, with respect to realism and diversity. Several important applications, such as unsupervised feature learning and shape arithmetic operations, are also demonstrated.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sun_PointGrow_Autoregressively_Learned_Point_Cloud_Generation_with_Self-Attention_WACV_2020_paper.pdf",
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; The Chinese University of Hong Kong; Michigan State University; Massachusetts Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1498162,
        "gs_citation": 248,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11174852044297249411&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mit.edu;mit.edu;link.cuhk.edu.hk;msu.edu;mit.edu",
        "email": "mit.edu;mit.edu;link.cuhk.edu.hk;msu.edu;mit.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;The Chinese University of Hong Kong;Michigan State University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://web.mit.edu;https://www.cuhk.edu.hk;https://www.msu.edu",
        "aff_unique_abbr": "MIT;CUHK;MSU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "PointPoseNet: Point Pose Network for Robust 6D Object Pose Estimation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_PonitPoseNet_Point_Pose_Network_for_Robust_6D_Object_Pose_Estimation_WACV_2020_paper.html",
        "author": "Wei Chen;  Jinming Duan;  Hector Basevi;  Hyung Jin Chang;  Ales Leonardis",
        "abstract": "In this paper, we propose a novel pipeline to estimate 6D object pose from RGB-D images of known objects present in complex scenes. The pipeline directly operates on raw point clouds extracted from RGB-D scans. Specifically, our method takes the point cloud as input and regresses the point-wise unit vectors pointing to the 3D keypoints. We then use these vectors to generate keypoint hypotheses from which the 6D object pose hypotheses are computed. Finally, we select the best 6D object pose from the hypotheses based on a proposed scoring mechanism with geometry constraints. Extensive experiments show that the proposed method is robust against the variety in object shape and appearance as well as occlusions between objects, and that our method outperforms the state-of-the-art methods on the LINEMOD and Occlusion LINEMOD datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_PonitPoseNet_Point_Pose_Network_for_Robust_6D_Object_Pose_Estimation_WACV_2020_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6878108,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9146322039164532418&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Pose Guided Gated Fusion for Person Re-identification",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Bhuiyan_Pose_Guided_Gated_Fusion_for_Person_Re-identification_WACV_2020_paper.html",
        "author": "Amran Bhuiyan;  Yang Liu;  Parthipan Siva;  Mehrsan Javan;  Ismail Ben Ayed;  Eric Granger",
        "abstract": "Person re-identification is an important yet challenging problem in visual recognition. Despite the recent advances with deep learning (DL) models for spatio-temporal and multi-modal fusion, re-identification approaches often fail to leverage the contextual information (e.g., pose and illu- mination) to dynamically select the most discriminant con- volutional filters (i.e., appearance features) for feature rep- resentation and inference. State-of-the-art techniques for gated fusion employ complex dedicated part- or attention- based architectures for late fusion, and do not incorpo- rate pose and appearance information to train the back- bone network. In this paper, a new DL model is proposed for pose-guided re-identification, comprised of a deep back- bone, pose estimation, and gated fusion network. Given a query image of an individual, the backbone convolutional NN produces a feature embedding required for pair-wise matching with embeddings for reference images, where fea- ture maps from the pose network and from mid-level CNN layers are combined by the gated fusion network to gen- erate pose-guided gating. The proposed framework al- lows to dynamically activate the most discriminant CNN filters based on pose information in order to perform a finer grained recognition. Extensive experiments on three challenging benchmark datasets indicate that integrating the pose-guided gated fusion into the state-of-the-art re- identification backbone architecture allows to improve their recognition accuracy. Experimental results also support our intuition on the advantages of gating backbone appear- ance information using the pose feature maps at mid-level CNN layers.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Bhuiyan_Pose_Guided_Gated_Fusion_for_Person_Re-identification_WACV_2020_paper.pdf",
        "aff": "LIVIA, Dept. of Systems Engineering, \u00b4Ecole de technologie sup \u00b4erieure, Montreal, Canada+Sportlogiq Inc., Montreal, Canada; LIVIA, Dept. of Systems Engineering, \u00b4Ecole de technologie sup \u00b4erieure, Montreal, Canada+Sportlogiq Inc., Montreal, Canada; LIVIA, Dept. of Systems Engineering, \u00b4Ecole de technologie sup \u00b4erieure, Montreal, Canada+Sportlogiq Inc., Montreal, Canada; LIVIA, Dept. of Systems Engineering, \u00b4Ecole de technologie sup \u00b4erieure, Montreal, Canada+Sportlogiq Inc., Montreal, Canada; LIVIA, Dept. of Systems Engineering, \u00b4Ecole de technologie sup \u00b4erieure, Montreal, Canada; LIVIA, Dept. of Systems Engineering, \u00b4Ecole de technologie sup \u00b4erieure, Montreal, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1094060,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12950315388363024594&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com;gmail.com;gmail.com;sportlogiq.com;etsmtl.ca;etsmtl.ca",
        "email": "gmail.com;gmail.com;gmail.com;sportlogiq.com;etsmtl.ca;etsmtl.ca",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1;0;0",
        "aff_unique_norm": "Ecole de technologie superieure;Sportlogiq Inc.",
        "aff_unique_dep": "Dept. of Systems Engineering;",
        "aff_unique_url": "https://www.etsmtl.ca;",
        "aff_unique_abbr": "ETS;",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Post-Mortem Iris Recognition Resistant to Biological Eye Decay Processes",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Trokielewicz_Post-Mortem_Iris_Recognition_Resistant_to_Biological_Eye_Decay_Processes_WACV_2020_paper.html",
        "author": "Mateusz Trokielewicz;  Adam Czajka;  Piotr Maciejewicz",
        "abstract": "This paper proposes an end-to-end iris recognition method designed specifically for post-mortem samples, and thus serving as a perfect application for iris biometrics in forensics. To our knowledge, it is the first method specific for verification of iris samples acquired after demise. We have fine-tuned a convolutional neural network-based segmentation model with a large set of diversified iris data (including post-mortem and diseased eyes), and combined Gabor kernels with newly designed, iris-specific kernels learnt by Siamese networks. The resulting method significantly outperforms the existing off-the-shelf iris recognition methods (both academic and commercial) on the newly collected database of post-mortem iris images and for all available time horizons since death. We make all models and the method itself available along with this paper.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Trokielewicz_Post-Mortem_Iris_Recognition_Resistant_to_Biological_Eye_Decay_Processes_WACV_2020_paper.pdf",
        "aff": "Research and Academic Computer Network NASK, Warsaw, Poland; University of Notre Dame, IN, USA; Medical University of Warsaw, Warsaw, Poland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Trokielewicz_Post-Mortem_Iris_Recognition_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2520615,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12462417978906648614&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "nask.pl;nd.edu;wum.edu.pl",
        "email": "nask.pl;nd.edu;wum.edu.pl",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Research and Academic Computer Network NASK;University of Notre Dame;Medical University of Warsaw",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nask.pl;https://www.nd.edu;https://www.muw.edu.pl",
        "aff_unique_abbr": "NASK;Notre Dame;MUW",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Notre Dame;Warsaw",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Poland;United States"
    },
    {
        "title": "Predicting the Physical Dynamics of Unseen 3D Objects",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Rempe_Predicting_the_Physical_Dynamics_of_Unseen_3D_Objects_WACV_2020_paper.html",
        "author": "Davis Rempe;  Srinath Sridhar;  He Wang;  Leonidas Guibas",
        "abstract": "Machines that can predict the effect of physical interactions on the dynamics of previously unseen object instances are important for creating better robots and interactive virtual worlds. In this work, we focus on predicting the dynamics of 3D objects on a plane that have just been subjected to an impulsive force. In particular, we predict the changes in state - 3D position, rotation, velocities, and stability. Different from previous work, our approach can generalize dynamics predictions to object shapes and initial conditions that were unseen during training. Our method takes the 3D object's shape as a point cloud and its initial linear and angular velocities as input. We extract shape features and use a recurrent neural network to predict the full change in state at each time step. Our model can support training with data from both a physics engine or the real world. Experiments show that we can accurately predict the changes in state for unseen object geometries and initial conditions.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Rempe_Predicting_the_Physical_Dynamics_of_Unseen_3D_Objects_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Rempe_Predicting_the_Physical_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2016144,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4755007055773142624&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Preference-Based Image Generation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kazemi_Preference-Based_Image_Generation_WACV_2020_paper.html",
        "author": "Hadi Kazemi;  Fariborz Taherkhani;  Nasser Nasrabadi",
        "abstract": "Deep generative models are a set of promising methods, that are able to model complex data and generate new samples. In principle, they learn to map a random latent code sampled from a prior distribution into high dimensional data space, such as image space. However, these models have limited utilities as the user has minimal control over what the network produces. Despite the success of some recent work in learning an interpretable latent code, the field still lacks a coherent framework to learn a fully interpretable latent code, without any random part for sample diversity.   Consequently, it is generally hard, if not impossible, for a non-expert user to produce the desired image by tuning the random and interpretable parts of the latent code. In this paper, we introduce the Preference-Based Image Generation (PbIG), a new method to retrieve the corresponding latent code of the user's mental image. We propose to adopt preference-based reinforcement learning, which learns from a user's judgment of the generated images by a pre-trained generative model. Since the proposed method is completely decoupled from the training stage of the underlying generative models, it can easily be adopted by any method, such as GANs and VAEs. We evaluate the effectiveness of PbIG framework using a set of experiments on baseline datasets using a pretraind StackGAN++.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kazemi_Preference-Based_Image_Generation_WACV_2020_paper.pdf",
        "aff": "West Virginia University; West Virginia University; West Virginia University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 863174,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15047059711856903166&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "mix.wvu.edu;gmail.com;mail.wvu.edu",
        "email": "mix.wvu.edu;gmail.com;mail.wvu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "West Virginia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wvu.edu",
        "aff_unique_abbr": "WVU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Print Defect Mapping with Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Valente_Print_Defect_Mapping_with_Semantic_Segmentation_WACV_2020_paper.html",
        "author": "Augusto Valente;  Cristina Wada;  Deangela Neves;  Deangeli Neves;  Fabio Perez;  Guilherme Megeto;  Marcos Cascone;  Otavio Gomes;  Qian Lin",
        "abstract": "Efficient automated print defect mapping is valuable to the printing industry since such defects directly influence customer-perceived printer quality and manually mapping them is cost-ineffective. Conventional methods consist of complicated and hand-crafted feature engineering techniques, usually targeting only one type of defect. In this paper, we propose the first end-to-end framework to map print defects at pixel level, adopting an approach based on semantic segmentation. Our framework uses Convolutional Neural Networks, specifically DeepLab-v3+, and achieves promising results in the identification of defects in printed images. We use synthetic training data by simulating two types of print defects and a print-scan effect with image processing and computer graphic techniques. Compared with conventional methods, our framework is versatile, allowing two inference strategies, one being near real-time and providing coarser results, and the other focusing on offline processing with more fine-grained detection. Our model is evaluated on a dataset of real printed images.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Valente_Print_Defect_Mapping_with_Semantic_Segmentation_WACV_2020_paper.pdf",
        "aff": "Instituto de Pesquisas Eldorado; Instituto de Pesquisas Eldorado; Instituto de Pesquisas Eldorado; Instituto de Pesquisas Eldorado; Instituto de Pesquisas Eldorado; Instituto de Pesquisas Eldorado; Instituto de Pesquisas Eldorado; Instituto de Pesquisas Eldorado; HP Labs, HP Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4251281,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3556841846033244792&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "eldorado.org.br;eldorado.org.br;eldorado.org.br;eldorado.org.br;eldorado.org.br;eldorado.org.br;eldorado.org.br;eldorado.org.br;hp.com",
        "email": "eldorado.org.br;eldorado.org.br;eldorado.org.br;eldorado.org.br;eldorado.org.br;eldorado.org.br;eldorado.org.br;eldorado.org.br;hp.com",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;0;1",
        "aff_unique_norm": "Instituto de Pesquisas Eldorado;HP Inc.",
        "aff_unique_dep": ";HP Labs",
        "aff_unique_url": "https://www.ipe.org.br;https://www.hp.com",
        "aff_unique_abbr": "IPE;HP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;1",
        "aff_country_unique": "Brazil;United States"
    },
    {
        "title": "Probabilistic Object Detection: Definition and Evaluation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hall_Probabilistic_Object_Detection_Definition_and_Evaluation_WACV_2020_paper.html",
        "author": "David Hall;  Feras Dayoub;  John Skinner;  Haoyang Zhang;  Dimity Miller;  Peter Corke;  Gustavo Carneiro;  Anelia Angelova;  Niko Suenderhauf",
        "abstract": "We introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections.  Given the lack of methods capable of assessing such probabilistic object detections, we present the new Probability-based Detection Quality measure (PDQ). Unlike AP-based measures, PDQ has no arbitrary thresholds and rewards spatial and label quality, and foreground/background separation quality while explicitly penalising false positive and false negative detections. We contrast PDQ with existing mAP and moLRP measures by evaluating state-of-the-art detectors and a Bayesian object detector based on Monte Carlo Dropout. Our experiments indicate that conventional object detectors tend to be spatially overconfident and thus perform poorly on the task of probabilistic object detection. Our paper aims to encourage the development of new object detection approaches that provide detections with accurately estimated spatial and label uncertainties and are of critical importance for deployment on robots and embodied AI systems in the real world.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hall_Probabilistic_Object_Detection_Definition_and_Evaluation_WACV_2020_paper.pdf",
        "aff": "Australian Centre for Robotic Vision+Queensland University of Technology; Australian Centre for Robotic Vision+Queensland University of Technology; Australian Centre for Robotic Vision+Queensland University of Technology; Australian Centre for Robotic Vision+Queensland University of Technology; Australian Centre for Robotic Vision+Queensland University of Technology; Australian Centre for Robotic Vision+Queensland University of Technology; Australian Centre for Robotic Vision+University of Adelaide; Google Brain; Australian Centre for Robotic Vision+Queensland University of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Hall_Probabilistic_Object_Detection_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 706224,
        "gs_citation": 171,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2504413666097728527&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "qut.edu.au;qut.edu.au;qut.edu.au;qut.edu.au;qut.edu.au;qut.edu.au;qut.edu.au;adelaide.edu.au;google.com",
        "email": "qut.edu.au;qut.edu.au;qut.edu.au;qut.edu.au;qut.edu.au;qut.edu.au;qut.edu.au;adelaide.edu.au;google.com",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1;0+1;0+2;3;0+1",
        "aff_unique_norm": "Australian Centre for Robotic Vision;Queensland University of Technology;University of Adelaide;Google",
        "aff_unique_dep": ";;;Google Brain",
        "aff_unique_url": "https://roboticvision.org/;https://www.qut.edu.au;https://www.adelaide.edu.au;https://brain.google.com",
        "aff_unique_abbr": "ACRV;QUT;Adelaide;Google Brain",
        "aff_campus_unique_index": ";;;;;;;1;",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0;0+0;1;0+0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "title": "Progressive Domain Adaptation for Object Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hsu_Progressive_Domain_Adaptation_for_Object_Detection_WACV_2020_paper.html",
        "author": "Han-Kai Hsu;  Chun-Han Yao;  Yi-Hsuan Tsai;  Wei-Chih Hung;  Hung-Yu Tseng;  Maneesh Singh;  Ming-Hsuan Yang",
        "abstract": "Recent deep learning methods for object detection rely on a large amount of bounding box annotations. Collecting these annotations is laborious and costly, yet supervised models do not generalize well when testing on images from a different distribution. Domain adaptation provides a solution by adapting existing labels to the target testing data. However, a large gap between domains could make adaptation a challenging task, which leads to unstable training processes and sub-optimal results. In this paper, we propose to bridge the domain gap with an intermediate domain and progressively solve easier adaptation subtasks. This intermediate domain is constructed by translating the source images to mimic the ones in the target domain. To tackle the domain-shift problem, we adopt adversarial learning to align distributions at the feature level. In addition, a weighted task loss is applied to deal with unbalanced image quality in the intermediate domain. Experimental results show that our method performs favorably against the state-of-the-art method in terms of the performance on the target domain.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hsu_Progressive_Domain_Adaptation_for_Object_Detection_WACV_2020_paper.pdf",
        "aff": "University of California, Merced; University of California, Merced; NEC Laboratories America; University of California, Merced; University of California, Merced; Verisk Analytics; Google",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 687355,
        "gs_citation": 421,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6531682860547120204&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0;2;3",
        "aff_unique_norm": "University of California, Merced;NEC Laboratories America;Verisk Analytics;Google",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ucmerced.edu;https://www.nec-labs.com;https://www.verisk.com;https://www.google.com",
        "aff_unique_abbr": "UC Merced;NEC Labs America;;Google",
        "aff_campus_unique_index": "0;0;0;0;2",
        "aff_campus_unique": "Merced;;Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Rodriguez_Proposal-free_Temporal_Moment_Localization_of_a_Natural-Language_Query_in_Video_WACV_2020_paper.html",
        "author": "Cristian Rodriguez;  Edison Marrese-Taylor;  Fatemeh Sadat Saleh;  HONGDONG LI;  Stephen Gould",
        "abstract": "This paper studies the problem of temporal moment localization in a long untrimmed video using natural language as the query. Given an untrimmed video and a query sentence, the goal is to determine the start and end of the relevant visual moment in the video that corresponds to the query sentence.  While most previous works have tackled this by a propose-and-rank approach, we introduce a more efficient, end-to-end trainable, and proposal-free approach that is built upon three key components: a dynamic filter which adaptively transfers language information to visual domain attention map, a new loss function to guide the model to attend the most relevant part of the video, and soft labels to cope with annotation uncertainties.  Our method is evaluated on three standard benchmark datasets, Charades-STA, TACoS and ActivityNet-Captions. Experimental results show our method outperforms state-of-the-art methods on these datasets, confirming the effectiveness of the method.  We believe the proposed dynamic filter-based guided attention mechanism will prove valuable for other vision and language tasks as well.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Rodriguez_Proposal-free_Temporal_Moment_Localization_of_a_Natural-Language_Query_in_Video_WACV_2020_paper.pdf",
        "aff": "Australian National University + Australian Centre for Robotic Vision (ACRV); Graduate School of Engineering, The University of Tokyo; Australian National University + Australian Centre for Robotic Vision (ACRV); Australian National University + Australian Centre for Robotic Vision (ACRV); Australian National University + Australian Centre for Robotic Vision (ACRV)",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Rodriguez_Proposal-free_Temporal_Moment_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 766461,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8919440901342594121&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "anu.edu.au;weblab.t-utokyo.ac.jp;anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;weblab.t-utokyo.ac.jp;anu.edu.au;anu.edu.au;anu.edu.au",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+1;0+1;0+1",
        "aff_unique_norm": "Australian National University;Australian Centre for Robotic Vision;The University of Tokyo",
        "aff_unique_dep": ";Centre for Robotic Vision;Graduate School of Engineering",
        "aff_unique_url": "https://www.anu.edu.au;https://roboticvision.org/;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "ANU;ACRV;UTokyo",
        "aff_campus_unique_index": ";1;;;",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "0+0;1;0+0;0+0;0+0",
        "aff_country_unique": "Australia;Japan"
    },
    {
        "title": "Propose-and-Attend Single Shot Detector",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Jang_Propose-and-Attend_Single_Shot_Detector_WACV_2020_paper.html",
        "author": "Ho-Deok Jang;  Sanghyun Woo;  Philipp Benz;  Jinsun Park;  In So Kweon",
        "abstract": "We present a simple yet effective prediction module for a one-stage detector. The main process is conducted in a coarse-to-fine manner. First, the module roughly adjusts the default boxes to well capture the extent of target objects in an image. Second, given the adjusted boxes, the module aligns the receptive field of the convolution filters accordingly, not requiring any embedding layers. Both steps build a propose-and-attend mechanism, mimicking two-stage detectors in a highly efficient manner. To verify its effectiveness, we apply the proposed module to a basic one-stage detector SSD. We empirically show that our module significantly lifts the detection accuracy with marginal parameter overhead. Our final model achieves an accuracy comparable to that of state-of-the-art detectors while using a fraction of their model parameter and computational overheads. Moreover, we found that the proposed module has two strong applications. 1) The module can be successfully integrated into a lightweight backbone, further pushing the efficiency of the one-stage detector. 2) The module also allows train-from-scratch without relying on any sophisticated base networks as previous methods do.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Jang_Propose-and-Attend_Single_Shot_Detector_WACV_2020_paper.pdf",
        "aff": "Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 597778,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15280040408963707748&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "QR-code Reconstruction from Event Data via Optimization in Code Subspace",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Nagata_QR-code_Reconstruction_from_Event_Data_via_Optimization_in_Code_Subspace_WACV_2020_paper.html",
        "author": "Jun Nagata;  Yusuke Sekikawa;  Kosuke Hara;  Teppei Suzuki;  Yoshimitsu Aoki",
        "abstract": "We propose an image reconstruction method from event data, assuming the target images belong to a prespecified class like QR codes. Instead of solving the reconstruction problem in the image space, we introduce a code space that covers all the noiseless target class images and solves the reconstruction problem on it. This restriction enormously reduces the number of optimizing parameters and makes the reconstruction problem well posed and robust to noise. We demonstrate fast and robust QR-code scanning in difficult, high-speed scenes with industrial high-speed cameras and other reconstruction methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Nagata_QR-code_Reconstruction_from_Event_Data_via_Optimization_in_Code_Subspace_WACV_2020_paper.pdf",
        "aff": "Keio University; Denso IT Laboratory; Denso IT Laboratory; Denso IT Laboratory; Keio University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1806356,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11155227840380998391&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "aoki-medialab.jp;d-itlab.co.jp;d-itlab.co.jp;d-itlab.co.jp;elec.keio.ac.jp",
        "email": "aoki-medialab.jp;d-itlab.co.jp;d-itlab.co.jp;d-itlab.co.jp;elec.keio.ac.jp",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Keio University;Denso Corporation",
        "aff_unique_dep": ";IT Laboratory",
        "aff_unique_url": "https://www.keio.ac.jp;https://www.denso.com",
        "aff_unique_abbr": "Keio;Denso",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "QUICKSAL: A small and sparse visual saliency model for efficient inference in resource constrained hardware",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ramanathan_QUICKSAL_A_small_and_sparse_visual_saliency_model_for_efficient_WACV_2020_paper.html",
        "author": "Vignesh Ramanathan;  Pritesh  Dwivedi;  Bharath Katabathuni;  Anirban Chakraborty;  Chetan  Singh Thakur",
        "abstract": "Visual saliency is an important problem in the field of cognitive science and computer vision with applications such as surveillance, adaptive compressing,  detecting unknown objects and scene understanding. In this paper, we propose a small and sparse neural network model for performing salient object segmentation that is suitable for use in mobile and embedded applications. Our model is built using depthwise separable convolutions and bottleneck inverted residuals which have been proven to perform very memory-efficient inference and can be easily implemented using standard functions available in all deep learning frameworks. The multiscale features extracted along with the layers with deep residuals allow our network to learn high-quality saliency maps. We present the quantitative results of our QUICKSAL model with multiple levels of model sparsity ranging from 0% to  96%, with the non-zero parameter count varying from 3.3M to  0.14M respectively - on publicly available benchmark datasets - showing that our highly constrained approach is comparable to other state-of-the-art approaches  (parameter count  35M).  We also present qualitative results on camouflage images and show that our model can successfully distinguish between the salient and non-salient parts even when both seem blended together.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ramanathan_QUICKSAL_A_small_and_sparse_visual_saliency_model_for_efficient_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Science; Indian Institute of Science; Indian Institute of Science + Archeron Group; Indian Institute of Science; Indian Institute of Science",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1161999,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5946477271893109324&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "iisc.ac.in;iisc.ac.in;iisc.ac.in + bharath;iisc.ac.in;iisc.ac.in",
        "email": "iisc.ac.in;iisc.ac.in;iisc.ac.in + bharath;iisc.ac.in;iisc.ac.in",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;0;0",
        "aff_unique_norm": "Indian Institute of Science;Archeron Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iisc.ac.in;",
        "aff_unique_abbr": "IISc;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India;"
    },
    {
        "title": "Quadtree Generating Networks: Efficient Hierarchical Scene Parsing with Sparse Convolutions",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chitta_Quadtree_Generating_Networks_Efficient_Hierarchical_Scene_Parsing_with_Sparse_Convolutions_WACV_2020_paper.html",
        "author": "Kashyap Chitta;  Jose M. Alvarez;  Martial Hebert",
        "abstract": "Semantic segmentation with Convolutional Neural Networks is a memory-intensive task due to the high spatial resolution of feature maps and output predictions. In this paper, we present Quadtree Generating Networks (QGNs), a novel approach able to drastically reduce the memory footprint of modern semantic segmentation networks. The key idea is to use quadtrees to represent the predictions and target segmentation masks instead of dense pixel grids. Our quadtree representation enables hierarchical processing of an input image, with the most computationally demanding layers only being used at regions in the image containing boundaries between classes. In addition, given a trained model, our representation enables flexible inference schemes to trade-off accuracy and computational cost, allowing the network to adapt in constrained situations such as embedded devices. We demonstrate the benefits of our approach on the Cityscapes, SUN-RGBD and ADE20k datasets. On Cityscapes, we obtain an relative 3% mIoU improvement compared to a dilated network with similar memory consumption; and only receive a 3% relative mIoU drop compared to a large dilated network, while reducing memory consumption by over 4x. Our code is available at https://github.com/kashyap7x/QGN.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chitta_Quadtree_Generating_Networks_Efficient_Hierarchical_Scene_Parsing_with_Sparse_Convolutions_WACV_2020_paper.pdf",
        "aff": "Autonomous Vision Group, MPI for Intelligent Systems and University of T \u00a8ubingen; NVIDIA; The Robotics Institute, Carnegie Mellon University",
        "project": "",
        "github": "https://github.com/kashyap7x/QGN",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Chitta_Quadtree_Generating_Networks_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 497836,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12729941795156799984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "tue.mpg.de;nvidia.com;cs.cmu.edu",
        "email": "tue.mpg.de;nvidia.com;cs.cmu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;NVIDIA Corporation;Carnegie Mellon University",
        "aff_unique_dep": "Autonomous Vision Group;;The Robotics Institute",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;https://www.nvidia.com;https://www.cmu.edu",
        "aff_unique_abbr": "MPI-IS;NVIDIA;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "title": "ROSS: Robust Learning of One-shot 3D Shape Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Yuan_ROSS_Robust_Learning_of_One-shot_3D_Shape_Segmentation_WACV_2020_paper.html",
        "author": "Shuaihang Yuan;  Yi Fang",
        "abstract": "3D shape segmentation is a fundamental computer vision task that partitions the object into labeled semantic parts. Recent approaches to 3D shape segmentation learning heavily rely on high-quality labeled training datasets. This limits their use in applications to handle the large scale unannotated datasets. In this paper, we proposed a novel semi-supervised approach, named Robust Learning of One-Shot 3D Shape Segmentation (ROSS), which only requires one single exemplar labeled shape for training. The proposed ROSS can generalize its ability from a one-shot training process to predict the segmentation for previously unseen 3D shape models. The proposed ROSS is composed of three major modules for 3D shape segmentation as follows. The global shape descriptor generator is the first module that utilizes the proposed reference weighted convolution to learn a 3D shape descriptor. The second module is a part-aware shape descriptor constructor that can generate weighted descriptors from a learned 3D shape descriptor according to semantic parts without supervision. The shape morphing with label transferring works as the last module. It morphs the exemplar shape and then transfers labels from the transformed exemplar shape to the target shape. The extensive experimental results on 3D mesh datasets demonstrate the ROSS is robust to noise and incomplete shapes and it can be applied to unannotated datasets. The experiment shows the proposed ROSS can achieve comparable performance with the supervised method.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Yuan_ROSS_Robust_Learning_of_One-shot_3D_Shape_Segmentation_WACV_2020_paper.pdf",
        "aff": "New York University; New York University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1525978,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7096374075142492401&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "RPM-Net: Robust Pixel-Level Matching Networks for Self-Supervised Video Object Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kim_RPM-Net_Robust_Pixel-Level_Matching_Networks_for_Self-Supervised_Video_Object_Segmentation_WACV_2020_paper.html",
        "author": "Youngeun Kim;  Seokeon Choi;  Hankyeol Lee;  Taekyung Kim;  Changick Kim",
        "abstract": "In this paper, we introduce a self-supervised approach for video object segmentation without human labeled data. Specifically, we present Robust Pixel-level Matching Networks (RPM-Net), a novel deep architecture that matches pixels between adjacent frames, using only color information from unlabeled videos for training. Technically, RPM-Net can be separated into two main modules. The embedding module first projects input images into high dimensional embedding space. Then the matching module with deformable convolution layers matches pixels between reference and target frames based on the embedding features. Unlike previous supervised methods using deformable convolution, our matching module adopts deformable convolution to focus on similar features in spatiotemporally neighboring pixels. We further propose an online updating module to refine the segmentation result by transferring knowledge from the given first frame. Also, we carry out comprehensive experiments on three public datasets (i.e., DAVIS-2017, SegTrack-v2, and Youtube- Objects) and achieve state-of-the-art performance on self-supervised video object segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kim_RPM-Net_Robust_Pixel-Level_Matching_Networks_for_Self-Supervised_Video_Object_Segmentation_WACV_2020_paper.pdf",
        "aff": "KAIST; KAIST; KAIST; KAIST; KAIST",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Kim_RPM-Net_Robust_Pixel-Level_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2254506,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12939530670692768890&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "ReStGAN: A step towards visually guided shopper experience via text-to-image synthesis",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Surya_ReStGAN_A_step_towards_visually_guided_shopper_experience_via_text-to-image_WACV_2020_paper.html",
        "author": "Shiv Surya;  Amrith Setlur;  Arijit Biswas;  Sumit Negi",
        "abstract": "E-commerce companies like Amazon, Alibaba and Flipkart have an extensive catalogue comprising of billions of products. Matching customer search queries to plausible products is challenging due to the size and diversity of the catalogue. These challenges are compounded in apparel due to the semantic complexity and a large variation of fashion styles, product attributes and colours.  Providing aids that can help the customer visualise the styles and colours matching their \"search queries\" will provide customers with necessary intuition about what can be done next. This helps the customer buy a product with the styles, embellishments and colours of their liking. In this work, we propose a Generative Adversarial Network (GAN) for generating images from text streams like customer search queries. Our GAN learns to incrementally generate possible images complementing the fine-grained style, colour of the apparel in the query. We incorporate a novel colour modelling approach enabling the GAN to render a wide spectrum of colours accurately. We compile a dataset from an e-commerce website to train our model. The proposed approach outperforms the baselines on qualitative and quantitative evaluations.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Surya_ReStGAN_A_step_towards_visually_guided_shopper_experience_via_text-to-image_WACV_2020_paper.pdf",
        "aff": "Amazon; Amazon + Carnegie Mellon University; Amazon; Amazon",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Surya_ReStGAN_A_step_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2255086,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5185663793121956156&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "amazon.com;cs.cmu.edu;amazon.com;amazon.com",
        "email": "amazon.com;cs.cmu.edu;amazon.com;amazon.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Amazon.com, Inc.;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.amazon.com;https://www.cmu.edu",
        "aff_unique_abbr": "Amazon;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Real-Time Multi-Person Pose Tracking using Data Assimilation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Buizza_Real-Time_Multi-Person_Pose_Tracking_using_Data_Assimilation_WACV_2020_paper.html",
        "author": "Caterina Buizza;  Tobias Fischer;  Yiannis Demiris",
        "abstract": "We propose a framework for the integration of data assimilation and machine learning methods in human pose estimation, with the aim of enabling any pose estimation method to be run in real-time, whilst also increasing consistency and accuracy. Data assimilation and machine learning are complementary methods: the former allows us to make use of information about the underlying dynamics of a system but lacks the flexibility of a data-based model, which we can instead obtain with the latter. Our framework presents a real-time tracking module for any single or multi-person pose estimation system. Specifically, tracking is performed by a number of Kalman filters initiated for each new person appearing in a motion sequence. This permits tracking of multiple skeletons and reduces the frequency that computationally expensive pose estimation has to be run, enabling online pose tracking. The module tracks for N frames while the pose estimates are calculated for frame (N+1). This also results in increased consistency of person identification and reduced inaccuracies due to missing joint locations and inversion of left-and right-side joints.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Buizza_Real-Time_Multi-Person_Pose_Tracking_using_Data_Assimilation_WACV_2020_paper.pdf",
        "aff": "Personal Robotics Laboratory, Dept. of Electrical and Electronic Engineering, Imperial College London; Personal Robotics Laboratory, Dept. of Electrical and Electronic Engineering, Imperial College London; Personal Robotics Laboratory, Dept. of Electrical and Electronic Engineering, Imperial College London",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Buizza_Real-Time_Multi-Person_Pose_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6710723,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12433236502628690485&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Imperial College London",
        "aff_unique_dep": "Dept. of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.imperial.ac.uk",
        "aff_unique_abbr": "ICL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Real-time Visual Object Tracking with Natural Language Description",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Feng_Real-time_Visual_Object_Tracking_with_Natural_Language_Description_WACV_2020_paper.html",
        "author": "Qi Feng;  Vitaly Ablavsky;  Qinxun Bai;  Guorong Li;  Stan Sclaroff",
        "abstract": "In this work, we argue that conditioning on the natural language (NL) description of a target provides information for longer-term invariance, and thus helps cope with typical tracking challenges. However, deriving a formulation to combine the strengths of appearance-based tracking with the language modality is not straightforward. Therefore, we propose a novel deep tracking-by-detection formulation that can take advantage of NL descriptions. Regions that are related to the given NL description are generated by a proposal network during the detection stage of the tracker. Our LSTM based tracker then predicts the update of the target from regions proposed by the NL based detection stage. Our method runs at over 30 fps on a single GPU. In benchmarks, our method is competitive with state of the art trackers that employ bounding boxes for initialization, while it outperforms all other trackers on targets given unambiguous and precise language annotations. When conditioned on NL descriptions only, our model doubles the performance of the previous best attempt.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Feng_Real-time_Visual_Object_Tracking_with_Natural_Language_Description_WACV_2020_paper.pdf",
        "aff": "Boston University; Boston University; Horizon Robotics + Hikvision Research America; University of Chinese Academy of Sciences; Boston University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2637973,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=102584957675527689&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "bu.edu;bu.edu;gmail.com;ucas.edu.cn;bu.edu",
        "email": "bu.edu;bu.edu;gmail.com;ucas.edu.cn;bu.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2;3;0",
        "aff_unique_norm": "Boston University;Horizon Robotics;Hikvision Research America;University of Chinese Academy of Sciences",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.bu.edu;https://www.horizon-robotics.com/;https://www.hikvision.com/en-US/;http://www.ucas.ac.cn",
        "aff_unique_abbr": "BU;Horizon Robotics;HRA;UCAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Real-time vehicle distance estimation using single view geometry",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ali_Real-time_vehicle_distance_estimation_using_single_view_geometry_WACV_2020_paper.html",
        "author": "Ahmed Ali;  Ali Hassan;  Afsheen Rafaqat Ali;  Hussam Ullah Khan;  Wajahat Kazmi;  Aamer Zaheer",
        "abstract": "Distance estimation is required for advanced driver as- sistance systems (ADAS) as well as self-driving cars. It is crucial for obstacle avoidance, tailgating detection and accident prevention. Currently, radars and lidars are pri- marily used for this purpose which are either expensive or offer poor resolution. Deep learning based depth or dis- tance estimation techniques require huge amount of data and ensuring domain invariance is a challenge. Therefore, in this paper, we propose a single view geometric approach which is lightweight and uses geometric features of the road lane markings for distance estimation that integrates well with the lane and vehicle detection modules of an existing ADAS. Our system introduces novelty on two fronts: (1) it uses cross-ratios of lane boundaries to estimate horizon (2) it determines an Inverse Perspective Mapping (IPM) and camera height from a known lane width and the detected horizon. Distances of the vehicles on the road are then cal- culated by back projecting image point to a ray intersecting the reconstructed road plane. For evaluation, we used li- dar data as ground truth and compare the performance of our algorithm with radar as well as the state-of-the-art deep learning based monocular depth prediction algorithms. The results on three public datasets (Kitti, nuScenes and Lyft level 5) showed that the proposed system maintains a con- sistent RMSE between 6.10 to 7.31. It outperforms other algorithms on two of the datasets while on KITTI it falls be- hind one (supervised) deep learning method. Furthermore, it is computationally inexpensive and is data-domain invari- ant.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ali_Real-time_vehicle_distance_estimation_using_single_view_geometry_WACV_2020_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1084904,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=535034649273275327&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Reconstructing Road Network Graphs from both Aerial Lidar and Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Parajuli_Reconstructing_Road_Network_Graphs_from_both_Aerial_Lidar_and_Images_WACV_2020_paper.html",
        "author": "Biswas Parajuli;  Ahana Roy Choudhury;  Piyush Kumar",
        "abstract": "We address the problem of reconstructing road networks as undirected graphs over large geographic regions in cold start scenarios where neither the preliminary graph nor any on-road trajectory information is available. The goal of this paper is to transform bimodal aerial data in the form of 3-dimensional Lidar scans and high resolution images into road network graphs. We use a fully convolutional architecture that fuses the two datasets by reducing the disparity in their modalities to segment out roads. We then apply a simple, disk-packing based algorithm that covers  the segmented regions with a minimal set of variably sized disks, connect the intersecting disks and use a provable curve reconstruction algorithm to obtain the road network graph. We show that our method is better at removing outliers and gives improved connectivity and topological accuracy than the existing state of the art thinning based method.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Parajuli_Reconstructing_Road_Network_Graphs_from_both_Aerial_Lidar_and_Images_WACV_2020_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Parajuli_Reconstructing_Road_Network_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1436670,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12668562678535217850&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Reducing Footskate in Human Motion Reconstruction with Ground Contact Constraints",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zou_Reducing_Footskate_in_Human_Motion_Reconstruction_with_Ground_Contact_Constraints_WACV_2020_paper.html",
        "author": "Yuliang Zou;  Jimei Yang;  Duygu Ceylan;  Jianming Zhang;  Federico Perazzi;  Jia-Bin Huang",
        "abstract": "In this paper, we aim to reduce the footskate artifacts when reconstructing human dynamics from monocular RGB videos. Recent work has made substantial progress in improving the temporal smoothness of the reconstructed motion trajectories. Their results, however, still suffer from severe foot skating and slippage artifacts. To tackle this issue, we present a neural network based detector for localizing ground contact events of human feet and use it to impose a physical constraint for optimization of the whole human dynamics in a video. We present a detailed study on the proposed ground contact detector and demonstrate high-quality human motion reconstruction results in various videos.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zou_Reducing_Footskate_in_Human_Motion_Reconstruction_with_Ground_Contact_Constraints_WACV_2020_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1392817,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1906750184955064055&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Reference Grid-assisted Network for 3D Point Signature Learning from Point Clouds",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhu_Reference_Grid-assisted_Network_for_3D_Point_Signature_Learning_from_Point_WACV_2020_paper.html",
        "author": "Jing  Zhu;  Yi Fang",
        "abstract": "Learning a robust 3D point signature from point clouds is an interesting but challenging task in the computer vision field due to the irregular and unordered structure characteristics of the point cloud data. In this paper, we propose to learn a 3D point signature by exploring the implicit relation between keypoints and their neighbors (grouped as patches) among the given scene point clouds. Specially, we design a uniform reference grid to represent the raw relation between each keypoint and its neighbors from the raw point clouds. In order to learn a 3D point signature gradually from a smaller perceptive region to a larger area, we create a novel framework with a MLP-based unit feature network and a 3D CNN-based grid feature network. Specifically, the unit feature network aims to dig the connections from points fallen into the same unit of the reference grid, while the grid feature network is used to discover the grid-wise relations across the whole reference grid with concatenation of the learned unit-wise features. Moreover, we introduce an MLP-based attention network upon the unit feature network to enhance the discriminative ability of our learned 3D point signature. All the components in our proposed model are implemented as siamese ones to better tackle the classic keypoint matching and geometric registration problems. Our proposed 3D point signature learning approach achieves superior performance over other state-of-the-art methods on keypoint matching and geometric registration on the real-world scenes datasets, e.g. SUN3D, 7-scenes and the synthetic scan augmented scenes in ICL-NUIM dataset. More importantly, our learned 3D point signature successfully handles the point cloud fragment alignment challenges by producing correct transformations with RANSAC algorithm.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhu_Reference_Grid-assisted_Network_for_3D_Point_Signature_Learning_from_Point_WACV_2020_paper.pdf",
        "aff": "NYU Multimedia and Visual Computing Lab, USA + New York University, USA + New York University Abu Dhabi, UAE; NYU Multimedia and Visual Computing Lab, USA + New York University, USA + New York University Abu Dhabi, UAE",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2227372,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15231103684749619640&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0+0;0+0+0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "Multimedia and Visual Computing Lab",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Abu Dhabi",
        "aff_country_unique_index": "0+0+1;0+0+1",
        "aff_country_unique": "United States;United Arab Emirates"
    },
    {
        "title": "Region Pooling with Adaptive Feature Fusion for End-to-End Person Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kumar_Region_Pooling_with_Adaptive_Feature_Fusion_for_End-to-End_Person_Recognition_WACV_2020_paper.html",
        "author": "Vijay Kumar;  Anoop Namboodiri;  C.V. Jawahar",
        "abstract": "Current approaches for person recognition train an ensemble of region specific convolutional neural networks for representation learning, and then adopt naive fusion strategies to combine their features or predictions during testing. In this paper, we propose an unified end-to-end architecture that generates a complete person representation based on pooling and aggregation of features from multiple body regions. Our network takes a person image and the pre-determined locations of body regions as input, and generates common feature maps that are shared across all the regions. Multiple features corresponding to different regions are then pooled and combined with an aggregation block, where the adaptive weights required for aggregation are obtained through an attention mechanism. Evaluations on three person recognition datasets - PIPA, Soccer and Hannah show that a single model trained end-to-end is computationally faster, requires fewer parameters and achieves improved performance over separately trained models.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kumar_Region_Pooling_with_Adaptive_Feature_Fusion_for_End-to-End_Person_Recognition_WACV_2020_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2263407,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10281469808373927714&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Regularize, Expand and Compress: NonExpansive Continual Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhang_Regularize_Expand_and_Compress_NonExpansive_Continual_Learning_WACV_2020_paper.html",
        "author": "Jie Zhang;  Junting Zhang;  Shalini Ghosh;  Dawei Li;  Jingwen  Zhu;  Heming Zhang;  Yalin Wang",
        "abstract": "Continual learning (CL), the problem of lifelong learning where tasks arrive in sequence, has attracted increasing attention in the computer vision community lately. The goal of CL is to learn new tasks while maintaining the performance on the previously learned tasks. There are two major obstacles for CL of deep neural networks: catastrophic forgetting and limited model capacity. Inspired by the recent breakthroughs in automatically learning good neural network architectures, we develop a nonexpansive AutoML framework for CL termed Regularize, Expand and Compress (REC) to solve the above issues. REC is a unified framework with three highlights: 1) a novel regularized weight consolidation (RWC) algorithm to avoid forgetting, where accessing the data seen in the previously learned tasks is not required; 2) an automatic neural architecture search (AutoML) engine to expand the network to increase model capability; 3) smart compression of the expanded model after a new task is learned to improve the model efficiency. The experimental results on four different image recognition datasets demonstrate the superior performance of the proposed REC over other CL algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Regularize_Expand_and_Compress_NonExpansive_Continual_Learning_WACV_2020_paper.pdf",
        "aff": "Arizona State University, Tempe, AZ, USA; University of Southern California, Los Angeles, CA, USA; Samsung Research America, Mountain View, CA, USA; Samsung Research America, Mountain View, CA, USA; Samsung Research America, Mountain View, CA, USA; University of Southern California, Los Angeles, CA, USA; Arizona State University, Tempe, AZ, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1265862,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12512292369483118260&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;2;1;0",
        "aff_unique_norm": "Arizona State University;University of Southern California;Samsung Research America",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.asu.edu;https://www.usc.edu;https://www.samsung.com/us/",
        "aff_unique_abbr": "ASU;USC;SRA",
        "aff_campus_unique_index": "0;1;2;2;2;1;0",
        "aff_campus_unique": "Tempe;Los Angeles;Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Relativistic Discriminator: A One-Class Classifier for Generalized Iris Presentation Attack Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Yadav_Relativistic_Discriminator_A_One-Class_Classifier_for_Generalized_Iris_Presentation_Attack_WACV_2020_paper.html",
        "author": "Shivangi Yadav;  Cunjian Chen;  Arun Ross",
        "abstract": "Iris based recognition systems are vulnerable to presentation attacks (PAs) where artifacts such as cosmetic contact lenses, artificial eyes and printed eyes can be used to fool the system. While many learning-based algorithms have been proposed to detect such attacks, very few are equipped to handle previously unseen or newly constructed PAs. In this research, we propose a presentation attack detection (PAD) method that utilizes a  discriminator that is trained to distinguish between bonafide iris images and synthetically generated iris images.  We hypothesize that such a discriminator will generate a tight boundary around the bonafide samples. This would allow the discriminator to better separate the bonafide samples from all types of PA samples. For generating synthetic irides, we train the Relativistic Average Standard Generative Adversarial Network (RaSGAN) that has been shown to generate higher resolution and better quality images than standard GANs. The relativistic discriminator (RD) component of the trained RaSGAN is then appropriated for PA detection and is referred to as RD-PAD. Experimental results convey the efficacy of the RD-PAD as a one-class anomaly detector.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Yadav_Relativistic_Discriminator_A_One-Class_Classifier_for_Generalized_Iris_Presentation_Attack_WACV_2020_paper.pdf",
        "aff": "Michigan State University; Michigan State University; Michigan State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 632548,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17336785017816995130&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "msu.edu;msu.edu;msu.edu",
        "email": "msu.edu;msu.edu;msu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Representing Objects in Video as Space-Time Volumes by Combining Top-Down and Bottom-Up Processes",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ilic_Representing_Objects_in_Video_as_Space-Time_Volumes_by_Combining_Top-Down_WACV_2020_paper.html",
        "author": "Filip Ilic;  Axel Pinz",
        "abstract": "As top-down based approaches of object recognition from video are getting more powerful, a structured way to combine them with bottom-up grouping processes becomes feasible. When done right, the resulting representation is able to describe objects and their decomposition into parts at appropriate spatio-temporal scales.We propose a method that uses a modern object detector to focus on salient structures in video, and a dense optical flow estimator to supplement feature extraction. From these structures we extract space-time volumes of interest (STVIs) by smoothing in spatio-temporal Gaussian Scale Space that guides bottom-up grouping.The resulting novel representation enables us to analyze and visualize the decomposition of an object into meaningful parts while preserving temporal object continuity. Our experimental validation is twofold. First, we achieve competitive results on a common video object segmentation benchmark. Second, we extend this benchmark with high quality object part annotations, DAVIS Parts, on which we establish a strong baseline by showing that our method yields spatio-temporally meaningful object parts. Our new representation will support applications that require high-level space-time reasoning at the parts level.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ilic_Representing_Objects_in_Video_as_Space-Time_Volumes_by_Combining_Top-Down_WACV_2020_paper.pdf",
        "aff": "Graz University of Technology; Graz University of Technology",
        "project": "http://f-ilic.github.io/STVI",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1499230,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:C3UgzXFMrHMJ:scholar.google.com/&scioq=Representing+Objects+in+Video+as+Space-Time+Volumes+by+Combining+Top-Down+and+Bottom-Up+Processes&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff_domain": "tugraz.at;tugraz.at",
        "email": "tugraz.at;tugraz.at",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TUGraz",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "Resisting Large Data Variations via Introspective Transformation Network",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhao_Resisting_Large_Data_Variations_via_Introspective_Transformation_Network_WACV_2020_paper.html",
        "author": "Yunhan Zhao;  Ye Tian;  Charless Fowlkes;  Wei Shen;  Alan Yuille",
        "abstract": "Training deep networks that generalize to a wide range of variations in test data is essential to building accurate and robust image classifiers. Data variations in this paper include but not limited to unseen affine transformations and warping in the training data. One standard strategy to overcome this problem is to apply data augmentation to synthetically enlarge the training set. However, data augmentation is essentially a brute-force method which generates uniform samples from some pre-defined set of transformations. In this paper, we propose a principled approach named introspective transformation network (ITN) that significantly improves network resistance to large variations between training and testing data. This is achieved by embedding a learnable transformation module into the introspective network, which is a convolutional neural network (CNN) classifier empowered with generative capabilities. Our approach alternates between synthesizing pseudo-negative samples and transformed positive examples based on the current model, and optimizing model predictions on these synthesized samples. Experimental results verify that our approach significantly improves the ability of deep networks to resist large variations between training and testing data and achieves classification accuracy improvements on several benchmark datasets, including MNIST, affNIST, SVHN, CIFAR-10 and miniImageNet.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhao_Resisting_Large_Data_Variations_via_Introspective_Transformation_Network_WACV_2020_paper.pdf",
        "aff": "University of California, Irvine; Johns Hopkins University + Tencent Hippocrates Research Labs; University of California, Irvine; Johns Hopkins University; Johns Hopkins University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Zhao_Resisting_Large_Data_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 554762,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10350718643378606728&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ics.uci.edu;jhu.edu;ics.uci.edu;jhu.edu;jhu.edu",
        "email": "ics.uci.edu;jhu.edu;ics.uci.edu;jhu.edu;jhu.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;0;1;1",
        "aff_unique_norm": "University of California, Irvine;Johns Hopkins University;Tencent",
        "aff_unique_dep": ";;Hippocrates Research Labs",
        "aff_unique_url": "https://www.uci.edu;https://www.jhu.edu;https://www.tencent.com",
        "aff_unique_abbr": "UCI;JHU;Tencent",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;0+1;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Reverse Variational Autoencoder for Visual Attribute Manipulation and Anomaly Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Lydia_Reverse_Variational_Autoencoder_for_Visual_Attribute_Manipulation_and_Anomaly_Detection_WACV_2020_paper.html",
        "author": "Gauerhof Lydia;  Nianlong Gu",
        "abstract": "In this paper, we introduce the `Reverse Variational Autoencoder\" (Reverse-VAE) which is a generative network. On the one hand, visual attributes can be manipulated and combined while generating images. On the other hand, anomalies, meaning deviations from the data space used for training, can be detected. During training the generator network maps samples from stochastic latent vectors to the data space. Meanwhile the encoder network takes these generated images to reconstruct the latent vector. The generator and discriminator are trained adversarially. The discriminator is trained to distinguish between real and generated data.   Overall, our model tries to match the joint latent/data-space distribution of the generator and the latent/data-space joint distribution of the encoder by minimizing their Kullback-Leibler divergence. Desired visual attributes of CelebA images are successfully manipulated. The performance of anomaly detection is competitive with state-of-the-art on MNIST and KDD 99 data set.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Lydia_Reverse_Variational_Autoencoder_for_Visual_Attribute_Manipulation_and_Anomaly_Detection_WACV_2020_paper.pdf",
        "aff": "Corporate Research, Robert Bosch GmbH; Institute of Neuroinformatics, ETH Zurich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4037327,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11398576571373470250&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff_domain": "de.bosch.com;ethz.ch",
        "email": "de.bosch.com;ethz.ch",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Robert Bosch GmbH;ETH Zurich",
        "aff_unique_dep": "Corporate Research;Institute of Neuroinformatics",
        "aff_unique_url": "https://www.bosch.com;https://www.ethz.ch",
        "aff_unique_abbr": "Bosch;ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "title": "Robust Explanations for Visual Question Answering",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Patro_Robust_Explanations_for_Visual_Question_Answering_WACV_2020_paper.html",
        "author": "Badri Patro;  Shivansh Patel;  Vinay Namboodiri",
        "abstract": "In this paper, we propose a method to obtain robust explanations for visual question answering(VQA) that correlate well with the answers. Our model explains the answers obtained through a VQA model by providing visual and textual explanations. The main challenges that we address are i) Answers and textual explanations obtained by current methods are not well correlated and ii) Current methods for visual explanation do not focus on the right location for explaining the answer. We address both these challenges by using a collaborative correlated module which ensures that even if we do not train for noise based attacks, the enhanced correlation ensures that the right explanation and answer can be generated. We further show that this also aids in improving the generated visual and textual explanations. The use of the correlated module can be thought of as a robust method to verify if the answer and explanations are coherent. We evaluate this model using VQA-X dataset. We observe that the proposed method yields better textual and visual justification that supports the decision. We showcase the robustness of the model against a noise-based perturbation attack using corresponding visual and textual explanations. A detailed empirical analysis is shown.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Patro_Robust_Explanations_for_Visual_Question_Answering_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Technology, Kanpur; Indian Institute of Technology, Kanpur; Indian Institute of Technology, Kanpur",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1611095,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13315968884160177520&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "email": "iitk.ac.in;iitk.ac.in;iitk.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Kanpur",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kanpur",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Robust Face Detection via Learning Small Faces on Hard Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhang_Robust_Face_Detection_via_Learning_Small_Faces_on_Hard_Images_WACV_2020_paper.html",
        "author": "Zhishuai Zhang;  Wei Shen;  Siyuan Qiao;  Yan Wang;  Bo Wang;  Alan Yuille",
        "abstract": "Recent anchor-based deep face detectors have achieved promising performance, but they are still struggling to detect hard faces, such as small, blurred and partially occluded faces. One reason is that they treat all images and faces equally, and ignore the imbalance between easy images and hard images; however large amounts of training images only contain easy faces, which are less helpful to learn robust detectors for hard faces. In this paper, we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images. Our intuitions are (1) hard images are the images which contain at least one hard face, thus they facilitate training robust face detectors; (2) most hard faces are small faces and other types of hard faces can be easily shrunk to small faces. To this end, we build an anchor-based deep face detector, which only outputs a single high-resolution feature map with small anchors, to specifically learn small faces and train it by a novel hard image mining strategy which automatically adjusts training weights on images according to their difficulties. Extensive experiments have been conducted on WIDER FACE, FDDB, Pascal Faces, and AFW datasets and our method achieves APs of 95.7, 94.9 and 89.7 on easy, medium and hard WIDER FACE val dataset respectively, which verify the effectiveness of our methods, especially on detecting hard faces. Our detector is also lightweight and enjoys a fast inference speed. Code and model are available at https://github.com/bairdzhang/smallhardface.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Robust_Face_Detection_via_Learning_Small_Faces_on_Hard_Images_WACV_2020_paper.pdf",
        "aff": "Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Stanford University; Johns Hopkins University",
        "project": "",
        "github": "https://github.com/bairdzhang/smallhardface",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2197332,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5301073940205823295&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com;gmail.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "Johns Hopkins University;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jhu.edu;https://www.stanford.edu",
        "aff_unique_abbr": "JHU;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Robust Facial Landmark Detection via Aggregation on Geometrically Manipulated Faces",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Iranmanesh_Robust_Facial_Landmark_Detection_via_Aggregation_on_Geometrically_Manipulated_Faces_WACV_2020_paper.html",
        "author": "Seyed Mehdi Iranmanesh;  Ali Dabouei;  Sobhan Soleymani;  Hadi Kazemi;  Nasser Nasrabadi",
        "abstract": "In this work, we present a practical approach to the problem of facial landmark detection. The proposed method can deal with large shape and appearance variations under the rich shape deformation. To handle the shape variations we equip our method with the aggregation of manipulated face images. The proposed framework generates different manipulated faces using only one given face image. The approach utilizes the fact that small but carefully crafted geometric manipulation in the input domain can fool deep face recognition models.   We propose three different approaches to generate manipulated faces in which two of them perform the manipulations via adversarial attacks and the other one uses known transformations. Aggregating the manipulated faces provides a more robust landmark detection approach which is able to capture more important deformations and variations of the face shapes. Our approach is demonstrated its superiority compared to the state-of-the-art method on benchmark datasets AFLW, 300-W, and COFW.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Iranmanesh_Robust_Facial_Landmark_Detection_via_Aggregation_on_Geometrically_Manipulated_Faces_WACV_2020_paper.pdf",
        "aff": "West Virginia University; West Virginia University; West Virginia University; West Virginia University; West Virginia University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1882944,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11959000109564409308&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mix.wvu.edu;mix.wvu.edu;mix.wvu.edu;mix.wvu.edu;mail.wvu.edu",
        "email": "mix.wvu.edu;mix.wvu.edu;mix.wvu.edu;mix.wvu.edu;mail.wvu.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "West Virginia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wvu.edu",
        "aff_unique_abbr": "WVU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Robust Feature Tracking in DVS Event Stream using Bezier Mapping",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Seok_Robust_Feature_Tracking_in_DVS_Event_Stream_using_Bezier_Mapping_WACV_2020_paper.html",
        "author": "Hochang Seok;  Jongwoo Lim",
        "abstract": "Unlike conventional cameras, event cameras capture the intensity changes at each pixel with very little delay. Such changes are recorded as an event stream with their positions, timestamps, and polarities continuously, thus there is no notion of 'frame' as in conventional cameras. As many applications including 3D pose estimation use 2D trajectories of feature points, it is necessary to detect and track the feature points robustly and accurately in a continuous event stream. In conventional feature tracking algorithms for event streams, the events in fixed time intervals are converted into the event images by stacking the events at their pixel locations, and the features are tracked in the event images. Such simple stacking of events yields blurry event images due to the camera motion, and it can significantly degrade the tracking quality. We propose to align the events in the time intervals along Bezier curves to minimize the misalignment. Since the camera motion is unknown, the Bezier curve is estimated to maximize the variance of the warped event pixels. Instead of the initial patches for tracking, we use the temporally integrated template patches, as it captures rich texture information from accurately aligned events. Extensive experimental evaluations in 2D feature tracking as well as 3D pose estimation show that our method significantly outperforms the conventional approaches.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Seok_Robust_Feature_Tracking_in_DVS_Event_Stream_using_Bezier_Mapping_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science, Hanyang University, Seoul, Korea; Department of Computer Science, Hanyang University, Seoul, Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Seok_Robust_Feature_Tracking_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2964441,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4309036477501144553&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "hanyang.ac.kr;hanyang.ac.kr",
        "email": "hanyang.ac.kr;hanyang.ac.kr",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hanyang University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "http://www.hanyang.ac.kr",
        "aff_unique_abbr": "HYU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Robust Template-Based Non-Rigid Motion Tracking Using Local Coordinate Regularization",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Li_Robust_Template-Based_Non-Rigid_Motion_Tracking_Using_Local_Coordinate_Regularization_WACV_2020_paper.html",
        "author": "Wei Li;  Shang Zhao;  Xiao Xiao;  James Hahn",
        "abstract": "In this paper, we propose our template-based non-rigid registration algorithm to address the misalignments in the frame-to-frame motion tracking with single or multiple commodity depth cameras. We analyze the deformation in the local coordinates of neighboring nodes and use this differential representation to formulate the regularization term for the deformation field in our non-rigid registration. The local coordinate regularizations vary for each pair of neighboring nodes based on the tracking status of the surface regions. We propose our tracking strategies for different surface regions to minimize misalignments and reduce error accumulation. This method can thus preserve local geometric features and prevent undesirable distortions. Moreover, we introduce a geodesic-based correspondence estimation algorithm to align surfaces with large displacements. Finally, we demonstrate the effectiveness of our proposed method with detailed experiments.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Li_Robust_Template-Based_Non-Rigid_Motion_Tracking_Using_Local_Coordinate_Regularization_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science, The George Washington University; Department of Computer Science, The George Washington University; Department of Computer Science, The George Washington University; Department of Computer Science, The George Washington University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1272384,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9602917833360508837&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gwu.edu;gwu.edu;gwu.edu;gwu.edu",
        "email": "gwu.edu;gwu.edu;gwu.edu;gwu.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The George Washington University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.gwu.edu",
        "aff_unique_abbr": "GWU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Robust estimation of local affine maps and its applications to image matching",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/RODRIGUEZ_Robust_estimation_of_local_affine_maps_and_its_applications_to_WACV_2020_paper.html",
        "author": "Mariano RODRIGUEZ;  Gabriele Facciolo;  Rafael Grompone von Gioi;  Pablo Muse;  Julie Delon",
        "abstract": "The classic approach to image matching consists in the detection, description and matching of keypoints. This defines a zero-order approximation of the mapping between two images, determined by corresponding point coordinates. But the patches around keypoints typically contain more information, which may be exploited to obtain a first-order approximation of the mapping, incorporating local affine maps between corresponding keypoints. In this work, we propose a LOCal Affine Transform Estimator (LOCATE) method based on neural networks. We show that LOCATE drastically improves the accuracy of local geometry estimation by tracking inverse maps. A second contribution on guided matching and refinement is presented. The novelty here consists in the use of LOCATE to propose new SIFT-keypoint correspondences with precise locations, orientations and scales. Our experiments show that the precision gain provided by LOCATE does play an important role in applications such as guided matching. The third contribution of this paper consists in a modification to the RANSAC algorithm, that use LOCATE to improve the homography estimation between a pair of images. These approaches outperform RANSAC for different choices of image descriptors and image datasets, and permit to increase the probability of success in identifying image pairs in challenging matching databases.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/RODRIGUEZ_Robust_estimation_of_local_affine_maps_and_its_applications_to_WACV_2020_paper.pdf",
        "aff": "CMLA, ENS Paris-Saclay, France; CMLA, ENS Paris-Saclay, France; CMLA, ENS Paris-Saclay, France; IIE, Universidad de la Rep \u00b4ublica, Uruguay; MAP5, Universit \u00b4e Paris Descartes, France",
        "project": "",
        "github": "https://rdguez-mariano.github.io/pages/locate",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2551815,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=870671073324891344&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "\u00c9cole Normale Sup\u00e9rieure Paris-Saclay;Universidad de la Republica;Universit\u00e9 Paris Descartes",
        "aff_unique_dep": "CMLA;Instituto de Ingenieria Electrica;MAP5",
        "aff_unique_url": "https://www.ens-paris-saclay.fr;https://www.unurepublica.edu.uy;https://www.univ-paris5.fr",
        "aff_unique_abbr": "ENS Paris-Saclay;Udelar;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Paris-Saclay;",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "France;Uruguay"
    },
    {
        "title": "Rotation-invariant Mixed Graphical Model Network for 2D Hand Pose Estimation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kong_Rotation-invariant_Mixed_Graphical_Model_Network_for_2D_Hand_Pose_Estimation_WACV_2020_paper.html",
        "author": "Deying Kong;  Haoyu Ma;  Yifei Chen;  Xiaohui Xie",
        "abstract": "In this paper, we propose a new architecture named Rotation-invariant Mixed Graphical Model Network (R-MGMN) to solve the problem of 2D hand pose estimation from a monocular RGB image.  By integrating a rotation net, the R-MGMN is invariant to rotations of the hand in the image. It also has a pool of graphical models, from which a combination of graphical models could be selected, conditioning on the input image. Belief propagation is performed on each graphical model separately, generating a set of marginal distributions, which are taken as the confidence maps of hand keypoint positions.  Final confidence maps are obtained by aggregating these confidence maps together.  We evaluate the R-MGMN on two public hand pose datasets. Experiment results show our model outperforms the state-of-the-art algorithm which is widely used in 2D hand pose estimation by a noticeable margin.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kong_Rotation-invariant_Mixed_Graphical_Model_Network_for_2D_Hand_Pose_Estimation_WACV_2020_paper.pdf",
        "aff": "University of California, Irvine; University of California, Irvine; Tencent Hippocrates Research Lab; University of California, Irvine",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 649116,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9023176448151885637&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uci.edu;uci.edu;tencent.com;uci.edu",
        "email": "uci.edu;uci.edu;tencent.com;uci.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of California, Irvine;Tencent",
        "aff_unique_dep": ";Hippocrates Research Lab",
        "aff_unique_url": "https://www.uci.edu;https://www.tencent.com",
        "aff_unique_abbr": "UCI;Tencent",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "SINet: Extreme Lightweight Portrait Segmentation Networks with Spatial Squeeze Module and Information Blocking Decoder",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Park_SINet_Extreme_Lightweight_Portrait_Segmentation_Networks_with_Spatial_Squeeze_Module_WACV_2020_paper.html",
        "author": "Hyojin Park;  Lars Sjosund;  Youngjoon Yoo;  Nicolas Monet;  Jihwan Bang;  Nojun Kwak",
        "abstract": "Designing a lightweight and robust portrait segmentation algorithm is an important task for a wide range of face applications. However, the problem has been considered as a subset of the object segmentation and less handled in this field. Obviously, portrait segmentation has its unique requirements. First, because the portrait segmentation is performed in the middle of a whole process, it requires extremely lightweight models. Second, there has not been any public datasets in this domain that contain a sufficient number of images. To solve the first problem, we introduce the new extremely lightweight portrait segmentation model SINet, containing an information blocking decoder and spatial squeeze modules. The information blocking decoder uses confidence estimation to recover local spatial information without spoiling global consistency. The spatial squeeze module uses multiple receptive fields to cope with various sizes of consistency. To tackle the second problem, we propose a simple method to create additional portrait segmentation data, which can improve accuracy. In our qualitative and quantitative analysis on the EG1800 dataset, we show that our method outperforms various existing lightweight models. Our method reduces the number of parameters from 2:1M to 86:9K (around 95.9% reduction), while maintaining the accuracy under an 1% margin from the state-of-the-art method. We also show our model is successfully executed on a real mobile device with 100.6 FPS. In addition, we demonstrate that our method can be used for general semantic segmentation on the Cityscapes dataset. The code and dataset are available in https://github.com/HYOJINPARK/ExtPortraitSeg.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Park_SINet_Extreme_Lightweight_Portrait_Segmentation_Networks_with_Spatial_Squeeze_Module_WACV_2020_paper.pdf",
        "aff": "Seoul National University; Clova AI, NA VER Corp; Clova AI, NA VER Corp; NA VER LABS Europe; Search Solutions, Inc; Seoul National University",
        "project": "",
        "github": "https://github.com/HYOJINPARK/ExtPortraitSeg",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1221012,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4352595058074674941&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "snu.ac.kr;navercorp.com;navercorp.com;naverlabs.com;navercorp.com;snu.ac.kr",
        "email": "snu.ac.kr;navercorp.com;navercorp.com;naverlabs.com;navercorp.com;snu.ac.kr",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2;3;0",
        "aff_unique_norm": "Seoul National University;Clova AI;NAVER LABS Europe;Search Solutions, Inc",
        "aff_unique_dep": ";Clova AI;;",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.clova.ai;https://www.naverlabs.com/europe;",
        "aff_unique_abbr": "SNU;Clova AI;NAVER LABS Europe;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;2;0",
        "aff_country_unique": "South Korea;Unknown;United States"
    },
    {
        "title": "SVIRO: Synthetic Vehicle Interior Rear Seat Occupancy Dataset and Benchmark",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Da_Cruz_SVIRO_Synthetic_Vehicle_Interior_Rear_Seat_Occupancy_Dataset_and_Benchmark_WACV_2020_paper.html",
        "author": "Steve Dias Da Cruz;  Oliver Wasenmuller;  Hans-Peter Beise;  Thomas Stifter;  Didier Stricker",
        "abstract": "We release SVIRO, a synthetic dataset for sceneries in the passenger compartment of ten different vehicles, in order to analyze machine learning-based approaches for their generalization capacities and reliability when trained on a limited number of variations (e.g. identical backgrounds and textures, few instances per class). This is in contrast to the intrinsically high variability of common benchmark datasets, which focus on improving the state-of-the-art of general tasks. Our dataset contains bounding boxes for object detection, instance segmentation masks, keypoints for pose estimation and depth images for each synthetic scenery as well as images for each individual seat for classification. The advantage of our use-case is twofold: The proximity to a realistic application to benchmark new approaches under novel circumstances while reducing the complexity to a more tractable environment, such that applications and theoretical questions can be tested on a more challenging dataset as toy problems. The data and evaluation server are available under https://sviro.kl.dfki.de.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Da_Cruz_SVIRO_Synthetic_Vehicle_Interior_Rear_Seat_Occupancy_Dataset_and_Benchmark_WACV_2020_paper.pdf",
        "aff": "IEE S.A. + DFKI - German Research Center for Artificial Intelligence + University of Kaiserslautern; DFKI - German Research Center for Artificial Intelligence; Trier University of Applied Sciences; IEE S.A.; DFKI - German Research Center for Artificial Intelligence + University of Kaiserslautern",
        "project": "https://sviro.kl.dfki.de",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Da_Cruz_SVIRO_Synthetic_Vehicle_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1176420,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7498832204955105052&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "iee.lu;dfki.de;inf.hochschule-trier.de;iee.lu;dfki.de",
        "email": "iee.lu;dfki.de;inf.hochschule-trier.de;iee.lu;dfki.de",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;1;3;0;1+2",
        "aff_unique_norm": "IEE S.A.;German Research Center for Artificial Intelligence;University of Kaiserslautern;Trier University of Applied Sciences",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";https://www.dFKI.de;https://www.uni-kl.de;https://www.fh-trier.de",
        "aff_unique_abbr": ";DFKI;Uni KL;FH Trier",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1+1;1;1;0;1+1",
        "aff_country_unique": "Unknown;Germany"
    },
    {
        "title": "ScaIL: Classifier Weights Scaling for Class Incremental Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Belouadah_ScaIL_Classifier_Weights_Scaling_for_Class_Incremental_Learning_WACV_2020_paper.html",
        "author": "Eden Belouadah;  Adrian Popescu",
        "abstract": "Incremental learning is useful if an AI agent needs to integrate data from a stream.  The problem is non trivial if the agent runs on a limited computational budget and has a bounded memory of past data. In a deep learning approach, the constant computational budget requires the use of a fixed architecture for all incremental states. The bounded memory generates imbalance in favor of new classes and a prediction bias toward them appears.  This bias is commonly countered by introducing a data balancing step in addition to the basic network training. We depart from this approach and propose simple but efficient scaling of past classifiers' weights to make them more comparable to those of new classes. Scaling exploits incremental state statistics and is applied to the classifiers learned in the initial state of classes to profit from all their available data.  We also question the utility of the widely used distillation loss component of incremental learning algorithms by comparing it to  vanilla fine tuning in presence of a bounded memory.  Evaluation is done against competitive baselines using four public datasets. Results show that the classifier weights scaling and the removal of the distillation are both beneficial.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Belouadah_ScaIL_Classifier_Weights_Scaling_for_Class_Incremental_Learning_WACV_2020_paper.pdf",
        "aff": "Universit\u00e9 Paris-Saclay, CEA, D\u00e9partement Intelligence Ambiante et Syst\u00e8mes Interactifs; Universit\u00e9 Paris-Saclay, CEA, D\u00e9partement Intelligence Ambiante et Syst\u00e8mes Interactifs",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Belouadah_ScaIL_Classifier_Weights_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 708532,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12764127239873399701&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cea.fr;cea.fr",
        "email": "cea.fr;cea.fr",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 Paris-Saclay",
        "aff_unique_dep": "D\u00e9partement Intelligence Ambiante et Syst\u00e8mes Interactifs",
        "aff_unique_url": "https://www.universite-paris-saclay.fr",
        "aff_unique_abbr": "UPS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Scalable Detection of Offensive and Non-compliant Content / Logo in Product Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Gandhi_Scalable_Detection_of_Offensive_and_Non-compliant_Content__Logo_in_WACV_2020_paper.html",
        "author": "Shreyansh Gandhi;  Samrat Kokkula;  Abon Chaudhuri;  Alessandro Magnani;  Theban Stanley;  Behzad Ahmadi;  Venkatesh  Kandaswamy;  Omer Ovenc;  Shie Mannor",
        "abstract": "In e-commerce, product content, especially product images have a significant influence on a customer's journey from product discovery to evaluation and finally, purchase decision. Since many e-commerce retailers sell items from other third-party marketplace sellers besides their own, the content published by both internal and external content creators needs to be monitored and enriched, wherever possible. Despite guidelines and warnings, product listings that contain offensive and non-compliant images continue to enter catalogs. Offensive and non-compliant content can include a wide range of objects, logos, and banners conveying violent, sexually explicit, racist, or promotional messages. Such images can severely damage the customer experience, lead to legal issues, and erode the company brand. In this paper, we present a computer vision driven offensive and non-compliant image detection system for extremely large image datasets. This paper delves into the unique challenges of applying deep learning to real-world product image data from retail world. We demonstrate how we resolve a number of technical challenges such as lack of training data, severe class imbalance, fine-grained class definitions etc. using a number of practical yet unique technical strategies. Our system combines state-of-the-art image classification and object detection techniques with budgeted crowdsourcing to develop a solution customized for a massive, diverse, and constantly evolving product catalog.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Gandhi_Scalable_Detection_of_Offensive_and_Non-compliant_Content__Logo_in_WACV_2020_paper.pdf",
        "aff": "Walmart Labs; Walmart Labs; Walmart Labs; Walmart Labs; Walmart Labs; Walmart Labs; Walmart Labs; Walmart Labs; Technion",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2098302,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4541941397720803705&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "walmartlabs.com; ; ; ; ; ; ; ;ee.technion.ac.il",
        "email": "walmartlabs.com; ; ; ; ; ; ; ;ee.technion.ac.il",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;0;1",
        "aff_unique_norm": "Walmart;Technion - Israel Institute of Technology",
        "aff_unique_dep": "Walmart Labs;",
        "aff_unique_url": "https://www.walmart.com;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Walmart Labs;Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "title": "Scale Match for Tiny Person Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Yu_Scale_Match_for_Tiny_Person_Detection_WACV_2020_paper.html",
        "author": "Xuehui Yu;  Yuqi Gong;  Nan Jiang;  Qixiang Ye;  Zhenjun  Han",
        "abstract": "Visual object detection has achieved unprecedented advance with the rise of deep convolutional neural networks.However,  detecting tiny objects (for example tiny persons less than 20 pixels) in large-scale images remains challenging. The extremely small objects raise a grand challenge about  feature  representation  while  the  massive  and  complex  backgrounds  aggregates  the  risk  of  false  detections. In this paper,  we introduce a new benchmark,  referred to as  TinyPerson,  opening  up  a  promising  direction  for  tiny object detection in a long distance and with massive back-grounds. We experimentally find that the scale mismatch be-tween the dataset for network pretraining and the dataset for detector learning could deteriorate the feature representation and the detectors.  Accordingly, we propose a simple yet effective Scale Match approach to align the object scales between the two datasets for favorable tiny-object representation.  Experiments show the significant performance gain of  our  proposed  approach  over  state-of-the-art  detectors, and the challenging aspects of TinyPerson related to real-world scenarios. The TinyPerson benchmark and the code for our approach will be publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Yu_Scale_Match_for_Tiny_Person_Detection_WACV_2020_paper.pdf",
        "aff": "University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China",
        "project": "",
        "github": "https://github.com/ucas-vg/TinyBenchmark",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3926165,
        "gs_citation": 360,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7608309077228430075&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mails.ucas.ac.cn;mails.ucas.ac.cn;mails.ucas.ac.cn;ucas.ac.cn;ucas.ac.cn",
        "email": "mails.ucas.ac.cn;mails.ucas.ac.cn;mails.ucas.ac.cn;ucas.ac.cn;ucas.ac.cn",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Chinese Academy of Sciences",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ucas.ac.cn",
        "aff_unique_abbr": "UCAS",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Scale-aware Conditional Generative Adversarial Network for Image Dehazing",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sharma_Scale-aware_Conditional_Generative_Adversarial_Network_for_Image_Dehazing_WACV_2020_paper.html",
        "author": "Prasen Sharma;  Priyankar Jain;  Arijit Sur",
        "abstract": "Outdoor images are often deteriorated due to the presence of haze in the atmosphere. Conventionally, the single image dehazing problem aims to restore the haze-free image. Previous successful approaches have utilized various hand-crafted features/priors. However, such images suffer from color degradation and halo artifacts. By way of analysis, these artifacts, in general, prevail around the regions with high-intensity variation, such as edgy structures. This finding inspires us to consider the Laplacians of Gaussian (LoG) of the images which exceptionally retains this information, to solve the problem of single image haze removal. In this line of thought, we present an end-to-end model that learns to remove the haze based on the per-pixel difference between LoGs of the dehazed and original haze-free images. The optimization of the proposed network is further enhanced by using the adversarial training and perceptual loss function. The proposed method has been appraised on Synthetic Objective Testing Set (SOTS) and benchmark realworld hazy images using 16 image quality measures. Based on the Color Difference (CIEDE 2000), an improvement of   15.89% has been observed over the state-of-the-art method, Yang et al. [50]. An ablation study has been presented at the end to illustrate the improvements achieved by various modules of the proposed network.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sharma_Scale-aware_Conditional_Generative_Adversarial_Network_for_Image_Dehazing_WACV_2020_paper.pdf",
        "aff": "IIT Guwahati; IIT Guwahati; IIT Guwahati",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Sharma_Scale-aware_Conditional_Generative_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 9068947,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14807735143391916339&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff_domain": "iitg.ac.in;iitg.ac.in;iitg.ac.in",
        "email": "iitg.ac.in;iitg.ac.in;iitg.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Guwahati",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitg.ac.in",
        "aff_unique_abbr": "IITG",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Guwahati",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "See the Sound, Hear the Pixels",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ramaswamy_See_the_Sound_Hear_the_Pixels_WACV_2020_paper.html",
        "author": "Janani Ramaswamy;  Sukhendu Das",
        "abstract": "For every event occurring in the real world, most often a sound is associated with the corresponding visual scene. Humans possess an inherent ability to automatically map the audio content with visual scenes leading to an effortless and enhanced understanding of the underlying event. This triggers an interesting question: Can this natural correspondence between video and audio, which has been diminutively explored so far, be learned by a machine and modeled jointly to localize the sound source in a visual scene? In this paper, we propose a novel algorithm that addresses the problem of localizing sound source in unconstrained videos, which uses efficient fusion and attention mechanisms. Two novel blocks namely, Audio Visual Fusion Block (AVFB) and Segment-Wise Attention Block (SWAB) have been developed for this purpose. Quantitative and qualitative evaluations show that it is feasible to use the same algorithm with minor modifications to serve the purpose of sound localization using three different types of learning: supervised, weakly supervised and unsupervised. A novel Audio Visual Triplet Gram Matrix Loss (AVTGML) has been proposed as a loss function to learn the localization in an unsupervised way. Our empirical evaluations demonstrate a significant increase in performance over the existing state-of-the-art methods, serving as a testimony to the superiority of our proposed approach.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ramaswamy_See_the_Sound_Hear_the_Pixels_WACV_2020_paper.pdf",
        "aff": "Visualization and Perception Lab, Dept. of Computer Science and Engineering, IIT Madras, India; Visualization and Perception Lab, Dept. of Computer Science and Engineering, IIT Madras, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Ramaswamy_See_the_Sound_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1104433,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18292754251048199977&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cse.iitm.ac.in;iitm.ac.in",
        "email": "cse.iitm.ac.in;iitm.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IIT Madras",
        "aff_unique_dep": "Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitm.ac.in",
        "aff_unique_abbr": "IITM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Self-Attention Network for Skeleton-based Human Action Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Cho_Self-Attention_Network_for_Skeleton-based_Human_Action_Recognition_WACV_2020_paper.html",
        "author": "Sangwoo Cho;  Muhammad Maqbool;  Fei Liu;  Hassan Foroosh",
        "abstract": "Skeleton-based action recognition has recently attracted a lot of attention. Researchers are coming up with new approaches for extracting spatio-temporal relations and making  considerable  progress  on  large-scale  skeleton based datasets. Most  of  the  architectures  being  proposed  are based  upon  recurrent  neural  networks  (RNNs),  convolutional  neural  networks  (CNNs)  and  graph-based  CNNs. When  it  comes  to  skeleton-based  action  recognition, the importance of long term contextual information is central which is not captured by the current architectures. In order to come up with a better representation and capturing of long term spatio-temporal relationships, we propose three variants of Self-Attention Network (SAN), namely, SAN-V1,SAN-V2 and SAN-V3. Our SAN variants has the impressive capability of extracting high-level semantics by capturing long-range correlations. We have also integrated the Temporal Segment Network (TSN) with our SAN variants which resulted in improved overall performance. Different configurations of Self-Attention Network (SAN) variants and Temporal Segment Network (TSN) are explored with extensive experiments. Our chosen configuration outperforms state-of-the-art Top-1 and Top-5 by 4.4% and 2.9% respectively on Kinetics and shows consistently better performance than state-of-the-art methods on NTU RGB+D.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Cho_Self-Attention_Network_for_Skeleton-based_Human_Action_Recognition_WACV_2020_paper.pdf",
        "aff": "Computer Science Department, University of Central Florida, Orlando, FL 32816, USA; Computer Science Department, University of Central Florida, Orlando, FL 32816, USA; Computer Science Department, University of Central Florida, Orlando, FL 32816, USA; Computer Science Department, University of Central Florida, Orlando, FL 32816, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 908295,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12903427979590443407&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "knights.ucf.edu;knights.ucf.edu;cs.ucf.edu;cs.ucf.edu",
        "email": "knights.ucf.edu;knights.ucf.edu;cs.ucf.edu;cs.ucf.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Central Florida",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.ucf.edu",
        "aff_unique_abbr": "UCF",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Orlando",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Self-Contained Stylization via Steganography for Reverse and Serial Style Transfer",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_Self-Contained_Stylization_via_Steganography_for_Reverse_and_Serial_Style_Transfer_WACV_2020_paper.html",
        "author": "Hung-Yu Chen;  I-Sheng Fang;  Chia-Ming Cheng;  Wei-Chen Chiu",
        "abstract": "Style transfer has been widely applied to give real-world images a new artistic look. However, given a stylized image, the attempts to use typical style transfer methods for de-stylization or transferring it again into another style usually lead to artifacts or undesired results. We realize that these issues are originated from the content inconsistency between the original image and its stylized output. Therefore, in this paper we advance to keep the content information of the input image during the process of style transfer by the power of steganography, with two approaches proposed: a two-stage model and an end-to-end model. We conduct extensive experiments to successfully verify the capacity of our models, in which both of them are able to not only generate stylized images of quality comparable with the ones produced by typical style transfer methods, but also effectively eliminate the artifacts introduced in reconstructing original input from a stylized image as well as performing multiple times of style transfer in series.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Self-Contained_Stylization_via_Steganography_for_Reverse_and_Serial_Style_Transfer_WACV_2020_paper.pdf",
        "aff": "National Chiao Tung University, Taiwan + Purdue University; National Chiao Tung University, Taiwan + National Cheng Chi University; MediaTek Inc., Taiwan; National Chiao Tung University, Taiwan",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Chen_Self-Contained_Stylization_via_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1934648,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17675004882709634727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "purdue.edu;gmail.com;cs.nctu.edu.tw; ",
        "email": "purdue.edu;gmail.com;cs.nctu.edu.tw; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+2;3;0",
        "aff_unique_norm": "National Chiao Tung University;Purdue University;National Cheng Chi University;MediaTek Inc.",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.nctu.edu.tw;https://www.purdue.edu;https://www.nccu.edu.tw;https://www.mediatek.com/",
        "aff_unique_abbr": "NCTU;Purdue;NCCU;MediaTek",
        "aff_campus_unique_index": "0;0+0;0;0",
        "aff_campus_unique": "Taiwan;",
        "aff_country_unique_index": "0+1;0+0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Self-Growing Spatial Graph Networks for Pedestrian Trajectory Prediction",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Haddad_Self-Growing_Spatial_Graph_Networks_for_Pedestrian_Trajectory_Prediction_WACV_2020_paper.html",
        "author": "Sirin Haddad;  Siew-Kei Lam",
        "abstract": "Intelligent vehicles and social robots need to navigate in crowded environments while avoiding collisions with pedestrians. To achieve this, pedestrian trajectory prediction is essential. However, predicting pedestrians' trajectory in crowded environments is nontrivial as human-to-human interactions among the crowd participants influence their motion.  In this work, we propose a novel end-to-end graph-centric gated learning model to estimate the existence of interactions between individuals. Accordingly, the model predicts pedestrians' future locations and velocities. Recent methods based on LSTM networks used thresholding techniques to define neighborhood boundaries and relationships. Other graph-structured methods grow edges in polynomial size. In contrast, our graph-based GRU network model employs an online data-driven criterion that can learn from interactions and grow connections between pedestrian nodes. The proposed model yields outperforming prediction accuracy over state-of-the-art works in two public datasets, i.e. Crowds and SDD.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Haddad_Self-Growing_Spatial_Graph_Networks_for_Pedestrian_Trajectory_Prediction_WACV_2020_paper.pdf",
        "aff": "Nanyang Technological University; Nanyang Technological University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1052820,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6188711622195945397&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "e.ntu.edu.sg;ntu.edu.sg",
        "email": "e.ntu.edu.sg;ntu.edu.sg",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Self-Guided Novel View Synthesis via Elastic Displacement Network",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Liu_Self-Guided_Novel_View_Synthesis_via_Elastic_Displacement_Network_WACV_2020_paper.html",
        "author": "Yicun Liu;  Jiawei Zhang;  Ye Ma;  Jimmy Ren",
        "abstract": "Synthesizing a novel view from different viewpoints has been an essential problem in 3D vision. Among a variety of view synthesis tasks, single image based view synthesis is particularly challenging. Recent works address this problem by a fixed number of image planes of discrete disparities, which tend to generate structurally inconsistent results on wide-baseline, scene-complicated datasets such as KITTI. In this paper, we propose the Self-Guided Elastic Displacement Network (SG-EDN), which explicitly models the geometric transformation by a novel non-discrete scene representation called layered displacement maps (LDM). To generate realistic views, we exploit the positional characteristics of the displacement maps and design a multi-scale structural pyramid for self-guided filtering on the displacement maps. To optimize efficiency and scene-adaptivity, we allow the effective range of each displacement map to be elastic, with fully learnable parameters. Experimental results confirm that our framework outperforms existing methods in both quantitative and qualitative tests.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Liu_Self-Guided_Novel_View_Synthesis_via_Elastic_Displacement_Network_WACV_2020_paper.pdf",
        "aff": "SenseTime Research+1; SenseTime Research; SenseTime Research; SenseTime Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6452910,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14243856138433949506&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "columbia.edu;sensetime.com;sensetime.com;sensetime.com",
        "email": "columbia.edu;sensetime.com;sensetime.com;sensetime.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "SenseTime;",
        "aff_unique_dep": "SenseTime Research;",
        "aff_unique_url": "https://www.sensetime.com;",
        "aff_unique_abbr": "SenseTime;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "title": "Self-Orthogonality Module: A Network Architecture Plug-in for Learning Orthogonal Filters",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zhang_Self-Orthogonality_Module_A_Network_Architecture_Plug-in_for_Learning_Orthogonal_Filters_WACV_2020_paper.html",
        "author": "Ziming Zhang;  WENCHI MA;  Yuanwei Wu;  Guanghui Wang",
        "abstract": "In this paper, we investigate the empirical impact of or- thogonality regularization (OR) in deep learning, either solo or collaboratively. Recent works on OR showed some promis- ing results on the accuracy. In our ablation study, however, we do not observe such significant improvement from exist- ing OR techniques compared with the conventional training based on weight decay, dropout, and batch normalization. To identify the real gain from OR, inspired by the locality sensitive hashing (LSH) in angle estimation, we propose to introduce an implicit self-regularization into OR to push the mean and variance of filter angles in a network towards 90 * and 0 * simultaneously to achieve (near) orthogonality among the filters, without using any other explicit regular- ization. Our regularization can be implemented as an archi- tectural plug-in and integrated with an arbitrary network. We reveal that OR helps stabilize the training process and leads to faster convergence and better generalization.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Self-Orthogonality_Module_A_Network_Architecture_Plug-in_for_Learning_Orthogonal_Filters_WACV_2020_paper.pdf",
        "aff": "Worcester Polytechnic Institute, MA + Mitsubishi Electric Research Laboratories (MERL); University of Kansas, KS; University of Kansas, KS; University of Kansas, KS",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1643853,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=580357277404971065&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "wpi.edu;ku.edu;ku.edu;ku.edu",
        "email": "wpi.edu;ku.edu;ku.edu;ku.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;2;2",
        "aff_unique_norm": "Worcester Polytechnic Institute;Mitsubishi Electric Research Laboratories;University of Kansas",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.wpi.edu;https://www.merl.com;https://www.ku.edu",
        "aff_unique_abbr": "WPI;MERL;KU",
        "aff_campus_unique_index": ";1;1;1",
        "aff_campus_unique": ";Lawrence",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Semantic Consistency and Identity Mapping Multi-Component Generative Adversarial Network for Person Re-Identification",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Khatun_Semantic_Consistency_and_Identity_Mapping_Multi-Component_Generative_Adversarial_Network_for_WACV_2020_paper.html",
        "author": "Amena Khatun;  SIMON DENMAN;  Sridha Sridharan;  Clinton  Fookes",
        "abstract": "In a real world environment, person re-identification (Re-ID) is a challenging task due to variations in lighting conditions, viewing angles, pose and occlusions. Despite recent performance gains, current person Re-ID algorithms still suffer heavily when encountering these variations. To address this problem, we propose a semantic consistency and identity mapping multi-component generative adversarial network (SC-IMGAN) which provides style adaptation from one to many domains. To ensure that transformed images are as realistic as possible, we propose novel identity mapping and semantic consistency losses to maintain identity across the diverse domains. For the Re-ID task, we propose a joint verification-identification quartet network which is trained with generated and real images, followed by an effective quartet loss for verification. Our proposed method outperforms state-of-the-art techniques on six challenging person Re-ID datasets: CUHK01, CUHK03, VIPeR, PRID2011, iLIDS and Market-1501.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Khatun_Semantic_Consistency_and_Identity_Mapping_Multi-Component_Generative_Adversarial_Network_for_WACV_2020_paper.pdf",
        "aff": "Image and Video Laboratory, Queensland University of Technology (QUT), Brisbane, QLD, Australia; Image and Video Laboratory, Queensland University of Technology (QUT), Brisbane, QLD, Australia; Image and Video Laboratory, Queensland University of Technology (QUT), Brisbane, QLD, Australia; Image and Video Laboratory, Queensland University of Technology (QUT), Brisbane, QLD, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2133145,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3463432625369838621&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "qut.edu.au;qut.edu.au;qut.edu.au;qut.edu.au",
        "email": "qut.edu.au;qut.edu.au;qut.edu.au;qut.edu.au",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Queensland University of Technology",
        "aff_unique_dep": "Image and Video Laboratory",
        "aff_unique_url": "https://www.qut.edu.au",
        "aff_unique_abbr": "QUT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Brisbane",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Shape Constrained Network for Eye Segmentation in the Wild",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Luo_Shape_Constrained_Network_for_Eye_Segmentation_in_the_Wild_WACV_2020_paper.html",
        "author": "Bingnan Luo;  Jie Shen;  Shiyang Cheng;  Yujiang Wang;  Maja Pantic",
        "abstract": "Semantic segmentation of eyes has long been a vital pre-processing step in many biometric applications. Majority of the works focus only on high resolution eye images, while little has been done to segment the eyes from low quality images in the wild. However, this is a particularly interesting and meaningful topic, as eyes play a crucial role in conveying the emotional state and mental well-being of a person. In this work, we take two steps toward solving this problem: (1) We collect and annotate a challenging eye segmentation dataset containing 8882 eye patches from 4461 facial images of different resolutions, illumination conditions and head poses; (2) We develop a novel eye segmentation method, Shape Constrained Network (SCN), that incorporates shape prior into the segmentation network training procedure. Specifically, we learn the shape prior from our dataset using VAE-GAN, and leverage the pre-trained encoder and discriminator to regularise the training of SegNet. To improve the accuracy and quality of predicted masks, we replace the loss of SegNet with three new losses: Intersection-over-Union (IoU) loss, shape discriminator loss and shape embedding loss. Extensive experiments shows that our method outperforms state-of-the-art segmentation and landmark detection methods in terms of mean IoU (mIoU) accuracy and the quality of segmentation masks. The dataset is available at https://ibug.doc.ic.ac.uk/resources/ibug-eye-segmentation-dataset/",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Luo_Shape_Constrained_Network_for_Eye_Segmentation_in_the_Wild_WACV_2020_paper.pdf",
        "aff": "Department of Computing, Imperial College London, UK+Samsung AI Centre, Cambridge, UK; Department of Computing, Imperial College London, UK+Samsung AI Centre, Cambridge, UK; Samsung AI Centre, Cambridge, UK; Department of Computing, Imperial College London, UK; Department of Computing, Imperial College London, UK+Samsung AI Centre, Cambridge, UK",
        "project": "https://ibug.doc.ic.ac.uk/resources/ibug-eye-segmentation-dataset/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2580275,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9681744923424562346&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;samsung.com;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;samsung.com;imperial.ac.uk;imperial.ac.uk",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;1;0;0+1",
        "aff_unique_norm": "Imperial College London;Samsung AI Centre",
        "aff_unique_dep": "Department of Computing;AI Centre",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.samsung.com/uk/",
        "aff_unique_abbr": "Imperial;",
        "aff_campus_unique_index": "0+1;0+1;1;0;0+1",
        "aff_campus_unique": "London;Cambridge",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Jandial_SieveNet_A_Unified_Framework_for_Robust_Image-Based_Virtual_Try-On_WACV_2020_paper.html",
        "author": "Surgan Jandial;  Ayush Chopra;  Kumar Ayush;  Mayur Hemani;  Balaji Krishnamurthy;  Abhijeet Halwai",
        "abstract": "Image-based virtual try-on for fashion has attracted considerable attention recently. The task requires trying on the desired clothing item on a target model. An efficient framework for this is composed of 2 stages: (1) warping (transforming) the try-on cloth to align with the pose and shape of the target model,  and  (2) a texture transfer module to seamlessly integrate the warped try-on cloth onto the target model image. Existing methods suffer from artifacts and distortions in their try-on output. In this work, we present SieveNet, a framework for robust image-based virtual try-on. Firstly, we introduce a multi-stage coarse-to-fine warping network to better model fine-grained intricacies in try-on clothing item and train it with a novel perceptual geometric matching loss. Next, we introduce a try-on cloth conditioned segmentation mask prior to improve the texture transfer network. Finally, we also introduce a dueling triplet strategy for training the texture transfer network which further improves the quality of the generated try-on result. We present extensive qualitative and quantitative evaluations on each component of the proposed pipeline and show significant performance improvements against existing state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Jandial_SieveNet_A_Unified_Framework_for_Robust_Image-Based_Virtual_Try-On_WACV_2020_paper.pdf",
        "aff": "Adobe+IIT Hyderabad; Adobe+Stanford University; Adobe+IIIT Hyderbad; Adobe; IIT Hyderabad; Adobe",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1092729,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13310042556151155637&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "adobe.com;adobe.com;adobe.com;adobe.com;iith.ac.in;adobe.com",
        "email": "adobe.com;adobe.com;adobe.com;adobe.com;iith.ac.in;adobe.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+2;0+3;0;1;0",
        "aff_unique_norm": "Adobe Inc.;Indian Institute of Technology, Hyderabad;Stanford University;International Institute of Information Technology, Hyderabad",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.adobe.com;https://www.iith.ac.in;https://www.stanford.edu;https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": "Adobe;IIT-H;Stanford;IIIT Hyderabad",
        "aff_campus_unique_index": "1;2;1;1",
        "aff_campus_unique": ";Hyderabad;Stanford",
        "aff_country_unique_index": "0+1;0+0;0+1;0;1;0",
        "aff_country_unique": "United States;India"
    },
    {
        "title": "Silhouette Guided Point Cloud Reconstruction beyond Occlusion",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Zou_Silhouette_Guided_Point_Cloud_Reconstruction_beyond_Occlusion_WACV_2020_paper.html",
        "author": "Chuhang Zou;  Derek Hoiem",
        "abstract": "One major challenge in 3D reconstruction is to infer the complete shape geometry from partial foreground occlusions. In this paper, we propose a method to reconstruct the complete 3D shape of an object from a single RGB image, with robustness to occlusion. Given the image and a silhouette of the visible region, our approach completes the silhouette of the occluded region and then generates a point cloud. We show improvements for reconstruction of non-occluded and partially occluded objects by providing the predicted complete silhouette as guidance. We also improve state-of-the-art for 3D shape prediction with a 2D reprojection loss from multiple synthetic views and a surface-based smoothing and refinement step. Experiments demonstrate the efficacy of our approach both quantitatively and qualitatively on synthetic and real scene datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Zou_Silhouette_Guided_Point_Cloud_Reconstruction_beyond_Occlusion_WACV_2020_paper.pdf",
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "https://github.com/zouchuhang/Silhouette-Guided-3D",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3032253,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=101551945696630811&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Simultaneous Detection and Removal of Dynamic Objects in Multi-view Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kanojia_Simultaneous_Detection_and_Removal_of_Dynamic_Objects_in_Multi-view_Images_WACV_2020_paper.html",
        "author": "Gagan Kanojia;  Shanmuganathan Raman",
        "abstract": "Consider a set of images of a scene consisting of moving objects captured using a hand-held camera. In this work, we propose an algorithm which takes this set of multi-view images as input, detects the dynamic objects present in the scene, and replaces them with the static regions which are being occluded by them. The proposed algorithm scans the reference image in the row-major order at the pixel level and classifies each pixel as static or dynamic. During the scan, when a pixel is classified as dynamic, the proposed algorithm replaces that pixel value with the corresponding pixel value of the static region which is being occluded by that dynamic region. We show that we achieve artifact-free removal of dynamic objects in multi-view images of several real-world scenes. To the best of our knowledge, we propose the first method which simultaneously detects and removes the dynamic objects present in multi-view images.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kanojia_Simultaneous_Detection_and_Removal_of_Dynamic_Objects_in_Multi-view_Images_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Technology Gandhinagar, India; Indian Institute of Technology Gandhinagar, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Kanojia_Simultaneous_Detection_and_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5498056,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16235440673157857529&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "iitgn.ac.in;iitgn.ac.in",
        "email": "iitgn.ac.in;iitgn.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Gandhinagar",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitgn.ac.in",
        "aff_unique_abbr": "IITGN",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Gandhinagar",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Single Satellite Optical Imagery Dehazing using SAR Image Prior Based on conditional Generative Adversarial Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Huang_Single_Satellite_Optical_Imagery_Dehazing_using_SAR_Image_Prior_Based_WACV_2020_paper.html",
        "author": "Binghui Huang;  Li Zhi;  Chao Yang;  Fuchun Sun;  Yixu Song",
        "abstract": "Satellite image dehazing aims at precisely retrieving the real situations of the obscured parts from the hazy remote sensing (RS) images, which is a challenging task since the hazy regions contain both ground features and haze components. Many approaches of removing haze focus on processing multi-spectral or RGB images, whereas few of them utilize multi-sensor data. The multi-sensor data fusion is significant to provide auxiliary information since RGB images are sensitive to atmospheric conditions. In this paper, a dataset called SateHaze1k is established and composed of 1200 pairs clear Synthetic Aperture Radar (SAR), hazy RGB, and corresponding ground truth images, which are divided into three degrees of the haze, i.e. thin, moderate, and thick fog. Moreover, we propose a novel fusion dehazing method to directly restore the haze-free RS images by using an end-to-end conditional generative adversarial network(cGAN). The proposed network combines the information of both RGB and SAR images to eliminate the image blurring. Besides, the dilated residual blocks of the generator can also sufficiently improve the dehazing effects. Our experiments demonstrate that the proposed method, which fuses the information of different sensors applied to the cloudy conditions, can achieve more precise results than other baseline models.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Huang_Single_Satellite_Optical_Imagery_Dehazing_using_SAR_Image_Prior_Based_WACV_2020_paper.pdf",
        "aff": "Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University; Shanghai University Of Engineering Science; Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University; Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University; Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 832613,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4810948151759978726&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "mails.tsinghua.edu.cn;foxmail.com;mails.tsinghua.edu.cn;tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;foxmail.com;mails.tsinghua.edu.cn;tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Tsinghua University;Shanghai University of Engineering Science",
        "aff_unique_dep": "Department of Computer Science and Technology;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;http://www.sues.edu.cn/",
        "aff_unique_abbr": "Tsinghua;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "SketchTransfer: A New Dataset for Exploring Detail-Invariance and the Abstractions Learned by Deep Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Lamb_SketchTransfer_A_New_Dataset_for_Exploring_Detail-Invariance_and_the_Abstractions_WACV_2020_paper.html",
        "author": "Alex Lamb;  Sherjil Ozair;  Vikas Verma;  David Ha",
        "abstract": "Deep networks have achieved excellent results in perceptual tasks, yet their ability to generalize to variations not seen during training has come under increasing scrutiny.  In this work we focus on their ability to have invariance towards the presence or absence of details.  For example, humans are able to watch cartoons, which are missing many visual details, without being explicitly trained to do so.  As another example, 3D rendering software is a relatively recent development, yet people are able to understand such rendered scenes even though they are missing details (consider a film like Toy Story).  This capability goes beyond visual data: humans are easily able to recognize isolated melodies from musical pieces when heard for the first time, even if the only piece they've listened to previously is from an orchestra.  Thus the failure of machine learning algorithms to do this indicates a significant gap in generalization between human abilities and the abilities of deep networks.  We propose a dataset that will make it easier to study the  detail-invariance problem concretely.  We produce a concrete task for this: SketchTransfer, and we show that state-of-the-art domain transfer algorithms still struggle with this task.  The state-of-the-art technique which achieves over 95% on MNIST \\xrightarrow   SVHN transfer only achieves 59% accuracy on the SketchTransfer task, which is much better than random (11% accuracy) but falls short of the 87% accuracy of a classifier trained directly on labeled sketches.  This indicates that this task is approachable with today's best methods but has substantial room for improvement.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Lamb_SketchTransfer_A_New_Dataset_for_Exploring_Detail-Invariance_and_the_Abstractions_WACV_2020_paper.pdf",
        "aff": "MILA; MILA; MILA + Aalto University, Finland; Google Brain",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 691901,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3674694237221402598&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "iro.umontreal.ca;gmail.com;aalto.fi;google.com",
        "email": "iro.umontreal.ca;gmail.com;aalto.fi;google.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;2",
        "aff_unique_norm": "MILA;Aalto University;Google",
        "aff_unique_dep": ";;Google Brain",
        "aff_unique_url": "https://mila.quebec;https://www.aalto.fi;https://brain.google.com",
        "aff_unique_abbr": "MILA;Aalto;Google Brain",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0+1;2",
        "aff_country_unique": "Canada;Finland;United States"
    },
    {
        "title": "Smart Hypothesis Generation for Efficient and Robust Room Layout Estimation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hirzer_Smart_Hypothesis_Generation_for_Efficient_and_Robust_Room_Layout_Estimation_WACV_2020_paper.html",
        "author": "Martin Hirzer;  Vincent Lepetit;  PETER ROTH",
        "abstract": "We propose a novel method to efficiently estimate the spatial layout of a room from a single monocular RGB image. As existing approaches based on low-level feature extraction, followed by a vanishing point estimation are very slow and often unreliable in realistic scenarios, we build on semantic segmentation of the input image. To obtain better segmentations, we introduce a robust, accurate and very efficient hypothesize-and-test scheme. The key idea is to use three segmentation hypotheses, each based on a different number of visible walls. For each hypothesis, we predict the image locations of the room corners and select the hypothesis for which the layout estimated from the room corners is consistent with the segmentation. We demonstrate the efficiency and robustness of our method on three challenging benchmark datasets, where we significantly outperform the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hirzer_Smart_Hypothesis_Generation_for_Efficient_and_Robust_Room_Layout_Estimation_WACV_2020_paper.pdf",
        "aff": "Institute of Computer Graphics and Vision, Graz University of Technology, Austria; Institute of Computer Graphics and Vision, Graz University of Technology, Austria; Laboratoire Bordelais de Recherche en Informatique, University of Bordeaux, France+Institute of Computer Graphics and Vision, Graz University of Technology, Austria",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1240814,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9679784587556530330&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at;u-bordeaux.fr",
        "email": "icg.tugraz.at;icg.tugraz.at;u-bordeaux.fr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "Graz University of Technology;University of Bordeaux",
        "aff_unique_dep": "Institute of Computer Graphics and Vision;Laboratoire Bordelais de Recherche en Informatique",
        "aff_unique_url": "https://www.tugraz.at;https://www.u-bordeaux.fr",
        "aff_unique_abbr": "TU Graz;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Graz;",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "Austria;France"
    },
    {
        "title": "SmartOverlays: A Visual Saliency Driven Label Placement for Intelligent Human-Computer Interfaces",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hegde_SmartOverlays_A_Visual_Saliency_Driven_Label_Placement_for_Intelligent_Human-Computer_WACV_2020_paper.html",
        "author": "Srinidhi Hegde;  Jitender Maurya;  Aniruddha Kalkar;  Ramya Hebbalaguppe",
        "abstract": "In augmented reality (AR), the computer generated labels assist in understanding a scene by addition of contextual information. However, naive label placement often results in clutter and occlusion impairing the effectiveness of AR visualization. For label placement, the main objectives to be satisfied are, non-occlusion to the scene of interest, the proximity of labels to the object, and, temporally coherent labels in a video/live feed. We present a novel method for the placement of labels corresponding to objects of interest in a video/live feed that satisfies the aforementioned objectives. Our proposed framework, SmartOverlays, first identifies the objects and generates corresponding labels using a YOLOv2 in a video frame; at the same time, Saliency Attention Model (SAM) learns eye fixation points that aid in predicting saliency maps; finally, computes Voronoi partitions of the video frame, choosing the centroids of objects as seed points, to place labels for satisfying the proximity constraints with the object of interest. In addition, our approach incorporates tracking the detected objects in a frame to facilitate temporal coherence between frames that enhances the readability of labels. We measure the effectiveness of SmartOverlays framework using three objective metrics: (a) Label Occlusion over Saliency (LOS), (b) temporal jitter metric to quantify jitter in the label placement, (c) computation time for label placement.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hegde_SmartOverlays_A_Visual_Saliency_Driven_Label_Placement_for_Intelligent_Human-Computer_WACV_2020_paper.pdf",
        "aff": "TCS Research; TCS Research; TCS Research; Walchand College of Engineering",
        "project": "https://ilab-ar.github.io/SmartOverlays/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2542557,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3811305356807065973&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "tcs.com;tcs.com;tcs.com;gmail.com",
        "email": "tcs.com;tcs.com;tcs.com;gmail.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Tata Consultancy Services;Walchand College of Engineering",
        "aff_unique_dep": "Research;",
        "aff_unique_url": "https://www.tcs.com;https://www.walchand.org",
        "aff_unique_abbr": "TCS;WCE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Dabouei_SmoothFool_An_Efficient_Framework_for_Computing_Smooth_Adversarial_Perturbations_WACV_2020_paper.html",
        "author": "Ali Dabouei;  Sobhan Soleymani;  Fariborz Taherkhani;  Jeremy Dawson;  Nasser Nasrabadi",
        "abstract": "Deep neural networks are susceptible to adversarial manipulations in the input domain. The extent of vulnerability has been explored intensively in cases of l_p-bounded and l_p-minimal adversarial perturbations. However, the vulnerability of DNNs to adversarial perturbations with specific statistical properties or frequency-domain characteristics has not been sufficiently explored. In this paper, we study the smoothness of perturbations and propose SmoothFool, a general and computationally efficient framework for computing smooth adversarial perturbations. Through extensive experiments, we validate the efficacy of the proposed method for both the white-box and black-box attack scenarios. In particular, we demonstrate that: (i) there exist extremely smooth adversarial perturbations for well-established and widely used network architectures, (ii) smoothness significantly enhances the robustness of perturbations against state-of-the-art defense mechanisms, (iii) smoothness improves the transferability of adversarial perturbations across both data points and network architectures, and (iv) class categories exhibit a variable range of susceptibility to smooth perturbations.  Our results suggest that smooth APs can play a significant role in exploring the vulnerability extent of DNNs to adversarial examples.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Dabouei_SmoothFool_An_Efficient_Framework_for_Computing_Smooth_Adversarial_Perturbations_WACV_2020_paper.pdf",
        "aff": "West Virginia University; West Virginia University; West Virginia University; West Virginia University; West Virginia University",
        "project": "",
        "github": "https://github.com/alldbi/SmoothFool",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1273078,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15784711322299896696&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "mix.wvu.edu;mix.wvu.edu;gmail.com;mail.wvu.edu;mail.wvu.edu",
        "email": "mix.wvu.edu;mix.wvu.edu;gmail.com;mail.wvu.edu;mail.wvu.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "West Virginia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wvu.edu",
        "aff_unique_abbr": "WVU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Spatial-Content Image Search in Complex Scenes",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ma_Spatial-Content_Image_Search_in_Complex_Scenes_WACV_2020_paper.html",
        "author": "Jin Ma;  Shanmin Pang;  Bo Yang;  Jihua Zhu;  Yaochen Li",
        "abstract": "Although the topic of image search has been heavily studied in the last two decades, many works have focused on either instance-level retrieval or semantic-level retrieval.  In this work, we develop a novel visually similar spatial-semantic method, namely spatial-content image search, to  search images that not only share the same spatial-semantics but also enjoy visual consistency as the query image in complex scenes. We achieve the goal by capturing spatial-semantic concepts as well as the visual representation of each concept contained in an image. Specifically, we first generate a set of bounding boxes and their category labels representing spatial-semantic constraints with YOLOV3, and then obtain visual content of each bounding box with deep features extracted from a convolutional  neural network. After that, we customize a similarity computation method that evaluates the relevance between dataset images and input queries according to the developed image representations. Experimental results on two large-scale benchmark retrieval datasets with images consisting of multiple objects demonstrate that our method provides an effective way to query image databases. Our code is available at https://github.com/MaJinWakeUp/spatial-content.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ma_Spatial-Content_Image_Search_in_Complex_Scenes_WACV_2020_paper.pdf",
        "aff": "Xi\u2019an Jiaotong University; Xi\u2019an Jiaotong University; Xi\u2019an Polytechnic University; Xi\u2019an Jiaotong University; Xi\u2019an Jiaotong University",
        "project": "",
        "github": "https://github.com/MaJinWakeUp/spatial-content",
        "supp": "",
        "arxiv": "",
        "pdf_size": 808339,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11039465148990447848&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "stu.xjtu.edu.cn;xjtu.edu.cn;stu.xjtu.edu.cn;xjtu.edu.cn;xjtu.edu.cn",
        "email": "stu.xjtu.edu.cn;xjtu.edu.cn;stu.xjtu.edu.cn;xjtu.edu.cn;xjtu.edu.cn",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Xi'an Jiaotong University;Xi'an Polytechnic University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.xjtu.edu.cn;http://www.xpu.edu.cn",
        "aff_unique_abbr": "XJTU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Spatio-Temporal Pyramid Graph Convolutions for Human Action Recognition and Postural Assessment",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Parsa_Spatio-Temporal_Pyramid_Graph_Convolutions_for_Human_Action_Recognition_and_Postural_WACV_2020_paper.html",
        "author": "Behnoosh Parsa;  Athmanarayanan Lakshmi narayanan;  Behzad Dariush",
        "abstract": "Recognition of human actions and associated interactions with objects and the environment is an important problem in computer vision due to its potential applications in a variety of domains. Recently, graph convolutional networks that extract features from the skeleton have demonstrated promising performance. In this paper, we propose a novel Spatio-Temporal Pyramid Graph Convolutional Network (ST-PGN) for online action recognition for ergonomics risk assessment that enables the use of features from all levels of the skeleton feature hierarchy. The proposed algorithm outperforms state-of-art action recognition algorithms tested on two public benchmark datasets typically used for postural assessment (TUM and UW-IOM). We also introduce a pipeline to enhance postural assessment methods with online action recognition techniques. Finally, the proposed algorithm is integrated with a traditional ergonomics risk index (REBA) to demonstrate the potential value for assessment of musculoskeletal disorders in occupational safety.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Parsa_Spatio-Temporal_Pyramid_Graph_Convolutions_for_Human_Action_Recognition_and_Postural_WACV_2020_paper.pdf",
        "aff": "University of Washington + Honda Research Institute; Honda Research Institute; Honda Research Institute",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Parsa_Spatio-Temporal_Pyramid_Graph_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2588836,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2878622832157749555&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "uw.edu;honda-ri.com;honda-ri.com",
        "email": "uw.edu;honda-ri.com;honda-ri.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "University of Washington;Honda Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.honda-ri.com",
        "aff_unique_abbr": "UW;HRI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "United States;Japan"
    },
    {
        "title": "Spatio-Temporal Ranked-Attention Networks for Video Captioning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Cherian_Spatio-Temporal_Ranked-Attention_Networks_for_Video_Captioning_WACV_2020_paper.html",
        "author": "Anoop Cherian;  Jue Wang;  Chiori Hori;  Tim Marks",
        "abstract": "Generating video descriptions automatically is a challenging task that involves a complex interplay between spatio-temporal visual features and language models. Given that videos consist of spatial (frame-level) features and their temporal evolutions, an effective captioning model should be able to attend to these different cues selectively. To this end, we propose a Spatio-Temporal and Temporo-Spatial (STaTS) attention model which, conditioned on the language state, hierarchically combines spatial and temporal attention to videos in two different orders: (i) a spatio-temporal (ST) sub-model, which first attends to regions that have temporal evolution, then temporally pools the features from these regions; and  (ii) a temporo-spatial (TS) sub-model, which first decides a single frame to attend to, then applies spatial attention within that frame. We propose a novel LSTM-based temporal ranking function, which we call ranked attention, for the ST model to capture action dynamics. Our entire framework is trained end-to-end. We provide experiments on two benchmark datasets: MSVD and MSR-VTT. Our results demonstrate the synergy between the ST and TS modules, outperforming recent state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Cherian_Spatio-Temporal_Ranked-Attention_Networks_for_Video_Captioning_WACV_2020_paper.pdf",
        "aff": "Mitsubishi Electric Research Labs, Cambridge, MA; Australian National University, Canberra; Mitsubishi Electric Research Labs, Cambridge, MA; Mitsubishi Electric Research Labs, Cambridge, MA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Cherian_Spatio-Temporal_Ranked-Attention_Networks_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1842681,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14922582449771594097&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "merl.com;anu.edu.au;merl.com;merl.com",
        "email": "merl.com;anu.edu.au;merl.com;merl.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Mitsubishi Electric Research Labs;Australian National University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.merl.com;https://www.anu.edu.au",
        "aff_unique_abbr": "MERL;ANU",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Cambridge;Canberra",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Australia"
    },
    {
        "title": "Stable Intrinsic Auto-Calibration from Fundamental Matrices of Devices with Uncorrelated Camera Parameters",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Fetzer_Stable_Intrinsic_Auto-Calibration_from_Fundamental_Matrices_of_Devices_with_Uncorrelated_WACV_2020_paper.html",
        "author": "Torben Fetzer;  Gerd Reis;  Didier Stricker",
        "abstract": "Auto-Calibration is an important task in computer vision and is necessary for many visual applications.  Methods like photogrammetry, depth map estimation, metrology, augmented/mixed reality or odometry are strongly dependent on well calibrated devices.  While classical calibration relies on tools like checkerboards or additional scene information, auto-calibration only takes epipolar relations into account. Classical calibration is often impractical, tends to de-adjust over time and distributes the error over the entire, limited working volume. Auto-calibration, on the other hand, does not require any information other than the image content itself, has a virtually unlimited working range and usually achieves highest accuracy at the objects' surfaces. Unfortunately, auto-calibration methods are sensitive to errors in the fundamental matrix and need good initialization to converge to the global solution.  In practice this leads to difficulties if optical parameters like principal point or focal length are unconstrained.  In such situations, even state-of-the-art auto-calibration methods tend to diverge and do not yield a valid calibration. This work assesses reasons for this behavior, in particular for the initialization method of Bougnoux [3] and Lourakis' state-of-the-art auto-calibration method [21]. Based on the analysis, a more stable method is proposed. A continuous and smooth energy functional is introduced, providing superior convergence properties. I.e. it can not diverge, converges faster, and has a significantly enlarged convergence region with respect to the global minimum.  Finally, a thorough evaluation has been conducted and a detailed comparison with the state of the art is presented.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Fetzer_Stable_Intrinsic_Auto-Calibration_from_Fundamental_Matrices_of_Devices_with_Uncorrelated_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science, TU Kaiserslautern + Department Augmented Vision, DFKI GmbH; Department of Computer Science, TU Kaiserslautern + Department Augmented Vision, DFKI GmbH; Department of Computer Science, TU Kaiserslautern + Department Augmented Vision, DFKI GmbH",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 689237,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10118768817730621922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "dfki.de;dfki.de;dfki.de",
        "email": "dfki.de;dfki.de;dfki.de",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "TU Kaiserslautern;DFKI GmbH",
        "aff_unique_dep": "Department of Computer Science;Department Augmented Vision",
        "aff_unique_url": "https://www.uni-kl.de;https://www.dfki.de",
        "aff_unique_abbr": "TU KL;DFKI",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Stacked Adversarial Network for Zero-Shot Sketch based Image Retrieval",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Pandey_Stacked_Adversarial_Network_for_Zero-Shot_Sketch_based_Image_Retrieval_WACV_2020_paper.html",
        "author": "Anubha Pandey;  Ashish Mishra;  Vinay Kumar Verma;  Anurag Mittal;  Hema Murthy",
        "abstract": "Conventional approaches to Sketch-Based Image Retrieval (SBIR) assume that the data of all the classes are available during training. The assumption may not always be practical since the data of a few classes may be unavailable, or the classes may not appear at the time of training. Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) relaxes this constraint and allows the algorithm to handle previously unseen classes during the test. This paper proposes a generative approach based on the Stacked Adversarial Network (SAN) and the advantage of Siamese Network (SN) for ZS-SBIR. While SAN generates a high-quality sample, SN learns a better distance metric compared to that of the nearest neighbor search. The capability of the generative model to synthesize image features based on the sketch reduces the SBIR problem to that of an image-to-image retrieval problem. We evaluate the efficacy of our proposed approach on TU-Berlin, and Sketchy database in both standard ZSL and generalized ZSL setting. The proposed method yields a significant improvement in standard ZSL as well as in a more challenging generalized ZSL setting (GZSL) for SBIR.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Pandey_Stacked_Adversarial_Network_for_Zero-Shot_Sketch_based_Image_Retrieval_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Technology Madras; Indian Institute of Technology Madras; Indian Institute of Technology Kanpur; Indian Institute of Technology Madras; Indian Institute of Technology Madras",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 728043,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9396701728168046413&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com;cse.iitm.ac.in;iitk.ac.in;cse.iitm.ac.in;cse.iitm.ac.in",
        "email": "gmail.com;cse.iitm.ac.in;iitk.ac.in;cse.iitm.ac.in;cse.iitm.ac.in",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Indian Institute of Technology Madras;Indian Institute of Technology Kanpur",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitm.ac.in;https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Madras;IIT Kanpur",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Madras;Kanpur",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Stacked Spatio-Temporal Graph Convolutional Networks for Action Segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ghosh_Stacked_Spatio-Temporal_Graph_Convolutional_Networks_for_Action_Segmentation_WACV_2020_paper.html",
        "author": "Pallabi Ghosh;  Yi Yao;  Larry Davis;  Ajay Divakaran",
        "abstract": "We propose novel Stacked Spatio-Temporal Graph Convolutional Networks (Stacked-STGCN) for action segmentation, i.e., predicting and localizing a sequence of actions over long videos. We extend the Spatio-Temporal Graph Convolutional Network (STGCN) originally proposed for skeleton-based action recognition to enable nodes with different characteristics (e.g., scene, actor, object, action), feature descriptors with varied lengths, and arbitrary temporal edge connections to account for large graph deformation commonly associated with complex activities. We further introduce the stacked hourglass architecture to STGCN to leverage the advantages of an encoder-decoder design for improved generalization performance and localization accuracy. We explore various descriptors such as frame-level VGG, segment-level I3D, RCNN-based object, etc. as node descriptors to enable action segmentation based on joint inference over comprehensive contextual information. We show results on CAD120 (which provides pre-computed node features and edge weights for fair performance comparison across algorithms) as well as a more complex real world activity dataset, Charades. Our Stacked-STGCN in general achieves improved performance over the state-of-the-art for both CAD120 and Charades. Moreover, due to its generic design, Stacked-STGCN can be applied to a wider range of applications that require structured inference over long sequences with heterogeneous data types and varied temporal extent.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ghosh_Stacked_Spatio-Temporal_Graph_Convolutional_Networks_for_Action_Segmentation_WACV_2020_paper.pdf",
        "aff": "University of Maryland; SRI International; University of Maryland; SRI International",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Ghosh_Stacked_Spatio-Temporal_Graph_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1058722,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10690180689084739213&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "umd.edu;sri.com;umiacs.umd.edu;sri.com",
        "email": "umd.edu;sri.com;umiacs.umd.edu;sri.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Maryland;SRI International",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://www.sri.com",
        "aff_unique_abbr": "UMD;SRI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Weigert_Star-convex_Polyhedra_for_3D_Object_Detection_and_Segmentation_in_Microscopy_WACV_2020_paper.html",
        "author": "Martin Weigert;  Uwe Schmidt;  Robert Haase;  Ko Sugawara;  Gene Myers",
        "abstract": "Accurate detection and segmentation of cell nuclei in volumetric (3D) fluorescence microscopy datasets is an important step in many biomedical research projects. Although many automated methods for these tasks exist, they often struggle for images with low signal-to-noise ratios and/or dense packing of nuclei. It was recently shown for 2D microscopy images that these issues can be alleviated by training a neural network to directly predict a suitable shape representation (star-convex polygon) for cell nuclei. In this paper, we adopt and extend this approach to 3D volumes by using star-convex polyhedra to represent cell nuclei and similar shapes. To that end, we overcome the challenges of 1) finding parameter-efficient star-convex polyhedra representations that can faithfully describe cell nuclei shapes, 2) adapting to anisotropic voxel sizes often found in fluorescence microscopy datasets, and 3) efficiently computing intersections between pairs of star-convex polyhedra (required for non-maximum suppression). Although our approach is quite general, since star-convex polyhedra include common shapes like bounding boxes and spheres as special cases, our focus is on accurate detection and segmentation of cell nuclei. Finally, we demonstrate on two challenging datasets that our approach (StarDist-3D) leads to superior results when compared to classical and deep learning based methods.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Weigert_Star-convex_Polyhedra_for_3D_Object_Detection_and_Segmentation_in_Microscopy_WACV_2020_paper.pdf",
        "aff": "Institute of Bioengineering, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Switzerland+Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG), Dresden, Germany+Center for Systems Biology Dresden (CSBD), Germany; Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG), Dresden, Germany+Center for Systems Biology Dresden (CSBD), Germany; Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG), Dresden, Germany+Center for Systems Biology Dresden (CSBD), Germany; Institut de G\u00e9nomique Fonctionnelle de Lyon (IGFL), \u00c9cole Normale Sup\u00e9rieure de Lyon, France+Centre National de la Recherche Scientifique (CNRS), Paris, France; Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG), Dresden, Germany+Center for Systems Biology Dresden (CSBD), Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1693711,
        "gs_citation": 541,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2139260067181824373&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "\u22c6; \u22c6; ; ; ",
        "email": "\u22c6; \u22c6; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;1+2;1+2;3+4;1+2",
        "aff_unique_norm": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne;Max Planck Institute of Molecular Cell Biology and Genetics;Center for Systems Biology Dresden;\u00c9cole Normale Sup\u00e9rieure de Lyon;Centre National de la Recherche Scientifique",
        "aff_unique_dep": "Institute of Bioengineering;Molecular Cell Biology and Genetics;Systems Biology;Institut de G\u00e9nomique Fonctionnelle de Lyon (IGFL);",
        "aff_unique_url": "https://www.epfl.ch;https://www.mpi-cbg.de;;https://www.ens-lyon.fr;https://www.cnrs.fr",
        "aff_unique_abbr": "EPFL;MPI-CBG;CSBD;ENS de Lyon;CNRS",
        "aff_campus_unique_index": "1;1;1;2+3;1",
        "aff_campus_unique": ";Dresden;Lyon;Paris",
        "aff_country_unique_index": "0+1+1;1+1;1+1;2+2;1+1",
        "aff_country_unique": "Switzerland;Germany;France"
    },
    {
        "title": "Stochastic Dynamics for Video Infilling",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xu_Stochastic_Dynamics_for_Video_Infilling_WACV_2020_paper.html",
        "author": "Qiangeng Xu;  Hanwang Zhang;  Weiyue Wang;  Peter Belhumeur;  Ulrich Neumann",
        "abstract": "In this paper, we introduce a stochastic dynamics video infilling (SDVI) framework to generate frames between long intervals in a video. Our task differs from video interpolation which aims to produce transitional frames for a short interval between every two frames and increase the temporal resolution. Our task, namely video infilling, however, aims to infill long intervals with plausible frame sequences. Our framework models the infilling as a constrained stochastic generation process and sequentially samples dynamics from the inferred distribution. SDVI consists of two parts: (1) a bi-directional constraint propagation module to guarantee the spatial-temporal coherence among frames, (2) a stochastic sampling process to generate dynamics from the inferred distributions. Experimental results show that SDVI can generate clear frame sequences with varying contents. Moreover, motions in the generated sequence are realistic and able to transfer smoothly from the given start frame to the terminal frame.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xu_Stochastic_Dynamics_for_Video_Infilling_WACV_2020_paper.pdf",
        "aff": "University of Southern California; Nanyang Technological University; University of Southern California; Columbia University; University of Southern California",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Xu_Stochastic_Dynamics_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1491683,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2367178316285526459&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "usc.edu;ntu.edu.sg;usc.edu;cs.columbia.edu;usc.edu",
        "email": "usc.edu;ntu.edu.sg;usc.edu;cs.columbia.edu;usc.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "University of Southern California;Nanyang Technological University;Columbia University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.usc.edu;https://www.ntu.edu.sg;https://www.columbia.edu",
        "aff_unique_abbr": "USC;NTU;Columbia",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "title": "Street Scene: A new dataset and evaluation protocol for video anomaly detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ramachandra_Street_Scene_A_new_dataset_and_evaluation_protocol_for_video_WACV_2020_paper.html",
        "author": "Bharathkumar Ramachandra;  Michael Jones",
        "abstract": "Progress  in  video  anomaly  detection  research  is  currently slowed by small datasets that lack a wide variety of activities as well as flawed evaluation criteria.  This paper aims to help move this research effort forward by introducing a large and varied new dataset called Street Scene, as well as two new evaluation criteria that provide a better estimate of how an algorithm will perform in practice. In addition to the new dataset and evaluation criteria, we present two variations of a novel baseline video anomaly detection algorithm and show they are much more accurate on Street Scene than two well known algorithms from the literature.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ramachandra_Street_Scene_A_new_dataset_and_evaluation_protocol_for_video_WACV_2020_paper.pdf",
        "aff": "North Carolina State University; Mitsubishi Electric Research Labs (MERL)",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Ramachandra_Street_Scene_A_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1221304,
        "gs_citation": 205,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1538068939775755798&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ncsu.edu;merl.com",
        "email": "ncsu.edu;merl.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "North Carolina State University;Mitsubishi Electric Research Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ncsu.edu;https://www.merl.com",
        "aff_unique_abbr": "NCSU;MERL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Structured Compression of Deep Neural Networks with Debiased Elastic Group LASSO",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Oyedotun_Structured_Compression_of_Deep_Neural_Networks_with_Debiased_Elastic_Group_WACV_2020_paper.html",
        "author": "Oyebade Oyedotun;  Djamila Aouada;  Bjorn Ottersten",
        "abstract": "State-of-the-art Deep Neural Networks (DNNs) are typically too cumbersome to be practically useful in portable electronic devices. As such, several works pursue model compression that seeks to drastically reduce computational memory footprints, FLOPS and memory for storage. Many of these works achieve unstructured compression, where the compressed models are not directly useful since dedicated hardware and specialized algorithms are required for storage of sparse weights and fast sparse matrix-vector multiplication respectively. In this paper, we propose structured compression of large DNNs using debiased elastic group LASSO (DEGL), which is motivated by different interesting characteristics of the individual components. That is, where group LASSO penalty enforces structured sparsity, l2-norm penalty promotes features grouping, and debiasing disentangles sparsity and shrinkage effects of group LASSO. We perform extensive experiments by applying DEGL to different DNN architectures including LeNet, VGG, AlexNet and ResNet on MNIST, CIFAR-10, CIFAR-100 and ImageNet datasets. Furthermore, we validate the effectiveness of our proposal on domain adaptation using Oxford-102 flower species and Food-5K datasets. Results show that DEGL can compress DNNs by several folds with small or no loss of performance. Particularly, DEGL outperforms conventional group LASSO and several other state-of-the-art methods that perform structured compression.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Oyedotun_Structured_Compression_of_Deep_Neural_Networks_with_Debiased_Elastic_Group_WACV_2020_paper.pdf",
        "aff": "Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, L-1855 Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, L-1855 Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, L-1855 Luxembourg",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Oyedotun_Structured_Compression_of_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 903180,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10267886733133372921&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "uni.lu;uni.lu;uni.lu",
        "email": "uni.lu;uni.lu;uni.lu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Luxembourg",
        "aff_unique_dep": "Interdisciplinary Centre for Security, Reliability and Trust (SnT)",
        "aff_unique_url": "https://wwwen.uniluxembourg.lu",
        "aff_unique_abbr": "Uni Lu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Luxembourg"
    },
    {
        "title": "Style Transfer for Light Field Photography",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Hart_Style_Transfer_for_Light_Field_Photography_WACV_2020_paper.html",
        "author": "David Hart;  Bryan Morse;  Jessica Greenland",
        "abstract": "As light field images continue to increase in use and application, it becomes necessary to adapt existing image processing methods to this unique form of photography. In this paper we explore methods for applying neural style transfer to light field images. Feed-forward style transfer networks provide fast, high-quality results for monocular images, but no such networks exist for full light field images. Because of the size of these images, current light field data sets are small and are insufficient for training purely feed-forward style-transfer networks from scratch. Thus, it is necessary to adapt existing monocular style transfer networks in a way that allows for the stylization of each view of the light field while maintaining visual consistencies between views. To do this, we first generate disparity maps for each view given a single depth image for the light field. Then in a fashion similar to neural stylization of stereo images, we use disparity maps to enforce a consistency loss between views and to warp feature maps during the feed forward stylization. Unlike previous work, however, light fields have too many views to train a purely feed-forward network that can stylize the entire light field with angular consistency. Instead, the proposed method uses an iterative optimization for each view of a single light field image that backpropagates the consistency loss through the network. Thus, the network architecture allows for the incorporation of pre-trained fast monocular stylization network while avoiding the need for a large light field training set.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Hart_Style_Transfer_for_Light_Field_Photography_WACV_2020_paper.pdf",
        "aff": "Brigham Young University; Brigham Young University; Brigham Young University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Hart_Style_Transfer_for_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3368567,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11559779547725747837&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "byu.edu;byu.edu;byu.edu",
        "email": "byu.edu;byu.edu;byu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brigham Young University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.byu.edu",
        "aff_unique_abbr": "BYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Super-resolved Chromatic Mapping of Snapshot Mosaic Image Sensors via a Texture Sensitive Residual Network",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Shoeiby_Super-resolved_Chromatic_Mapping_of_Snapshot_Mosaic_Image_Sensors_via_a_WACV_2020_paper.html",
        "author": "Mehrdad Shoeiby;  Lars Petersson;  Ali Armin;  Sadegh Aliakbarian;  antonio robbles-kelly",
        "abstract": "This paper introduces a novel method to simultaneously super-resolve and colour-predict images acquired by snapshot mosaic sensors. These sensors allow for spectral images to be acquired using low-power, small form factor, solid-state CMOS sensors that can operate at video frame rates without the need for complex optical setups. Despite their desirable traits, their main drawback stems from the fact that the spatial resolution of the imagery acquired by these sensors is low. Moreover, chromatic mapping in snapshot mosaic sensors is not straightforward since the bands delivered by the sensor tend to be narrow and unevenly distributed across the range in which they operate. We tackle this drawback as applied to chromatic mapping by using a residual channel attention network equipped with a texture sensitive block. Our method significantly outperforms the traditional approach of interpolating the image and, afterwards, applying a colour matching function. This work establishes  state-of-the-art in this domain while also making available to the research community a dataset containing 296 registered stereo multi-spectral/RGB images pairs.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Shoeiby_Super-resolved_Chromatic_Mapping_of_Snapshot_Mosaic_Image_Sensors_via_a_WACV_2020_paper.pdf",
        "aff": "DATA61-CSIRO, ACT, Australia; DATA61-CSIRO, ACT, Australia; DATA61-CSIRO, ACT, Australia; DATA61-CSIRO, ACT, Australia; Deakin University, VIC, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 16696130,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5625403042416242589&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 7,
        "aff_domain": "data61.csiro.au; ; ; ; ",
        "email": "data61.csiro.au; ; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Commonwealth Scientific and Industrial Research Organisation;Deakin University",
        "aff_unique_dep": "DATA61;",
        "aff_unique_url": "https://www.csiro.au;https://www.deakin.edu.au",
        "aff_unique_abbr": "CSIRO;Deakin",
        "aff_campus_unique_index": "0;0;0;0;1",
        "aff_campus_unique": "Canberra;VIC",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Supervised and Unsupervised Learning of Parameterized Color Enhancement",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chai__Supervised_and_Unsupervised_Learning_of_Parameterized_Color_Enhancement_WACV_2020_paper.html",
        "author": "Yoav Chai;  Raja Giryes;  Lior Wolf",
        "abstract": "We treat the problem of color enhancement as an image translation task, which we tackle using both supervised and unsupervised learning. Unlike traditional image to image generators, our translation is performed using a global parameterized color transformation instead of learning to directly map image information. In the supervised case, every training image is paired with a desired target image and a convolutional neural network (CNN) learns from the expert retouched images the parameters of the transformation. In the unpaired case, we employ two-way generative adversarial networks (GANs) to learn these parameters and apply a circularity constraint. We achieve state-of-the-art results compared to both supervised (paired data) and unsupervised (unpaired data) image enhancement methods on the MIT-Adobe FiveK benchmark. Moreover, we show the generalization capability of our method, by applying it on photos from the early 20th century and to dark video frames.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chai__Supervised_and_Unsupervised_Learning_of_Parameterized_Color_Enhancement_WACV_2020_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Chai__Supervised_and_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5197904,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=662962711355569763&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "SymGAN: Orientation Estimation without Annotation for Symmetric Objects",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ammirato_SymGAN_Orientation_Estimation_without_Annotation_for_Symmetric_Objects_WACV_2020_paper.html",
        "author": "Phil Ammirato;  Jonathan Tremblay;  Ming-Yu Liu;  Alexander Berg;  Dieter Fox",
        "abstract": "Training a computer vision system to predict an object's pose  is crucial to improving robotic manipulation, where robots can easily locate and then grasp objects. Some of the key challenges in pose estimation lie in obtaining labeled data and handling objects with symmetries. We explore both these problems of viewpoint estimation (object 3D orientation) by proposing a novel unsupervised training paradigm that only requires a 3D model of the object of interest. We show that we can successfully train an orientation detector, which simply consumes an RGB image, in an adversarial training framework, where the discriminator learns to provide a learning signal to retrieve the object orientation using a black-box non differentiable renderer. In order to overcome this non differentiability,  we introduce a randomized sampling method to obtain training gradients.  To our knowledge this is the first time an adversarial framework is employed to successfully train a viewpoint detector that can handle symmetric objects.Using this training framework we show state of the art results on 3D orientation prediction on T-LESS, a challenging dataset for texture-less and symmetric objects.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ammirato_SymGAN_Orientation_Estimation_without_Annotation_for_Symmetric_Objects_WACV_2020_paper.pdf",
        "aff": "UNC-Chapel Hill; NVIDIA; NVIDIA; UNC-Chapel Hill; NVIDIA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1117682,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8288378981968384742&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;NVIDIA Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unc.edu;https://www.nvidia.com",
        "aff_unique_abbr": "UNC;NVIDIA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Synthesizing human-like sketches from natural images using a conditional convolutional decoder",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kampelmuhler_Synthesizing_human-like_sketches_from_natural_images_using_a_conditional_convolutional_WACV_2020_paper.html",
        "author": "Moritz Kampelmuhler;  Axel Pinz",
        "abstract": "Humans are able to precisely communicate diverse concepts by employing sketches, a highly reduced and abstract shape based representation of visual content. We propose, for the first time, a fully convolutional end-to-end architecture that is able to synthesize human-like sketches of objects in natural images with potentially cluttered background. To enable an architecture to learn this highly abstract mapping, we employ the following key components: (1) a fully convolutional encoder-decoder structure, (2) a perceptual similarity loss function operating in an abstract feature space and (3) conditioning of the decoder on the label of the object that shall be sketched. Given the combination of these architectural concepts, we can train our structure in an end-to-end supervised fashion on a collection of sketch-image pairs. The generated sketches of our architecture can be classified with 85.6% Top-5 accuracy and we verify their visual quality via a user study. We find that deep features as a perceptual similarity metric enable image translation with large domain gaps and our findings further show that convolutional neural networks trained on image classification tasks implicitly learn to encode shape information.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kampelmuhler_Synthesizing_human-like_sketches_from_natural_images_using_a_conditional_convolutional_WACV_2020_paper.pdf",
        "aff": "Graz University of Technology; Graz University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1229427,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15616643975363964352&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tugraz.at; ",
        "email": "tugraz.at; ",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TUGraz",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "Synthetic Examples Improve Generalization for Rare Classes",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Beery_Synthetic_Examples_Improve_Generalization_for_Rare_Classes_WACV_2020_paper.html",
        "author": "Sara Beery;  Yang Liu;  Dan Morris;  Jim Piavis;  Ashish Kapoor;  Neel Joshi;  Markus Meister;  Pietro Perona",
        "abstract": "The ability to detect and classify rare occurrences in images has important applications -- for example, counting rare and endangered species when studying biodiversity, or detecting infrequent traffic scenarios that pose a danger to self-driving cars. Few-shot learning is an open problem: current computer vision systems struggle to categorize objects they have seen only rarely during training, and collecting a sufficient number of training examples of rare events is often challenging and expensive, and sometimes outright impossible. We explore in depth an approach to this problem: complementing the few available training images with ad-hoc simulated data.         Our testbed is animal species classification, which has a real-world long-tailed distribution.  We analyze the effect of different axes of variation in simulation, such as pose, lighting, model, and simulation method, and we prescribe best practices for efficiently incorporating simulated data for real-world performance gain. Our experiments reveal that synthetic data can considerably reduce error rates for classes that are rare, that as the amount of simulated data is increased, accuracy on the target class improves, and that high variation of simulated data provides maximum performance gain.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Beery_Synthetic_Examples_Improve_Generalization_for_Rare_Classes_WACV_2020_paper.pdf",
        "aff": ";;;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Beery_Synthetic_Examples_Improve_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 8188498,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10066533240153187672&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "author_num": 8,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "TKD: Temporal Knowledge Distillation for Active Perception",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Bajestani_TKD_Temporal_Knowledge_Distillation_for_Active_Perception_WACV_2020_paper.html",
        "author": "Mohammad Farhadi Bajestani;  Yezhou Yang",
        "abstract": "Deep neural network-based methods have been proved to achieve outstanding performance on object detection and classification tasks. Despite the significant performance improvement using the deep structures, they still require prohibitive runtime to process images and maintain the highest possible performance for real-time applications. Observing the phenomenon that human visual system (HVS) relies heavily on the temporal dependencies among frames from the visual input to conduct recognition efficiently, we propose a novel framework dubbed as TKD: temporal knowledge distillation. This framework distills the temporal knowledge from a heavy neural network-based model over selected video frames (the perception of the moments) to a light-weight model. To enable the distillation, we put forward two novel procedures: 1) a Long-short Term Memory (LSTM)-based key frame selection method; and 2) a novel teacher-bounded loss design. To validate our approach, we conduct comprehensive empirical evaluations using different object detection methods over multiple datasets including Youtube Objects and Hollywood scene dataset. Our results show consistent improvement in accuracy-speed trade-offs for object detection over the frames of the dynamic scene, compared to other modern object recognition methods. It can maintain the desired accuracy with the throughput of around 220 images per second. Implementation: https://github.com/mfarhadi/TKD-Cloud.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Bajestani_TKD_Temporal_Knowledge_Distillation_for_Active_Perception_WACV_2020_paper.pdf",
        "aff": "Arizona State University; Arizona State University",
        "project": "",
        "github": "https://github.com/mfarhadi/TKD-Cloud",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1112823,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5600555275440439942&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "asu.edu;asu.edu",
        "email": "asu.edu;asu.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "TailorGAN: Making User-Defined Fashion Designs",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_TailorGAN_Making_User-Defined_Fashion_Designs_WACV_2020_paper.html",
        "author": "Lele Chen;  Justin Tian;  Guo Li;  Cheng-Haw Wu;  Erh-Kan King;  Kuan-Ting Chen;  Shao-Hang Hsieh;  Chenliang Xu",
        "abstract": "Attribute editing has become an important and emerging topic of computer vision. In this paper, we consider a task: given a reference garment image A and another image B with target attribute (collar/sleeve), generate a photo-realistic image which combines the texture from reference A and the new attribute from reference B. The highly convoluted attributes and the lack of paired data are the main challenges to the task. To overcome those limitations, we propose a novel self-supervised model to synthesize garment images with disentangled attributes (e.g., collar and sleeves) without paired data. Our method consists of reconstruction learning step and adversarial learning step. The model learns texture and location information through reconstruction learning. And the model capability is generalized to achieve single-attribute manipulation by adversarial learning. Meanwhile, we compose a new dataset, named GarmentSet, with annotation of landmarks of collar and sleeves on clean garment images. Thoughtful experiments on this dataset and real-world samples demonstrate that our method can synthesize significantly better results than the state-of-the-art methods in both quantitative and qualitative comparisons. The code is available at: https://github.com/gli-27/TailorGAN.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_TailorGAN_Making_User-Defined_Fashion_Designs_WACV_2020_paper.pdf",
        "aff": "University of Rochester; University of Rochester; University of Rochester; Viscovery; Viscovery; Viscovery; Viscovery; University of Rochester",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1795165,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12257427092492238380&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ur.rochester.edu;pas.rochester.edu;ur.rochester.edu;viscovery.com;viscovery.com;viscovery.com;viscovery.com;rochester.edu",
        "email": "ur.rochester.edu;pas.rochester.edu;ur.rochester.edu;viscovery.com;viscovery.com;viscovery.com;viscovery.com;rochester.edu",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1;1;1;0",
        "aff_unique_norm": "University of Rochester;Viscovery",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rochester.edu;",
        "aff_unique_abbr": "U of R;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Template-Based Automatic Search of Compact Semantic Segmentation Architectures",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Nekrasov_Template-Based_Automatic_Search_of_Compact_Semantic_Segmentation_Architectures_WACV_2020_paper.html",
        "author": "Vladimir Nekrasov;  Chunhua Shen;  Ian Reid",
        "abstract": "Automatic search of neural architectures for various vision and natural language tasks is becoming a prominent tool as it allows to discover high-performing structures on any dataset of interest. Nevertheless, on more difficult domains, such as dense per-pixel classification, current automatic approaches are limited in their scope -- due to their strong reliance on existing image classifiers they tend to search only for a handful of additional layers with discovered architectures still containing a large number of parameters. In contrast, in this work we propose a novel solution able to find light-weight and accurate segmentation architectures starting from only few blocks of a pre-trained classification network. To this end, we progressively build up a methodology that relies on templates of sets of operations, predicts which template and how many times should be applied at each step, while also generating the connectivity structure and downsampling factors. All these decisions are being made by a recurrent neural network that is rewarded based on the score of the emitted architecture on the holdout set and trained using reinforcement learning. One discovered architecture achieves 63.2% mean IoU on CamVid and 67.8% on CityScapes having only 270K parameters.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Nekrasov_Template-Based_Automatic_Search_of_Compact_Semantic_Segmentation_Architectures_WACV_2020_paper.pdf",
        "aff": "The University of Adelaide, Australia; The University of Adelaide, Australia; The University of Adelaide, Australia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Nekrasov_Template-Based_Automatic_Search_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1912821,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6557433767400710627&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The University of Adelaide",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Temporal Aggregation with Clip-level Attention for Video-based Person Re-identification",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Li_Temporal_Aggregation_with_Clip-level_Attention_for_Video-based_Person_Re-identification_WACV_2020_paper.html",
        "author": "Mengliu Li;  Han Xu;  Jinjun Wang;  Wenpeng Li;  Yongli Sun",
        "abstract": "Video-based person re-identification (Re-ID) methods can extract richer features than image-based ones from short video clips. The existing methods usually apply simple strategies, such as average/max pooling, to obtain the tracklet-level features, which has been proved hard to aggregate the information from all video frames. In this paper, we propose a simple yet effective Temporal Aggregation with Clip-level Attention Network (TACAN) to solve the temporal aggregation problem in a hierarchal way. Specifically, a tracklet is firstly broken into different numbers of clips, through a two-stage temporal aggregation network we can get the tracklet-level feature representation. A novel min-max loss is introduced to learn both a clip-level attention extractor and a clip-level feature representer in the training process. Afterwards, the resulting clip-level weights are further taken to average the clip-level features, which can generate a robust tracklet-level feature representation at the testing stage. Experimental results on four benchmark datasets, including the MARS, iLIDS-VID, PRID-2011 and DukeMTMC-VideoReID, show that our TACAN has achieved significant improvements as compared with the state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Li_Temporal_Aggregation_with_Clip-level_Attention_for_Video-based_Person_Re-identification_WACV_2020_paper.pdf",
        "aff": "Institute of Arti\ufb01cial Intelligence and Robotics, Xi\u2019an Jiao Tong University; Deep North Inc., Xi\u2019an, Shaanxi, China; Institute of Arti\ufb01cial Intelligence and Robotics, Xi\u2019an Jiao Tong University; Deep North Inc., Xi\u2019an, Shaanxi, China; Deep North Inc., Xi\u2019an, Shaanxi, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5274872,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14429550584904245272&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "stu.xjtu.edu.cn;deepnorth.cn;mail.xjtu.edu.cn;deepnorth.cn;deepnorth.cn",
        "email": "stu.xjtu.edu.cn;deepnorth.cn;mail.xjtu.edu.cn;deepnorth.cn;deepnorth.cn",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;1",
        "aff_unique_norm": "Xi'an Jiao Tong University;Deep North Inc.",
        "aff_unique_dep": "Institute of Arti\ufb01cial Intelligence and Robotics;",
        "aff_unique_url": "http://www.xjtu.edu.cn;",
        "aff_unique_abbr": "XJTU;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Xi'an;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Temporal Contrastive Pretraining for Video Action Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/LORRE_Temporal_Contrastive_Pretraining_for_Video_Action_Recognition_WACV_2020_paper.html",
        "author": "Guillaume LORRE;  Jaonary Rabarisoa;  Astrid Orcesi;  Samia Ainouz;  Stephane Canu",
        "abstract": "In this paper, we propose a self-supervised method for video representation learning based on Contrastive Predictive Coding (CPC) [27]. Previously, CPC has been used to learn representations for different signals (audio, text or image). It benefits from the use of an autoregressive modeling and contrastive estimation to learn long-term relations inside raw signal while remaining robust to local noise. Our self-supervised task consists in predicting the latent representation of future segments of the video. As opposed to generative models, predicting directly in the feature space is easier and avoid incertitude problems for long-term predictions. Today, using CPC to learn representations for videos remains challenging due to the structure and the high dimensionality of the signal. We demonstrate experimentally that the representations learned by the network are useful for action recognition. We test it with different input types such as optical flows, image differences and raw images on different datasets (UCF-101 and HMDB51). It gives consistent results across the modalities. At last, we notice the utility of our pre-training method by achieving competitive results for action recognition using few labeled data.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/LORRE_Temporal_Contrastive_Pretraining_for_Video_Action_Recognition_WACV_2020_paper.pdf",
        "aff": "CEA, LIST, Laboratoire Vision et Apprentissage pour l\u2019Analyse de Scene, Gif-sur-Yvette, France; CEA, LIST, Laboratoire Vision et Apprentissage pour l\u2019Analyse de Scene, Gif-sur-Yvette, France; CEA, LIST, Laboratoire Vision et Apprentissage pour l\u2019Analyse de Scene, Gif-sur-Yvette, France; Normandie Univ, INSA Rouen, UNIROUEN, UNIHA VRE, LITIS, France; Normandie Univ, INSA Rouen, UNIROUEN, UNIHA VRE, LITIS, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 648686,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=137088362469241076&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cea.fr;cea.fr;cea.fr;insa\u2212rouen.fr;insa\u2212rouen.fr",
        "email": "cea.fr;cea.fr;cea.fr;insa\u2212rouen.fr;insa\u2212rouen.fr",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "CEA;Normandie University",
        "aff_unique_dep": "LIST, Laboratoire Vision et Apprentissage pour l\u2019Analyse de Scene;INSA Rouen",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";Normandie Univ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Temporal Similarity Analysis of Remote Photoplethysmography for Fast 3D Mask Face Presentation Attack Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Liu_Temporal_Similarity_Analysis_of_Remote_Photoplethysmography_for_Fast_3D_Mask_WACV_2020_paper.html",
        "author": "Siqi Liu;  Xiangyuan Lan;  PongChi Yuen",
        "abstract": "To tackle the 3D mask face presentation attack, remote Photoplethysmography (rPPG), a biomedical technique that can detect heartbeat signal remotely, is employed as an intrinsic liveness cue. Although existing rPPG-based methods exhibit encouraging results, they require long observation time (10-12 seconds) to identify the heartbeat information, which limits their employment in real applications such as smartphone unlock and e-payment. To shorten the observation time (within 1-second) while keeping the performance, we propose a fast rPPG-based 3D mask presentation attack detection (PAD) method by analyzing the similarity of local facial rPPG signals in the time domain. In particular, a set of temporal similarity features of facial and background local rPPG signals are designed and fused to adapt the real world variations based on rPPG shape and phase properties.  For better evaluation under practical variations, we build the HKBU-MARsV2+ dataset that includes 16 masks from 2 types and 6 lighting conditions.  Finally, extensive experiments are conducted on 11092 short-term video slots from 4 datasets with a large number of real-world variations, in terms of mask type, lighting condition, camera, resolution of face region, and compression setting. Results show that the proposed TSrPPG outperforms the state-of-the-art competitors dramatically on discriminability and generalizability. To our best knowledge, this is the first work that addresses the length of observation time issue of rPPG-based 3D mask PAD.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Liu_Temporal_Similarity_Analysis_of_Remote_Photoplethysmography_for_Fast_3D_Mask_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science, Hong Kong Baptist University, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2039446,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=137808208599692796&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "comp.hkbu.edu.hk;comp.hkbu.edu.hk;comp.hkbu.edu.hk",
        "email": "comp.hkbu.edu.hk;comp.hkbu.edu.hk;comp.hkbu.edu.hk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hong Kong Baptist University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.hkbu.edu.hk",
        "aff_unique_abbr": "HKBU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Text-based Person Search via Attribute-aided Matching",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Aggarwal_Text-based_Person_Search_via_Attribute-aided_Matching_WACV_2020_paper.html",
        "author": "Surbhi Aggarwal;  Venkatesh Babu RADHAKRISHNAN;  Anirban Chakraborty",
        "abstract": "Text-based person search aims to retrieve the pedestrian images that best match a given text query. Existing methods utilize class-id information to get discriminative and identity-preserving features. However, it is not well-explored whether it is beneficial to explicitly ensure that the semantics of the data are retained. In the proposed work, we aim to create semantics-preserving embeddings through an additional task of attribute prediction. Since attribute annotation is typically unavailable in text-based person search, we first mine them from the text corpus. These attributes are then used as a means to bridge the modality gap between the image-text inputs, as well as to improve the representation learning. In summary, we propose an approach for text-based person search by learning an attribute-driven space along with a class-information driven space, and utilize both for obtaining the retrieval results. Our experiments on benchmark dataset, CUHK-PEDES, show that learning the attribute-space not only helps in improving performance, giving us state-of-the-art Rank-1 accuracy of 56.68%, but also yields humanly-interpretable features.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Aggarwal_Text-based_Person_Search_via_Attribute-aided_Matching_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Aggarwal_Text-based_Person_Search_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1236378,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10957827647730091999&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "email": "iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "The Overlooked Elephant of Object Detection: Open Set",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Dhamija_The_Overlooked_Elephant_of_Object_Detection_Open_Set_WACV_2020_paper.html",
        "author": "Akshay Dhamija;  Manuel Gunther;  Jonathan Ventura;  Terrance Boult",
        "abstract": "Even though object detection is a popular area of research that has found considerable applications in the real world, it has some fundamental aspects that have never been formally discussed and experimented. One of the core aspects of evaluating object detectors has been the ability to avoid false detections. While major datasets like PASCAL VOC or MSCOCO extensively test the detectors on their ability to avoid false positives, they do not differentiate between their closed-set and open-set performance. Despite systems being trained to reject everything other than the classes of interest, unknown objects from the open world end up being incorrectly detected as known objects, often with very high confidence. This paper is the first to formalize the problem of open-set object detection and propose the first open-set object detection protocol. Moreover, the paper provides a new evaluation metric to analyze the performance of some state-of-the-art detectors and discusses their performance differences.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Dhamija_The_Overlooked_Elephant_of_Object_Detection_Open_Set_WACV_2020_paper.pdf",
        "aff": "Vision and Security Technology Lab, University of Colorado Colorado Springs; Vision and Security Technology Lab, University of Colorado Colorado Springs; Department of Computer Science & Software Engineering, California Polytechnic State University; Vision and Security Technology Lab, University of Colorado Colorado Springs",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Dhamija_The_Overlooked_Elephant_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2183428,
        "gs_citation": 198,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14535120296504073640&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "vast.uccs.edu;vast.uccs.edu;calpoly.edu;vast.uccs.edu",
        "email": "vast.uccs.edu;vast.uccs.edu;calpoly.edu;vast.uccs.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Colorado Colorado Springs;California Polytechnic State University",
        "aff_unique_dep": "Vision and Security Technology Lab;Department of Computer Science & Software Engineering",
        "aff_unique_url": "https://www.uccs.edu;https://www.calpoly.edu",
        "aff_unique_abbr": "UCCS;Cal Poly",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Colorado Springs;San Luis Obispo",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "The Synthinel-1 dataset: a collection of high resolution synthetic overhead imagery for building segmentation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kong_The_Synthinel-1_dataset_a_collection_of_high_resolution_synthetic_overhead_WACV_2020_paper.html",
        "author": "Fanjie Kong;  Bohao Huang;  Kyle Bradbury;  Jordan Malof",
        "abstract": "Recently deep learning - namely convolutional neural networks (CNNs) - have yielded impressive performance for the task of building segmentation on large overhead (e.g., satellite) imagery benchmarks. However, these benchmark datasets only capture a small fraction of the variability present in real-world overhead imagery, limiting the ability to properly train, or evaluate, models for real-world application. Unfortunately, developing a dataset that captures even a small fraction of real-world variability is typically infeasible due to the cost of imagery, and manual pixel-wise labeling of the imagery.  In this work we develop an approach to rapidly and cheaply generate large and diverse synthetic overhead imagery for training segmentation CNNs.  Using this approach, we generate and publicly-release a collection of synthetic overhead imagery, termed Synthinel-1, with full pixel-wise building labels.  We use several benchmark datasets to demonstrate that Synthinel-1 is consistently beneficial when used to augment real-world training imagery, especially when CNNs are tested on novel geographic locations or conditions.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kong_The_Synthinel-1_dataset_a_collection_of_high_resolution_synthetic_overhead_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 996257,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2452520669407425820&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Toward Explainable Fashion Recommendation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Tangseng_Toward_Explainable_Fashion_Recommendation_WACV_2020_paper.html",
        "author": "Pongsate Tangseng;  Takayuki Okatani",
        "abstract": "Many studies have been conducted so far to build systems for recommending fashion items and outfits. Although they achieve good performances in their respective tasks, most of them cannot explain their judgments to the users, which compromises their usefulness. Toward explainable fashion recommendation, this study proposes a system that is able not only to provide a goodness score for an outfit but also to explain the score by providing reason behind it. For this purpose, we propose a method for quantifying how influential each feature of each item is to the score.  Using this influence value, we can identify which item and what feature make the outfit good or bad. We represent the image of each item with a combination of human-interpretable features, and thereby the identification of the most influential item-feature pair gives useful explanation of the output score. To evaluate the performance of this approach, we design an experiment that can be performed without human annotation; we replace a single item-feature pair in an outfit so that the score will decrease, and then we test if the proposed method can detect the replaced item-feature pair correctly using the above influence values. The experimental results show that the proposed method can accurately detect bad items in outfits lowering their scores.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Tangseng_Toward_Explainable_Fashion_Recommendation_WACV_2020_paper.pdf",
        "aff": "Graduate School of Information Sciences, Tohoku University; RIKEN Center for AIP",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Tangseng_Toward_Explainable_Fashion_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3160713,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14178051595909857090&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "vision.is.tohoku.ac.jp;vision.is.tohoku.ac.jp",
        "email": "vision.is.tohoku.ac.jp;vision.is.tohoku.ac.jp",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Tohoku University;RIKEN",
        "aff_unique_dep": "Graduate School of Information Sciences;Center for AIP",
        "aff_unique_url": "https://www.tohoku.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "Tohoku U;RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Toward Interactive Self-Annotation For Video Object Bounding Box: Recurrent Self-Learning And Hierarchical Annotation Based Framework",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Le_Toward_Interactive_Self-Annotation_For_Video_Object_Bounding_Box_Recurrent_Self-Learning_WACV_2020_paper.html",
        "author": "Trung-Nghia Le;  Akihiro Sugimoto;  Shintaro Ono;  Hiroshi Kawasaki",
        "abstract": "Amount and variety of training data drastically affect the performance of CNNs. Thus, annotation methods are becoming more and more critical to collect data efficiently. In this paper, we propose a simple yet efficient Interactive Self-Annotation framework to cut down both time and human labor cost for video object bounding box annotation. Our method is based on recurrent self-supervised learning and consists of two processes: automatic process and interactive process, where the automatic process aims to build a supported detector to speed up the interactive process. In the Automatic Recurrent Annotation, we let an off-the-shelf detector watch unlabeled videos repeatedly to reinforce itself automatically. At each iteration, we utilize the trained model from the previous iteration to generate better pseudo ground-truth bounding boxes than those at the previous iteration, recurrently improving self-supervised training the detector. In the Interactive Recurrent Annotation, we tackle the human-in-the-loop annotation scenario where the detector receives feedback from the human annotator. To this end, we propose a novel Hierarchical Correction module, where the annotated frame-distance binarizedly decreases at each time step, to utilize the strength of CNN for neighbor frames. Experimental results on various video datasets demonstrate the advantages of the proposed framework in generating high-quality annotations while reducing annotation time and human labor costs.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Le_Toward_Interactive_Self-Annotation_For_Video_Object_Bounding_Box_Recurrent_Self-Learning_WACV_2020_paper.pdf",
        "aff": "University of Tokyo, Japan; National Institute of Informatics, Japan; University of Tokyo, Japan; Kyushu University, Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2033309,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4145488529667536837&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "its.iis.u-tokyo.ac.jp; ; ; ",
        "email": "its.iis.u-tokyo.ac.jp; ; ; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of Tokyo;National Institute of Informatics;Kyushu University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.nii.ac.jp;https://www.kyushu-u.ac.jp",
        "aff_unique_abbr": "UTokyo;NII;Kyushu U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Towards Good Practice for CNN-Based Monocular Depth Estimation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Fang_Towards_Good_Practice_for_CNN-Based_Monocular_Depth_Estimation_WACV_2020_paper.html",
        "author": "Zhicheng Fang;  Xiaoran Chen;  Yuhua Chen;  Luc Van Gool",
        "abstract": "Monocular depth estimation has gained increasing attention in recent years, and various techniques have been proposed to tackle this problem. In this work, we aim to provide a comprehensive study on the techniques widely used in monocular depth estimation, and examine their individual influence on the performance. More specifically, we provide a study on: 1) network architectures, including different combinations of encoders/decoders. 2) supervision losses, including fully supervised losses and self-supervised losses and 3) other practices such as input resolution. The experiments are conducted on two commonly used public datasets, KITTI and NYU Depth v2. We also provide an analysis on the errors produced by different models, to reveal the limitations of current methods. Furthermore, by a careful redesign, we present a model for depth estimation, which achieves competitive performance on KITTI and state-of-the-art performance on NYU Depth v2. Our code is publicly available at https://github.com/zenithfang/supervised_dispnet.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Fang_Towards_Good_Practice_for_CNN-Based_Monocular_Depth_Estimation_WACV_2020_paper.pdf",
        "aff": "Computer Vision Laboratory, ETH Zurich; Computer Vision Laboratory, ETH Zurich; Computer Vision Laboratory, ETH Zurich; Computer Vision Laboratory, ETH Zurich+VISICS, ESAT/PSI, KU Leuven",
        "project": "",
        "github": "https://github.com/zenithfang/supervised_dispnet",
        "supp": "",
        "arxiv": "",
        "pdf_size": 898597,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7143383508159780052&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "student.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "student.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "ETH Zurich;KU Leuven",
        "aff_unique_dep": "Computer Vision Laboratory;VISICS, ESAT/PSI",
        "aff_unique_url": "https://www.ethz.ch;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;KU Leuven",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Zurich;",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Switzerland;Belgium"
    },
    {
        "title": "Towards Learning Affine-Invariant Representations via Data-Efficient CNNs",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xu_Towards_Learning_Affine-Invariant_Representations_via_Data-Efficient_CNNs_WACV_2020_paper.html",
        "author": "Wenju Xu;  Guanghui Wang;  Alan Sullivan;  Ziming Zhang",
        "abstract": "In this paper we propose integrating a priori knowledge into both design and training of convolutional neural networks (CNNs) to learn object representations that are invariant to affine transformations (i.e. translation, scale, rotation). Accordingly we propose a novel multi-scale maxout CNN and train it end-to-end with a novel rotation-invariant regularizer. This regularizer aims to enforce the weights in each 2D spatial filter to approximate circular patterns. In this way, we manage to handle affine transformations in training using convolution, multi-scale maxout, and circular filters. Empirically we demonstrate that such knowledge can significantly improve the data-efficiency as well as generalization and robustness of learned models. For instance, on the Traffic Sign data set and trained with only 10 images per class, our method can achieve 84.15% that outperforms the state-of-the-art by 29.80% in terms of test accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xu_Towards_Learning_Affine-Invariant_Representations_via_Data-Efficient_CNNs_WACV_2020_paper.pdf",
        "aff": "University of Kansas, KS; University of Kansas, KS; Mitsubishi Electric Research Laboratories (MERL), MA; Worcester Polytechnic Institute, MA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 827636,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16081840415321478858&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ku.edu;ku.edu;merl.com;wpi.edu",
        "email": "ku.edu;ku.edu;merl.com;wpi.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of Kansas;Mitsubishi Electric Research Laboratories;Worcester Polytechnic Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ku.edu;https://www.merl.com;https://www.wpi.edu",
        "aff_unique_abbr": "KU;MERL;WPI",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Lawrence;MA;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Towards Photographic Image Manipulation with Balanced Growing of Generative Autoencoders",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Heljakka_Towards_Photographic_Image_Manipulation_with_Balanced_Growing_of_Generative_Autoencoders_WACV_2020_paper.html",
        "author": "Ari Heljakka;  Arno Solin;  Juho  Kannala",
        "abstract": "We present a generative autoencoder that provides fast encoding, faithful reconstructions (e.g. retaining the identity of a face), sharp generated/reconstructed samples in high resolutions, and a well-structured latent space that supports semantic manipulation of the inputs. There are no current autoencoder or GAN models that satisfactorily achieve all of these. We build on the progressively growing autoencoder model PIONEER, for which we completely alter the training dynamics based on a careful analysis of recently introduced normalization schemes. We show significantly improved visual and quantitative results for face identity conservation in CelebA-HQ. Our model achieves state-of-the-art disentanglement of latent space, both quantitatively and via realistic image attribute manipulations. On the LSUN Bedrooms dataset, we improve the disentanglement performance of the vanilla PIONEER, despite having a simpler model. Overall, our results indicate that the PIONEER networks provide a way towards photorealistic face manipulation.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Heljakka_Towards_Photographic_Image_Manipulation_with_Balanced_Growing_of_Generative_Autoencoders_WACV_2020_paper.pdf",
        "aff": "Department of Computer Science, Aalto University, Espoo, Finland + GenMind Ltd., Espoo, Finland; Department of Computer Science, Aalto University, Espoo, Finland; Department of Computer Science, Aalto University, Espoo, Finland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Heljakka_Towards_Photographic_Image_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2310851,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12744532749622533312&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "aalto.fi;aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi;aalto.fi",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Aalto University;GenMind Ltd.",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.aalto.fi;",
        "aff_unique_abbr": "Aalto;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Espoo;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "title": "Towards Preserving the Ephemeral: Texture-Based Background Modelling for Capturing Back-of-the-Napkin Notes",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Cote_Towards_Preserving_the_Ephemeral_Texture-Based_Background_Modelling_for_Capturing_Back-of-the-Napkin_WACV_2020_paper.html",
        "author": "Melissa Cote;  Alexandra Branzan Albu",
        "abstract": "A back-of-the-napkin idea is typically created on the spur of the moment and captured via a few hand-sketched notes on whatever material is available, which often happens to be an actual paper napkin. This paper explores the preservation of such back-of-the-napkin ideas. Hand-sketched notes, reflecting those flashes of inspiration, are not limited to text; they can also include drawings and graphics. Napkin backgrounds typically exhibit diverse textural and colour motifs/patterns that may have high visual saliency from a low-level vision standpoint. We thus frame the extraction of hand-sketched notes as a background modelling and removal task. We propose a novel document background model based on texture mixtures constructed from the document itself via texture synthesis, which allows us to identify background pixels and extract hand-sketched data as foreground elements. Experiments on a novel napkin image dataset yield excellent results and showcase the robustness of our method with respect to the napkin contents. A texture-based background modelling approach, such as ours, is generic enough to cope with any type of hand-sketched notes.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Cote_Towards_Preserving_the_Ephemeral_Texture-Based_Background_Modelling_for_Capturing_Back-of-the-Napkin_WACV_2020_paper.pdf",
        "aff": "University of Victoria, Victoria, BC, Canada; University of Victoria, Victoria, BC, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1538949,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:uw02AU9j_4gJ:scholar.google.com/&scioq=Towards+Preserving+the+Ephemeral:+Texture-Based+Background+Modelling+for+Capturing+Back-of-the-Napkin+Notes&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff_domain": "uvic.ca;uvic.ca",
        "email": "uvic.ca;uvic.ca",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Victoria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uvic.ca",
        "aff_unique_abbr": "UVic",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Victoria",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Towards a Unified Framework for Visual Compatibility Prediction",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Singhal_Towards_a_Unified_Framework_for_Visual_Compatibility_Prediction_WACV_2020_paper.html",
        "author": "Anirudh Singhal;  Ayush Chopra;  Kumar Ayush;  Utkarsh Patel Govind;  Balaji Krishnamurthy",
        "abstract": "Visual compatibility prediction refers to the task of determining if a set of items go well together. Existing techniques for compatibility prediction prioritize sensitivity to type or context in item representations and evaluate using a fill-in-the-blank (FITB) task. We scale the FITB task to stress-test existing methods which highlight the need for a compatibility prediction framework that is sensitive to multiple modalities of item relationships. In this work, we introduce a unified framework for compatibility learning that is jointly conditioned on the type, context, and style. The framework is composed of TC-GAE, a graph-based network that models type & context; SAE, an autoencoder that models style; and a reinforcement-learning based search technique that incorporates these modalities to learn a unified compatibility measure. We conduct experiments on two standard datasets and significantly outperform existing state-of-the-art methods. We also present qualitative analysis and discussions to study the impact of components of the proposed framework.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Singhal_Towards_a_Unified_Framework_for_Visual_Compatibility_Prediction_WACV_2020_paper.pdf",
        "aff": "IIT Bombay\u20202 + Adobe\u20211; Adobe\u20211 + IIT Madras\u20203; Stanford University\u00a74 + Adobe; IIT Madras\u20203; Adobe",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 560787,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12468782539834647947&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iitb.ac.in;adobe.com;stanford.edu;iitm.ac.in;adobe.com",
        "email": "iitb.ac.in;adobe.com;stanford.edu;iitm.ac.in;adobe.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1+2;3+1;2;1",
        "aff_unique_norm": "Indian Institute of Technology Bombay;Adobe Inc.;Indian Institute of Technology Madras;Stanford University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.adobe.com;https://www.iitm.ac.in;https://www.stanford.edu",
        "aff_unique_abbr": "IIT-B;Adobe;IIT Madras;Stanford",
        "aff_campus_unique_index": "0;2;;2",
        "aff_campus_unique": "Mumbai;;Madras",
        "aff_country_unique_index": "0+1;1+0;1+1;0;1",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "Training with Noise Adversarial Network: A Generalization Method for Object Detection on Sonar Image",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Ma_Training_with_Noise_Adversarial_Network_A_Generalization_Method_for_Object_WACV_2020_paper.html",
        "author": "Qixiang Ma;  Longyu Jiang;  Wenxue Yu;  Rui Jin;  Zhixiang Wu;  Fangjin Xu",
        "abstract": "Object detection tasks for sonar image confront two major challenges, scarcity of dataset and perturbation of noise, which cause overfitting to models. The state-of-the-art object detection designed for optical images cannot address the issues because of the inherent differentiation between the optical image and sonar image. To tackle this problem, in this paper, we propose an adversarial training method to generalize the detector by introducing perturbation with specific noise property of sonar images during training stage. We design a sideway network which we name Noise Adversarial Network (NAN). The NAN is embedded into the state-of-the-art detector to generate adversarial examples which serve as assistant decision-making items to predict both class and bounding box, aiming to improve the generalization and noise robustness of the detector. To provide prior knowledge of noise perturbation to NAN, we also design a Noise Block (NB) for introducing noise in the upstream layers, which further improves noise robustness. Following the Faster R-CNN framework, the results of our experiments indicate a 8.9% mAP boost on our sonar image dataset. The detector equipped with NAN and NB also outperforms the baseline on noised test sets. Furthermore, it gains a 2.4% mAP boost on the optical image dataset PASCAL VOC 2007.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Ma_Training_with_Noise_Adversarial_Network_A_Generalization_Method_for_Object_WACV_2020_paper.pdf",
        "aff": "Laboratory of Image Science and Technology, Southeast University, Nanjing 210096, China; Laboratory of Image Science and Technology, Southeast University, Nanjing 210096, China + Acoustic Science and Technology Laboratory, Harbin Engineering University, Harbin 150001, China; Laboratory of Image Science and Technology, Southeast University, Nanjing 210096, China; Laboratory of Image Science and Technology, Southeast University, Nanjing 210096, China; Laboratory of Image Science and Technology, Southeast University, Nanjing 210096, China; Laboratory of Image Science and Technology, Southeast University, Nanjing 210096, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Ma_Training_with_Noise_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2936638,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11221241058200617064&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0;0;0",
        "aff_unique_norm": "Southeast University;Harbin Engineering University",
        "aff_unique_dep": "Laboratory of Image Science and Technology;Acoustic Science and Technology Laboratory",
        "aff_unique_url": "https://www.seu.edu.cn/;",
        "aff_unique_abbr": "SEU;",
        "aff_campus_unique_index": "0;0+1;0;0;0;0",
        "aff_campus_unique": "Nanjing;Harbin",
        "aff_country_unique_index": "0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Transductive Zero-Shot Learning for 3D Point Cloud Classification",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Cheraghian_Transductive_Zero-Shot_Learning_for_3D_Point_Cloud_Classification_WACV_2020_paper.html",
        "author": "Ali Cheraghian;  Shafin Rahman;  Dylan Campbell;  Lars Petersson",
        "abstract": "Zero-shot learning, the task of learning to recognize new classes not seen during training, has received considerable attention in the case of 2D image classification. However despite the increasing ubiquity of 3D sensors, the corresponding 3D point cloud classification problem has not been meaningfully explored and introduces new challenges. This paper extends, for the first time, transductive Zero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL) approaches to the domain of 3D point cloud classification. To this end, a novel triplet loss is developed that takes advantage of unlabeled test data. While designed for the task of 3D point cloud classification, the method is also shown to be applicable to the more common use case of 2D image classification. An extensive set of experiments is carried out, establishing state-of-the-art for ZSL and GZSL in the 3D point cloud domain, as well as demonstrating the applicability of the approach to the image domain.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Cheraghian_Transductive_Zero-Shot_Learning_for_3D_Point_Cloud_Classification_WACV_2020_paper.pdf",
        "aff": "Australian National University + Data61-CSIRO; Australian National University + Data61-CSIRO; Australian National University + Australian Centre for Robotic Vision; Australian National University + Data61-CSIRO",
        "project": "",
        "github": "https://github.com/ali-chr/Transductive_ZSL_3D_Point_Cloud",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Cheraghian_Transductive_Zero-Shot_Learning_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2397858,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9267371596801629574&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+2;0+1",
        "aff_unique_norm": "Australian National University;Commonwealth Scientific and Industrial Research Organisation;Australian Centre for Robotic Vision",
        "aff_unique_dep": ";Data61;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.csiro.au;https://roboticvision.org/",
        "aff_unique_abbr": "ANU;CSIRO;ACRV",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Triple-SGM: Stereo Processing using Semi-Global Matching with Cost Fusion",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kallwies_Triple-SGM_Stereo_Processing_using_Semi-Global_Matching_with_Cost_Fusion_WACV_2020_paper.html",
        "author": "Jan Kallwies;  Torsten Engler;  Bianca Forkel;  Hans-Joachim Wuensche",
        "abstract": "In this work, we propose an extension of the Semi-Global Matching framework for three images from a stereo rig consisting of a horizontal and vertical camera pair. After calculating the matching costs separately for both image pairs, these are merged at cost level using cubic spline interpolation. For cost values near the left/bottom image boundaries, we propose an advanced weighting strategy. Subsequently, the fused matching can be used directly for the cost aggregation and disparity estimation. The benefits of the proposed fusion strategy are demonstrated by an evaluation based on synthetic and real-world data. To encourage further comparisons on triple stereo algorithms, the dataset used for evaluation is made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kallwies_Triple-SGM_Stereo_Processing_using_Semi-Global_Matching_with_Cost_Fusion_WACV_2020_paper.pdf",
        "aff": "Autonomous Systems Technology (TAS); Autonomous Systems Technology (TAS); Autonomous Systems Technology (TAS); Autonomous Systems Technology (TAS)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1744117,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1815215248802261083&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "unibw.de;unibw.de;unibw.de;unibw.de",
        "email": "unibw.de;unibw.de;unibw.de;unibw.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Autonomous Systems Technology",
        "aff_unique_dep": "TAS",
        "aff_unique_url": "",
        "aff_unique_abbr": "TAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Two-Grid Preconditioned Solver for Bundle Adjustment",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Katyan_Two-Grid_Preconditioned_Solver_for_Bundle_Adjustment_WACV_2020_paper.html",
        "author": "Siddhant Katyan;  Shrutimoy Das;  Pawan Kumar",
        "abstract": "We present the design and implementation of Two-Grid Preconditioned Bundle Adjustment (TPBA), a robust and efficient technique for solving the non-linear least squares problem that arises in bundle adjustment. Bundle adjustment (BA) methods for multi-view reconstruction formulate the BA problem as a non-linear least squares problem which is solved by some variant of the traditional Levenberg-Marquardt (LM) algorithm. Most of the computation in LM goes into repeatedly solving the normal equations that arise as a result of linearizing the objective function. To solve these  system of equations we use the Generalized Minimal Residual (GMRES) method, which is preconditioned using a deflated algebraic two-grid method. To the best of our knowledge this is the first time that a deflated algebraic two-grid preconditioner has been used along with GMRES, for solving a problem in the computer vision domain. We show that the proposed method is several times faster than the direct method and block Jacobi preconditioned GMRES.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Katyan_Two-Grid_Preconditioned_Solver_for_Bundle_Adjustment_WACV_2020_paper.pdf",
        "aff": "International Institute of Information Technology, Hyderabad; International Institute of Information Technology, Hyderabad; International Institute of Information Technology, Hyderabad",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 498794,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9118799680295609287&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "International Institute of Information Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": "IIIT Hyderabad",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "TwoStreamVAN: Improving Motion Modeling in Video Generation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Sun_TwoStreamVAN_Improving_Motion_Modeling_in_Video_Generation_WACV_2020_paper.html",
        "author": "Ximeng Sun;  Huijuan Xu;  Kate Saenko",
        "abstract": "Video generation is an inherently challenging task, as it requires modeling realistic temporal dynamics as well as spatial content. Existing methods entangle the two intrinsically different tasks of motion and content creation in a single generator network, but this approach struggles to simultaneously generate plausible motion and content. To im-prove motion modeling in video generation tasks, we propose a two-stream model that disentangles motion generation from content generation, called a Two-Stream Variational Adversarial Network (TwoStreamVAN). Given an action label and a noise vector, our model is able to create clear and consistent motion, and thus yields photorealistic videos. The key idea is to progressively generate and fuse multi-scale motion with its corresponding spatial content. Our model significantly outperforms existing methods on the standard Weizmann Human Action, MUG Facial Expression, and VoxCeleb datasets, as well as our new dataset of diverse human actions with challenging and complex motion. Our code is available at https://github.com/sunxm2357/TwoStreamVAN/.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Sun_TwoStreamVAN_Improving_Motion_Modeling_in_Video_Generation_WACV_2020_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "https://github.com/sunxm2357/TwoStreamVAN/",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Sun_TwoStreamVAN_Improving_Motion_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1805259,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12752499263958920151&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "ULSAM: Ultra-Lightweight Subspace Attention Module for Compact Convolutional Neural Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Saini_ULSAM_Ultra-Lightweight_Subspace_Attention_Module_for_Compact_Convolutional_Neural_Networks_WACV_2020_paper.html",
        "author": "Rajat Saini;  Nandan Kumar Jha;  Bedanta Das;  Sparsh Mittal;  C . Krishna Mohan",
        "abstract": "The capability of the self-attention mechanism to model the long-range dependencies has catapulted its deployment in vision models. Unlike convolution operators, self-attention offers infinite receptive field and enables compute-efficient modeling of global dependencies. However, the existing state-of-the-art attention mechanisms incur high compute and/or parameter overheads, and hence unfit for compact convolutional neural networks (CNNs). In this work, we propose a simple yet effective \"Ultra-Lightweight Subspace Attention Mechanism\" (ULSAM), which infers different attention maps for each feature map subspace. We argue that leaning separate attention maps for each feature subspace enables multi-scale and multi-frequency feature representation, which is more desirable for fine-grained  image classification. Our method of subspace attention is orthogonal and complementary to the existing state-of-the-arts attention mechanisms used in vision models. ULSAM is end-to-end trainable and can be deployed as a plug-and- play module in the pre-existing compact CNNs. Notably, our work is the first attempt that uses a subspace attention mechanism to increase the efficiency of compact CNNs. To show the efficacy of ULSAM, we perform experiments with MobileNet-V1 and MobileNet-V2 as backbone architectures on ImageNet-1K and three fine-grained image classification datasets. We achieve [?]13% and [?]25% reduction in both the FLOPs and parameter counts of MobileNet-V2 with a 0.27% and more than 1% improvement in top-1 accuracy on the ImageNet-1K and fine-grained image classification datasets (respectively). Code and trained models are available at https://github.com/Nandan91/ULSAM .",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Saini_ULSAM_Ultra-Lightweight_Subspace_Attention_Module_for_Compact_Convolutional_Neural_Networks_WACV_2020_paper.pdf",
        "aff": "IIT Hyderabad; IIT Hyderabad; IIT Hyderabad; IIT Roorkee; IIT Hyderabad",
        "project": "",
        "github": "https://github.com/Nandan91/ULSAM",
        "supp": "",
        "arxiv": "",
        "pdf_size": 849612,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5133594243473641436&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "iith.ac.in;iith.ac.in;iith.ac.in;iitr.ac.in;iith.ac.in",
        "email": "iith.ac.in;iith.ac.in;iith.ac.in;iitr.ac.in;iith.ac.in",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Indian Institute of Technology, Hyderabad;Indian Institute of Technology Roorkee",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iith.ac.in;https://www.iitr.ac.in",
        "aff_unique_abbr": "IIT-H;IITR",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hyderabad;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Luiten_UnOVOST_Unsupervised_Offline_Video_Object_Segmentation_and_Tracking_WACV_2020_paper.html",
        "author": "Jonathon Luiten;  Idil Esen Zulfikar;  Bastian Leibe",
        "abstract": "We address Unsupervised Video Object Segmentation (UVOS), the task of automatically generating accurate pixel masks for salient objects in a video sequence and of tracking these objects consistently through time, without any input about which objects should be tracked. Towards solving this task, we present UnOVOST (Unsupervised Offline Video Object Segmentation and Tracking) as a simple and generic algorithm which is able to track and segment a large variety of objects. This algorithm builds up tracks in a number stages, first grouping segments into short tracklets that are spatio-temporally consistent, before merging these tracklets into long-term consistent object tracks based on their visual similarity. In order to achieve this we introduce a novel tracklet-based Forest Path Cutting data association algorithm which builds up a decision forest of track hypotheses before cutting this forest into paths that form long-term consistent object tracks. When evaluating our approach on the DAVIS 2017 Unsupervised dataset we obtain state-of-the-art performance with a mean J &F score of 67.9% on the val, 58% on the test-dev and 56.4% on the test-challenge benchmarks, obtaining first place in the DAVIS 2019 Unsupervised Video Object Segmentation Challenge. UnOVOST even performs competitively with many semi-supervised video object segmentation algorithms even though it is not given any input as to which objects should be tracked and segmented.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Luiten_UnOVOST_Unsupervised_Offline_Video_Object_Segmentation_and_Tracking_WACV_2020_paper.pdf",
        "aff": "Computer Vision Group, RWTH Aachen University; Computer Vision Group, RWTH Aachen University; Computer Vision Group, RWTH Aachen University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Luiten_UnOVOST_Unsupervised_Offline_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4951234,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3792022592588739089&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "vision.rwth-aachen.de;rwth-aachen.de;vision.rwth-aachen.de",
        "email": "vision.rwth-aachen.de;rwth-aachen.de;vision.rwth-aachen.de",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "RWTH Aachen University",
        "aff_unique_dep": "Computer Vision Group",
        "aff_unique_url": "https://www.rwth-aachen.de",
        "aff_unique_abbr": "RWTH",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Aachen",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Uncertainty in Model-Agnostic Meta-Learning using Variational Inference",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Nguyen_Uncertainty_in_Model-Agnostic_Meta-Learning_using_Variational_Inference_WACV_2020_paper.html",
        "author": "Cuong Nguyen;  Thanh-Toan Do;  Gustavo Carneiro",
        "abstract": "We introduce a new, rigorously-formulated Bayesian meta-learning algorithm that learns a probability distribution of model parameter prior for few-shot learning. The proposed algorithm employs a gradient-based variational inference to infer the posterior of model parameters for a new task. Our algorithm can be applied to any model architecture and can be implemented in various machine learning paradigms, including regression and classification. We show that the models trained with our proposed meta-learning algorithm are well calibrated and accurate, with state-of-the-art calibration and classification results on three few-shot classification benchmarks (Omniglot, mini-ImageNet and tiered-ImageNet), and competitive results in a multi-modal task-distribution regression.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Nguyen_Uncertainty_in_Model-Agnostic_Meta-Learning_using_Variational_Inference_WACV_2020_paper.pdf",
        "aff": "University of Adelaide; University of Liverpool; University of Adelaide",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Nguyen_Uncertainty_in_Model-Agnostic_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 747358,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=823334953574253907&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "adelaide.edu.au;liverpool.ac.uk;adelaide.edu.au",
        "email": "adelaide.edu.au;liverpool.ac.uk;adelaide.edu.au",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Adelaide;University of Liverpool",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.adelaide.edu.au;https://www.liverpool.ac.uk",
        "aff_unique_abbr": "Adelaide;Liv Uni",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Australia;United Kingdom"
    },
    {
        "title": "Uncertainty-aware Short-term Motion Prediction of Traffic Actors for Autonomous Driving",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Djuric_Uncertainty-aware_Short-term_Motion_Prediction_of_Traffic_Actors_for_Autonomous_Driving_WACV_2020_paper.html",
        "author": "Nemanja Djuric;  Vladan Radosavljevic;  Henggang Cui;  Thi Nguyen;  Fang-Chieh Chou;  Tsung-Han Lin;  NITIN SINGH;  Jeff Schneider",
        "abstract": "We address one of the crucial aspects necessary for safe and efficient operations of autonomous vehicles, namely predicting future state of traffic actors in the autonomous vehicle's surroundings. We introduce a deep learning-based approach that takes into account a current world state and produces raster images of each actor's vicinity. The rasters are then used as inputs to deep convolutional models to infer future movement of actors while also accounting for and capturing inherent uncertainty of the prediction task. Extensive experiments on real-world data strongly suggest benefits of the proposed approach. Moreover, following successful tests the system was deployed to a fleet of autonomous vehicles.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Djuric_Uncertainty-aware_Short-term_Motion_Prediction_of_Traffic_Actors_for_Autonomous_Driving_WACV_2020_paper.pdf",
        "aff": "Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Djuric_Uncertainty-aware_Short-term_Motion_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 753567,
        "gs_citation": 267,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3569895507286843060&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "uber.com;uber.com;uber.com;uber.com;uber.com;uber.com;uber.com;uber.com",
        "email": "uber.com;uber.com;uber.com;uber.com;uber.com;uber.com;uber.com;uber.com",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Uber",
        "aff_unique_dep": "Advanced Technologies Group",
        "aff_unique_url": "https://www.uber.com",
        "aff_unique_abbr": "Uber ATG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Unsupervised Cross-Dataset Adaptation via Probabilistic Amodal 3D Human Pose Completion",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kundu_Unsupervised_Cross-Dataset_Adaptation_via_Probabilistic_Amodal_3D_Human_Pose_Completion_WACV_2020_paper.html",
        "author": "Jogendra Nath Kundu;  Rahul M V;  Jay Patravali;  Venkatesh Babu RADHAKRISHNAN",
        "abstract": "Despite remarkable success of supervised deep learning models for 3D human pose estimation, performance of such models is mostly limited to constrained laboratory settings. Such models not only exhibit an alarming level of dataset bias but also fail to operate on unconstrained videos in the presence of external variations such as camera motion, partial body visibility, occlusion, etc. Acknowledging these shortcomings, firstly, we aim to formalize a motion representation learning framework by effectively utilizing both constrained and artificially generated unconstrained video samples for datasets with 3D pose annotation. Without ignoring the inherent uncertainty in pose estimation for the truncated video frames, we devise a novel probabilistic amodal pose completion framework to enable generation of multiple plausible pose-filling outcomes. Secondly, to address dataset bias, the probabilistic amodal framework is re-utilized to design novel self-supervised objectives. This not only enables adaptation of the model to target unannotated datasets (wild YouTube videos) but also encourages learning of generic motion representations beyond the available supervised data even in unconstrained scenarios. Such a training regime helps us achieve state-of-the-art performance on unsupervised cross-dataset pose estimation, with a significant improvement in partially-visible unconstrained scenarios.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kundu_Unsupervised_Cross-Dataset_Adaptation_via_Probabilistic_Amodal_3D_Human_Pose_Completion_WACV_2020_paper.pdf",
        "aff": "Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India; Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India; Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India; Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 17289226,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8129916341606503737&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "iisc.ac.in;andrew.cmu.edu;gmail.com;iisc.ac.in",
        "email": "iisc.ac.in;andrew.cmu.edu;gmail.com;iisc.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Computational Data Systems",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Unsupervised Domain Adaptation in Person re-ID via k-Reciprocal Clustering and Large-Scale Heterogeneous Environment Synthesis",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kumar_Unsupervised_Domain_Adaptation_in_Person_re-ID_via_k-Reciprocal_Clustering_and_WACV_2020_paper.html",
        "author": "Devinder Kumar;  Parthipan Siva;  Paul Marchwica;  Alexander Wong",
        "abstract": "An ongoing major challenge in computer vision is the task of person re-identification, where the goal is to match individuals across different, non-overlapping camera views. While recent success has been achieved via supervised learning using deep neural networks, such methods have limited widespread adoption due to the need for large-scale, customized data annotation. As such, there has been a re- cent focus on unsupervised learning approaches to mitigate the data annotation issue; however, current approaches in literature have limited performance compared to supervised learning approaches as well as limited applicability for adoption in new environments. In this paper, we address the aforementioned challenges faced in person re-identification for real-world, practical scenarios by introducing a novel, unsupervised domain adaptation approach for person re- identification. This is accomplished through the introduction of: i) k-reciprocal tracklet Clustering for Unsupervised Domain Adaptation (ktCUDA) (for pseudo-label generation on target domain), and ii) Synthesized Heterogeneous RE-id Domain (SHRED) composed of large-scale heterogeneous independent source environments (for improving robustness and adaptability to a wide diversity of target environments). Experimental results across four different image and video benchmark datasets show that the pro- posed ktCUDA and SHRED approach achieves an average improvement of +5.7 mAP in re-identification performance when compared to existing state-of-the-art methods, as well as demonstrate better adaptability to different types of environments.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kumar_Unsupervised_Domain_Adaptation_in_Person_re-ID_via_k-Reciprocal_Clustering_and_WACV_2020_paper.pdf",
        "aff": "University of Waterloo; Sportlogiq; Sportlogiq; University of Waterloo",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 720507,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6800115920716090536&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "uwaterloo.ca;sportlogiq.com;sportlogiq.com;uwaterloo.ca",
        "email": "uwaterloo.ca;sportlogiq.com;sportlogiq.com;uwaterloo.ca",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Waterloo;Sportlogiq",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://uwaterloo.ca;https://www.sportlogiq.com",
        "aff_unique_abbr": "UW;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Unsupervised Image Style Embeddings for Retrieval and Recognition Tasks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Gairola_Unsupervised_Image_Style_Embeddings_for_Retrieval_and_Recognition_Tasks_WACV_2020_paper.html",
        "author": "Siddhartha Gairola;  Rajvi Shah;  P. J.  Narayanan",
        "abstract": "We propose an unsupervised protocol for learning a neural embedding of visual style of images. Style similarity is an important measure for many applications such as style transfer, fashion search, art exploration, etc. However, computational modeling of style is a difficult task owing to its vague and subjective nature. Most methods for style based retrieval use supervised training with pre-defined categorization of images according to style. While this paradigm is suitable for applications where style categories are well-defined and curating large datasets according to such a categorization is feasible, in several other cases such a categorization is either ill-defined or does not exist. Our protocol for learning style based representations does not leverage categorical labels but a proxy measure for forming triplets of anchor, similar, and dissimilar images. Using these triplets, we learn a compact style embedding that is useful for style-based search and retrieval. The learned embeddings outperform other unsupervised representations for style-based image retrieval task on six datasets that capture different meanings of style. We also show that by fine-tuning the learned features with dataset-specific style labels, we obtain best results for image style recognition task on five of six datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Gairola_Unsupervised_Image_Style_Embeddings_for_Retrieval_and_Recognition_Tasks_WACV_2020_paper.pdf",
        "aff": "CVIT, KCIS, IIIT Hyderabad, India+Facebook Reality Labs, Redmond, WA, USA; CVIT, KCIS, IIIT Hyderabad, India; CVIT, KCIS, IIIT Hyderabad, India",
        "project": "https://sidgairo18.github.io/style",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9926241,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12672602247929708082&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff_domain": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad;Facebook Reality Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://iiit Hyderabad.ac.in;https://www.facebook.com/realitylabs",
        "aff_unique_abbr": "IIIT Hyderabad;FRL",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Hyderabad;Redmond",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "Unsupervised Learning of Camera Pose with Compositional Re-estimation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Nabavi_Unsupervised_Learning_of_Camera_Pose_with_Compositional_Re-estimation_WACV_2020_paper.html",
        "author": "Seyed shahabeddin Nabavi;  Mehrdad  Hosseinzadeh;  Ramin Fahimi;  Yang Wang",
        "abstract": "We consider the problem of unsupervised camera pose estimation. Given an input video sequence, our goal is to estimate the camera pose (i.e. the camera motion) between consecutive frames. Traditionally, this problem is tackled by placing strict constraints on the transformation vector or by incorporating optical flow through a complex pipeline. We propose an alternative approach that utilizes a compositional re-estimation process for camera pose estimation. Given an input, we first estimate a depth map. Our method then iteratively estimates the camera motion based on the estimated depth map. Our approach significantly improves the predicted camera motion both quantitatively and visually. Furthermore, the re-estimation resolves the problem of out-of-boundaries pixels in a novel and simple way. Another advantage of our approach is that it is adaptable to other camera pose estimation approaches. Experimental analysis on KITTI benchmark dataset demonstrates that our method outperforms existing state-of-the-art approaches in unsupervised camera ego-motion estimation.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Nabavi_Unsupervised_Learning_of_Camera_Pose_with_Compositional_Re-estimation_WACV_2020_paper.pdf",
        "aff": "York University; University of Manitoba; Ryerson University; University of Manitoba",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Nabavi_Unsupervised_Learning_of_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 650611,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9229856606563487735&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "yorku.ca;cs.umanitoba.ca;ryerson.ca;cs.umanitoba.ca",
        "email": "yorku.ca;cs.umanitoba.ca;ryerson.ca;cs.umanitoba.ca",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "York University;University of Manitoba;Ryerson University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.yorku.ca;https://umanitoba.ca;https://www.ryerson.ca",
        "aff_unique_abbr": "York U;U of M;Ryerson",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Unsupervised Writer Adaptation for Synthetic-to-Real Handwritten Word Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kang_Unsupervised_Writer_Adaptation_for_Synthetic-to-Real_Handwritten_Word_Recognition_WACV_2020_paper.html",
        "author": "Lei Kang;  Marcal Rusinol;  Alicia Fornes;  Pau Riba;  Mauricio Villegas",
        "abstract": "Handwritten Text Recognition (HTR) is still a challenging problem because it must deal with two important difficulties: the variability among writing styles, and the scarcity of labelled data. To alleviate such problems, synthetic data generation and data augmentation are typically used to train HTR systems. However, training with such data produces encouraging but still inaccurate transcriptions in real words. In this paper, we propose an unsupervised writer adaptation approach that is able to automatically adjust a generic handwritten word recognizer, fully trained with synthetic fonts, towards a new incoming writer. We have experimentally validated our proposal using five different datasets, covering several challenges (i) the document source: modern and historic samples, which may involve paper degradation problems; (ii) different handwriting styles: single and multiple writer collections; and (iii) language, which involves different character combinations. Across these challenging collections, we show that our system is able to maintain its performance, thus, it provides a practical and generic approach to deal with new document collections without requiring any expensive and tedious manual annotation s",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kang_Unsupervised_Writer_Adaptation_for_Synthetic-to-Real_Handwritten_Word_Recognition_WACV_2020_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2724378,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=325206602897492255&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Unsupervised and Semi-Supervised Domain Adaptation for Action Recognition from Drones",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Choi_Unsupervised_and_Semi-Supervised_Domain_Adaptation_for_Action_Recognition_from_Drones_WACV_2020_paper.html",
        "author": "Jinwoo Choi;  Gaurav Sharma;  Manmohan Chandraker;  Jia-Bin Huang",
        "abstract": "We address the problem of human action classification in drone videos. Due to the high cost of capturing and labeling large-scale drone videos with diverse actions, we present unsupervised and semi-supervised domain adaptation approaches that leverage both the existing fully annotated action recognition datasets and unannotated (or only a few annotated) videos from drones. To study the emerging problem of drone-based action recognition, we create a new dataset, NEC-Drone, containing 5,250 videos to evaluate the task. We tackle both problem settings with 1) same and 2) different action label sets for the source (e.g., Kinectics dataset) and target domains (drone videos). We present a combination of video and instance-based adaptation methods, paired with either a classifier or an embedding-based framework to transfer the knowledge from source to target. Our results show that the proposed adaptation approach substantially improves the performance on these challenging and practical tasks. We further demonstrate the applicability of our method for learning cross-view action recognition on the Charades-Ego dataset. We provide qualitative analysis to understand the behaviors of our approaches.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Choi_Unsupervised_and_Semi-Supervised_Domain_Adaptation_for_Action_Recognition_from_Drones_WACV_2020_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Choi_Unsupervised_and_Semi-Supervised_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6363914,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15704040222441632516&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "VRT-Net: Real-Time Scene Parsing via Variable Resolution Transform",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Kundu_VRT-Net_Real-Time_Scene_Parsing_via_Variable_Resolution_Transform_WACV_2020_paper.html",
        "author": "Jogendra Nath Kundu;  Gaurav Singh Rajput;  Venkatesh Babu RADHAKRISHNAN",
        "abstract": "Urban scene parsing is a basic requirement for various autonomous navigation systems especially in self-driving. Most of the available approaches employ generic image parsing architectures designed for segmentation of object focused scene captured in indoor setups. However, images captured in car-mounted cameras exhibit an extreme effect of perspective geometry, causing a significant scale disparity between near and farther objects. Recognizing this, we formalize a unique Variable Resolution Transform (VRT) technique motivated from the foveal magnification in human eye. Following this, we design a Fovea Estimation Network (FEN) which is trained to estimate a single most convenient fixation location along with the associated magnification factor, best suited for a given input image. The proposed framework is designed to enable its usage as a wrapper over the available real-time scene parsing models, thereby demonstrating a superior trade-off between speed and quality as compared to the prior state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kundu_VRT-Net_Real-Time_Scene_Parsing_via_Variable_Resolution_Transform_WACV_2020_paper.pdf",
        "aff": "Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India; Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India; Video Analytics Lab, CDS, Indian Institute of Science, Bangalore, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3811869,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=961139546327551556&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 4,
        "aff_domain": "iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "email": "iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Computational Data Systems",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Variational Image Deraining",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Du_Variational_Image_Deraining_WACV_2020_paper.html",
        "author": "Yingjun Du;  Jun Xu;  Qiang Qiu;  Xiantong Zhen;  Lei Zhang",
        "abstract": "Images captured in severe weather such as rain and snow significantly degrade the accuracy of vision systems, e.g., for outdoor video surveillance or autonomous driving. Image deraining is a critical yet highly challenging task, due to the fact that rain density varies across spatial locations, while the distribution patterns simultaneously vary across color channels. In this paper, we propose a variational image deraining (VID) method by formulating image deraining in a conditional variational auto-encoder framework. To achieve adaptive deraining to spatial rain density, we generate a density estimation map for each color channel, which can largely avoid over and under deraining. In addition, to address cross-channel variations, we conduct channel-wise deraining, motivated by our observation that bright pixels do not tend to remain bright after deraining unless there color channels are handled separately. Experimental results show that the proposed deraining method achieves superior performance on both synthesized and real rainy images, surpassing previous state-of-the-art methods by large margins. The code will be publicly released.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Du_Variational_Image_Deraining_WACV_2020_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3044967,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3482104392045222467&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Very Power Efficient Neural Time-of-Flight",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_Very_Power_Efficient_Neural_Time-of-Flight_WACV_2020_paper.html",
        "author": "Yan Chen;  Jimmy Ren;  Xuanye Cheng;  Keyuan Qian;  Luyang Wang;  Jinwei Gu",
        "abstract": "Time-of-Flight (ToF) cameras require active illumination to obtain depth information thus the power of illumination directly affects the performance of ToF cameras. Traditional ToF imaging algorithms are very sensitive to illumination and the depth accuracy degenerates rapidly with the power of it. Therefore, the design of a power efficient ToF camera always creates a painful dilemma for the illumination and the performance trade-off. In this paper, we show that despite the weak signals in many areas under extreme short exposure setting, these signals as a whole can be well utilized through a learning process which directly translates the weak and noisy ToF camera raw to depth map. This creates an opportunity to tackle the aforementioned dilemma and make a very power efficient ToF camera possible. To enable the learning, we collect a comprehensive dataset under a variety of scenes and photographic conditions by a specialized ToF camera. Experiments show that our method is able to robustly process ToF camera raw with the exposure time of one order of magnitude shorter than that used in conventional ToF cameras. In addition to evaluating our approach both quantitatively and qualitatively, we also discuss its implication to designing the next generation power efficient ToF cameras.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Very_Power_Efficient_Neural_Time-of-Flight_WACV_2020_paper.pdf",
        "aff": "SenseTime Research; SenseTime Research; SenseTime Research; Tsinghua University; SenseTime Research; SenseTime Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3104604,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12842503047572189550&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "sensetime.com;sensetime.com;sensetime.com;sz.tsinghua.edu.cn;sensetime.com;sensetime.com",
        "email": "sensetime.com;sensetime.com;sensetime.com;sz.tsinghua.edu.cn;sensetime.com;sensetime.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "SenseTime;Tsinghua University",
        "aff_unique_dep": "SenseTime Research;",
        "aff_unique_url": "https://www.sensetime.com;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "SenseTime;THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "ViP: Virtual Pooling for Accelerating CNN-based Image Classification and Object Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chen_ViP_Virtual_Pooling_for_Accelerating_CNN-based_Image_Classification_and_Object_WACV_2020_paper.html",
        "author": "Zhuo Chen;  Jiyuan Zhang;  Ruizhou Ding;  Diana Marculescu",
        "abstract": "In recent years, Convolutional Neural Networks (CNNs) have shown superior capability in visual learning tasks. While accuracy-wise CNNs provide unprecedented performance, they are also known to be computationally intensive and energy demanding for modern computer systems. In this paper, we propose Virtual Pooling (ViP), a model-level approach to improve speed and energy consumption of CNN-based image classification and object detection tasks, with a provable error bound. We show the efficacy of ViP through experiments on four CNN models, three representative datasets, both desktop and mobile platforms, and two visual learning tasks, i.e., image classification and object detection. For example, ViP delivers 2.1x speedup with less than 1.5% accuracy degradation in ImageNet classification on VGG16, and 1.8x speedup with 0.025 mAP degradation in PASCAL VOC object detection with Faster-RCNN. ViP also reduces mobile GPU and CPU energy consumption by up to 55% and  70%, respectively. As a complementary method to existing acceleration approaches, ViP achieves 1.9x speedup on ThiNet leading to a combined speedup of 5.23x on VGG16. Furthermore, ViP provides a knob for machine learning practitioners to generate a set of CNN models with varying trade-offs between system speed/energy consumption and accuracy to better accommodate the requirements of their tasks. Code is available at https://github.com/cmu-enyac/VirtualPooling.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_ViP_Virtual_Pooling_for_Accelerating_CNN-based_Image_Classification_and_Object_WACV_2020_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; University of Texas at Austin + Carnegie Mellon University",
        "project": "",
        "github": "https://github.com/cmu-enyac/VirtualPooling",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Chen_ViP_Virtual_Pooling_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1439202,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12540936366118901948&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;utexas.edu + dianam",
        "email": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;utexas.edu + dianam",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1+0",
        "aff_unique_norm": "Carnegie Mellon University;University of Texas at Austin",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.utexas.edu",
        "aff_unique_abbr": "CMU;UT Austin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Video Object Segmentation-based Visual Servo Control and Object Depth Estimation on a Mobile Robot",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Griffin_Video_Object_Segmentation-based_Visual_Servo_Control_and_Object_Depth_Estimation_WACV_2020_paper.html",
        "author": "Brent Griffin;  Victoria Florence;  Jason Corso",
        "abstract": "To be useful in everyday environments, robots must be able to identify and locate real-world objects. In recent years, video object segmentation has made significant progress on densely separating such objects from background in real and challenging videos. Building off of this progress, this paper addresses the problem of identifying generic objects and locating them in 3D using a mobile robot with an RGB camera. We achieve this by, first, introducing a video object segmentation-based approach to visual servo control and active perception and, second, developing a new Hadamard-Broyden update formulation. Our segmentation-based methods are simple but effective, and our update formulation lets a robot quickly learn the relationship between actuators and visual features without any camera calibration. We validate our approach in experiments by learning a variety of actuator-camera configurations on a mobile HSR robot, which subsequently identifies, locates, and grasps objects from the YCB dataset and tracks people and other dynamic articulated objects in real-time.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Griffin_Video_Object_Segmentation-based_Visual_Servo_Control_and_Object_Depth_Estimation_WACV_2020_paper.pdf",
        "aff": "University of Michigan; University of Michigan; University of Michigan",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Griffin_Video_Object_Segmentation-based_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3701112,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15694679643685134706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Video Person Re-Identification using Learned Clip Similarity Aggregation",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Matiyali_Video_Person_Re-Identification_using_Learned_Clip_Similarity_Aggregation_WACV_2020_paper.html",
        "author": "Neeraj  Matiyali;  Gaurav Sharma",
        "abstract": "We address the challenging task of video-based person re-identification. Recent works have shown that splitting the video sequences into clips and then aggregating clip based similarity is appropriate for the task. We show that using a learned clip similarity aggregation function allows filtering out hard clip pairs, e.g. where the person is not clearly visible, is in a challenging pose, or where the poses in the two clips are too different to be informative. This allows the method to focus on clip-pairs which are more informative for the task. We also introduce the use of 3D CNNs for video-based re-identification and show their effectiveness by performing equivalent to previous works, which use optical flow in addition to RGB, while using RGB inputs only. We give quantitative results on three challenging public benchmarks and show better or competitive performance. We also validate our method qualitatively.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Matiyali_Video_Person_Re-Identification_using_Learned_Clip_Similarity_Aggregation_WACV_2020_paper.pdf",
        "aff": "IIT Kanpur; NEC Labs America",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Matiyali_Video_Person_Re-Identification_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1258906,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12394457008371533746&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cse.iitk.ac.in;nec-labs.com",
        "email": "cse.iitk.ac.in;nec-labs.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;NEC Labs America",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitk.ac.in;https://www.nec-labs.com",
        "aff_unique_abbr": "IITK;NEC LA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Kanpur;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "Visual Question Answering on 360deg Images",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Chou_Visual_Question_Answering_on_360deg_Images_WACV_2020_paper.html",
        "author": "Shih-Han Chou;  Wei-Lun Chao;  Wei-Sheng Lai;  Min Sun;  Ming-Hsuan Yang",
        "abstract": "In this work, we introduce VQA 360deg, a novel task of visual question answering on 360deg images. Unlike a normal field-of-view image, a 360deg image captures the entire visual content around the optical center of a camera, demanding more sophisticated spatial understanding and reasoning. To address this problem, we collect the first VQA 360deg dataset, containing around 17,000 real-world image-question-answer triplets for a variety of question types. We then study two different VQA models on VQA 360deg, including one conventional model that takes an equirectangular image (with intrinsic distortion) as input and one dedicated model that first projects a 360deg image onto cubemaps and subsequently aggregates the information from multiple spatial resolutions. We demonstrate that the cubemap-based model with multi-level fusion and attention diffusion performs favorably against other variants and the equirectangular-based models. Nevertheless, the gap between the humans' and machines' performance reveals the need for more advanced VQA 360deg algorithms. We, therefore, expect our dataset and studies to serve as the benchmark for future development in this challenging task. Dataset, code, and pre-trained models are available online.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Chou_Visual_Question_Answering_on_360deg_Images_WACV_2020_paper.pdf",
        "aff": "University of British Columbia+National Tsing Hua University; The Ohio State University; Google; National Tsing Hua University; University of California at Merced+Google",
        "project": "http://aliensunmin.github.io/project/360-VQA/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2471074,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3360921644448971742&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3;1;4+3",
        "aff_unique_norm": "University of British Columbia;National Tsing Hua University;The Ohio State University;Google;University of California, Merced",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.ubc.ca;https://www.nthu.edu.tw;https://www.osu.edu;https://www.google.com;https://www.ucmerced.edu",
        "aff_unique_abbr": "UBC;NTHU;OSU;Google;UC Merced",
        "aff_campus_unique_index": "1;2;1;3+2",
        "aff_campus_unique": ";Taiwan;Mountain View;Merced",
        "aff_country_unique_index": "0+1;2;2;1;2+2",
        "aff_country_unique": "Canada;China;United States"
    },
    {
        "title": "Watch to Listen Clearly: Visual Speech Enhancement Driven Multi-modality Speech Recognition",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Xu_Watch_to_Listen_Clearly_Visual_Speech_Enhancement_Driven_Multi-modality_Speech_WACV_2020_paper.html",
        "author": "Bo Xu;  Jacob Wang;  Cheng Lu;  Yandong Guo",
        "abstract": "Multi-modality (talking face video and audio) information helps improve speech recognition performance compared to the single modality. In noisy environments, the effect of audio modality is weakened, which further affects the performance of multi-modality speech recognition (MSR). Most of the MSR methods use noisy audio signal as input of the audio modality without any enhancement (filtering the noisy components in the audio signal). In this paper, we propose an audio-enhanced multi-modality speech recognition model. In particular, the proposed model consists of two sub-networks, one is the visual speech enhancement (VE) sub-network and the other is the multi-modality speech recognition (MSR) sub-network. The VE sub-network is able to separate a speaker's voice from background noises when given the corresponding talking face to enhance audio modality. Then the audio modality together with video modality are fed into the MSR sub-network to produce characters. We introduce a pseudo-3D residual network (P3D)-based visual front-end to extract more advantageous visual features. The MSR sub-network is built on top of the Element-wise-Attention Gated Recurrent Unit (EleAtt-GRU) architecture which is more effective than Transformer in long sequences. We demonstrate the effectiveness of audio enhancement for MSR by extensive experiments. The proposed method surpasses the state-of-the-art MSR models on the LRS3-TED dataset and the LRW dataset.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Xu_Watch_to_Listen_Clearly_Visual_Speech_Enhancement_Driven_Multi-modality_Speech_WACV_2020_paper.pdf",
        "aff": "Xpeng motors; Xpeng motors; Xpeng motors; Xpeng motors",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Xu_Watch_to_Listen_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1496891,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3965442103755881176&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 3,
        "aff_domain": "gmail.com; ; ; ",
        "email": "gmail.com; ; ; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Xpeng Motors",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.xpengmotor.com",
        "aff_unique_abbr": "Xpeng",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Weakly Supervised Gaussian Networks for Action Detection",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Fernando_Weakly_Supervised_Gaussian_Networks_for_Action_Detection_WACV_2020_paper.html",
        "author": "Basura Fernando;  Cheston Tan;  Hakan Bilen",
        "abstract": "Detecting temporal extents of human actions in videos is a challenging computer vision problem that requires detailed manual supervision including frame-level labels. This expensive annotation process limits deploying action detectors to a limited number of categories. We propose a novel method, called WSGN, that learns to detect actions from weak supervision, using only video-level labels.  WSGN learns to exploit both video-specific and dataset-wide statistics to predict relevance of each frame to an action category. This strategy leads to significant gains in action detection for two standard benchmarks THUMOS14 and Charades. Our method obtains excellent results compared to state-of-the-art methods that uses similar features and loss functions on THUMOS14 dataset. Similarly, our  weakly supervised method is only 0.3% mAP behind a state-of-the-art supervised method on challenging Charades dataset for action localization.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Fernando_Weakly_Supervised_Gaussian_Networks_for_Action_Detection_WACV_2020_paper.pdf",
        "aff": "A*AI, A*STAR Singapore; I2R, A*STAR Singapore; VICO, University of Edinburgh",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 501068,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11511651767815840039&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "scei.a-star.edu.sg;i2r.a-star.edu.sg;ed.ac.uk",
        "email": "scei.a-star.edu.sg;i2r.a-star.edu.sg;ed.ac.uk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "A*STAR Singapore;A*STAR Institute of High Performance Computing;University of Edinburgh",
        "aff_unique_dep": "A*AI;Institute of High Performance Computing;VICO",
        "aff_unique_url": "https://www.a-star.edu.sg;https://www.a-star.edu.sg;https://www.ed.ac.uk",
        "aff_unique_abbr": "A*STAR;I2R;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Singapore;United Kingdom"
    },
    {
        "title": "Weakly Supervised Graph Convolutional Neural Network for Human Action Localization",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Miki_Weakly_Supervised_Graph_Convolutional_Neural_Network_for_Human_Action_Localization_WACV_2020_paper.html",
        "author": "Daisuke Miki;  Shi Chen;  Kazuyuki Demachi",
        "abstract": "Skeleton-based human action recognition from video sequences is currently an active topic of research. Conventionally, human action recognition is performed after conducting feature extraction on a given spatial-temporal representation of a human pose by using statistical methods or deep learning methods. The spatial and temporal features are globally evaluated by a classifier and used to determine which action is closest. However, the conventional methodology does not identify the temporal location of the action that determines the classification. To address this problem, we propose a skeleton-based human action recognition and localization method using weakly supervised graph convolutional neural networks, which are both spatially and temporally connected. In this method, human action localization is accomplished using time series data of human joint positions as input and then applying regression to find an expected value for each action at each time frame. Our weakly supervised training is based on multiple-instance learning inspired by deep ranking, and we devise a loss function so that high scores can be spontaneously learned for temporally important time frames. In this paper, we first explain the network architecture and then present a multiple-instance learning method for its optimization. In the experiment, we performed localization and classification of human actions by using this method and confirmed the temporal localization efficacy of the method.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Miki_Weakly_Supervised_Graph_Convolutional_Neural_Network_for_Human_Action_Localization_WACV_2020_paper.pdf",
        "aff": "The University of Tokyo, Japan + Tokyo Metropolitan Industrial Technology Research Institute, Japan; The University of Tokyo, Japan; The University of Tokyo, Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5164399,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5918602629644319208&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iri-tokyo.jp;g.ecc.u-tokyo.ac.jp;nuclear.jp",
        "email": "iri-tokyo.jp;g.ecc.u-tokyo.ac.jp;nuclear.jp",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "The University of Tokyo;Tokyo Metropolitan Industrial Technology Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.tMRI.or.jp",
        "aff_unique_abbr": "UTokyo;TMRI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Weakly Supervised Temporal Action Localization Using Deep Metric Learning",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Islam_Weakly_Supervised_Temporal_Action_Localization_Using_Deep_Metric_Learning_WACV_2020_paper.html",
        "author": "Ashraful Islam;  Richard Radke",
        "abstract": "Temporal action localization is an important step towards video understanding. Most current action localization methods depend on untrimmed videos with full temporal annotations of action instances. However, it is expensive and time-consuming to annotate both action labels and temporal boundaries of videos. To this end, we propose a weakly supervised temporal action localization method that only requires video-level action instances as supervision during training. We propose a classification module to generate action labels for each segment in the video, and a deep metric learning module to learn the similarity between different action instances. We jointly optimize a balanced binary cross-entropy loss and a metric loss using a standard backpropagation algorithm. Extensive experiments demonstrate the effectiveness of both of these components in temporal localization. We evaluate our algorithm on two challenging untrimmed video datasets: THUMOS14 and ActivityNet1.2. Our approach improves the current state-of-the-art result for THUMOS14 by 6.5% mAP at IoU threshold 0.5, and achieves competitive performance for ActivityNet1.2.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Islam_Weakly_Supervised_Temporal_Action_Localization_Using_Deep_Metric_Learning_WACV_2020_paper.pdf",
        "aff": "Rensselaer Polytechnic Institute; Rensselaer Polytechnic Institute",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Islam_Weakly_Supervised_Temporal_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1262875,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18422278029905010959&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "rpi.edu;ecse.rpi.edu",
        "email": "rpi.edu;ecse.rpi.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rpi.edu",
        "aff_unique_abbr": "RPI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Weakly-Supervised Multi-Person Action Recognition in 360$^{\\circ}$ Videos",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Li_Weakly-Supervised_Multi-Person_Action_Recognition_in_360circ_Videos_WACV_2020_paper.html",
        "author": "Junnan Li;  Jianquan Liu;  Wong Yongkang;  Shoji Nishimura;  Mohan Kankanhalli",
        "abstract": "The recent development of commodity 360^ \\circ  cameras have enabled a single video to capture an entire scene, which endows promising potentials in surveillance scenarios. However, research in omnidirectional video analysis has lagged behind the hardware advances. In this work, we address the important problem of action recognition in top-view 360^ \\circ  videos. Due to the wide filed-of-view, 360^ \\circ  videos usually capture multiple people performing actions at the same time. Furthermore, the appearance of people are deformed. The proposed framework first transforms omnidirectional videos into panoramic videos, then it extracts spatial-temporal features using region-based 3D CNNs for action recognition. We propose a weakly-supervised method based on multi-instance multi-label learning, which trains the model to recognize and localize multiple actions in a video using only video-level action labels as supervision. We perform experiments to quantitatively validate the efficacy of the proposed method and qualitatively demonstrate action localization results. To enable research in this direction, we introduce 360Action, the first omnidirectional video dataset for multi-person action recognition.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Li_Weakly-Supervised_Multi-Person_Action_Recognition_in_360circ_Videos_WACV_2020_paper.pdf",
        "aff": "School of Computing, National University of Singapore; NEC Corporation; School of Computing, National University of Singapore; NEC Corporation; School of Computing, National University of Singapore",
        "project": "",
        "github": "https://github.com/ryukenzen/360action",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Li_Weakly-Supervised_Multi-Person_Action_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2761875,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14944389056201058949&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "u.nus.edu;ct.jp.nec.com;nus.edu.sg;bk.jp.nec.com;comp.nus.edu.sg",
        "email": "u.nus.edu;ct.jp.nec.com;nus.edu.sg;bk.jp.nec.com;comp.nus.edu.sg",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "National University of Singapore;NEC Corporation",
        "aff_unique_dep": "School of Computing;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.nec.com",
        "aff_unique_abbr": "NUS;NEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "Singapore;Japan"
    },
    {
        "title": "Wide Hidden Expansion Layer for Deep Convolutional Neural Networks",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Wang_Wide_Hidden_Expansion_Layer_for_Deep_Convolutional_Neural_Networks_WACV_2020_paper.html",
        "author": "Min Wang;  Baoyuan Liu;  Hassan Foroosh",
        "abstract": "Non-linearity is an essential factor contributing to the success of deep convolutional neural networks. Increasing the non-linearity in the network will enhance the network's learning capability, attributing to better performance. We present a novel Wide Hidden Expansion (WHE) layer that can significantly increase (by an order of magnitude ) the number of activation functions in the network, with very little increase of computational complexity and memory consumption. It can be flexibly embedded with different network architectures to boost the performance of the original networks. The WHE layer is composed of a wide hidden layer, in which each channel only connects with two input channels and one output channel. Before connecting to the output channel, each intermediate channel in the WHE layer is followed by one activation function. In this manner, the number of activation functions can grow along with the number of channels in the hidden layer. We apply the WHE layer to ResNet, WideResNet, SENet, and MobileNet architectures and evaluate on ImageNet, CIFAR-100, and Tiny ImageNet dataset. On the ImageNet dataset, models with the WHE layer can achieve up to 2.01% higher Top-1 accuracy than baseline models, with less than 4% computation increase and less than 2% more parameters. On CIFAR-100 and Tiny ImageNet, when applying the WHE layer to ResNet models, it demonstrates consistent improvement in the accuracy of the networks. Applying the WHE layer to ResNet backbone of the CenterNet object detection model can also boost its performance on COCO and Pascal VOC datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_Wide_Hidden_Expansion_Layer_for_Deep_Convolutional_Neural_Networks_WACV_2020_paper.pdf",
        "aff": "University of Central Florida; Amazon; University of Central Florida",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 784632,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18104433051485087566&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "cs.ucf.edu;amazon.com;cs.ucf.edu",
        "email": "cs.ucf.edu;amazon.com;cs.ucf.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Central Florida;Amazon.com, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucf.edu;https://www.amazon.com",
        "aff_unique_abbr": "UCF;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Li_Word-level_Deep_Sign_Language_Recognition_from_Video_A_New_Large-scale_WACV_2020_paper.html",
        "author": "DONGXU LI;  Cristian Rodriguez;  Xin Yu;  HONGDONG LI",
        "abstract": "Vision-based sign language recognition aims at helping the hearing-impaired people to communicate with others. However, most existing sign language datasets are limited to a small number of words. This makes the migration of recognition systems to real-life scenarios difficult. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge, it is by far the largest public ASL dataset to facilitate word-level sign recognition research.   Based on this new large-scale dataset, we are able to experiment with several deep learning methods for word-level sign recognition and evaluate their performances in large scale scenarios. Specifically we implement and compare two different models,i.e., (i) holistic visual appearance based approach, and (ii) 2D human pose based approach. Both models are valuable baselines that will benefit the community for method benchmarking.  Moreover, we also propose a novel pose-based temporal graph convolution networks (Pose-TGCN) that model spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method.  Our results show that pose-based and appearance-based models achieve comparable performances up to 62.63% at top-10 accuracy on 2,000 words/glosses, demonstrating the validity and challenges of our dataset. Our dataset and baseline deep mod- els are available at https://dxli94.github.io/WLASL/.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Li_Word-level_Deep_Sign_Language_Recognition_from_Video_A_New_Large-scale_WACV_2020_paper.pdf",
        "aff": "The Australian National University, Australian Centre for Robotic Vision (ACRV); The Australian National University, Australian Centre for Robotic Vision (ACRV); The Australian National University, Australian Centre for Robotic Vision (ACRV); The Australian National University, Australian Centre for Robotic Vision (ACRV)",
        "project": "",
        "github": "https://dxli94.github.io/WLASL/",
        "supp": "",
        "arxiv": "",
        "pdf_size": 719075,
        "gs_citation": 657,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13137825799206138070&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The Australian National University",
        "aff_unique_dep": "Australian Centre for Robotic Vision",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "microbatchGAN: Stimulating Diversity with Multi-Adversarial Discrimination",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Mordido_microbatchGAN_Stimulating_Diversity_with_Multi-Adversarial_Discrimination_WACV_2020_paper.html",
        "author": "Goncalo Mordido;  Haojin Yang;  Christoph  Meinel",
        "abstract": "We propose to tackle the mode collapse problem in generative adversarial networks (GANs) by using multiple discriminators and assigning a different portion of each minibatch, called microbatch, to each discriminator. We gradually change each discriminator's task from distinguishing between real and fake samples to discriminating samples coming from inside or outside its assigned microbatch by using a diversity parameter \\alpha. The generator is then forced to promote variety in each minibatch to make the microbatch discrimination harder to achieve by each discriminator. Thus, all models in our framework benefit from having variety in the generated set to reduce their respective losses. We show evidence that our solution promotes sample diversity since early training stages on multiple datasets.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Mordido_microbatchGAN_Stimulating_Diversity_with_Multi-Adversarial_Discrimination_WACV_2020_paper.pdf",
        "aff": "Hasso Plattner Institute; Hasso Plattner Institute; Hasso Plattner Institute",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_WACV_2020/supplemental/Mordido_microbatchGAN_Stimulating_Diversity_WACV_2020_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 819174,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12486612875682961660&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "hpi.de; ; ",
        "email": "hpi.de; ; ",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hasso Plattner Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hpi.de",
        "aff_unique_abbr": "HPI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "s-SBIR: Style Augmented Sketch based Image Retrieval",
        "site": "https://openaccess.thecvf.com/content_WACV_2020/html/Dutta_s-SBIR_Style_Augmented_Sketch_based_Image_Retrieval_WACV_2020_paper.html",
        "author": "Titir Dutta;  Soma  Biswas",
        "abstract": "Sketch-based image retrieval (SBIR) is gaining increasing popularity because of its flexibility to search natural images using unrestricted hand-drawn sketch query. Here, we address a related, but relatively unexplored problem, where the users can also specify their preferred styles of the images they want to retrieve, e.g., color, shape, etc., as key-words, whose information is not present in the sketch. The contribution of this work is three-fold. First, we propose a deep network for the problem of style-augmented SBIR (or s-SBIR) having three main components - category module, style module and mixer module, which are trained in an end-to-end manner. Second, we propose a quintuplet loss, which takes into consideration both the category and style, while giving appropriate importance to the two components. Third, we propose a composite evaluation metric or ncMAP which can quantitatively evaluate s-SBIR approaches. Extensive experiments on subsets of two benchmark image-sketch datasets, Sketchy and TU-Berlin show the effectiveness of the proposed approach.",
        "pdf": "https://openaccess.thecvf.com/content_WACV_2020/papers/Dutta_s-SBIR_Style_Augmented_Sketch_based_Image_Retrieval_WACV_2020_paper.pdf",
        "aff": "Indian Institute of Science, Bangalore; Indian Institute of Science, Bangalore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1738976,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3479642152866448881&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff_domain": "iisc.ac.in;iisc.ac.in",
        "email": "iisc.ac.in;iisc.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    }
]