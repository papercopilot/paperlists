[
    {
        "id": "8930f3719f",
        "title": "3D Modeling Beneath Ground: Plant Root Detection and Reconstruction Based on Ground-Penetrating Radar",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lu_3D_Modeling_Beneath_Ground_Plant_Root_Detection_and_Reconstruction_Based_WACV_2022_paper.html",
        "author": "Yawen Lu; Guoyu Lu",
        "abstract": "3D object reconstruction based on deep neural networks has been gaining attention in recent years. However, recovering 3D shapes of hidden and buried objects remains to be a challenge. Ground Penetrating Radar (GPR) is among the most powerful and widely used instruments for detecting and locating underground objects such as plant roots and pipes, with affordable prices and continually evolving technology. This paper first proposes a deep convolution neural network-based anchor-free GPR curve signal detection network utilizing B-scans from a GPR sensor. The detection results can help obtain precisely fitted parabola curves. Furthermore, a graph neural network-based root shape reconstruction network is designated in order to progressively recover major taproot and then fine root branches' geometry. Our results on the gprMax simulated root data as well as the real-world GPR data collected from apple orchards demonstrate the potential of using the proposed framework as a new approach for fine-grained underground object shape reconstruction in a non-destructive way.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lu_3D_Modeling_Beneath_Ground_Plant_Root_Detection_and_Reconstruction_Based_WACV_2022_paper.pdf",
        "aff": "Intelligent Vision and Sensing Lab, Rochester Institute of Technology, USA; Intelligent Vision and Sensing Lab, Rochester Institute of Technology, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1588808,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11817375197621513822&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rochester Institute of Technology",
        "aff_unique_dep": "Intelligent Vision and Sensing Lab",
        "aff_unique_url": "https://www.rit.edu",
        "aff_unique_abbr": "RIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "18c55fc562",
        "title": "3DFaceFill: An Analysis-by-Synthesis Approach To Face Completion",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Dey_3DFaceFill_An_Analysis-by-Synthesis_Approach_To_Face_Completion_WACV_2022_paper.html",
        "author": "Rahul Dey; Vishnu Naresh Boddeti",
        "abstract": "Existing face completion solutions are primarily driven by end-to-end models that directly generate 2D completions of 2D masked faces. By having to implicitly account for geometric and photometric variations in facial shape and appearance, such approaches result in unrealistic completions, especially under large variations in pose, shape, illumination and mask sizes. To alleviate these limitations, we introduce 3DFaceFill, an analysis-by-synthesis approach for face completion that explicitly considers the image formation process. It comprises three components, (1) an encoder that disentangles the face into its constituent 3D mesh, 3D pose, illumination and albedo factors, (2) an autoencoder that inpaints the UV representation of facial albedo, and (3) a renderer that resynthesizes the completed face. By operating on the UV representation, 3DFaceFill affords the power of correspondence and allows us to naturally enforce geometrical priors (e.g. facial symmetry) more effectively. Quantitatively, 3DFaceFill improves the state-of-the-art by up to 4dB higher PSNR and 25% better LPIPS for large masks. And, qualitatively, it leads to demonstrably more photorealistic face completions over a range of masks and occlusions while preserving consistency in global and component-wise shape, pose, illumination and eye-gaze.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Dey_3DFaceFill_An_Analysis-by-Synthesis_Approach_To_Face_Completion_WACV_2022_paper.pdf",
        "aff": "Michigan State University; Michigan State University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Dey_3DFaceFill_An_Analysis-by-Synthesis_WACV_2022_supplemental.pdf",
        "arxiv": "2110.10395",
        "pdf_size": 5066632,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3164316308015228451&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "msu.edu;msu.edu",
        "email": "msu.edu;msu.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "27b025e07a",
        "title": "3DRefTransformer: Fine-Grained Object Identification in Real-World Scenes Using Natural Language",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Abdelreheem_3DRefTransformer_Fine-Grained_Object_Identification_in_Real-World_Scenes_Using_Natural_Language_WACV_2022_paper.html",
        "author": "Ahmed Abdelreheem; Ujjwal Upadhyay; Ivan Skorokhodov; Rawan Al Yahya; Jun Chen; Mohamed Elhoseiny",
        "abstract": "In this paper, we study fine-grained 3D object identification in real-world scenes described by a textual query. The task aims to discriminatively understand an instance of a particular 3D object described by natural language utterances among other instances of 3D objects of the same class appearing in a visual scene. We introduce the 3DRefTransformer net, a transformer-based neural network that identifies 3D objects described by linguistic utterances in real-world scenes. The network's input is 3D object segmented point cloud images representing a real-world scene and a language utterance that refers to one of the scene objects. The goal is to identify the referred object. Compared to the state-of-the-art models that are mostly based on graph convolutions and LSTMs, our 3DRefTransformer net offers two key advantages. First, it is an end-to-end transformer model that operates both on language and 3D visual objects. Second, it has a natural ability to ground textual terms in the utterance to the learning representation of 3D objects in the scene. We further incorporate object pairwise spatial relation loss and contrastive learning during model training. We show in our experiments that our model improves the performance upon the current SOTA significantly on Referit3D Nr3D and Sr3D datasets. Code and Models will be made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Abdelreheem_3DRefTransformer_Fine-Grained_Object_Identification_in_Real-World_Scenes_Using_Natural_Language_WACV_2022_paper.pdf",
        "aff": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "project": "https://vision-cair.github.io/3dreftransformer/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Abdelreheem_3DRefTransformer_Fine-Grained_Object_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 8058796,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13223810357407852283&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "kaust.edu.sa;kaust.edu.sa;kaust.edu.sa;kaust.edu.sa;kaust.edu.sa;kaust.edu.sa",
        "email": "kaust.edu.sa;kaust.edu.sa;kaust.edu.sa;kaust.edu.sa;kaust.edu.sa;kaust.edu.sa",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaust.edu.sa",
        "aff_unique_abbr": "KAUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Saudi Arabia"
    },
    {
        "id": "da4585312f",
        "title": "A Context-Enriched Satellite Imagery Dataset and an Approach for Parking Lot Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yin_A_Context-Enriched_Satellite_Imagery_Dataset_and_an_Approach_for_Parking_WACV_2022_paper.html",
        "author": "Yifang Yin; Wenmiao Hu; An Tran; Hannes Kruppa; Roger Zimmermann; See-Kiong Ng",
        "abstract": "Automatic detection of geoinformation from satellite images has been a fundamental yet challenging problem, which aims to reduce the manual effort of human annotators in maintaining an up-to-date digital map. There are currently several high-resolution satellite imagery datasets that are publicly available. However, the associated ground-truth annotations are limited to road, building, and land use, while the annotations of other geographic objects or attributes are mostly not available. To bridge the gap, we present Grab-Pklot, the first high-resolution and context-enriched satellite imagery dataset for parking lot detection. Our dataset consists of 1344 satellite images with the ground-truth annotations of carparks in Singapore. Motivated by the observation that carparks are mostly co-appear with other geographic objects, we associate each satellite image in our dataset with the surrounding contextual information of road and building, given in the format of multi-channel images. As a side contribution, we present a fusion-based segmentation approach to demonstrate that the parking lot detection accuracy can be improved by modeling the correlations between parking lots and other geographic objects. Experiments on our dataset provide baseline results as well as new insights into the challenges and opportunities in parking lot detection from satellite images.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yin_A_Context-Enriched_Satellite_Imagery_Dataset_and_an_Approach_for_Parking_WACV_2022_paper.pdf",
        "aff": "Grab-NUS AI Lab, National University of Singapore; Grab-NUS AI Lab, National University of Singapore+GrabTaxi Holdings, Singapore; GrabTaxi Holdings, Singapore; GrabTaxi Holdings, Singapore; Grab-NUS AI Lab, National University of Singapore; Grab-NUS AI Lab, National University of Singapore",
        "project": "geo.grabpklot@grabtaxi.com",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7813243,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4061021169299736266&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "nus.edu.sg;nus.edu.sg;nus.edu.sg;grabtaxi.com;grabtaxi.com;grabtaxi.com",
        "email": "nus.edu.sg;nus.edu.sg;nus.edu.sg;grabtaxi.com;grabtaxi.com;grabtaxi.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;1;1;0;0",
        "aff_unique_norm": "National University of Singapore;GrabTaxi Holdings",
        "aff_unique_dep": "Grab-NUS AI Lab;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.grab.com",
        "aff_unique_abbr": "NUS;Grab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "c94aa150cd",
        "title": "A Deep Insight Into Measuring Face Image Utility With General and Face-Specific Image Quality Metrics",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Fu_A_Deep_Insight_Into_Measuring_Face_Image_Utility_With_General_WACV_2022_paper.html",
        "author": "Biying Fu; Cong Chen; Olaf Henniger; Naser Damer",
        "abstract": "Quality scores provide a measure to evaluate the utility of biometric samples for biometric recognition. Biometric recognition systems require high-quality samples to achieve optimal performance. This paper focuses on face images and the measurement of face image utility with general and face-specific image quality metrics. While face-specific metrics rely on features of aligned face images, general image quality metrics can be used on the global image and relate to human perceptions. In this paper, we analyze the gap between the general image quality metrics and the face image quality metrics. Our contribution lies in a thorough examination of how different the image quality assessment algorithms relate to the utility for the face recognition task. The results of image quality assessment algorithms are further compared with those of dedicated face image quality assessment algorithms. In total, 25 different quality metrics are evaluated on three face image databases, BioSecure, LFW, and VGGFace2 using three open-source face recognition solutions, SphereFace, ArcFace, and FaceNet. Our results reveal a clear correlation between learned image metrics to face image utility even without being specifically trained as a face utility measure. Individual handcrafted features lack general stability and perform significantly worse than general face-specific quality metrics. We additionally provide a visual insight into the image areas contributing to the quality score of a selected set of quality assessment methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Fu_A_Deep_Insight_Into_Measuring_Face_Image_Utility_With_General_WACV_2022_paper.pdf",
        "aff": "Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany + Department of Computer Science, TU Darmstadt, Darmstadt, Germany",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Fu_A_Deep_Insight_WACV_2022_supplemental.pdf",
        "arxiv": "2110.11111",
        "pdf_size": 6336487,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17466110468236173341&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "igd.fraunhofer.de; ; ; ",
        "email": "igd.fraunhofer.de; ; ; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Fraunhofer Institute for Computer Graphics Research IGD;TU Darmstadt",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": "https://www.igd.fraunhofer.de;https://www.tu-darmstadt.de",
        "aff_unique_abbr": "IGD;TUD",
        "aff_campus_unique_index": "0;0;0;0+0",
        "aff_campus_unique": "Darmstadt",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "ca3cd0650c",
        "title": "A Fast Partial Video Copy Detection Using KNN and Global Feature Database",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tan_A_Fast_Partial_Video_Copy_Detection_Using_KNN_and_Global_WACV_2022_paper.html",
        "author": "Weijun Tan; Hongwei Guo; Rushuai Liu",
        "abstract": "Unlike in most previous partial video copy detection (PVCD) algorithms, where reference videos are scanned one by one, we treat the PVCD as a video search/retrieval problem. We propose a fast partial video copy detection framework in this paper. In this framework, all frame CNN features of the reference videos are organized in a KNN searchable database. Instead of scanning all reference videos, the query video segment does a fast KNN search in the global feature database. The returned results are used to generate a shortlist of candidate videos. A modified temporal network is then used to localize the copy segment in the candidate videos. Furthermore, We propose to use a transformer encoder to improve the CNN feature. We evaluate our algorithm on the VCDB dataset. Our benchmark F1 scores exceed state-of-the-art by a big margin. The speed of our algorithm is also improved significantly.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tan_A_Fast_Partial_Video_Copy_Detection_Using_KNN_and_Global_WACV_2022_paper.pdf",
        "aff": "LinkSprite Technologies, Longmont, CO 80501, USA + Shenzhen Deepcam Information Technologies, Shenzhen, China; Shenzhen Deepcam Information Technologies, Shenzhen, China; Shenzhen Deepcam Information Technologies, Shenzhen, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2105.01713",
        "pdf_size": 2273925,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5287084486162926202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "linksprite.com;deepcam.com;deepcam.com",
        "email": "linksprite.com;deepcam.com;deepcam.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "LinkSprite Technologies;Shenzhen Deepcam Information Technologies",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Longmont;",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "f9b93d08d5",
        "title": "A Modular and Unified Framework for Detecting and Localizing Video Anomalies",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Doshi_A_Modular_and_Unified_Framework_for_Detecting_and_Localizing_Video_WACV_2022_paper.html",
        "author": "Keval Doshi; Yasin Yilmaz",
        "abstract": "Anomaly detection in videos has been attracting an increasing amount of attention. Despite the competitive performance of recent methods on benchmark datasets, they typically lack desirable features such as modularity, cross-domain adaptivity, interpretability, and real-time anomalous event detection. Furthermore, current state-of-the-art approaches are evaluated using the standard instance-based detection metric by considering video frames as independent instances, which is not ideal for video anomaly detection. Motivated by these research gaps, we propose a modular and unified approach to the online video anomaly detection and localization problem, called MOVAD, which consists of a novel transfer learning based plug-and-play architecture, a sequential anomaly detector, a mathematical framework for selecting the detection threshold, and a suitable performance metric for real-time anomalous event detection in videos. Extensive performance evaluations on benchmark datasets show that the proposed framework significantly outperforms the current state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Doshi_A_Modular_and_Unified_Framework_for_Detecting_and_Localizing_Video_WACV_2022_paper.pdf",
        "aff": "University of South Florida; University of South Florida",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Doshi_A_Modular_and_WACV_2022_supplemental.pdf",
        "arxiv": "2103.11299",
        "pdf_size": 884745,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14463645777164397840&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "usf.edu;usf.edu",
        "email": "usf.edu;usf.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of South Florida",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usf.edu",
        "aff_unique_abbr": "USF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2c709eb298",
        "title": "A Pixel-Level Meta-Learner for Weakly Supervised Few-Shot Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lee_A_Pixel-Level_Meta-Learner_for_Weakly_Supervised_Few-Shot_Semantic_Segmentation_WACV_2022_paper.html",
        "author": "Yuan-Hao Lee; Fu-En Yang; Yu-Chiang Frank Wang",
        "abstract": "Few-shot semantic segmentation addresses the learning task in which only few images with ground truth pixel-level labels are available for the novel classes of interest. One is typically required to collect a large mount of data (i.e., base classes) with such ground truth information, followed by meta-learning strategies to address the above learning task. When only image-level semantic labels can be observed during both training and testing, it is considered as an even more challenging task of weakly supervised few-shot semantic segmentation. To address this problem, we propose a novel meta-learning framework, which predicts pseudo pixel-level segmentation masks from a limited amount of data and their semantic labels. More importantly, our learning scheme further exploits the produced pixel-level information for query image inputs with segmentation guarantees. Thus, our proposed learning model can be viewed as a pixel-level meta-learner. Through extensive experiments on benchmark datasets, we show that our model achieves satisfactory performances under fully supervised settings, yet performs favorably against state-of-the-art methods under weakly supervised settings.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lee_A_Pixel-Level_Meta-Learner_for_Weakly_Supervised_Few-Shot_Semantic_Segmentation_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Lee_A_Pixel-Level_Meta-Learner_WACV_2022_supplemental.pdf",
        "arxiv": "2111.01418",
        "pdf_size": 3699864,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7597896605653682190&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ba3041fe5d",
        "title": "A Riemannian Framework for Analysis of Human Body Surface",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Pierson_A_Riemannian_Framework_for_Analysis_of_Human_Body_Surface_WACV_2022_paper.html",
        "author": "Emery Pierson; Mohamed Daoudi; Alice-Barbara Tumpach",
        "abstract": "We propose a novel framework for comparing 3D human shapes under the change of shape and pose. This problem is challenging since 3D human shapes vary significantly across subjects and body postures. We solve this problem by using a Riemannian approach. Our core contribution is the mapping of the human body surface to the space of metrics and normals. We equip this space with a family of Riemannian metrics, called Ebin (or DeWitt) metrics. We treat a human body surface as a point in a \"shape space\" equipped with a family of Riemmanian metrics. The family of metrics is invariant under rigid motions and reparametrizations; hence it induces a metric on the \"shape space\" of surfaces. Using the alignment of human bodies with a given template, we show that this family of metrics allows us to distinguish the changes in shape and pose. The proposed framework has several advantages. First, we define a family of metrics with desired invariant properties for the comparison of human shape. Second, we present an efficient framework to compute geodesic paths between human shape given the chosen metric. Third, this framework provides some basic tools for statistical shape analysis of human body surfaces. Finally, we demonstrate the utility of the proposed framework in pose and shape retrieval of human body.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Pierson_A_Riemannian_Framework_for_Analysis_of_Human_Body_Surface_WACV_2022_paper.pdf",
        "aff": "Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France; IMT Nord Europe, Institut Mines-T\u00e9l\u00e9com, Univ. Lille, Centre for Digital Systems, F-59000 Lille, France + Univ. Lille, CNRS, Centrale Lille, Institut Mines-T\u00e9l\u00e9com, UMR 9189 CRIStAL, F-59000 Lille, France; Univ. Lille, CNRS, UMR 8524 Laboratoire Painlev\u00e9, F-59000 Lille, France + Wolfgang Pauli Institut, Vienna, Austria",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Pierson_A_Riemannian_Framework_WACV_2022_supplemental.pdf",
        "arxiv": "2108.11449",
        "pdf_size": 9217284,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1649432598329344016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "univ-lille.fr;imt-nord-europe.fr;univ-lille.fr",
        "email": "univ-lille.fr;imt-nord-europe.fr;univ-lille.fr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0+2",
        "aff_unique_norm": "University of Lille;IMT Nord Europe;Wolfgang Pauli Institute",
        "aff_unique_dep": "UMR 9189 CRIStAL;Centre for Digital Systems;",
        "aff_unique_url": "https://www.univ-lille.fr;https://www.imt-nord-europe.fr;https://www.univie.ac.at/WPI",
        "aff_unique_abbr": "Univ. Lille;IMT Nord Europe;WPI",
        "aff_campus_unique_index": "0;0+0;0+1",
        "aff_campus_unique": "Lille;Vienna",
        "aff_country_unique_index": "0;0+0;0+1",
        "aff_country_unique": "France;Austria"
    },
    {
        "id": "ae0219cfc0",
        "title": "A Semi-Supervised Generalized VAE Framework for Abnormality Detection Using One-Class Classification",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sharma_A_Semi-Supervised_Generalized_VAE_Framework_for_Abnormality_Detection_Using_One-Class_WACV_2022_paper.html",
        "author": "Renuka Sharma; Satvik Mashkaria; Suyash P. Awate",
        "abstract": "Anomaly detection is a one-class classification (OCC) problem where the methods learn either a generative model of the inlier class (e.g., in the variants of kernel principal component analysis) or a decision boundary to encapsulate the inlier class (e.g., in the one-class variants of the support vector machine). Learning schemes for OCC typically rely on training data solely from the inlier class, but some recent approaches have proposed semi-supervised extensions, e.g., variants of semi-supervised anomaly detection that also leverage a small amount of training data from outlier classes. Other recent methods extend existing principles to employ deep neural network (DNN) modeling that relies on learning (for the inlier class) either latent-space distributions or autoencoders, but not both. We propose a novel semi-supervised variational formulation, leveraging generalized-Gaussian models leading to data-adaptive, robust, and uncertainty-aware distribution modeling in both latent space and image space. For variational learning, we propose a novel reparameterization for sampling from the latent-space generalized-Gaussian to enable backpropagation-based optimization. Results on several public image sets show the benefits of our method over state of the art.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sharma_A_Semi-Supervised_Generalized_VAE_Framework_for_Abnormality_Detection_Using_One-Class_WACV_2022_paper.pdf",
        "aff": "Computer Science and Engineering Department, Indian Institute of Technology Bombay, Mumbai + IITB-Monash Research Academy, Mumbai; Computer Science and Engineering Department, Indian Institute of Technology Bombay, Mumbai; Computer Science and Engineering Department, Indian Institute of Technology Bombay, Mumbai",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3191523,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2964075524043185252&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay;IITB-Monash Research Academy",
        "aff_unique_dep": "Computer Science and Engineering Department;",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.iitbmonash.org",
        "aff_unique_abbr": "IIT Bombay;",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "Mumbai",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "fc84ade646",
        "title": "A Structure-Aware Method for Direct Pose Estimation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Blanton_A_Structure-Aware_Method_for_Direct_Pose_Estimation_WACV_2022_paper.html",
        "author": "Hunter Blanton; Scott Workman; Nathan Jacobs",
        "abstract": "Estimating camera pose from a single image is a fundamental problem in computer vision. Existing methods for solving this task fall into two distinct categories, which we refer to as direct and indirect. Direct methods, such as PoseNet, regress pose from the image as a fixed function, for example using a feed-forward convolutional network. Such methods are desirable because they are deterministic and run in constant time. Indirect methods for pose regression are often non-deterministic, with various external dependencies such as image retrieval and hypothesis sampling. We propose a direct method that takes inspiration from structure-based approaches to incorporate explicit 3D constraints into the network. Our approach maintains the desirable qualities of other direct methods while achieving much lower error in general. Code is available https://github.com/mvrl/structure-aware-pose-estimation.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Blanton_A_Structure-Aware_Method_for_Direct_Pose_Estimation_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Blanton_A_Structure-Aware_Method_WACV_2022_supplemental.pdf",
        "arxiv": "2012.12360",
        "pdf_size": 1119513,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11652900100573438266&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "fedd2d2a0b",
        "title": "ADC: Adversarial Attacks Against Object Detection That Evade Context Consistency Checks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yin_ADC_Adversarial_Attacks_Against_Object_Detection_That_Evade_Context_Consistency_WACV_2022_paper.html",
        "author": "Mingjun Yin; Shasha Li; Chengyu Song; M. Salman Asif; Amit K. Roy-Chowdhury; Srikanth V. Krishnamurthy",
        "abstract": "Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial examples, which are slightly perturbed input images which lead DNNs to make wrong predictions. To protect from such examples, various defense strategies have been proposed. A very recent defense strategy for detecting adversarial examples, that has been shown to be robust to current attacks, is to check for intrinsic context consistencies in the input data, where context refers to various relationships (e.g., object-to-object co-occurrence relationships) in images. In this paper, we show that even context consistency checks can be brittle to properly crafted adversarial examples and to the best of our knowledge, we are the first to do so. Specifically, we propose an adaptive framework to generate examples that subvert such defenses, namely, Adversarial attacks against object Detection that evade Context consistency checks (ADC). In ADC, we formulate a joint optimization problem which has two attack goals, viz., (i) fooling the object detector and (ii) evading the context consistency check system, at the same time. Experiments on both PASCAL VOC and MS COCO datasets show that examples generated with ADC fool the object detector with a success rate of over 85% in most cases, and at the same time evade the recently proposed context consistency checks, with a bypassing rate of over 80% in most cases. Our results suggest that how to robustly model context and check its consistency, is still an open problem.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yin_ADC_Adversarial_Attacks_Against_Object_Detection_That_Evade_Context_Consistency_WACV_2022_paper.pdf",
        "aff": "University of California, Riverside; University of California, Riverside; University of California, Riverside; University of California, Riverside; University of California, Riverside; University of California, Riverside",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yin_ADC_Adversarial_Attacks_WACV_2022_supplemental.pdf",
        "arxiv": "2110.12321",
        "pdf_size": 2381781,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1972760505355584022&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ucr.edu;ucr.edu;cs.ucr.edu;ect.ucr.edu;ect.ucr.edu;cs.ucr.edu",
        "email": "ucr.edu;ucr.edu;cs.ucr.edu;ect.ucr.edu;ect.ucr.edu;cs.ucr.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Riverside",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucr.edu",
        "aff_unique_abbr": "UCR",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Riverside",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e53728d745",
        "title": "AE-StyleGAN: Improved Training of Style-Based Auto-Encoders",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Han_AE-StyleGAN_Improved_Training_of_Style-Based_Auto-Encoders_WACV_2022_paper.html",
        "author": "Ligong Han; Sri Harsha Musunuri; Martin Renqiang Min; Ruijiang Gao; Yu Tian; Dimitris Metaxas",
        "abstract": "StyleGANs have shown impressive results on data generation and manipulation in recent years, thanks to its disentangled style latent space. A lot of efforts have been made in inverting a pre-trained generator, where an encoder is trained ad hoc after the generator is trained in a two-stage fashion. In this paper, we focus on style-based generators asking a scientific question: Does forcing such a generator to reconstruct real data lead to more disentangled latent space and make the inversion process from image to latent space easy? We describe a new methodology to train a style-based autoencoder where the encoder and generator are optimized end-to-end. We show that our proposed model consistently outperforms baselines in terms of image inversion and generation quality.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Han_AE-StyleGAN_Improved_Training_of_Style-Based_Auto-Encoders_WACV_2022_paper.pdf",
        "aff": "Rutgers University; Rutgers University; NEC Labs America; The University of Texas at Austin; Rutgers University; Rutgers University",
        "project": "",
        "github": "https://github.com/phymhan/stylegan2-pytorch",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3530535,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11311221007079849228&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "rutgers.edu;rutgers.edu;nec-labs.com;utexas.edu;cs.rutgers.edu;cs.rutgers.edu",
        "email": "rutgers.edu;rutgers.edu;nec-labs.com;utexas.edu;cs.rutgers.edu;cs.rutgers.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "Rutgers University;NEC Labs America;University of Texas at Austin",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.rutgers.edu;https://www.nec-labs.com;https://www.utexas.edu",
        "aff_unique_abbr": "Rutgers;NEC LA;UT Austin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "edc6a2a372",
        "title": "AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yan_AFTer-UNet_Axial_Fusion_Transformer_UNet_for_Medical_Image_Segmentation_WACV_2022_paper.html",
        "author": "Xiangyi Yan; Hao Tang; Shanlin Sun; Haoyu Ma; Deying Kong; Xiaohui Xie",
        "abstract": "Recent advances in transformer-based models have drawn attention to exploring these techniques in medical image segmentation, especially in conjunction with the U-Net model (or its variants), which has shown great success in medical image segmentation, under both 2D and 3D settings. Current 2D based methods either directly replace convolutional layers with pure transformers or consider a transformer as an additional intermediate encoder between the encoder and decoder of U-Net. However, these approaches only consider the attention encoding within one single slice and do not utilize the axial-axis information naturally provided by a 3D volume. In the 3D setting, convolution on volumetric data and transformers both consume large GPU memory. One has to either downsample the image or use cropped local patches to reduce GPU memory usage, which limits its performance. In this paper, we propose Axial Fusion Transformer UNet (AFTer-UNet), which takes both advantages of convolutional layers' capability of extracting detailed features and transformers' strength on long sequence modeling. It considers both intra-slice and inter-slice long-range cues to guide the segmentation. Meanwhile, it has fewer parameters and takes less GPU memory to train than the previous transformer-based models. Extensive experiments on three multi-organ segmentation datasets demonstrate that our method outperforms current state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yan_AFTer-UNet_Axial_Fusion_Transformer_UNet_for_Medical_Image_Segmentation_WACV_2022_paper.pdf",
        "aff": "University of California, Irvine; University of California, Irvine; University of California, Irvine; University of California, Irvine; University of California, Irvine; University of California, Irvine",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1637521,
        "gs_citation": 194,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11271469627934119428&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "uci.edu;uci.edu;uci.edu;uci.edu;uci.edu;uci.edu",
        "email": "uci.edu;uci.edu;uci.edu;uci.edu;uci.edu;uci.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4bb48644f2",
        "title": "Action Anticipation Using Latent Goal Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Roy_Action_Anticipation_Using_Latent_Goal_Learning_WACV_2022_paper.html",
        "author": "Debaditya Roy; Basura Fernando",
        "abstract": "To get something done, humans perform a sequence of actions dictated by a goal. So, predicting the next action in the sequence becomes easier once we know the goal that guides the entire activity. We present an action anticipation model that uses goal information in an effective manner. Specifically, we use a latent goal representation as a proxy for the \"real goal\" of the sequence and use this goal information when predicting the next action. We design a model to compute the latent goal representation from the observed video and use it to predict the next action. We also exploit two properties of goals to propose new losses for training the model. First, the effect of the next action should be closer to the latent goal than the observed action, termed as \"goal closeness\". Second, the latent goal should remain consistent before and after the execution of the next action which we coined as \"goal consistency\". Using this technique, we obtain state-of-the-art action anticipation performance on scripted datasets 50Salads and Breakfast that have predefined goals in all their videos. We also evaluate the latent goal-based model on EPIC-KITCHENS55 which is an unscripted dataset with multiple goals being pursued simultaneously. Even though this is not an ideal setup for using latent goals, our model is able to predict the next noun better than existing approaches on both seen and unseen kitchens in the test set.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Roy_Action_Anticipation_Using_Latent_Goal_Learning_WACV_2022_paper.pdf",
        "aff": "Social and Cognitive Computing, IHPC, A*STAR, Singapore; Social and Cognitive Computing, IHPC, A*STAR, Singapore",
        "project": "",
        "github": "https://github.com/debadityaroy/LatentGoal",
        "supp": "",
        "arxiv": "",
        "pdf_size": 678086,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9112750858219333985&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "A*STAR",
        "aff_unique_dep": "Social and Cognitive Computing",
        "aff_unique_url": "https://www.a-star.edu.sg",
        "aff_unique_abbr": "A*STAR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "7af478488a",
        "title": "Active Learning for Improved Semi-Supervised Semantic Segmentation in Satellite Images",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Desai_Active_Learning_for_Improved_Semi-Supervised_Semantic_Segmentation_in_Satellite_Images_WACV_2022_paper.html",
        "author": "Shasvat Desai; Debasmita Ghose",
        "abstract": "Remote sensing data is crucial for applications ranging from monitoring forest fires and deforestation to tracking urbanization. Most of these tasks require dense pixel-level annotations for the model to parse visual information from limited labeled data available for these satellite images. Due to the dearth of high-quality labeled training data in this domain, there is a need to focus on semi-supervised techniques. These techniques generate pseudo-labels from a small set of labeled examples which are used to augment the labeled training set. This makes it necessary to have a highly representative and diverse labeled training set. Therefore, we propose to use an active learning-based sampling strategy to select a highly representative set of labeled training data. We demonstrate our proposed method's effectiveness on two existing semantic segmentation datasets containing satellite images: UC Merced Land Use Classification Dataset and DeepGlobe Land Cover Classification Dataset. We report a 27% improvement in mIoU with as little as 2% labeled data using active learning sampling strategies over randomly sampling the small set of labeled training data.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Desai_Active_Learning_for_Improved_Semi-Supervised_Semantic_Segmentation_in_Satellite_Images_WACV_2022_paper.pdf",
        "aff": "Orbital Insight; Yale University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Desai_Active_Learning_for_WACV_2022_supplemental.pdf",
        "arxiv": "2110.07782",
        "pdf_size": 11189182,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6525268228754201080&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "orbitalinsight.com;yale.edu",
        "email": "orbitalinsight.com;yale.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Orbital Insight;Yale University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.orbitalinsight.com;https://www.yale.edu",
        "aff_unique_abbr": ";Yale",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b52cc316f1",
        "title": "Addressing Out-of-Distribution Label Noise in Webly-Labelled Data",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Albert_Addressing_Out-of-Distribution_Label_Noise_in_Webly-Labelled_Data_WACV_2022_paper.html",
        "author": "Paul Albert; Diego Ortego; Eric Arazo; Noel E. O'Connor; Kevin McGuinness",
        "abstract": "A recurring focus of the deep learning community is towards reducing the labeling effort. Data gathering and annotation using a search engine is a simple alternative to generating a fully human-annotated and human-gathered dataset. Although web crawling is very time efficient, some of the retrieved images are unavoidably noisy, i.e. incorrectly labeled. Designing robust algorithms for training on noisy data gathered from the web is an important research perspective that would render the building of datasets easier. In this paper we conduct a study to understand the type of label noise to expect when building a dataset using a search engine. We review the current limitations of state-of-the-art methods for dealing with noisy labels for image classification tasks in the case of web noise distribution. We propose a simple solution to bridge the gap with a fully clean dataset using Dynamic Softening of Out-of-distribution Samples (DSOS), which we design on corrupted versions of the CIFAR-100 dataset, and compare against state-of-the-art algorithms on the web noise perturbated MiniImageNet and Stanford datasets and on real label noise datasets: WebVision 1.0 and Clothing1M. Our work is fully reproducible https://git.io/JKGcj.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Albert_Addressing_Out-of-Distribution_Label_Noise_in_Webly-Labelled_Data_WACV_2022_paper.pdf",
        "aff": "School of Electronic Engineering, Insight SFI Centre for Data Analytics, Dublin City University (DCU); School of Electronic Engineering, Insight SFI Centre for Data Analytics, Dublin City University (DCU); School of Electronic Engineering, Insight SFI Centre for Data Analytics, Dublin City University (DCU); School of Electronic Engineering, Insight SFI Centre for Data Analytics, Dublin City University (DCU); School of Electronic Engineering, Insight SFI Centre for Data Analytics, Dublin City University (DCU)",
        "project": "",
        "github": "https://git.io/JKGcj",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Albert_Addressing_Out-of-Distribution_Label_WACV_2022_supplemental.pdf",
        "arxiv": "2110.13699",
        "pdf_size": 906092,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7194300370578564513&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "insight-centre.org; ; ; ; ",
        "email": "insight-centre.org; ; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Dublin City University",
        "aff_unique_dep": "School of Electronic Engineering",
        "aff_unique_url": "https://www.dcu.ie",
        "aff_unique_abbr": "DCU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Dublin",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Ireland"
    },
    {
        "id": "e8d671f82d",
        "title": "Adversarial Branch Architecture Search for Unsupervised Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Robbiano_Adversarial_Branch_Architecture_Search_for_Unsupervised_Domain_Adaptation_WACV_2022_paper.html",
        "author": "Luca Robbiano; Muhammad Rameez Ur Rahman; Fabio Galasso; Barbara Caputo; Fabio Maria Carlucci",
        "abstract": "Unsupervised Domain Adaptation (UDA) is a key issue in visual recognition, as it allows to bridge different visual domains enabling robust performances in the real world. To date, all proposed approaches rely on human expertise to manually adapt a given UDA method (e.g. DANN) to a specific backbone architecture (e.g. ResNet). This dependency on handcrafted designs limits the applicability of a given approach in time, as old methods need to be constantly adapted to novel backbones. Existing Neural Architecture Search (NAS) approaches cannot be directly applied to mitigate this issue, as they rely on labels that are not available in the UDA setting. Furthermore, most NAS methods search for full architectures, which precludes the use of pre-trained models, essential in a vast range of UDA settings for reaching SOTA results. To the best of our knowledge, no prior work has addressed these aspects in the context of NAS for UDA. Here we tackle both aspects with an Adversarial Branch Architecture Search for UDA (ABAS): i. we address the lack of target labels by a novel data-driven ensemble approach for model selection; and ii. we search for an auxiliary adversarial branch, attached to a pre-trained backbone, which drives the domain alignment. We extensively validate ABAS to improve two modern UDA techniques, DANN and ALDA, on three standard visual recognition datasets (Office31, Office-Home and PACS). In all cases, ABAS robustly finds the adversarial branch architectures and parameters which yield best performances. https://github.com/lr94/abas",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Robbiano_Adversarial_Branch_Architecture_Search_for_Unsupervised_Domain_Adaptation_WACV_2022_paper.pdf",
        "aff": "Politecnico di Torino; Sapienza University of Rome; Sapienza University of Rome; Politecnico di Torino + CINI Consortium; Huawei Noah\u2019s Ark Lab",
        "project": "",
        "github": "https://github.com/lr94/abas",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Robbiano_Adversarial_Branch_Architecture_WACV_2022_supplemental.pdf",
        "arxiv": "2102.06679",
        "pdf_size": 3522108,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=822480724252480601&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "polito.it;di.uniroma1.it;di.uniroma1.it;polito.it;gmail.com",
        "email": "polito.it;di.uniroma1.it;di.uniroma1.it;polito.it;gmail.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0+2;3",
        "aff_unique_norm": "Politecnico di Torino;Sapienza University of Rome;CINI Consortium;Huawei",
        "aff_unique_dep": ";;;Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.polito.it;https://www.uniroma1.it;http://www.cini.it;https://www.huawei.com",
        "aff_unique_abbr": "Polito;Sapienza;CINI;Huawei",
        "aff_campus_unique_index": "1;1;",
        "aff_campus_unique": ";Rome",
        "aff_country_unique_index": "0;0;0;0+0;1",
        "aff_country_unique": "Italy;China"
    },
    {
        "id": "fb5b58aa90",
        "title": "Adversarial Open Domain Adaptation for Sketch-to-Photo Synthesis",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Xiang_Adversarial_Open_Domain_Adaptation_for_Sketch-to-Photo_Synthesis_WACV_2022_paper.html",
        "author": "Xiaoyu Xiang; Ding Liu; Xiao Yang; Yiheng Zhu; Xiaohui Shen; Jan P. Allebach",
        "abstract": "In this paper, we explore open-domain sketch-to-photo translation, which aims to synthesize a realistic photo from a freehand sketch with its class label, even if the sketches of that class are missing in the training data. It is challenging due to the lack of training supervision and the large geometric distortion between the freehand sketch and photo domains. To synthesize the absent freehand sketches from photos, we propose a framework that jointly learns sketch-to-photo and photo-to-sketch generation. However, the generator trained from fake sketches might lead to unsatisfying results when dealing with sketches of missing classes, due to the domain gap between synthesized sketches and real ones. To alleviate this issue, we further propose a simple yet effective open-domain sampling and optimization strategy to \"fool\" the generator into treating fake sketches as real ones. Our method takes advantage of the learned sketch-to-photo and photo-to-sketch mapping of in-domain data and generalizes it to the open-domain classes. We validate our method on the Scribble and SketchyCOCO datasets. Compared with the recent competing methods, our approach shows impressive results in synthesizing realistic color, texture, and maintaining the geometric composition for various categories of open-domain sketches.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Xiang_Adversarial_Open_Domain_Adaptation_for_Sketch-to-Photo_Synthesis_WACV_2022_paper.pdf",
        "aff": "Purdue University+ByteDance Inc.; ByteDance Inc.; ByteDance Inc.; ByteDance Inc.; ByteDance Inc.; Purdue University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Xiang_Adversarial_Open_Domain_WACV_2022_supplemental.pdf",
        "arxiv": "2104.05703",
        "pdf_size": 5717700,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8579431718377214872&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "purdue.edu;bytedance.com;bytedance.com;bytedance.com;bytedance.com;purdue.edu",
        "email": "purdue.edu;bytedance.com;bytedance.com;bytedance.com;bytedance.com;purdue.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;1;1;0",
        "aff_unique_norm": "Purdue University;ByteDance",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.purdue.edu;https://www.bytedance.com",
        "aff_unique_abbr": "Purdue;ByteDance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "a88489bddd",
        "title": "Adversarial Robustness of Deep Sensor Fusion Models",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wang_Adversarial_Robustness_of_Deep_Sensor_Fusion_Models_WACV_2022_paper.html",
        "author": "Shaojie Wang; Tong Wu; Ayan Chakrabarti; Yevgeniy Vorobeychik",
        "abstract": "We experimentally study the robustness of deep camera-LiDAR fusion architectures for 2D object detection in autonomous driving. First, we find that the fusion model is usually both more accurate, and more robust against single-source attacks than single-sensor deep neural networks. Furthermore, we show that without adversarial training, early fusion is more robust than late fusion, whereas the two perform similarly after adversarial training. However, we note that single-channel adversarial training of deep fusion is often detrimental even to robustness. Moreover, we observe cross-channel externalities, where single-channel adversarial training reduces robustness to attacks on the other channel. Additionally, we observe that the choice of adversarial model in adversarial training is critical: using attacks restricted to cars' bounding boxes is more effective in adversarial training and exhibits less significant cross-channel externalities. Finally, we find that joint-channel adversarial training helps mitigate many of the issues above, but does not significantly boost adversarial robustness.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wang_Adversarial_Robustness_of_Deep_Sensor_Fusion_Models_WACV_2022_paper.pdf",
        "aff": "Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2006.13192",
        "pdf_size": 5395169,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10305035162572511539&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "wustl.edu;wustl.edu;gmail.com;wustl.edu",
        "email": "wustl.edu;wustl.edu;gmail.com;wustl.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WashU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b243d2c9a5",
        "title": "Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tjio_Adversarial_Semantic_Hallucination_for_Domain_Generalized_Semantic_Segmentation_WACV_2022_paper.html",
        "author": "Gabriel Tjio; Ping Liu; Joey Tianyi Zhou; Rick Siow Mong Goh",
        "abstract": "Convolutional neural networks typically perform poorly when the test (target domain) and training (source domain) data have significantly different distributions. While this problem can be mitigated by using the target domain data to align the source and target domain feature representations, the target domain data may be unavailable due to privacy concerns. Consequently, there is a need for methods that generalize well despite restricted access to target domain data during training. In this work, we propose an adversarial semantic hallucination approach (ASH), which combines a class-conditioned hallucination module and a semantic segmentation module. Since the segmentation performance varies across different classes, we design a semantic-conditioned style hallucination module to generate affine transformation parameters from semantic information in the segmentation probability maps of the source domain image. Unlike previous adaptation approaches, which treat all classes equally, ASH considers the class-wise differences. The segmentation module and the hallucination module compete adversarially, with the hallucination module generating increasingly \"difficult\" stylized images to challenge the segmentation module. In response, the segmentation module improves as it is trained with generated samples at an appropriate class-wise difficulty level. Our results on the Cityscapes and Mapillary benchmark datasets show that our method is competitive with state of the art work. Code is made available at https://github.com/gabriel-tjio/ASH.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tjio_Adversarial_Semantic_Hallucination_for_Domain_Generalized_Semantic_Segmentation_WACV_2022_paper.pdf",
        "aff": "Institute of High Performance Computing, A*STAR, Singapore; Institute of High Performance Computing, A*STAR, Singapore; Institute of High Performance Computing, A*STAR, Singapore; Institute of High Performance Computing, A*STAR, Singapore",
        "project": "",
        "github": "https://github.com/gabriel-tjio/ASH",
        "supp": "",
        "arxiv": "2106.04144",
        "pdf_size": 1778784,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13688032060281401766&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ihpc.a-star.edu.sg;gmail.com;ihpc.a-star.edu.sg;ihpc.a-star.edu.sg",
        "email": "ihpc.a-star.edu.sg;gmail.com;ihpc.a-star.edu.sg;ihpc.a-star.edu.sg",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "A*STAR Institute of High Performance Computing",
        "aff_unique_dep": "Institute of High Performance Computing",
        "aff_unique_url": "https://www.ihpc.a-star.edu.sg",
        "aff_unique_abbr": "IHPC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "79a3f1782f",
        "title": "Agree To Disagree: When Deep Learning Models With Identical Architectures Produce Distinct Explanations",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Watson_Agree_To_Disagree_When_Deep_Learning_Models_With_Identical_Architectures_WACV_2022_paper.html",
        "author": "Matthew Watson; Bashar Awwad Shiekh Hasan; Noura Al Moubayed",
        "abstract": "Deep Learning of neural networks has progressively become more prominent in healthcare with models reaching, or even surpassing, expert accuracy levels. However, these success stories are tainted by concerning reports on the lack of model transparency and bias against some medical conditions or patients' sub-groups. Explainable methods are considered the gateway to alleviate many of these concerns. In this study we demonstrate that the generated explanations are volatile to changes in model training that are perpendicular to the classification task and model structure. This raises further questions about trust in deep learning models for healthcare. Mainly, whether the models capture underlying causal links in the data or just rely on spurious correlations that are made visible via explanation methods. We demonstrate that the output of explainability methods on deep neural networks can vary significantly by changes of hyper-parameters, such as the random seed or how the training set is shuffled. We introduce a measure of explanation consistency which we use to highlight the identified problems on the MIMIC-CXR dataset. We find explanations of identical models but with different training setups have a low consistency: approximately 33% on average. On the contrary, kernel methods are robust against any orthogonal changes, with explanation consistency at 94%. We conclude that current trends in model explanation are not sufficient to mitigate the risks of deploying models in real life healthcare applications.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Watson_Agree_To_Disagree_When_Deep_Learning_Models_With_Identical_Architectures_WACV_2022_paper.pdf",
        "aff": "Durham University; Durham University; Durham University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Watson_Agree_To_Disagree_WACV_2022_supplemental.pdf",
        "arxiv": "2105.06791",
        "pdf_size": 1729203,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1020018505331333473&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "durham.ac.uk;durham.ac.uk;durham.ac.uk",
        "email": "durham.ac.uk;durham.ac.uk;durham.ac.uk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Durham University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.dur.ac.uk",
        "aff_unique_abbr": "Durham",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "fdd3046ab6",
        "title": "AirCamRTM: Enhancing Vehicle Detection for Efficient Aerial Camera-Based Road Traffic Monitoring",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Makrigiorgis_AirCamRTM_Enhancing_Vehicle_Detection_for_Efficient_Aerial_Camera-Based_Road_Traffic_WACV_2022_paper.html",
        "author": "Rafael Makrigiorgis; Nicolas Hadjittoouli; Christos Kyrkou; Theocharis Theocharides",
        "abstract": "Efficient road traffic monitoring is playing a fundamental role in successfully resolving traffic congestion in cities.Unmanned Aerial Vehicles (UAVs) or drones equipped with cameras are an attractive proposition to provide flexible and infrastructure-free traffic monitoring. However, real-time traffic monitoring from UAV imagery poses several challenges, due to the large image sizes and presence of non relevant targets. In this paper, we propose the AirCam-RTM framework that combines road segmentation and vehicle detection to focus only on relevant vehicles, which as a result, improves the monitoring performance by approximately 2x and provides approximately 18% accuracy improvement. Furthermore,through a real experimental setup we qualitatively evaluate the performance of the proposed approach, and also demonstrate how it can be used for real-time traffic monitoring and management using UAVs.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Makrigiorgis_AirCamRTM_Enhancing_Vehicle_Detection_for_Efficient_Aerial_Camera-Based_Road_Traffic_WACV_2022_paper.pdf",
        "aff": "KIOS Research and Innovation Center of Excellence, Department of Electrical and Computer Engineering, University of Cyprus; KIOS Research and Innovation Center of Excellence, Department of Electrical and Computer Engineering, University of Cyprus; KIOS Research and Innovation Center of Excellence, Department of Electrical and Computer Engineering, University of Cyprus; KIOS Research and Innovation Center of Excellence, Department of Electrical and Computer Engineering, University of Cyprus",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3392249,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8794484257938191057&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ucy.ac.cy;ucy.ac.cy;ucy.ac.cy;ucy.ac.cy",
        "email": "ucy.ac.cy;ucy.ac.cy;ucy.ac.cy;ucy.ac.cy",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Cyprus",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucy.ac.cy",
        "aff_unique_abbr": "UCY",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Cyprus"
    },
    {
        "id": "45f71c6b18",
        "title": "All the Attention You Need: Global-Local, Spatial-Channel Attention for Image Retrieval",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Song_All_the_Attention_You_Need_Global-Local_Spatial-Channel_Attention_for_Image_WACV_2022_paper.html",
        "author": "Chull Hwan Song; Hye Joo Han; Yannis Avrithis",
        "abstract": "We address representation learning for large-scale instance-level image retrieval. Apart from backbone, training pipelines and loss functions, popular approaches have focused on different spatial pooling and attention mechanisms, which are at the core of learning a powerful global image representation. There are different forms of attention according to the interaction of elements of the feature tensor (local and global) and the dimensions where it is applied (spatial and channel). Unfortunately, each study addresses only one or two forms of attention and applies it to different problems like classification, detection or retrieval. We present global-local attention module (GLAM), which is attached at the end of a backbone network and incorporates all four forms of attention: local and global, spatial and channel. We obtain a new feature tensor and, by spatial pooling, we learn a powerful embedding for image retrieval. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Song_All_the_Attention_You_Need_Global-Local_Spatial-Channel_Attention_for_Image_WACV_2022_paper.pdf",
        "aff": "Odd Concepts Inc.; Odd Concepts Inc.; Athena RC",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2107.08000",
        "pdf_size": 4653455,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14959374972088271626&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "oddconcepts.kr;oddconcepts.kr;athenarc.gr",
        "email": "oddconcepts.kr;oddconcepts.kr;athenarc.gr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Odd Concepts Inc.;Athena RC",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "ec0dc52701",
        "title": "An Experimental Comparison of Multi-View Stereo Approaches on Satellite Images",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gomez_An_Experimental_Comparison_of_Multi-View_Stereo_Approaches_on_Satellite_Images_WACV_2022_paper.html",
        "author": "Alvaro G\u00f3mez; Gregory Randall; Gabriele Facciolo; Rafael Grompone von Gioi",
        "abstract": "Different methods can be applied to satellite images to derive an altitude map from a set of images. In this article we evaluate a set of representative methods from different approaches. We consider true multi-view stereo methods as well as pair-wise ones, classic methods and deep learning based ones, methods already in use on satellite images and others that were originally devised for close range imaging and are adapted to satellite imagery. While deep learning (DL) methods have taken over multi-view stereo reconstruction in the last years, this tendency has not fully reached satellite stereo pipelines that still largely rely on pair-wise classic algorithms. For the comparison, we set-up a framework that allows to interface a DL-based stereo method taken from the computer vision literature with a satellite stereo pipeline. For multi-view stereo algorithms we build on a recently proposed framework originally devised to apply Colmap method to satellite images. Methods are compared on several datasets that include sets of images taken within a few days and sets of images taken months apart. Results show that DL methods have, in general, a good generalization power. In particular, the use of the GANet DL method as the matching step in a pair-wise stereo pipeline is promising as it already performs better than the classic counterpart, even without a specific training.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gomez_An_Experimental_Comparison_of_Multi-View_Stereo_Approaches_on_Satellite_Images_WACV_2022_paper.pdf",
        "aff": "Facultad de Ingenier \u00b4\u0131a, Universidad de la Rep \u00b4ublica, Uruguay; Facultad de Ingenier \u00b4\u0131a, Universidad de la Rep \u00b4ublica, Uruguay; Centre Borelli, ENS Paris-Saclay, France; Centre Borelli, ENS Paris-Saclay, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10163407,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12700815488195397072&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "fing.edu.uy; ; ; ",
        "email": "fing.edu.uy; ; ; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Universidad de la Rep\u00fablica;\u00c9cole Normale Sup\u00e9rieure Paris-Saclay",
        "aff_unique_dep": "Facultad de Ingenier\u00eda;Centre Borelli",
        "aff_unique_url": "https://www.unorte.edu.uy;https://www.ens-paris-saclay.fr",
        "aff_unique_abbr": ";ENS Paris-Saclay",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Paris-Saclay",
        "aff_country_unique_index": "0;0;1;1",
        "aff_country_unique": "Uruguay;France"
    },
    {
        "id": "b360355fa4",
        "title": "An Investigation of Critical Issues in Bias Mitigation Techniques",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Shrestha_An_Investigation_of_Critical_Issues_in_Bias_Mitigation_Techniques_WACV_2022_paper.html",
        "author": "Robik Shrestha; Kushal Kafle; Christopher Kanan",
        "abstract": "A critical problem in deep learning is that systems learn inappropriate biases, resulting in their inability to perform well on minority groups. This has led to the creation of multiple algorithms that endeavor to mitigate bias. However, it is not clear how effective these methods are. This is because study protocols differ among papers, systems are tested on datasets that fail to test many forms of bias, and systems have access to hidden knowledge or are tuned specifically to the test set. To address this, we introduce an improved evaluation protocol, sensible metrics, and a new dataset, which enables us to ask and answer critical questions about bias mitigation algorithms. We evaluate seven state-of-the-art algorithms using the same network architecture and hyperparameter selection policy across three benchmark datasets. We introduce a new dataset called BiasedMNIST that enables the assessment of robustness to multiple bias sources. We use BiasedMNIST and a visual question answering (VQA) benchmark to assess robustness to hidden biases. Rather than only tuning to the test set distribution, we study robustness across different tuning distributions, which is critical because for many applications the test distribution may not be known during development. We find that algorithms exploit hidden biases, are unable to scale to multiple forms of bias, and are highly sensitive to the choice of tuning set. Based on our findings, we implore the community to adopt more rigorous assessment of future bias mitigation methods. All data, code and results will be made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Shrestha_An_Investigation_of_Critical_Issues_in_Bias_Mitigation_Techniques_WACV_2022_paper.pdf",
        "aff": "Rochester Institute of Technology; Adobe Research; Paige+Cornell Tech",
        "project": "",
        "github": "https://github.com/erobic/bias-mitigators",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Shrestha_An_Investigation_of_WACV_2022_supplemental.pdf",
        "arxiv": "2104.00170",
        "pdf_size": 885681,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4934042672996992445&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "rit.edu;rit.edu;rit.edu",
        "email": "rit.edu;rit.edu;rit.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3",
        "aff_unique_norm": "Rochester Institute of Technology;Adobe;Paige;Cornell University",
        "aff_unique_dep": ";Adobe Research;;",
        "aff_unique_url": "https://www.rit.edu;https://research.adobe.com;;https://tech.cornell.edu",
        "aff_unique_abbr": "RIT;Adobe;;Cornell Tech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York City",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "59ef9182ae",
        "title": "Approximate Neural Architecture Search via Operation Distribution Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wan_Approximate_Neural_Architecture_Search_via_Operation_Distribution_Learning_WACV_2022_paper.html",
        "author": "Xingchen Wan; Binxin Ru; Pedro M. Esparan\u00e7a; Fabio Maria Carlucci",
        "abstract": "The standard paradigm in neural architecture search (NAS) is to search for a fully deterministic architecture with specific operations and connections. In this work, we instead propose to search for the optimal operation distribution, thus providing a stochastic and approximate solution, which can be used to sample architectures of arbitrary length. We propose and show, that given an architectural cell, its performance largely depends on the ratio of used operations, rather than any specific connection pattern; that is, small changes in the ordering of the operations are often irrelevant. This intuition is orthogonal to any specific search strategy and can be applied to a diverse set of NAS algorithms. Through extensive validation on 4 data-sets and 4 NAS techniques (Bayesian optimisation, differentiable search, local search and random search), we show that the operation distribution (1) holds enough discriminating power to reliably identify a solution and (2) is significantly easier to optimise than traditional encodings, leading to large speed-ups at little to no cost in performance. Indeed, this simple intuition significantly reduces the cost of current approaches and potentially enable NAS to be used in a broader range of research applications.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wan_Approximate_Neural_Architecture_Search_via_Operation_Distribution_Learning_WACV_2022_paper.pdf",
        "aff": "Huawei Noah\u2019s Ark Lab, London, UK+Machine Learning Research Group, University of Oxford, Oxford, UK; Huawei Noah\u2019s Ark Lab, London, UK+Machine Learning Research Group, University of Oxford, Oxford, UK; Huawei Noah\u2019s Ark Lab, London, UK; Huawei Noah\u2019s Ark Lab, London, UK",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Wan_Approximate_Neural_Architecture_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 969075,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11557368530295101297&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk;huawei.com;gmail.com",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk;huawei.com;gmail.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0;0",
        "aff_unique_norm": "Huawei;University of Oxford",
        "aff_unique_dep": "Huawei Noah\u2019s Ark Lab;Machine Learning Research Group",
        "aff_unique_url": "https://www.huawei.com/en/ai;https://www.ox.ac.uk",
        "aff_unique_abbr": "HNA Lab;Oxford",
        "aff_campus_unique_index": "0+1;0+1;0;0",
        "aff_campus_unique": "London;Oxford",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "4a8a948592",
        "title": "AttWalk: Attentive Cross-Walks for Deep Mesh Analysis",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Izhak_AttWalk_Attentive_Cross-Walks_for_Deep_Mesh_Analysis_WACV_2022_paper.html",
        "author": "Ran Ben Izhak; Alon Lahav; Ayellet Tal",
        "abstract": "Mesh representation by random walks has been shown to benefit deep learning. Randomness is indeed a powerful concept. However, it comes with a price--some walks might wander around non-characteristic regions of the mesh, which might be harmful to shape analysis, especially when only a few walks are utilized. We propose a novel walk-attention mechanism that leverages the fact that multiple walks are used for a single mesh representation. The key idea is that the walks may provide each other with information regarding the meaningful (attentive) features of the mesh. We utilize this mutual information to extract a single descriptor of the mesh. This differs from common attention mechanisms that use attention to improve the representation of each individual descriptor. Our approach achieves SoTA results for two basic 3D shape analysis tasks: classification and retrieval. Even a handful of walks along a mesh suffice for learning. Furthermore, our approach provides insight into mesh importance detection.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Izhak_AttWalk_Attentive_Cross-Walks_for_Deep_Mesh_Analysis_WACV_2022_paper.pdf",
        "aff": "Technion, Israel; Technion, Israel; Technion, Israel",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Izhak_AttWalk_Attentive_Cross-Walks_WACV_2022_supplemental.pdf",
        "arxiv": "2104.11571",
        "pdf_size": 7121284,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2593143730520686033&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;gmail.com;ee.technion.ac.il",
        "email": "gmail.com;gmail.com;ee.technion.ac.il",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "3a377d8452",
        "title": "Attack Agnostic Detection of Adversarial Examples via Random Subspace Analysis",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Drenkow_Attack_Agnostic_Detection_of_Adversarial_Examples_via_Random_Subspace_Analysis_WACV_2022_paper.html",
        "author": "Nathan Drenkow; Neil Fendley; Philippe Burlina",
        "abstract": "Whilst adversarial attack detection has received considerable attention, it remains a fundamentally challenging problem from two perspectives. First, while threat models can be well-defined, attacker strategies may still vary widely within those constraints. Therefore, detection should be considered as an open-set problem, standing in contrast to most current detection approaches. These methods take a closed-set view and train binary detectors, thus biasing detection toward attacks seen during detector training. Second, limited information is available at test time and typically confounded by nuisance factors including the label and underlying content of the image. We address these challenges via a novel strategy based on random subspace analysis. We present a technique that utilizes properties of random projections to characterize the behavior of clean and adversarial examples across a diverse set of subspaces. The self-consistency (or inconsistency) of model activations is leveraged to discern clean from adversarial examples. Performance evaluations demonstrate that our technique (AUC [0.92, 0.98]) outperforms competing detection strategies (AUC [0.30,0.79]), while remaining truly agnostic to the attack strategy (for both targeted/untargeted attacks). It also requires significantly less calibration data (composed only of clean examples) than competing approaches to achieve this performance.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Drenkow_Attack_Agnostic_Detection_of_Adversarial_Examples_via_Random_Subspace_Analysis_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2012.06405",
        "pdf_size": 970260,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17544089002426451995&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5875fab97d",
        "title": "Attribute-Based Deep Periocular Recognition: Leveraging Soft Biometrics to Improve Periocular Recognition",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Talreja_Attribute-Based_Deep_Periocular_Recognition_Leveraging_Soft_Biometrics_to_Improve_Periocular_WACV_2022_paper.html",
        "author": "Veeru Talreja; Nasser M. Nasrabadi; Matthew C. Valenti",
        "abstract": "In recent years, periocular recognition has been developed as a valuable biometric identification approach, especially in wild environments (for example, masked faces due to COVID-19 pandemic) where facial recognition may not be applicable. This paper presents a new deep periocular recognition framework called attribute-based deep periocular recognition (ADPR), which predicts soft biometrics and incorporates the prediction into a periocular recognition algorithm to determine identity from periocular images with high accuracy. We propose an end-to-end framework, which uses several shared convolutional neural network (CNN) layers (a common network) whose output feeds two separate dedicated branches (modality dedicated layers); the first branch classifies periocular images while the second branch predicts soft biometrics. Next, the features from these two branches are fused together for a final periocular recognition. The proposed method is different from existing methods as it not only uses a shared CNN feature space to train these two tasks jointly, but it also fuses predicted soft biometric features with the periocular features in the training step to improve the overall periocular recognition performance. Our proposed model is extensively evaluated using four different publicly available datasets. Experimental results indicate that our soft biometric based periocular recognition approach outperforms other state-of-the-art methods for periocular recognition in wild environments.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Talreja_Attribute-Based_Deep_Periocular_Recognition_Leveraging_Soft_Biometrics_to_Improve_Periocular_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2111.01325",
        "pdf_size": 2207795,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2235141180619183457&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7f800ecd20",
        "title": "Auditing Saliency Cropping Algorithms",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Birhane_Auditing_Saliency_Cropping_Algorithms_WACV_2022_paper.html",
        "author": "Abeba Birhane; Vinay Uday Prabhu; John Whaley",
        "abstract": "In this paper, we audit saliency cropping algorithms used by Twitter, Google and Apple to investigate issues pertaining to the male-gaze cropping phenomenon as well as race-gender biases that emerge in post-cropping survival ratios of face-images constituting 3 x 1 grid images. In doing so, we present the first formal empirical study which suggests that the worry of a male-gaze-like image cropping phenomenon on Twitter is not at all far-fetched and it does occur with worryingly high prevalence rates in real-world full-body single-female-subject images shot with logo-littered backdrops. We uncover that while all three saliency cropping frameworks considered in this paper do exhibit acute racial and gender biases, Twitter's saliency cropping framework uniquely elicits high male-gaze cropping prevalence rates. In order to facilitate reproducing the results presented here, we are open-sourcing both the code and the datasets that we curated at shorturl.at/iuzK9. We hope the computer vision community and saliency cropping researchers will build on the results presented here and extend these investigations to similar frameworks deployed in the real world by other companies such as Microsoft and Facebook.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Birhane_Auditing_Saliency_Cropping_Algorithms_WACV_2022_paper.pdf",
        "aff": "University College Dublin & Lero; UnifyID Labs; UnifyID Labs",
        "project": "shorturl.at/iuzK9",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Birhane_Auditing_Saliency_Cropping_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5302938,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10412517825270520862&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "ucdconnect.ie;unify.id;unify.id",
        "email": "ucdconnect.ie;unify.id;unify.id",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University College Dublin;UnifyID Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucd.ie;",
        "aff_unique_abbr": "UCD;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Ireland;United States"
    },
    {
        "id": "35092f2b9a",
        "title": "Auto White-Balance Correction for Mixed-Illuminant Scenes",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Afifi_Auto_White-Balance_Correction_for_Mixed-Illuminant_Scenes_WACV_2022_paper.html",
        "author": "Mahmoud Afifi; Marcus A. Brubaker; Michael S. Brown",
        "abstract": "Auto white balance (AWB) is applied by camera hardware at capture time to remove the color cast caused by the scene illumination. The vast majority of white-balance algorithms assume a single light source illuminates the scene; however, real scenes often have mixed lighting conditions. This paper presents an effective AWB method to deal with such mixed-illuminant scenes. A unique departure from conventional AWB, our method does not require illuminant estimation, as is the case in traditional camera AWB modules. Instead, our method proposes to render the captured scene with a small set of predefined white-balance settings. Given this set of rendered images, our method learns to estimate weighting maps that are used to blend the rendered images to generate the final corrected image. Through extensive experiments, we show this proposed method produces promising results compared to other alternatives for single- and mixed-illuminant scene color correction.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Afifi_Auto_White-Balance_Correction_for_Mixed-Illuminant_Scenes_WACV_2022_paper.pdf",
        "aff": "York University; York University + Vector Institute; York University + Vector Institute",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Afifi_Auto_White-Balance_Correction_WACV_2022_supplemental.pdf",
        "arxiv": "2109.08750",
        "pdf_size": 5679597,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1226652016489698421&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "eecs.yorku.ca;eecs.yorku.ca;eecs.yorku.ca",
        "email": "eecs.yorku.ca;eecs.yorku.ca;eecs.yorku.ca",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0+1",
        "aff_unique_norm": "York University;Vector Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.yorku.ca;https://vectorinstitute.ai/",
        "aff_unique_abbr": "York U;Vector Institute",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "c0d7d9eb76",
        "title": "Auto-X3D: Ultra-Efficient Video Understanding via Finer-Grained Neural Architecture Search",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Jiang_Auto-X3D_Ultra-Efficient_Video_Understanding_via_Finer-Grained_Neural_Architecture_Search_WACV_2022_paper.html",
        "author": "Yifan Jiang; Xinyu Gong; Junru Wu; Humphrey Shi; Zhicheng Yan; Zhangyang Wang",
        "abstract": "Efficient video architecture is the key to the deployment of video action recognition systems on devices with limited computing capabilities. Unfortunately, existing video architectures are often computationally intensive and not suitable for such applications. The recent X3D work presents a new family of efficient video models by expanding a hand-crafted image architecture along multiple axes, such as space, time, width, and depth. Although operating in a conceptually large space, X3D searched one axis at a time, and merely explored a small set of 30 architectures in total, which does not sufficiently explore the space. This paper bypasses existing 2D architectures, and directly searched for 3D architectures in a fine-grained space, where block type, filter number, expansion ratio and attention block are jointly searched. A probabilistic neural architecture search method is adopted to efficiently search in such a large space. Evaluations on Kinetics and Something-Something-V2 benchmarks confirm our \\autoxthreed models outperform existing ones in accuracy up to 1.7% under similar FLOPs, and reduce the computational cost up to 1.74 times to reach similar performance. Code will be publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Jiang_Auto-X3D_Ultra-Efficient_Video_Understanding_via_Finer-Grained_Neural_Architecture_Search_WACV_2022_paper.pdf",
        "aff": "University of Texas at Austin; University of Texas at Austin; Texas A&M University; University of Oregon+Picsart AI Research (PAIR); Facebook AI; University of Texas at Austin",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Jiang_Auto-X3D_Ultra-Efficient_Video_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 772571,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6252987615857225669&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "utexas.edu;utexas.edu; ;uoregon.edu; ;utexas.edu",
        "email": "utexas.edu;utexas.edu; ;uoregon.edu; ;utexas.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2+3;4;0",
        "aff_unique_norm": "University of Texas at Austin;Texas A&M University;University of Oregon;Picsart AI Research;Meta",
        "aff_unique_dep": ";;;AI Research;Facebook AI",
        "aff_unique_url": "https://www.utexas.edu;https://www.tamu.edu;https://www.uoregon.edu;https://research.picsart.com;https://www.facebook.com",
        "aff_unique_abbr": "UT Austin;TAMU;UO;PAIR;Facebook AI",
        "aff_campus_unique_index": "0;0;;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b9e7a4f65a",
        "title": "Automated Defect Inspection in Reverse Engineering of Integrated Circuits",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bette_Automated_Defect_Inspection_in_Reverse_Engineering_of_Integrated_Circuits_WACV_2022_paper.html",
        "author": "Ann-Christin Bette; Patrick Brus; Gabor Balazs; Matthias Ludwig; Alois Knoll",
        "abstract": "In the semiconductor industry, reverse engineering is used to extract information from microchips. Circuit extraction is becoming increasingly difficult due to the continuous technology shrinking. A high quality reverse engineering process is challenged by various defects coming from chip preparation and imaging errors. Currently, no automated, technology-agnostic defect inspection framework is available. To meet the requirements of the mostly manual reverse engineering process, the proposed automated framework needs to handle highly imbalanced data, as well as unknown and multiple defect classes. We propose a network architecture that is composed of a shared Xception-based feature extractor and multiple, individually trainable binary classification heads: the HydREnet. We evaluated our defect classifier on three challenging industrial datasets and achieved accuracies of over 85 %, even for underrepresented classes. With this framework, the manual inspection effort can be reduced down to 5 %.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bette_Automated_Defect_Inspection_in_Reverse_Engineering_of_Integrated_Circuits_WACV_2022_paper.pdf",
        "aff": "Technical University of Munich, Germany+Infineon Technologies AG, Munich, Germany; Infineon Technologies AG, Munich, Germany; Technical University of Munich, Germany+Infineon Technologies AG, Munich, Germany; Technical University of Munich, Germany+Infineon Technologies AG, Munich, Germany; Technical University of Munich, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2914014,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13848702289646644011&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "tum.de;infineon.com;in.tum.de; ; ",
        "email": "tum.de;infineon.com;in.tum.de; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0+1;0+1;0",
        "aff_unique_norm": "Technical University of Munich;Infineon Technologies AG",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tum.de;https://www.infineon.com/",
        "aff_unique_abbr": "TUM;Infineon",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Munich",
        "aff_country_unique_index": "0+0;0;0+0;0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "a7914ff9f3",
        "title": "AuxAdapt: Stable and Efficient Test-Time Adaptation for Temporally Consistent Video Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zhang_AuxAdapt_Stable_and_Efficient_Test-Time_Adaptation_for_Temporally_Consistent_Video_WACV_2022_paper.html",
        "author": "Yizhe Zhang; Shubhankar Borse; Hong Cai; Fatih Porikli",
        "abstract": "In video segmentation, generating temporally consistent results across frames is as important as achieving frame-wise accuracy. Existing methods rely either on optical flow regularization or fine-tuning with test data to attain temporal consistency. However, optical flow is not always avail-able and reliable. Besides, it is expensive to compute. Fine-tuning the original model in test time is cost sensitive. This paper presents an efficient, intuitive, and unsupervised online adaptation method, AuxAdapt, for improving the temporal consistency of most neural network models. It does not require optical flow and only takes one pass of the video. Since inconsistency mainly arises from the model's uncertainty in its output, we propose an adaptation scheme where the model learns from its own segmentation decisions as it streams a video, which allows producing more confident and temporally consistent labeling for similarly-looking pixels across frames. For stability and efficiency, we leverage a small auxiliary segmentation network (AuxNet) to assist with this adaptation. More specifically, AuxNet readjusts the decision of the original segmentation network (Main-Net) by adding its own estimations to that of MainNet. At every frame, only AuxNet is updated via back-propagation while keeping MainNet fixed. We extensively evaluate our test-time adaptation approach on standard video benchmarks, including Cityscapes, CamVid, and KITTI. The results demonstrate that our approach provides label-wise accurate, temporally consistent, and computationally efficient adaptation (5+ folds overhead reduction comparing to state-of-the-art test-time adaptation methods).",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zhang_AuxAdapt_Stable_and_Efficient_Test-Time_Adaptation_for_Temporally_Consistent_Video_WACV_2022_paper.pdf",
        "aff": "Qualcomm AI Research*\u2020; Qualcomm AI Research\u2021 \u00a7; Qualcomm AI Research\u2021 \u00a7; Qualcomm AI Research\u2021",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Zhang_AuxAdapt_Stable_and_WACV_2022_supplemental.pdf",
        "arxiv": "2110.12369",
        "pdf_size": 5550098,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14457673197786991325&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;qti.qualcomm.com;qti.qualcomm.com;qti.qualcomm.com",
        "email": "gmail.com;qti.qualcomm.com;qti.qualcomm.com;qti.qualcomm.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Qualcomm;Qualcomm AI Research",
        "aff_unique_dep": "Qualcomm AI Research;AI Research",
        "aff_unique_url": "https://www.qualcomm.com/research;https://www.qualcomm.com/research",
        "aff_unique_abbr": "Qualcomm AI Research;QAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9d6fb158f0",
        "title": "Batch Normalization Tells You Which Filter Is Important",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Oh_Batch_Normalization_Tells_You_Which_Filter_Is_Important_WACV_2022_paper.html",
        "author": "Junghun Oh; Heewon Kim; Sungyong Baik; Cheeun Hong; Kyoung Mu Lee",
        "abstract": "The goal of filter pruning is to search for unimportant filters to remove in order to make convolutional neural networks (CNNs) efficient without sacrificing the performance in the process. The challenge lies in finding information that can help determine how important or relevant each filter is with respect to the final output of neural networks. In this work, we share our observation that the batch normalization (BN) parameters of pre-trained CNNs can be used to estimate the feature distribution of activation outputs, without processing of training data. Upon observation, we propose a simple yet effective filter pruning method by evaluating the importance of each filter based on the BN parameters of pre-trained CNNs. The experimental results on CIFAR-10 and ImageNet demonstrate that the proposed method can achieve outstanding performance with and without fine-tuning in terms of the trade-off between the accuracy drop and the reduction in computational complexity and number of parameters of pruned networks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Oh_Batch_Normalization_Tells_You_Which_Filter_Is_Important_WACV_2022_paper.pdf",
        "aff": "Department of ECE, ASRI, Seoul National University; Department of ECE, ASRI, Seoul National University; Department of ECE, ASRI, Seoul National University; Department of ECE, ASRI, Seoul National University; Department of ECE, ASRI, Seoul National University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Oh_Batch_Normalization_Tells_WACV_2022_supplemental.pdf",
        "arxiv": "2112.01155",
        "pdf_size": 1798757,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10250659119727388043&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "d80b02164a",
        "title": "Bayesian Uncertainty and Expected Gradient Length - Regression: Two Sides of the Same Coin?",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Shukla_Bayesian_Uncertainty_and_Expected_Gradient_Length_-_Regression_Two_Sides_WACV_2022_paper.html",
        "author": "Megh Shukla",
        "abstract": "Active learning algorithms select a subset of data for annotation to maximize the model performance on a budget. One such algorithm is Expected Gradient Length, which as the name suggests uses the approximate gradient induced per example in the sampling process. While Expected Gradient Length has been successfully used for classification and regression, the formulation for regression remains intuitively driven. Hence, our theoretical contribution involves deriving this formulation, thereby supporting experimental evidence [4, 5]. Subsequently, we show that expected gradient length in regression is equivalent to Bayesian uncertainty [22]. If certain assumptions are infeasible, our algorithmic contribution (EGL++) approximates the effect of ensembles with a single deterministic network. Instead of computing multiple possible inferences per input, we leverage previously annotated samples to quantify the probability of previous labels being the true label. Such an approach allows us to extend expected gradient length to a new task: human pose estimation. We perform experimental validation on two human pose datasets (MPII and LSP/LSPET), highlighting the interpretability and competitiveness of EGL++ with different active learning algorithms for human pose estimation.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Shukla_Bayesian_Uncertainty_and_Expected_Gradient_Length_-_Regression_Two_Sides_WACV_2022_paper.pdf",
        "aff": "Mercedes-Benz Research and Development India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Shukla_Bayesian_Uncertainty_and_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5240655,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2007700534967226222&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "daimler.com",
        "email": "daimler.com",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Mercedes-Benz Research and Development India",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mercedes-benz.com/in/",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "India"
    },
    {
        "id": "a7daabf577",
        "title": "Beyond Mono to Binaural: Generating Binaural Audio From Mono Audio With Depth and Cross Modal Attention",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Parida_Beyond_Mono_to_Binaural_Generating_Binaural_Audio_From_Mono_Audio_WACV_2022_paper.html",
        "author": "Kranti Kumar Parida; Siddharth Srivastava; Gaurav Sharma",
        "abstract": "Binaural audio gives the listener an immersive experience and can enhance augmented and virtual reality. However, recording binaural audio requires specialized setup with a dummy human head having microphones in left and right ears. Such a recording setup is difficult to build and setup, therefore mono audio has become the preferred choice in common devices. To obtain the same impact as binaural audio, recent efforts have been directed towards lifting mono audio to binaural audio conditioned on the visual input from the scene. Such approaches have not used an important cue for the task: the distance of different sound producing objects from the microphones. In this work, we argue that depth map of the scene can act as a proxy for inducing distance information of different objects in the scene, for the task of audio binauralization. We propose a novel encoder-decoder architecture with a hierarchical attention mechanism to encode image, depth and audio feature jointly. We design the network on top of state-of-the-art transformer networks for image and depth representation. We show empirically that the proposed method outperforms state-of-the-art methods comfortably for two challenging public datasets FAIR-Play and MUSIC- Stereo. We also demonstrate with qualitative results that the method is able to focus on the right information required for the task. The qualitative results are available at our project page https://krantiparida.github.io/projects/bmonobinaural.html",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Parida_Beyond_Mono_to_Binaural_Generating_Binaural_Audio_From_Mono_Audio_WACV_2022_paper.pdf",
        "aff": "IIT Kanpur; CDAC Noida; IIT Kanpur + TensorTour Inc.",
        "project": "https://krantiparida.github.io/projects/bmonobinaural.html",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Parida_Beyond_Mono_to_WACV_2022_supplemental.pdf",
        "arxiv": "2111.08046",
        "pdf_size": 1393509,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1617400962369500634&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cse.iitk.ac.in;cdac.in;cse.iitk.ac.in",
        "email": "cse.iitk.ac.in;cdac.in;cse.iitk.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;Centre for Development of Advanced Computing;TensorTour Inc.",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.iitk.ac.in;https://www.cdac.in;",
        "aff_unique_abbr": "IITK;CDAC;",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Kanpur;Noida;",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2d7c701d07",
        "title": "BiHPF: Bilateral High-Pass Filters for Robust Deepfake Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Jeong_BiHPF_Bilateral_High-Pass_Filters_for_Robust_Deepfake_Detection_WACV_2022_paper.html",
        "author": "Yonghyun Jeong; Doyeon Kim; Seungjai Min; Seongho Joe; Youngjune Gwon; Jongwon Choi",
        "abstract": "The advancement in numerous generative models has a two-fold effect: a simple and easy generation of realistic synthesized images, but also an increased risk of malicious abuse of those images. Thus, it is important to develop a generalized detector for synthesized images of any GAN model or object category, including those unseen during the training phase. However, the conventional methods heavily depend on the training settings, which cause a dramatic decline in performance when tested with unknown domains. To resolve the issue and obtain a generalized detection ability, we propose Bilateral High-Pass Filters (BiHPF), which amplify the effect of the frequency-level artifacts that are generally found in the synthesized images of generative models. Also, to find the properties of the general frequency-level artifacts, we develop an additional method to adversarially extract the artifact compression map. Numerous experimental results validate that our method outperforms other state-of-the-art methods, even when tested with unseen domains.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Jeong_BiHPF_Bilateral_High-Pass_Filters_for_Robust_Deepfake_Detection_WACV_2022_paper.pdf",
        "aff": "Samsung SDS, Seoul, Korea; Samsung SDS, Seoul, Korea; Samsung SDS, Seoul, Korea; Samsung SDS, Seoul, Korea; Samsung SDS, Seoul, Korea; Dept. of Advanced Imaging, Chung-Ang University, Seoul, Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Jeong_BiHPF_Bilateral_High-Pass_WACV_2022_supplemental.zip",
        "arxiv": "2109.00911",
        "pdf_size": 883265,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3481382362325267125&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "samsung.com;samsung.com;samsung.com;samsung.com;samsung.com;cau.ac.kr",
        "email": "samsung.com;samsung.com;samsung.com;samsung.com;samsung.com;cau.ac.kr",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Samsung;Chung-Ang University",
        "aff_unique_dep": "Samsung SDS;Dept. of Advanced Imaging",
        "aff_unique_url": "https://www.samsungsds.com;http://www.cau.ac.kr",
        "aff_unique_abbr": "Samsung SDS;CAU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seoul",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "4188f6da7b",
        "title": "Billion-Scale Pretraining With Vision Transformers for Multi-Task Visual Representations",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Beal_Billion-Scale_Pretraining_With_Vision_Transformers_for_Multi-Task_Visual_Representations_WACV_2022_paper.html",
        "author": "Josh Beal; Hao-Yu Wu; Dong Huk Park; Andrew Zhai; Dmitry Kislyuk",
        "abstract": "Large-scale pretraining of visual representations has led to state-of-the-art performance on a range of benchmark computer vision tasks, yet the benefits of these techniques at extreme scale in complex production systems has been relatively unexplored. We consider the case of a popular visual discovery product, where these representations are trained with multi-task learning, from use-case specific visual understanding (e.g. skin tone classification) to general representation learning for all visual content (e.g. embeddings for retrieval). In this work, we describe how we (1) generate a dataset with over a billion images via large weakly-supervised pretraining to improve the performance of these visual representations, and (2) leverage Transformers to replace the traditional convolutional backbone, with insights into both system and performance improvements, especially at 1B+ image scale. To support this backbone model, we detail a systematic approach to deriving weakly-supervised image annotations from heterogenous text signals, demonstrating the benefits of clustering techniques to handle the long-tail distribution of image labels. Through a comprehensive study of offline and online evaluation, we show that large-scale Transformer-based pretraining provides significant benefits to industry computer vision applications. The model is deployed in a production visual shopping system, with 36% improvement in top-1 relevance and 23% improvement in click-through volume. We conduct extensive experiments to better understand the empirical relationships between Transformer-based architectures, dataset scale, and the performance of production vision systems.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Beal_Billion-Scale_Pretraining_With_Vision_Transformers_for_Multi-Task_Visual_Representations_WACV_2022_paper.pdf",
        "aff": "Pinterest; Pinterest; Pinterest; Pinterest; Pinterest",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Beal_Billion-Scale_Pretraining_With_WACV_2022_supplemental.pdf",
        "arxiv": "2108.05887",
        "pdf_size": 1297494,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9822679265470597549&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "pinterest.com;pinterest.com;pinterest.com;pinterest.com;pinterest.com",
        "email": "pinterest.com;pinterest.com;pinterest.com;pinterest.com;pinterest.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Pinterest",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.pinterest.com",
        "aff_unique_abbr": "Pinterest",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "391d52ce38",
        "title": "Biomass Prediction With 3D Point Clouds From LiDAR",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Pan_Biomass_Prediction_With_3D_Point_Clouds_From_LiDAR_WACV_2022_paper.html",
        "author": "Liyuan Pan; Liu Liu; Anthony G. Condon; Gonzalo M. Estavillo; Robert A. Coe; Geoff Bull; Eric A. Stone; Lars Petersson; Vivien Rolland",
        "abstract": "With population growth and a shrinking rural workforce, agricultural technologies have become increasingly important. Above-ground biomass (AGB) is a key trait relevant to breeding, agronomy and crop physiology field experiments. However, measuring the biomass of a cereal plot requires cutting, drying and weighing processes, which are laborious, expensive and destructive tasks. This paper proposes a non-destructive and high-throughput method to predict biomass from field samples based on Light Detection and Ranging (LiDAR). Unlike previous methods that are based on the density of a point cloud or plant height, our biomass prediction network (BioNet) additionally considers plant structure. Our BioNet contains three modules: 1) a completion module to predict missing points due to canopy occlusion; 2) a regularization module to regularize the neural representation of the whole plot; and 3) a projection module to learn the salient structures from a bird's eye view of the point cloud. An attention-based fusion block is used to achieve final biomass predictions. In addition, the complete dataset, including hand-measured biomass and LiDAR data, is made available to the community. Experiments show that our BioNet achieves approximately 33% improvement over current state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Pan_Biomass_Prediction_With_3D_Point_Clouds_From_LiDAR_WACV_2022_paper.pdf",
        "aff": "CSIRO A&F+ANU; ANU; CSIRO A&F+Australian Plant Phenomics Facility; CSIRO A&F; CSIRO A&F+Australian Plant Phenomics Facility; CSIRO A&F+Australian Plant Phenomics Facility; CSIRO A&F+ANU; CSIRO Data61; CSIRO A&F",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9195497,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9935513701995387653&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "csiro.au; ; ; ; ; ; ; ;csiro.au",
        "email": "csiro.au; ; ; ; ; ; ; ;csiro.au",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0+2;0;0+2;0+2;0+1;3;0",
        "aff_unique_norm": "Commonwealth Scientific and Industrial Research Organisation;Australian National University;Australian Plant Phenomics Facility;CSIRO",
        "aff_unique_dep": "Agriculture and Food;;;Data61",
        "aff_unique_url": "https://www.csiro.au;https://www.anu.edu.au;https://www.plantphenomics.org.au;https://www.csiro.au",
        "aff_unique_abbr": "CSIRO;ANU;APPF;CSIRO",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0;0+0;0+0;0+0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "479f23e9bf",
        "title": "Boosting Contrastive Self-Supervised Learning With False Negative Cancellation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Huynh_Boosting_Contrastive_Self-Supervised_Learning_With_False_Negative_Cancellation_WACV_2022_paper.html",
        "author": "Tri Huynh; Simon Kornblith; Matthew R. Walter; Michael Maire; Maryam Khademi",
        "abstract": "Self-supervised representation learning has made significant leaps fueled by progress in contrastive learning, which seeks to learn transformations that embed positive input pairs nearby, while pushing negative pairs far apart. While positive pairs can be generated reliably (e.g., as different views of the same image), it is difficult to accurately establish negative pairs, defined as samples from different images regardless of their semantic content or visual features. A fundamental problem in contrastive learning is mitigating the effects of false negatives. Contrasting false negatives induces two critical issues in representation learning: discarding semantic information and slow convergence. In this paper, we propose novel approaches to identify false negatives, as well as two strategies to mitigate their effect, i.e. false negative elimination and attraction, while systematically performing rigorous evaluations to study this problem in detail. Our method exhibits consistent improvements over existing contrastive learning-based methods. Without labels, we identify false negatives with  40% accuracy among 1000 semantic classes on ImageNet, and achieve 5.8% absolute improvement in top-1 accuracy over the previous state-of-the-art when finetuning with 1% labels.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Huynh_Boosting_Contrastive_Self-Supervised_Learning_With_False_Negative_Cancellation_WACV_2022_paper.pdf",
        "aff": "Google; Google Research; TTI-Chicago; University of Chicago; Google",
        "project": "",
        "github": "https://github.com/google-research/fnc",
        "supp": "",
        "arxiv": "2011.11765",
        "pdf_size": 1736454,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9603814736596946986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "google.com; ; ; ;google.com",
        "email": "google.com; ; ; ;google.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Google;Toyota Technological Institute at Chicago;University of Chicago",
        "aff_unique_dep": "Google;;",
        "aff_unique_url": "https://www.google.com;https://www.tti-chicago.org;https://www.uchicago.edu",
        "aff_unique_abbr": "Google;TTI;UChicago",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Mountain View;Chicago;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c08abdb96c",
        "title": "Busy-Quiet Video Disentangling for Video Classification",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Huang_Busy-Quiet_Video_Disentangling_for_Video_Classification_WACV_2022_paper.html",
        "author": "Guoxi Huang; Adrian G. Bors",
        "abstract": "In video data, busy motion details from moving regions are conveyed within a specific frequency bandwidth in the frequency domain. Meanwhile, the rest of the frequencies of video data are encoded with quiet information with substantial redundancy, which causes low processing efficiency in existing video models that take as input raw RGB frames. In this paper, we consider allocating intenser computation for the processing of the important busy information and less computation for that of the quiet information. We design a trainable Motion Band-Pass Module (MBPM) for separating busy information from quiet information in raw video data. By embedding the MBPM into a two-pathway CNN architecture, we define a Busy-Quiet Net (BQN). The efficiency of BQN is determined by avoiding redundancy in the feature space processed by the two pathways: one operating on Quiet features of low-resolution, while the other processes Busy features. The proposed BQN outperforms many recent video processing models on Something-Something V1, Kinetics400, UCF101 and HMDB51 datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Huang_Busy-Quiet_Video_Disentangling_for_Video_Classification_WACV_2022_paper.pdf",
        "aff": "Department of Computer Science, University of York, York YO10 5GH, UK; Department of Computer Science, University of York, York YO10 5GH, UK",
        "project": "",
        "github": "https://github.com/guoxih/busy-quiet-net",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Huang_Busy-Quiet_Video_Disentangling_WACV_2022_supplemental.pdf",
        "arxiv": "2103.15584",
        "pdf_size": 2841109,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8199640524917999879&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "york.ac.uk;york.ac.uk",
        "email": "york.ac.uk;york.ac.uk",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of York",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.york.ac.uk",
        "aff_unique_abbr": "York",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "fa58fb1086",
        "title": "C-VTON: Context-Driven Image-Based Virtual Try-On Network",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Fele_C-VTON_Context-Driven_Image-Based_Virtual_Try-On_Network_WACV_2022_paper.html",
        "author": "Benjamin Fele; Ajda Lampe; Peter Peer; Vitomir Struc",
        "abstract": "Image-based virtual try-on techniques have shown great promise for enhancing the user-experience and improving customer satisfaction on fashion-oriented e-commerce platforms. However, they are currently still limited in the quality of the try-on results they are able to produce from input images of diverse characteristics. In this work, we propose a Context-Driven Virtual Try-On Network (C-VTON) that addresses these limitations and convincingly transfers selected clothing items to the target subjects even under challenging pose configurations and in the presence of self-occlusions. At the core of the C-VTON pipeline are: (i) a geometric matching procedure that efficiently aligns the target clothing with the pose of the person in the input images, and (ii) a powerful image generator that utilizes various types of contextual information when synthesizing the final try-on result. C-VTON is evaluated in rigorous experiments on the VITON and MPV datasets and in comparison to state-of-the-art techniques from the literature. Experimental results show that the proposed approach is able to produce photo-realistic and visually convincing results and significantly improves on the existing state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Fele_C-VTON_Context-Driven_Image-Based_Virtual_Try-On_Network_WACV_2022_paper.pdf",
        "aff": "Faculty of Electrical Engineering + Faculty of Computer and Information Science; Faculty of Electrical Engineering + Faculty of Computer and Information Science; Faculty of Computer and Information Science; Faculty of Electrical Engineering",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Fele_C-VTON_Context-Driven_Image-Based_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6034542,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8730212571712667788&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "fe.uni-lj.si;fri.uni-lj.si;fri.uni-lj.si;fe.uni-lj.si",
        "email": "fe.uni-lj.si;fri.uni-lj.si;fri.uni-lj.si;fe.uni-lj.si",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;1;0",
        "aff_unique_norm": "Faculty of Electrical Engineering;Faculty of Computer and Information Science",
        "aff_unique_dep": "Electrical Engineering;Computer and Information Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "544b3e6f54",
        "title": "CFLOW-AD: Real-Time Unsupervised Anomaly Detection With Localization via Conditional Normalizing Flows",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gudovskiy_CFLOW-AD_Real-Time_Unsupervised_Anomaly_Detection_With_Localization_via_Conditional_Normalizing_WACV_2022_paper.html",
        "author": "Denis Gudovskiy; Shun Ishizaka; Kazuki Kozuka",
        "abstract": "Unsupervised anomaly detection with localization has many practical applications when labeling is infeasible and, moreover, when anomaly examples are completely missing in the train data. While recently proposed models for such data setup achieve high accuracy metrics, their complexity is a limiting factor for real-time processing. In this paper, we propose a real-time model and analytically derive its relationship to prior methods. Our CFLOW-AD model is based on a conditional normalizing flow framework adopted for anomaly detection with localization. In particular, CFLOW-AD consists of a discriminatively pretrained encoder followed by a multi-scale generative decoders where the latter explicitly estimate likelihood of the encoded features. Our approach results in a computationally and memory-efficient model: CFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art with the same input setting. Our experiments on the MVTec dataset show that CFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by 1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source our code with fully reproducible experiments.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gudovskiy_CFLOW-AD_Real-Time_Unsupervised_Anomaly_Detection_With_Localization_via_Conditional_Normalizing_WACV_2022_paper.pdf",
        "aff": "Panasonic AI Lab, USA; Panasonic Technology Division, Japan; Panasonic Technology Division, Japan",
        "project": "",
        "github": "github.com/gudovskiy/cflow-ad",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Gudovskiy_CFLOW-AD_Real-Time_Unsupervised_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3786420,
        "gs_citation": 585,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5644429611266863213&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "us.panasonic.com;jp.panasonic.com;jp.panasonic.com",
        "email": "us.panasonic.com;jp.panasonic.com;jp.panasonic.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Panasonic AI Lab;Panasonic",
        "aff_unique_dep": "AI Lab;Technology Division",
        "aff_unique_url": "https://www.panasonic.com/global/innovation/research-development/ai.html;https://panasonic.com",
        "aff_unique_abbr": "Panasonic AI Lab;Panasonic",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "8e7c7d0ef0",
        "title": "COCOA: Context-Conditional Adaptation for Recognizing Unseen Classes in Unseen Domains",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Mangla_COCOA_Context-Conditional_Adaptation_for_Recognizing_Unseen_Classes_in_Unseen_Domains_WACV_2022_paper.html",
        "author": "Puneet Mangla; Shivam Chandhok; Vineeth N Balasubramanian; Fahad Shahbaz Khan",
        "abstract": "Recent progress towards designing models that can generalize to unseen domains (i.e domain generalization) or unseen classes (i.e zero-shot learning) has embarked interest towards building models that can tackle both domain-shift and semantic shift simultaneously (i.e zero-shot domain generalization). For models to generalize to unseen classes in unseen domains, it is crucial to learn feature representation that preserves class-level (domain-invariant) as well as domain-specific information. Motivated from the success of generative zero-shot approaches, we propose a feature generative framework integrated with a COntext COnditional Adaptive (COCOA) Batch-Normalization layer to seamlessly integrate class-level semantic and domain-specific information. The generated visual features better capture the underlying data distribution enabling us to generalize to unseen classes and domains at test-time. We thoroughly evaluate our approach on established large-scale benchmarks -- DomainNet, DomainNet-LS (Limited Sources) -- as well as a new CUB-Corruptions benchmark, and demonstrate promising performance over baselines and state-of-the-art methods. We show detailed ablations and analysis to verify that our proposed approach indeed allows us to generate better quality visual features relevant for zero-shot domain generalization.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Mangla_COCOA_Context-Conditional_Adaptation_for_Recognizing_Unseen_Classes_in_Unseen_Domains_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Mangla_COCOA_Context-Conditional_Adaptation_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 941305,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14098472488690600941&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4355fb3006",
        "title": "Calibrating CNNs for Few-Shot Meta Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yang_Calibrating_CNNs_for_Few-Shot_Meta_Learning_WACV_2022_paper.html",
        "author": "Peng Yang; Shaogang Ren; Yang Zhao; Ping Li",
        "abstract": "Although few-shot meta learning has been extensively studied in machine learning community, the fast adaptation towards new tasks remains a challenge in the few-shot learning scenario. The neuroscience research reveals that the capability of evolving neural network formulation is essential for task adaptation, which has been broadly studied in recent meta-learning researches. In this paper, we present a novel forward-backward meta-learning framework (FBM) to facilitate the model generalization in few-shot learning from a new perspective, i.e., neuron calibration. In particular, FBM models the neurons in deep neural network-based model as calibrated units under a general formulation, where neuron calibration could empower fast adaptation capability to the neural network-based models through influencing both their forward inference path and backward propagation path. The proposed calibration scheme is lightweight and applicable to various feed-forward neural network architectures. Extensive empirical experiments on the challenging few-shot learning benchmarks validate that our approach training with neuron calibration achieves a promising performance, which demonstrates that neuron calibration plays a vital role in improving the few-shot learning performance.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yang_Calibrating_CNNs_for_Few-Shot_Meta_Learning_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1087581,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6796962547842873256&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "777cab349a",
        "title": "CeyMo: See More on Roads - A Novel Benchmark Dataset for Road Marking Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Jayasinghe_CeyMo_See_More_on_Roads_-_A_Novel_Benchmark_Dataset_WACV_2022_paper.html",
        "author": "Oshada Jayasinghe; Sahan Hemachandra; Damith Anhettigama; Shenali Kariyawasam; Ranga Rodrigo; Peshala Jayasekara",
        "abstract": "In this paper, we introduce a novel road marking benchmark dataset for road marking detection, addressing the limitations in the existing publicly available datasets such as lack of challenging scenarios, prominence given to lane markings, unavailability of an evaluation script, lack of annotation formats and lower resolutions. Our dataset consists of 2887 total images with 4706 road marking instances belonging to 11 classes. The images have a high resolution of 1920 x 1080 and capture a wide range of traffic, lighting and weather conditions. We provide road marking annotations in polygons, bounding boxes and pixel-level segmentation masks to facilitate a diverse range of road marking detection algorithms. The evaluation metrics and the evaluation script we provide, will further promote direct comparison of novel approaches for road marking detection with existing methods. Furthermore, we evaluate the effectiveness of using both instance segmentation and object detection based approaches for the road marking detection task. Speed and accuracy scores for two instance segmentation models and two object detector models are provided as a performance baseline for our benchmark dataset. The dataset and the evaluation script is publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Jayasinghe_CeyMo_See_More_on_Roads_-_A_Novel_Benchmark_Dataset_WACV_2022_paper.pdf",
        "aff": "Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka",
        "project": "",
        "github": "https://github.com/oshadajay/CeyMo",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8513178,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17074936537345290523&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;uom.lk;uom.lk",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;uom.lk;uom.lk",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Moratuwa",
        "aff_unique_dep": "Department of Electronic and Telecommunication Engineering",
        "aff_unique_url": "https://www.mrt.ac.lk",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Sri Lanka"
    },
    {
        "id": "51d0ba3a77",
        "title": "Challenges in Procedural Multimodal Machine Comprehension: A Novel Way To Benchmark",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sahu_Challenges_in_Procedural_Multimodal_Machine_Comprehension_A_Novel_Way_To_WACV_2022_paper.html",
        "author": "Pritish Sahu; Karan Sikka; Ajay Divakaran",
        "abstract": "We focus on Multimodal Machine Reading Comprehension (M3C) where a model is expected to answer questions based on given passage (or context), and the context and the questions can be in different modalities. Previous works such as RecipeQA have proposed datasets and cloze-style tasks for evaluation. However, we identify three critical biases stemming from the question-answer generation process and memorization capabilities of large deep models. These biases makes it easier for a model to overfit by relying on spurious correlations or naive data patterns. We propose a systematic framework to address these biases through three Control-Knobs that enable us to generate a test bed of datasets of progressive difficulty levels. We believe that our benchmark (referred to as Meta- RecipeQA) will provide, for the first time, a fine grained estimate of a model's generalization capabilities. We also propose a generalM3C model that is used to realize several prior SOTA models and motivate a novel hierarchical transformer based reasoning network (HTRN). We perform a detailed evaluation of these models with different language and visual features on our benchmark. We observe a consistent improvement with HTRN over SOTA (  18% in Visual Cloze task and   13% in average over all the tasks). We also observe a drop in performance across all the models when testing on RecipeQA and proposed Meta-RecipeQA (e.g. 83.6% versus 67.1% for HTRN), which shows that the proposed dataset is relatively less biased. We conclude by highlighting the impact of the control knobs with some quantitative results.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sahu_Challenges_in_Procedural_Multimodal_Machine_Comprehension_A_Novel_Way_To_WACV_2022_paper.pdf",
        "aff": "SRI International+Rutgers University; SRI International; SRI International",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Sahu_Challenges_in_Procedural_WACV_2022_supplemental.pdf",
        "arxiv": "2110.11899",
        "pdf_size": 1052648,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2524635712680884162&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "rutgers.edu;sri.com;sri.com",
        "email": "rutgers.edu;sri.com;sri.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "SRI International;Rutgers University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sri.com;https://www.rutgers.edu",
        "aff_unique_abbr": "SRI;Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1c6ee9880a",
        "title": "Channel Pruning via Lookahead Search Guided Reinforcement Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wang_Channel_Pruning_via_Lookahead_Search_Guided_Reinforcement_Learning_WACV_2022_paper.html",
        "author": "Zi Wang; Chengcheng Li",
        "abstract": "Channel pruning has become an effective yet still challenging approach to achieve compact neural networks. It aims to prune the optimal set of filters whose removal results in minimal performance degradation of the slimmed network. Due to the prohibitively vast search space of filter combinations, existing approaches usually use various criteria to estimate the filter importance while sacrificing some precision. Here we present a new approach to optimizing the filter selection in channel pruning with lookahead search guided reinforcement learning (RL). A neural network that takes as input filter-related features is trained with RL to prune the optimal sequence of filters and maximize the performance of the remaining network. In addition, we employ Monte Carlo tree search (MCTS) to provide a lookahead search for filter selection, which increases the sample efficiency for the RL training. Experiments on MNIST, CIFAR-10, and ILSVRC-2012 validate the effectiveness of our approach compared to both traditional and automated existing channel pruning approaches.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wang_Channel_Pruning_via_Lookahead_Search_Guided_Reinforcement_Learning_WACV_2022_paper.pdf",
        "aff": "University of Tennessee, Knoxville, TN, USA; University of Tennessee, Knoxville, TN, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Wang_Channel_Pruning_via_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 598620,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17240213683235797645&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "vols.utk.edu;vols.utk.edu",
        "email": "vols.utk.edu;vols.utk.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tennessee",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utk.edu",
        "aff_unique_abbr": "UT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Knoxville",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7bf2626286",
        "title": "CharacterGAN: Few-Shot Keypoint Character Animation and Reposing",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hinz_CharacterGAN_Few-Shot_Keypoint_Character_Animation_and_Reposing_WACV_2022_paper.html",
        "author": "Tobias Hinz; Matthew Fisher; Oliver Wang; Eli Shechtman; Stefan Wermter",
        "abstract": "We introduce CharacterGAN, a generative model that can be trained on only a few samples (8 - 15) of a given character. Our model generates novel poses based on keypoint locations, which can be modified in real time while providing interactive feedback, allowing for intuitive reposing and animation. Since we only have very limited training samples, one of the key challenges lies in how to address (dis)occlusions, e.g. when a hand moves behind or in front of a body. To address this, we introduce a novel layering approach which explicitly splits the input keypoints into different layers which are processed independently. These layers represent different parts of the character and provide a strong implicit bias that helps to obtain realistic results even with strong (dis)occlusions. To combine the features of individual layers we use an adaptive scaling approach conditioned on all keypoints. Finally, we introduce a mask connectivity constraint to reduce distortion artifacts that occur with extreme out-of-distribution poses at test time. We show that our approach outperforms recent baselines and creates realistic animations for diverse characters. We also show that our model can handle discrete state changes, for example a profile facing left or right, that the different layers do indeed learn features specific for the respective keypoints in those layers, and that our model scales to larger datasets when more data is available. Code is available at https://github.com/tohinz/CharacterGAN.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hinz_CharacterGAN_Few-Shot_Keypoint_Character_Animation_and_Reposing_WACV_2022_paper.pdf",
        "aff": "University of Hamburg + Adobe Research; Adobe Research; Adobe Research; Adobe Research; University of Hamburg",
        "project": "",
        "github": "https://github.com/tohinz/CharacterGAN",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hinz_CharacterGAN_Few-Shot_Keypoint_WACV_2022_supplemental.pdf",
        "arxiv": "2102.03141",
        "pdf_size": 15903226,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3805749653633997396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;1;0",
        "aff_unique_norm": "University of Hamburg;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.uni-hamburg.de;https://research.adobe.com",
        "aff_unique_abbr": "UHH;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "adcb3e1e96",
        "title": "Class-Balanced Active Learning for Image Classification",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bengar_Class-Balanced_Active_Learning_for_Image_Classification_WACV_2022_paper.html",
        "author": "Javad Zolfaghari Bengar; Joost van de Weijer; Laura Lopez Fuentes; Bogdan Raducanu",
        "abstract": "Active learning aims to reduce the labeling effort that is required to train algorithms by learning an acquisition function selecting the most relevant data for which a label should be requested from a large unlabeled data pool. Active learning is generally studied on balanced datasets where an equal amount of images per class is available. However, real-world datasets suffer from severe imbalanced classes, the so called long-tail distribution. We argue that this further complicates the active learning process, since the imbalanced data pool can result in suboptimal classifiers. To address this problem in the context of active learning, we proposed a general optimization framework that explicitly takes class-balancing into account. Results on three datasets showed that the method is general (it can be combined with most existing active learning algorithms) and can be effectively applied to boost the performance of both informative and representative-based active learning methods. In addition, we showed that also on balanced datasets our method generally results in a performance gain.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bengar_Class-Balanced_Active_Learning_for_Image_Classification_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "https://github.com/Javadzb/Class-Balanced-AL.git",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Bengar_Class-Balanced_Active_Learning_WACV_2022_supplemental.pdf",
        "arxiv": "2110.04543",
        "pdf_size": 756749,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6743589452522543848&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "57c89a8301",
        "title": "Cleaning Noisy Labels by Negative Ensemble Learning for Source-Free Unsupervised Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ahmed_Cleaning_Noisy_Labels_by_Negative_Ensemble_Learning_for_Source-Free_Unsupervised_WACV_2022_paper.html",
        "author": "Waqar Ahmed; Pietro Morerio; Vittorio Murino",
        "abstract": "Conventional Unsupervised Domain Adaptation (UDA) methods presume source and target domain data to be simultaneously available during training. Such an assumption may not hold in practice, as source data is often inaccessible (e.g., due to privacy reasons). On the contrary, a pre-trained source model is usually available, which performs poorly on target due to the well-known domain shift problem. This translates into a significant amount of misclassifications, which can be interpreted as structured noise affecting the inferred target pseudo-labels. In this work, we cast UDA as a pseudo-label refinery problem in the challenging source-free scenario. We propose Negative Ensemble Learning (NEL) technique, a unified method for adaptive noise filtering and progressive pseudo-label refinement. NEL is devised to tackle noisy pseudo-labels by enhancing diversity in ensemble members with different stochastic (i) input augmentation and (ii) feedback. The latter is achieved by leveraging the novel concept of Disjoint Residual Labels, which allow propagating diverse information to the different members. Eventually, a single model is trained with the refined pseudo-labels, which leads to a robust performance on the target domain. Extensive experiments show that the proposed method achieves state-of-the-art performance on major UDA benchmarks, such as Digit5, PACS, Visda-C, and DomainNet, without using source data samples at all.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ahmed_Cleaning_Noisy_Labels_by_Negative_Ensemble_Learning_for_Source-Free_Unsupervised_WACV_2022_paper.pdf",
        "aff": "Pattern Analysis & Computer Vision, Istituto Italiano di Tecnologia, Genova, Italy + Dipartimento di Ingegneria Navale, Elettrica, Elettronica e delle Telecomunicazioni, University of Genova, Italy; Pattern Analysis & Computer Vision, Istituto Italiano di Tecnologia, Genova, Italy; Pattern Analysis & Computer Vision, Istituto Italiano di Tecnologia, Genova, Italy + Dipartimento di Informatica, University of Verona, Italy",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ahmed_Cleaning_Noisy_Labels_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4948461,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17709195455145675886&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "iit.it;iit.it;iit.it",
        "email": "iit.it;iit.it;iit.it",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0+2",
        "aff_unique_norm": "Istituto Italiano di Tecnologia;University of Genova;University of Verona",
        "aff_unique_dep": "Pattern Analysis & Computer Vision;Dipartimento di Ingegneria Navale, Elettrica, Elettronica e delle Telecomunicazioni;Dipartimento di Informatica",
        "aff_unique_url": "https://www.iit.it;https://www.unige.it;https://www.univr.it",
        "aff_unique_abbr": "IIT;;UniVR",
        "aff_campus_unique_index": "0;0;0+2",
        "aff_campus_unique": "Genova;;Verona",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "8798321f7a",
        "title": "Co-Net: A Collaborative Region-Contour-Driven Network for Fine-to-Finer Medical Image Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Liu_Co-Net_A_Collaborative_Region-Contour-Driven_Network_for_Fine-to-Finer_Medical_Image_Segmentation_WACV_2022_paper.html",
        "author": "Anran Liu; Xiangsheng Huang; Tong Li; Pengcheng Ma",
        "abstract": "In this paper, a fine-to-finer segmentation task is investigated driven by region and contour features collaboratively on Glomerular Electron-Dense Deposits (GEDD) in view of the complementary nature of these two types of features. To this end, a novel network (Co-Net) is presented to dynamically use fine saliency segmentation to guide finer segmentation on boundaries. The whole architecture contains double mutually boosted decoders sharing one common encoder. Specifically, a new structure named Global-guided Interaction Module (GIM) is designed to effectively control the information flow and reduce redundancy in the cross-level feature fusion process. At the same time, the global features are used in it to make the features of each layer gain access to richer context, and a fine segmentation map is obtained initially; Discontinuous Boundary Supervision (DBS) strategy is applied to pay more attention to discontinuity positions and modifying segmentation errors on boundaries. At last, Selective Kernel (SK) is used for dynamical aggregation of the region and contour features to obtain a finer segmentation. Our proposed approach is evaluated on an independent GEDD dataset labeled by pathologists and also on open polyp datasets to test the generalization. Ablation studies show the effectiveness of different modules. On all datasets, our proposal achieves high segmentation accuracy and surpasses previous methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Liu_Co-Net_A_Collaborative_Region-Contour-Driven_Network_for_Fine-to-Finer_Medical_Image_Segmentation_WACV_2022_paper.pdf",
        "aff": "School of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an, China+Institute of Automation, Chinese Academy of Science, Beijing, China; Institute of Automation, Chinese Academy of Science, Beijing, China; School of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an, China+Institute of Automation, Chinese Academy of Science, Beijing, China; Institute of Automation, Chinese Academy of Science, Beijing, China+Department of Radiation Oncology, Nanfang Hospital, Southern Medical University, Guangzhou, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1555513,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17076708954319377732&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "xjtu.edu.cn;ucas.ac.cn;xjtu.edu.cn;smu.edu.cn",
        "email": "xjtu.edu.cn;ucas.ac.cn;xjtu.edu.cn;smu.edu.cn",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0+1;1+2",
        "aff_unique_norm": "Xi'an Jiao Tong University;Chinese Academy of Sciences;Southern Medical University",
        "aff_unique_dep": "School of Mathematics and Statistics;Institute of Automation;Department of Radiation Oncology",
        "aff_unique_url": "http://en.xjtu.edu.cn/;http://www.ia.cas.cn;http://www.fsmu.edu.cn",
        "aff_unique_abbr": "XJTU;CAS;SMU",
        "aff_campus_unique_index": "0+1;1;0+1;1+2",
        "aff_campus_unique": "Xi'an;Beijing;Guangzhou",
        "aff_country_unique_index": "0+0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "6ebd48a603",
        "title": "Co-Segmentation Aided Two-Stream Architecture for Video Captioning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Vaidya_Co-Segmentation_Aided_Two-Stream_Architecture_for_Video_Captioning_WACV_2022_paper.html",
        "author": "Jayesh Vaidya; Arulkumar Subramaniam; Anurag Mittal",
        "abstract": "The goal of video captioning is to generate captions for a video by understanding visual and temporal cues. A general video captioning model consists of an Encoder-Decoder framework where Encoder generally captures the visual and temporal information while the decoder generates captions. Recent works have incorporated object-level information into the Encoder by a pretrained off-the-shelf object detector, significantly improving performance. However, using an object detector comes with the following downsides: 1) object detectors may not exhaustively capture all the object categories. 2) In a realistic setting, the performance may be influenced by the domain gap between the object detector and the visual-captioning dataset. To remedy this, we argue that using an external object detector could be eliminated if the model is equipped with the capability of automatically finding salient regions. To achieve this, we propose a novel architecture that learns to attend to salient regions such as objects, persons automatically using a co-segmentation inspired attention module. Then, we utilize a novel salient region interaction module to promote information propagation between salient regions of adjacent frames. Further, we incorporate this salient region-level information into the model using knowledge distillation. We evaluate our model on two benchmark datasets MSR-VTT and MSVD, and show that our model achieves competitive performance without using any object detector.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Vaidya_Co-Segmentation_Aided_Two-Stream_Architecture_for_Video_Captioning_WACV_2022_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Indian Institute of Technology Madras; Department of Computer Science and Engineering, Indian Institute of Technology Madras; Department of Computer Science and Engineering, Indian Institute of Technology Madras",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1496002,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12022547281840715794&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cse.iitm.ac.in;cse.iitm.ac.in;cse.iitm.ac.in",
        "email": "cse.iitm.ac.in;cse.iitm.ac.in;cse.iitm.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Madras",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitm.ac.in",
        "aff_unique_abbr": "IIT Madras",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Madras",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "66235d0ab4",
        "title": "Compensation Tracker: Reprocessing Lost Object for Multi-Object Tracking",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zou_Compensation_Tracker_Reprocessing_Lost_Object_for_Multi-Object_Tracking_WACV_2022_paper.html",
        "author": "Zhibo Zou; Junjie Huang; Ping Luo",
        "abstract": "Tracking by detection paradigm is one of the most popular object tracking methods. However, it is very dependent on the performance of the detector. When the detector has a behavior of missing detection, the tracking result will be directly affected. In this paper, we analyze the phenomenon of the lost tracking object in real-time tracking model on MOT2020 dataset. Based on simple and traditional methods, we propose a compensation tracker to further alleviate the lost tracking problem caused by missing detection. It consists of a motion compensation module and an object selection module. The proposed method not only can re-track missing tracking objects from lost objects, but also does not require additional networks so as to maintain speed-accuracy trade-off of the real-time model. Our method only needs to be embedded into the tracker to work without re-training the network. Experiments show that the compensation tracker can efficaciously improve the performance of the model and reduce identity switches. With limited costs, the compensation tracker successfully enhances the baseline tracking performance by a large margin and reaches 66% of MOTA and 67% of IDF1 on MOT2020 dataset.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zou_Compensation_Tracker_Reprocessing_Lost_Object_for_Multi-Object_Tracking_WACV_2022_paper.pdf",
        "aff": "School of Automation, Chongqing University of Posts and Telecommunications, Chongqing 400065, China; School of Automation, Chongqing University of Posts and Telecommunications, Chongqing 400065, China; School of Automation, Chongqing University of Posts and Telecommunications, Chongqing 400065, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5409185,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16962316583928480970&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "stu.cqupt.edu.cn;stu.cqupt.edu.cn;cqupt.edu.cn",
        "email": "stu.cqupt.edu.cn;stu.cqupt.edu.cn;cqupt.edu.cn",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Chongqing University of Posts and Telecommunications",
        "aff_unique_dep": "School of Automation",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chongqing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "ccbb26eba7",
        "title": "Complete Face Recovery GAN: Unsupervised Joint Face Rotation and De-Occlusion From a Single-View Image",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ju_Complete_Face_Recovery_GAN_Unsupervised_Joint_Face_Rotation_and_De-Occlusion_WACV_2022_paper.html",
        "author": "Yeong-Joon Ju; Gun-Hee Lee; Jung-Ho Hong; Seong-Whan Lee",
        "abstract": "Although various face-related tasks have significantly advanced in recent years, occlusion and extreme pose still impede the achievement of higher performance. Existing face rotation or de-occlusion methods only have emphasized the aspect of each problem. In addition, the lack of high-quality paired data remains an obstacle for both methods. In this work, we present a self-supervision strategy called Swap-R&R to overcome the lack of ground-truth in a fully unsupervised manner for joint face rotation and de-occlusion. To generate an input pair for self-supervision, we transfer the occlusion from a face in an image to an estimated 3D face and create a damaged face image, as if rotated from a different pose by rotating twice with the roughly de-occluded face. Furthermore, we propose Complete Face Recovery GAN (CFR-GAN) to restore the collapsed textures and disappeared occlusion areas by leveraging the structural and textural differences between two rendered images. Unlike previous works, which have selected occlusion-free images to obtain ground-truths, our approach does not require human intervention and paired data. We show that our proposed method can generate a de-occluded frontal face image from an occluded profile face image. Moreover, extensive experiments demonstrate that our approach can boost the performance of facial recognition and facial expression recognition. The code is publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ju_Complete_Face_Recovery_GAN_Unsupervised_Joint_Face_Rotation_and_De-Occlusion_WACV_2022_paper.pdf",
        "aff": "Department of Artificial Intelligence, Korea University, Seoul, South Korea; Department of Computer and Radio Communications Engineering, Korea University, Seoul, South Korea; Department of Artificial Intelligence, Korea University, Seoul, South Korea; Department of Artificial Intelligence, Korea University, Seoul, South Korea",
        "project": "",
        "github": "https://github.com/yeongjoonJu/CFR-GAN",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ju_Complete_Face_Recovery_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 10551032,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16599877910925556613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "email": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Korea University",
        "aff_unique_dep": "Department of Artificial Intelligence",
        "aff_unique_url": "https://www.korea.ac.kr",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "0201693823",
        "title": "Compressed Sensing MRI Reconstruction With Co-VeGAN: Complex-Valued Generative Adversarial Network",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Vasudeva_Compressed_Sensing_MRI_Reconstruction_With_Co-VeGAN_Complex-Valued_Generative_Adversarial_Network_WACV_2022_paper.html",
        "author": "Bhavya Vasudeva; Puneesh Deora; Saumik Bhattacharya; Pyari Mohan Pradhan",
        "abstract": "Compressed sensing (CS) is extensively used to reduce magnetic resonance imaging (MRI) acquisition time. State-of-the-art deep learning-based methods have proven effective in obtaining fast, high-quality reconstruction of CS-MR images. However, they treat the inherently complex-valued MRI data as real-valued entities by extracting the magnitude content or concatenating the complex-valued data as two real-valued channels for processing. In both cases, the phase content is discarded. To address the fundamental problem of real-valued deep networks, i.e. their inability to process complex-valued data, we propose a complex-valued generative adversarial network (Co-VeGAN) framework, which is the first-of-its-kind generative model exploring the use of complex-valued weights and operations. Further, since real-valued activation functions do not generalize well to the complex-valued space, we propose a novel complex-valued activation function that is sensitive to the input phase and has a learnable profile. Extensive evaluation of the proposed approach on different datasets demonstrates that it significantly outperforms the existing CS-MRI reconstruction techniques.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Vasudeva_Compressed_Sensing_MRI_Reconstruction_With_Co-VeGAN_Complex-Valued_Generative_Adversarial_Network_WACV_2022_paper.pdf",
        "aff": "ISI Kolkata; ISI Kolkata; IIT Kharagpur; IIT Roorkee",
        "project": "",
        "github": "https://github.com/estija/Co-VeGAN",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Vasudeva_Compressed_Sensing_MRI_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1659082,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13111571786862255488&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Indian Statistical Institute;Indian Institute of Technology Kharagpur;Indian Institute of Technology Roorkee",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.isical.ac.in;https://www.iitkgp.ac.in;https://www.iitr.ac.in",
        "aff_unique_abbr": "ISI;IIT KGP;IITR",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Kolkata;Kharagpur;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "b1b670f20a",
        "title": "Consistent Cell Tracking in Multi-Frames With Spatio-Temporal Context by Object-Level Warping Loss",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hayashida_Consistent_Cell_Tracking_in_Multi-Frames_With_Spatio-Temporal_Context_by_Object-Level_WACV_2022_paper.html",
        "author": "Junya Hayashida; Kazuya Nishimura; Ryoma Bise",
        "abstract": "Multi-object tracking is essential in biomedical image analysis. Most multi-object tracking methods follow a tracking-by-detection approach that involves using object detectors and learning the appearance feature models of the detected regions for association. Although these methods can learn the appearance similarity features to identify the same objects among frames, they have difficulties identifying the same cells because cells have a similar appearance and their shapes change as they migrate. In addition, cells often partially overlap for several frames. In this case, even an expert biologist would require knowledge of the spatial-temporal context in order to identify individual cells. To tackle such difficult situations, we propose a cell-tracking method that can effectively use the spatial-temporal context in multiple frames by using long-term motion estimation and an object-level warping loss. We conducted experiments showing that the proposed method outperformed state-of-the-art methods under various conditions on real biological images.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hayashida_Consistent_Cell_Tracking_in_Multi-Frames_With_Spatio-Temporal_Context_by_Object-Level_WACV_2022_paper.pdf",
        "aff": "Kyushu University; Kyushu University; Kyushu University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5286402,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16793400641094885242&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ait.kyushu-u.ac.jp; ; ",
        "email": "ait.kyushu-u.ac.jp; ; ",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Kyushu University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kyushu-u.ac.jp",
        "aff_unique_abbr": "Kyushu U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "bc2bf98c8c",
        "title": "Contextual Gradient Scaling for Few-Shot Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lee_Contextual_Gradient_Scaling_for_Few-Shot_Learning_WACV_2022_paper.html",
        "author": "Sanghyuk Lee; Seunghyun Lee; Byung Cheol Song",
        "abstract": "Model-agnostic meta-learning (MAML) is a well-known optimization-based meta-learning algorithm that works well in various computer vision tasks, e.g., few-shot classification. MAML is to learn an initialization so that a model can adapt to a new task in a few steps. However, since the gradient norm of a classifier (head) is much bigger than those of backbone layers, the model focuses on learning the decision boundary of the classifier with similar representations. Furthermore, gradient norms of high-level layers are small than those of the other layers. So, the backbone of MAML usually learns task-generic features, which results in deteriorated adaptation performance in the inner-loop. To resolve or mitigate this problem, we propose contextual gradient scaling (CxGrad), which scales gradient norms of the backbone to facilitate learning task-specific knowledge in the inner-loop. Since the scaling factors are generated from task-conditioned parameters, gradient norms of the backbone can be scaled in a task-wise fashion. Experimental results show that CxGrad effectively encourages the backbone to learn task-specific knowledge in the inner-loop and improves the performance of MAML up to a significant margin in both same- and cross-domain few-shot classification.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lee_Contextual_Gradient_Scaling_for_Few-Shot_Learning_WACV_2022_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, Inha University; Department of Electrical and Computer Engineering, Inha University; Department of Electrical and Computer Engineering, Inha University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Lee_Contextual_Gradient_Scaling_WACV_2022_supplemental.pdf",
        "arxiv": "2110.10353",
        "pdf_size": 1603151,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12960184019299561890&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;gmail.com;inha.ac.kr",
        "email": "gmail.com;gmail.com;inha.ac.kr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Inha University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.inha.edu",
        "aff_unique_abbr": "Inha",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "0072c9129c",
        "title": "Contextual Proposal Network for Action Localization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hsieh_Contextual_Proposal_Network_for_Action_Localization_WACV_2022_paper.html",
        "author": "He-Yen Hsieh; Ding-Jie Chen; Tyng-Luh Liu",
        "abstract": "This paper investigates the problem of Temporal Action Proposal (TAP) generation, which aims to provide a set of high-quality video segments that potentially contain actions events locating in long untrimmed videos. Based on the goal to distill available contextual information, we introduce a Contextual Proposal Network (CPN) composing of two context-aware mechanisms. The first mechanism, i.e., feature enhancing, integrates the inception-like module with long-range attention to capture the multi-scale temporal contexts for yielding a robust video segment representation. The second mechanism, i.e., boundary scoring, employs the bi-directional recurrent neural networks (RNN) to capture bi-directional temporal contexts that explicitly model actionness, background, and confidence of proposals. While generating and scoring proposals, such bi-directional temporal contexts are helpful to retrieve high-quality proposals of low false positives for covering the video action instances. We conduct experiments on two challenging datasets of ActivityNet-1.3 and THUMOS-14 to demonstrate the effectiveness of the proposed Contextual Proposal Network (CPN). In particular, our method respectively surpasses state-of-the-art TAP methods by 1.54% AUC on ActivityNet-1.3 test split and by 0.61% AR@200 on THUMOS-14 dataset.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hsieh_Contextual_Proposal_Network_for_Action_Localization_WACV_2022_paper.pdf",
        "aff": "Institute of Information Science, Academia Sinica, Taiwan; Institute of Information Science, Academia Sinica, Taiwan; Institute of Information Science, Academia Sinica, Taiwan",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hsieh_Contextual_Proposal_Network_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1614344,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13263686042659478786&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "iis.sinica.edu.tw;gmail.com;iis.sinica.edu.tw",
        "email": "iis.sinica.edu.tw;gmail.com;iis.sinica.edu.tw",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Academia Sinica",
        "aff_unique_dep": "Institute of Information Science",
        "aff_unique_url": "https://www.sinica.edu.tw",
        "aff_unique_abbr": "AS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "382a36d1f6",
        "title": "Contrast To Divide: Self-Supervised Pre-Training for Learning With Noisy Labels",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zheltonozhskii_Contrast_To_Divide_Self-Supervised_Pre-Training_for_Learning_With_Noisy_Labels_WACV_2022_paper.html",
        "author": "Evgenii Zheltonozhskii; Chaim Baskin; Avi Mendelson; Alex M. Bronstein; Or Litany",
        "abstract": "The success of learning with noisy labels (LNL) methods relies heavily on the success of a warm-up stage where standard supervised training is performed using the full (noisy) training set. In this paper, we identify a \"warm-up obstacle\": the inability of standard warm-up stages to train high quality feature extractors and avert memorization of noisy labels. We propose \"Contrast to Divide\" (C2D), a simple framework that solves this problem by pre-training the feature extractor in a self-supervised fashion. Using self-supervised pre-training boosts the performance of existing LNL approaches by drastically reducing the warm-up stage's susceptibility to noise level, shortening its duration, and improving extracted feature quality. C2D works out of the box with existing methods and demonstrates markedly improved performance, especially in the high noise regime, where we get a boost of more than 27% for CIFAR-100 with 90% noise over the previous state of the art. In real-life noise settings, C2D trained on mini-WebVision outperforms previous works both in WebVision and ImageNet validation sets by 3% top-1 accuracy. We perform an in-depth analysis of the framework, including investigating the performance of different pre-training approaches and estimating the effective upper bound of the LNL performance with semi-supervised learning. Code for reproducing our experiments is available at https://github.com/ContrastToDivide/C2D",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zheltonozhskii_Contrast_To_Divide_Self-Supervised_Pre-Training_for_Learning_With_Noisy_Labels_WACV_2022_paper.pdf",
        "aff": "Technion - Israel Institute of Technology; Technion - Israel Institute of Technology; Technion - Israel Institute of Technology; Technion - Israel Institute of Technology; NVIDIA",
        "project": "",
        "github": "https://github.com/ContrastToDivide/C2D",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Zheltonozhskii_Contrast_To_Divide_WACV_2022_supplemental.pdf",
        "arxiv": "2103.13646",
        "pdf_size": 17323531,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18164524478326052564&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "campus.technion.ac.il;campus.technion.ac.il;technion.ac.il;cs.technion.ac.il;gmail.com",
        "email": "campus.technion.ac.il;campus.technion.ac.il;technion.ac.il;cs.technion.ac.il;gmail.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;NVIDIA",
        "aff_unique_dep": ";NVIDIA Corporation",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://www.nvidia.com",
        "aff_unique_abbr": "Technion;NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "1d21d9d945",
        "title": "Controlled GAN-Based Creature Synthesis via a Challenging Game Art Dataset - Addressing the Noise-Latent Trade-Off",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Vavilala_Controlled_GAN-Based_Creature_Synthesis_via_a_Challenging_Game_Art_Dataset_WACV_2022_paper.html",
        "author": "Vaibhav Vavilala; David Forsyth",
        "abstract": "The state-of-the-art StyleGAN2 network supports powerful methods to create and edit art, including generating random images, finding images \"like\" some query, and modifying content or style. Further, recent advancements enable training with small datasets. We apply these methods to synthesize card art, by training on a novel Yu-Gi-Oh dataset. While noise inputs to StyleGAN2 are essential for good synthesis, we find that coarse-scale noise interferes with latent variables on this dataset because both control long-scale image effects. We observe over-aggressive variation in art with changes in noise and weak content control via latent variable edits. Here, we demonstrate that training a modified StyleGAN2, where coarse-scale noise is suppressed, removes these unwanted effects. We obtain a superior FID; changes in noise result in local exploration of style; and identity control is markedly improved. These results and analysis lead towards a GAN-assisted art synthesis tool for digital artists of all skill levels, which can be used in film, games, or any creative industry for artistic ideation.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Vavilala_Controlled_GAN-Based_Creature_Synthesis_via_a_Challenging_Game_Art_Dataset_WACV_2022_paper.pdf",
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10089610,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6175539894426235405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f304776a6f",
        "title": "CoordiNet: Uncertainty-Aware Pose Regressor for Reliable Vehicle Localization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Moreau_CoordiNet_Uncertainty-Aware_Pose_Regressor_for_Reliable_Vehicle_Localization_WACV_2022_paper.html",
        "author": "Arthur Moreau; Nathan Piasco; Dzmitry Tsishkou; Bogdan Stanciulescu; Arnaud de La Fortelle",
        "abstract": "In this paper, we investigate visual-based camera relocalization with neural networks for robotics and autonomous vehicles applications. Our solution is a CNN-based algorithm which predicts camera pose (3D translation and 3D rotation) directly from a single image. It also provides an uncertainty estimate of the pose. Pose and uncertainty are learned together with a single loss function and are fused at test time with an EKF. Furthermore, we propose a new fully convolutional architecture, named CoordiNet, designed to embed some of the scene geometry. Our framework outperforms comparable methods on the largest available benchmark, the Oxford RobotCar dataset, with an average error of 8 meters where previous best was 19 meters. We have also investigated the performance of our method on large scenes for real time (18 fps) vehicle localization. In this setup, structure-based methods require a large database, and we show that our proposal is a reliable alternative, achieving 29cm median error in a 1.9km loop in a busy urban area.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Moreau_CoordiNet_Uncertainty-Aware_Pose_Regressor_for_Reliable_Vehicle_Localization_WACV_2022_paper.pdf",
        "aff": "IoV team, Paris Research Center, Huawei Technologies France + MINES ParisTech, PSL University, Center for robotics; IoV team, Paris Research Center, Huawei Technologies France; IoV team, Paris Research Center, Huawei Technologies France; MINES ParisTech, PSL University, Center for robotics; MINES ParisTech, PSL University, Center for robotics",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2103.10796",
        "pdf_size": 6668718,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10897621977622672540&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "mines-paristech.fr; ; ; ; ",
        "email": "mines-paristech.fr; ; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;1;1",
        "aff_unique_norm": "Huawei;MINES ParisTech",
        "aff_unique_dep": "IoV team;Center for robotics",
        "aff_unique_url": "https://www.huawei.com/fr;https://www.mines-paristech.fr",
        "aff_unique_abbr": "Huawei;MPT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "f9dabe8013",
        "title": "Coupled Training for Multi-Source Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Amosy_Coupled_Training_for_Multi-Source_Domain_Adaptation_WACV_2022_paper.html",
        "author": "Ohad Amosy; Gal Chechik",
        "abstract": "Unsupervised domain adaptation is often addressed by learning a joint representation of labeled samples from a source domain and unlabeled samples from a target domain. Unfortunately, hard sharing of representation may hurt adaptation because of negative transfer, where features that are useful for source domains are learned even if they hurt inference on the target domain. Here, we propose an alternative, soft sharing scheme. We train separate but weakly-coupled models for the source and the target data, while encouraging their predictions to agree. Training the two coupled models jointly effectively exploits the distribution over unlabeled target data and achieves high accuracy on the target. Specifically, we show analytically and empirically that the decision boundaries of the target model converge to low-density \"valleys\" of the target distribution. We evaluate our approach on four multi-source domain adaptation (MSDA) benchmarks, digits, amazon text reviews, Office-Caltech, and images (DomainNet). We find that it consistently outperforms current MSDA SoTA, sometimes by a very large margin.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Amosy_Coupled_Training_for_Multi-Source_Domain_Adaptation_WACV_2022_paper.pdf",
        "aff": "Bar-Ilan University, Israel; Bar-Ilan University, Israel + NVIDIA Research, Israel",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Amosy_Coupled_Training_for_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1507499,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7368177663568613166&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "biu.ac.il;biu.ac.il",
        "email": "biu.ac.il;biu.ac.il",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Bar-Ilan University;NVIDIA",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.biu.ac.il;https://research.nvidia.com",
        "aff_unique_abbr": "BIU;NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "1aa2353505",
        "title": "Creating and Reenacting Controllable 3D Humans With Differentiable Rendering",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gomes_Creating_and_Reenacting_Controllable_3D_Humans_With_Differentiable_Rendering_WACV_2022_paper.html",
        "author": "Thiago L. Gomes; Thiago M. Coutinho; Rafael Azevedo; Renato Martins; Erickson R. Nascimento",
        "abstract": "This paper proposes a new end-to-end neural rendering architecture to transfer appearance and reenact human actors. Our method leverages a carefully designed graph convolutional network (GCN) to model the human body manifold structure, jointly with differentiable rendering, to synthesize new videos of people in different contexts from where they were initially recorded. Unlike recent appearance transferring methods, our approach can reconstruct a fully controllable 3D texture-mapped model of a person, while taking into account the manifold structure from body shape and texture appearance in the view synthesis. Specifically, our approach models mesh deformations with a three-stage GCN trained in a self-supervised manner on rendered silhouettes of the human body. It also infers texture appearance with a convolutional network in the texture domain, which is trained in an adversarial regime to reconstruct human texture from rendered images of actors in different poses. Experiments on different videos show that our method successfully infers specific body deformations and avoid creating texture artifacts while achieving the best values for appearance in terms of Structural Similarity (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), Mean Squared Error (MSE), and Frechet Video Distance (FVD). By taking advantages of both differentiable rendering and the 3D parametric model, our method is fully controllable, which allows controlling the human synthesis from both pose and rendering parameters. The source code is available at https://www.verlab.dcc.ufmg.br/retargeting-motion/wacv2022.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gomes_Creating_and_Reenacting_Controllable_3D_Humans_With_Differentiable_Rendering_WACV_2022_paper.pdf",
        "aff": "Universidade Federal de Minas Gerais (UFMG), Brazil; Universidade Federal de Minas Gerais (UFMG), Brazil; Universidade Federal de Minas Gerais (UFMG), Brazil; Universit Bourgogne Franche-Comt, France; Universidade Federal de Minas Gerais (UFMG), Brazil",
        "project": "https://www.verlab.dcc.ufmg.br/retargeting-motion/wacv2022",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Gomes_Creating_and_Reenacting_WACV_2022_supplemental.zip",
        "arxiv": "2110.11746",
        "pdf_size": 6292839,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6974043528342954467&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "dcc.ufmg.br;dcc.ufmg.br;dcc.ufmg.br;u-bourgogne.fr;dcc.ufmg.br",
        "email": "dcc.ufmg.br;dcc.ufmg.br;dcc.ufmg.br;u-bourgogne.fr;dcc.ufmg.br",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Universidade Federal de Minas Gerais;University of Burgundy",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ufmg.br;https://www.ubourgogne.fr",
        "aff_unique_abbr": "UFMG;UBFC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Brazil;France"
    },
    {
        "id": "0a3a642bd3",
        "title": "Cross-Modal Adversarial Reprogramming",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Neekhara_Cross-Modal_Adversarial_Reprogramming_WACV_2022_paper.html",
        "author": "Paarth Neekhara; Shehzeen Hussain; Jinglong Du; Shlomo Dubnov; Farinaz Koushanfar; Julian McAuley",
        "abstract": "With the abundance of large-scale deep learning models, it has become possible to repurpose pre-trained networks for new tasks. Recent works on adversarial reprogramming have shown that it is possible to repurpose neural networks for alternate tasks without modifying the network architecture or parameters. However these works only consider original and target tasks within the same data domain. In this work, we broaden the scope of adversarial reprogramming beyond the data modality of the original task. We analyze the feasibility of adversarially repurposing image classification neural networks for Natural Language Processing (NLP) and other sequence classification tasks. We design an efficient adversarial program that maps a sequence of discrete tokens into an image which can be classified to the desired class by an image classification model. We demonstrate that by using highly efficient adversarial programs, we can reprogram image classifiers to achieve competitive performance on a variety of text and sequence classification benchmarks without retraining the network.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Neekhara_Cross-Modal_Adversarial_Reprogramming_WACV_2022_paper.pdf",
        "aff": "University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Neekhara_Cross-Modal_Adversarial_Reprogramming_WACV_2022_supplemental.pdf",
        "arxiv": "2102.07325",
        "pdf_size": 1019522,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1027403710474518762&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ucsd.edu;ucsd.edu; ; ; ; ",
        "email": "ucsd.edu;ucsd.edu; ; ; ; ",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3f5cfd8b68",
        "title": "CrossLocate: Cross-Modal Large-Scale Visual Geo-Localization in Natural Environments Using Rendered Modalities",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tomesek_CrossLocate_Cross-Modal_Large-Scale_Visual_Geo-Localization_in_Natural_Environments_Using_Rendered_WACV_2022_paper.html",
        "author": "Jan Tome\u0161ek; Martin \u010cad\u00edk; Jan Brejcha",
        "abstract": "We propose a novel approach to visual geo-localization in natural environments. This is a challenging problem due to vast localization areas, the variable appearance of outdoor environments and the scarcity of available data. In order to make the research of new approaches possible, we first create two databases containing \"synthetic\" images of various modalities. These image modalities are rendered from a 3D terrain model and include semantic segmentations, silhouette maps and depth maps. By combining the rendered database views with existing datasets of photographs (used as \"queries\" to be localized), we create a unique benchmark for visual geo-localization in natural environments, which contains correspondences between query photographs and rendered database imagery. The distinct ability to match photographs to synthetically rendered databases defines our task as \"cross-modal\". On top of this benchmark, we provide thorough ablation studies analysing the localization potential of the database image modalities. We reveal the depth information as the best choice for outdoor localization. Finally, based on our observations, we carefully develop a fully-automatic method for large-scale cross-modal localization using image retrieval. We demonstrate its localization performance outdoors in the entire state of Switzerland. Our method reveals a large gap between operating within a single image domain (e.g. photographs) and working across domains (e.g. photographs matched to rendered images), as gained knowledge is not transferable between the two. Moreover, we show that modern localization methods fail when applied to such a cross-modal task and that our method achieves significantly better results than state-of-the-art approaches. The datasets, code and trained models are available on the project website: http://cphoto.fit.vutbr.cz/crosslocate/.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tomesek_CrossLocate_Cross-Modal_Large-Scale_Visual_Geo-Localization_in_Natural_Environments_Using_Rendered_WACV_2022_paper.pdf",
        "aff": "Brno University of Technology, Faculty of Information Technology, CPhoto@FIT; Brno University of Technology, Faculty of Information Technology, CPhoto@FIT; Brno University of Technology, Faculty of Information Technology, CPhoto@FIT",
        "project": "http://cphoto.fit.vutbr.cz/crosslocate/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Tomesek_CrossLocate_Cross-Modal_Large-Scale_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5559081,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6408686330438726997&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff_domain": "fit.vutbr.cz;fit.vutbr.cz;fit.vutbr.cz",
        "email": "fit.vutbr.cz;fit.vutbr.cz;fit.vutbr.cz",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brno University of Technology",
        "aff_unique_dep": "Faculty of Information Technology",
        "aff_unique_url": "https://www.fit.vutbr.cz",
        "aff_unique_abbr": "Brno UoT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Czech Republic"
    },
    {
        "id": "03bf2f97c1",
        "title": "D2Conv3D: Dynamic Dilated Convolutions for Object Segmentation in Videos",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Schmidt_D2Conv3D_Dynamic_Dilated_Convolutions_for_Object_Segmentation_in_Videos_WACV_2022_paper.html",
        "author": "Christian Schmidt; Ali Athar; Sabarinath Mahadevan; Bastian Leibe",
        "abstract": "Despite receiving significant attention from the research community, the task of segmenting and tracking objects in monocular videos still has much room for improvement. Existing works have simultaneously justified the efficacy of dilated and deformable convolutions for various image-level segmentation tasks. This gives reason to believe that 3D extensions of such convolutions should also yield performance improvements for video-level segmentation tasks. However, this aspect has not yet been explored thoroughly in existing literature. In this paper, we propose Dynamic Dilated Convolutions (D2Conv3D): a novel type of convolution which draws inspiration from dilated and deformable convolutions and extends them to the 3D (spatio-temporal) domain. We experimentally show that D2Conv3D can be used to improve the performance of multiple 3D CNN architectures across multiple video segmentation related benchmarks by simply employing D2Conv3D as a drop-in replacement for standard convolutions. We further show that D2Conv3D out-performs trivial extensions of existing dilated and deformable convolutions to 3D. Lastly, we set a new state-of-the-art on the DAVIS 2016 Unsupervised Video Object Segmentation benchmark. Code is made publicly available at https://github.com/Schmiddo/d2conv3d.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Schmidt_D2Conv3D_Dynamic_Dilated_Convolutions_for_Object_Segmentation_in_Videos_WACV_2022_paper.pdf",
        "aff": "Computer Vision Group, RWTH Aachen University; Computer Vision Group, RWTH Aachen University; Computer Vision Group, RWTH Aachen University; Computer Vision Group, RWTH Aachen University",
        "project": "",
        "github": "https://github.com/Schmiddo/d2conv3d",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Schmidt_D2Conv3D_Dynamic_Dilated_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 9087141,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3116842529613551677&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de",
        "email": "rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "RWTH Aachen University",
        "aff_unique_dep": "Computer Vision Group",
        "aff_unique_url": "https://www.rwth-aachen.de",
        "aff_unique_abbr": "RWTH",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Aachen",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "7fedc4cb6e",
        "title": "DAD: Data-Free Adversarial Defense at Test Time",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Nayak_DAD_Data-Free_Adversarial_Defense_at_Test_Time_WACV_2022_paper.html",
        "author": "Gaurav Kumar Nayak; Ruchit Rawal; Anirban Chakraborty",
        "abstract": "Deep models are highly susceptible to adversarial attacks. Such attacks are carefully crafted imperceptible noises that can fool the network and can cause severe consequences when deployed. To encounter them, the model requires training data for adversarial training or explicit regularization-based techniques. However, privacy has become an important concern, restricting access to only trained models but not the training data (e.g. biometric data). Also, data curation is expensive and companies may have proprietary rights over it. To handle such situations, we propose a completely novel problem of \"test-time adversarial defense in absence of training data and even their statistics\". We solve it in two stages: a) detection and b) correction of adversarial samples. Our adversarial sample detection framework is initially trained on arbitrary data and is subsequently adapted to the unlabelled test data through unsupervised domain adaptation. We further correct the predictions on detected adversarial samples by transforming them in Fourier domain and obtaining their low frequency component at our proposed suitable radius for model prediction. We demonstrate the efficacy of our proposed technique via extensive experiments against several adversarial attacks and for different model architectures and datasets. For a non-robust Resnet-18 model pretrained on CIFAR-10, our detection method correctly identifies 91.42% adversaries. Also, we significantly improve the adversarial accuracy from 0% to 37.37% with a minimal drop of 0.02% in clean accuracy on state-of-the-art \"Auto Attack\" without having to retrain the model.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Nayak_DAD_Data-Free_Adversarial_Defense_at_Test_Time_WACV_2022_paper.pdf",
        "aff": "Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India; Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India; Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Nayak_DAD_Data-Free_Adversarial_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1105495,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10752371513645112917&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "email": "iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computational and Data Sciences",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "326a5b4777",
        "title": "DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hong_DAQ_Channel-Wise_Distribution-Aware_Quantization_for_Deep_Image_Super-Resolution_Networks_WACV_2022_paper.html",
        "author": "Cheeun Hong; Heewon Kim; Sungyong Baik; Junghun Oh; Kyoung Mu Lee",
        "abstract": "Since the resurgence of deep neural networks (DNNs), image super-resolution (SR) has recently seen a huge progress in improving the quality of low resolution images, however at the great cost of computations and resources. Recently, there has been several efforts to make DNNs more efficient via quantization. However, SR demands pixel-level accuracy in the system, it is more difficult to perform quantization without significantly sacrificing SR performance. To this end, we introduce a new ultra-low precision yet effective quantization approach specifically designed for SR. In particular, we observe that in recent SR networks, each channel has different distribution characteristics. Thus we propose a channel-wise distribution-aware quantization scheme. Experimental results demonstrate that our proposed quantization, dubbed Distribution-Aware Quantization (DAQ), manages to greatly reduce the computational and resource costs without the significant sacrifice in SR performance, compared to other quantization methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hong_DAQ_Channel-Wise_Distribution-Aware_Quantization_for_Deep_Image_Super-Resolution_Networks_WACV_2022_paper.pdf",
        "aff": "Department of ECE, ASRI, Seoul National University; Department of ECE, ASRI, Seoul National University; Department of ECE, ASRI, Seoul National University; Department of ECE, ASRI, Seoul National University; Department of ECE, ASRI, Seoul National University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hong_DAQ_Channel-Wise_Distribution-Aware_WACV_2022_supplemental.pdf",
        "arxiv": "2012.11230",
        "pdf_size": 2506378,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8144856821225926732&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "06d5574bb6",
        "title": "DG-Labeler and DGL-MOTS Dataset: Boost the Autonomous Driving Perception",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Cui_DG-Labeler_and_DGL-MOTS_Dataset_Boost_the_Autonomous_Driving_Perception_WACV_2022_paper.html",
        "author": "Yiming Cui; Zhiwen Cao; Yixin Xie; Xingyu Jiang; Feng Tao; Yingjie Victor Chen; Lin Li; Dongfang Liu",
        "abstract": "Multi-object tracking and segmentation (MOTS) is a critical task for autonomous driving applications. The existing MOTS studies face two critical challenges: 1) the published datasets inadequately capture the real-world complexity for network training to address various driving settings; 2) the working pipeline annotation tool is under-studied in the literature to improve the quality of MOTS learning examples. In this work, we introduce the DG-Labeler and DGL-MOTS dataset to facilitate the training data annotation for the MOST task and accordingly improve network training accuracy and efficiency. To the best of our knowledge, our DG-Labeler is the first tool publicly available for MOTS data annotation. DG-Labeler uses the novel Depth-Granularity Module to depict the instance spatial relations and produce fine-grained instance masks. Annotated by DG-Labeler, our DGL-MOTS dataset exceeds the prior effort (i.e., KITTI MOTS and BDD100K) in data diversity, annotation quality, and temporal representations. Results on extensive cross-dataset evaluations indicate significant performance improvements for several state-of-the-art methods trained on our DGL-MOTS dataset. We believe our DGL-MOTS Dataset and DG-Labeler hold valuable potential to boost the visual perception of future transportation. Our dataset and code are available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Cui_DG-Labeler_and_DGL-MOTS_Dataset_Boost_the_Autonomous_Driving_Perception_WACV_2022_paper.pdf",
        "aff": "University of Florida; Purdue University; The University of Texas at El Paso; Purdue University; The University of Texas at San Antonio; Purdue University; The University of Texas at El Paso; Rochester Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.07790",
        "pdf_size": 6306123,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13405777810120847233&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ufl.edu;purdue.edu;miners.utep.edu;purdue.edu;my.utsa.edu;purdue.edu;utep.edu;rit.edu",
        "email": "ufl.edu;purdue.edu;miners.utep.edu;purdue.edu;my.utsa.edu;purdue.edu;utep.edu;rit.edu",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1;3;1;2;4",
        "aff_unique_norm": "University of Florida;Purdue University;University of Texas at El Paso;University of Texas at San Antonio;Rochester Institute of Technology",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.ufl.edu;https://www.purdue.edu;https://www.utep.edu;https://www.utsa.edu;https://www.rit.edu",
        "aff_unique_abbr": "UF;Purdue;UTEP;UTSA;RIT",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";El Paso;San Antonio",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "59f3d3b9fc",
        "title": "Danish Fungi 2020 - Not Just Another Image Recognition Dataset",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Picek_Danish_Fungi_2020_-_Not_Just_Another_Image_Recognition_Dataset_WACV_2022_paper.html",
        "author": "Luk\u00e1\u0161 Picek; Milan \u0160ulc; Ji\u0159\u00ed Matas; Thomas S. Jeppesen; Jacob Heilmann-Clausen; Thomas L\u00e6ss\u00f8e; Tobias Fr\u00f8slev",
        "abstract": "We introduce a novel fine-grained dataset and benchmark, the Danish Fungi 2020 (DF20). The dataset, constructed from observations submitted to the Atlas of Danish Fungi, is unique in its taxonomy-accurate class labels, small number of errors, highly unbalanced long-tailed class distribution, rich observation metadata, and well-defined class hierarchy. DF20 has zero overlap with ImageNet, allowing unbiased comparison of models fine-tuned from publicly available ImageNet checkpoints. The proposed evaluation protocol enables testing the ability to improve classification using metadata - e.g. precise geographic location, habitat, and substrate, facilitates classifier calibration testing, and finally allows to study the impact of the device settings on the classification performance. Experiments using Convolutional Neural Networks (CNN) and the recent Vision Transformers (ViT) show that DF20 presents a challenging task. Interestingly, ViT achieves results superior to CNN baselines with 80.45% accuracy and 0.743 macro F1 score, reducing the CNN error by 9% and 12% respectively. A simple procedure for including metadata into the decision process improves the classification accuracy by more than 2.95 percentage points, reducing the error rate by 15%. The source code for all methods and experiments is available at https://sites.google.com/view/danish-fungi-dataset.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Picek_Danish_Fungi_2020_-_Not_Just_Another_Image_Recognition_Dataset_WACV_2022_paper.pdf",
        "aff": "University of West Bohemia; CTU in Prague; CTU in Prague; GBIF; University of Copenhagen; University of Copenhagen; University of Copenhagen",
        "project": "https://sites.google.com/view/danish-fungi-dataset",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10249839,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10065264158824080026&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "kky.zcu.cz;fel.cvut.cz;fel.cvut.cz;gbif.org;snm.ku.dk;bio.ku.dk;sund.ku.dk",
        "email": "kky.zcu.cz;fel.cvut.cz;fel.cvut.cz;gbif.org;snm.ku.dk;bio.ku.dk;sund.ku.dk",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2;3;3;3",
        "aff_unique_norm": "University of West Bohemia;Czech Technical University;Global Biodiversity Information Facility;University of Copenhagen",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.zcu.cz;https://www.ctu.cz;https://www.gbif.org;https://www.ku.dk",
        "aff_unique_abbr": "ZCU;CTU;GBIF;UCPH",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Prague",
        "aff_country_unique_index": "0;0;0;1;1;1;1",
        "aff_country_unique": "Czech Republic;Denmark"
    },
    {
        "id": "54d4f99d1e",
        "title": "Data Augmented 3D Semantic Scene Completion With 2D Segmentation Priors",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Dourado_Data_Augmented_3D_Semantic_Scene_Completion_With_2D_Segmentation_Priors_WACV_2022_paper.html",
        "author": "Aloisio Dourado; Frederico Guth; Teofilo de Campos",
        "abstract": "Semantic scene completion (SSC) is a challenging Computer Vision task with many practical applications, from robotics to assistive computing. Its goal is to infer the 3D geometry in a field of view of a scene and the semantic labels of voxels, including occluded regions. In this work, we present SPAwN, a novel lightweight multimodal 3D deep CNN that seamlessly fuses structural data from the depth component of RGB-D images with semantic priors from a bimodal 2D segmentation network. A crucial difficulty in this field is the lack of fully labeled real-world 3D datasets which are large enough to train the current data-hungry deep 3D CNNs. In 2D computer vision tasks, many data augmentation strategies have been proposed to improve the generalization ability of CNNs. However those approaches cannot be directly applied to the RGB-D input and output volume of SSC solutions. In this paper, we introduce the use of a 3D data augmentation strategy that can be applied to multimodal SSC networks. We validate our contributions with a comprehensive and reproducible ablation study. Our solution consistently surpasses previous works with a similar level of complexity.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Dourado_Data_Augmented_3D_Semantic_Scene_Completion_With_2D_Segmentation_Priors_WACV_2022_paper.pdf",
        "aff": "University of Brasilia; University of Brasilia; University of Brasilia",
        "project": "https://cic.unb.br/~teodecampos/aloisio/3781",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Dourado_Data_Augmented_3D_WACV_2022_supplemental.pdf",
        "arxiv": "2111.13309",
        "pdf_size": 2197003,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17924282636091886663&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;fredguth.com;oxfordalumni.org",
        "email": "gmail.com;fredguth.com;oxfordalumni.org",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Brasilia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unb.br",
        "aff_unique_abbr": "UNB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Brazil"
    },
    {
        "id": "7f269cc31b",
        "title": "Data InStance Prior (DISP) in Generative Adversarial Networks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Mangla_Data_InStance_Prior_DISP_in_Generative_Adversarial_Networks_WACV_2022_paper.html",
        "author": "Puneet Mangla; Nupur Kumari; Mayank Singh; Balaji Krishnamurthy; Vineeth N. Balasubramanian",
        "abstract": "Recent advances in generative adversarial networks (GANs) have shown remarkable progress in generating high-quality images. However, this gain in performance depends on the availability of a large amount of training data. In limited data regimes, training typically diverges, and therefore the generated samples are of low quality and lack diversity. Previous works have addressed training in low data setting by leveraging transfer learning and data augmentation techniques. We propose a novel transfer learning method for GANs in the limited data domain by leveraging informative data prior derived from self-supervised/supervised pre-trained networks trained on a diverse source domain. We perform experiments on several standard vision datasets using various GAN architectures (BigGAN, SNGAN, StyleGAN2) to demonstrate that the proposed method effectively transfers knowledge to domains with few target images, outperforming existing state-of-the-art techniques in terms of image quality and diversity. We also show the utility of data instance prior in large-scale unconditional image generation.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Mangla_Data_InStance_Prior_DISP_in_Generative_Adversarial_Networks_WACV_2022_paper.pdf",
        "aff": "Adobe; CMU+Adobe; CMU+Adobe; Media and Data Science Research lab, Adobe; IIT Hyderabad, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Mangla_Data_InStance_Prior_WACV_2022_supplemental.pdf",
        "arxiv": "2012.04256",
        "pdf_size": 6418017,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2388384577652643688&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;gmail.com;gmail.com;adobe.com;iith.ac.in",
        "email": "gmail.com;gmail.com;gmail.com;adobe.com;iith.ac.in",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;1+0;0;2",
        "aff_unique_norm": "Adobe;Carnegie Mellon University;Indian Institute of Technology Hyderabad",
        "aff_unique_dep": "Adobe Inc.;;",
        "aff_unique_url": "https://www.adobe.com;https://www.cmu.edu;https://www.iith.ac.in",
        "aff_unique_abbr": "Adobe;CMU;IIT Hyderabad",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Hyderabad",
        "aff_country_unique_index": "0;0+0;0+0;0;1",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "5f3d56ddd1",
        "title": "Dataset Knowledge Transfer for Class-Incremental Learning Without Memory",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Slim_Dataset_Knowledge_Transfer_for_Class-Incremental_Learning_Without_Memory_WACV_2022_paper.html",
        "author": "Habib Slim; Eden Belouadah; Adrian Popescu; Darian Onchis",
        "abstract": "Incremental learning enables artificial agents to learn from sequential data. While important progress was made by exploiting deep neural networks, incremental learning remains very challenging. This is particularly the case when no memory of past data is allowed and catastrophic forgetting has a strong negative effect. We tackle class-incremental learning without memory by adapting prediction bias correction, a method which makes predictions of past and new classes more comparable. It was proposed when a memory is allowed and cannot be directly used without memory, since samples of past classes are required. We introduce a two-step learning process which allows the transfer of bias correction parameters between reference and target datasets. Bias correction is first optimized offline on reference datasets which have an associated validation memory. The obtained correction parameters are then transferred to target datasets, for which no memory is available. The second contribution is to introduce a finer modeling of bias correction by learning its parameters per incremental state instead of the usual past vs. new class modeling. The proposed dataset knowledge transfer is applicable to any incremental method which works without memory. We test its effectiveness by applying it to four existing methods. Evaluation with four target datasets and different configurations shows consistent improvement, with practically no computational and memory overhead.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Slim_Dataset_Knowledge_Transfer_for_Class-Incremental_Learning_Without_Memory_WACV_2022_paper.pdf",
        "aff": "Universit \u00b4e Paris-Saclay, CEA, List, F-91120, Palaiseau, France; Universit \u00b4e Paris-Saclay, CEA, List, F-91120, Palaiseau, France + IMT Atlantique, Lab-STICC, team RAMBO, UMR CNRS 6285, F-29328, Brest, France; Universit \u00b4e Paris-Saclay, CEA, List, F-91120, Palaiseau, France; West University of Timisoara, Timisoara, Romania",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Slim_Dataset_Knowledge_Transfer_WACV_2022_supplemental.pdf",
        "arxiv": "2110.08421",
        "pdf_size": 749791,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13464914416373576871&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff_domain": "grenoble-inp.org;cea.fr;cea.fr;e-uvt.ro",
        "email": "grenoble-inp.org;cea.fr;cea.fr;e-uvt.ro",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;2",
        "aff_unique_norm": "Universit\u00e9 Paris-Saclay;IMT Atlantique;West University of Timisoara",
        "aff_unique_dep": "CEA, List;Lab-STICC;",
        "aff_unique_url": "https://www.universite-paris-saclay.fr;https://www.imt-atlantique.fr;https://www.uvt.ro",
        "aff_unique_abbr": "UPS;IMT Atlantique;",
        "aff_campus_unique_index": "0;0+1;0;2",
        "aff_campus_unique": "Palaiseau;Brest;Timisoara",
        "aff_country_unique_index": "0;0+0;0;1",
        "aff_country_unique": "France;Romania"
    },
    {
        "id": "908603c42d",
        "title": "Deep Feature Prior Guided Face Deblurring",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Jung_Deep_Feature_Prior_Guided_Face_Deblurring_WACV_2022_paper.html",
        "author": "Soo Hyun Jung; Tae Bok Lee; Yong Seok Heo",
        "abstract": "Most recent face deblurring methods have focused on utilizing facial shape priors such as face landmarks and parsing maps. While these priors can provide facial geometric cues effectively, they are insufficient to contain local texture details that act as important clues to solve face deblurring problem. To deal with this, we focus on estimating the deep features of pre-trained face recognition networks (e.g., VGGFace network) that include rich information about sharp faces as a prior, and adopt a generative adversarial network (GAN) to learn it. To this end, we propose a deep feature prior guided network (DFPGnet) that restores facial details using the estimated the deep feature prior from a blurred image. In our DFPGnet, the generator is divided into two streams including prior estimation and deblurring streams. Since the estimated deep features of the prior estimation stream are learned from the VGGFace network which is trained for face recognition not for deblurring, we need to alleviate the discrepancy of feature distributions between the two streams. Therefore, we present feature transform modules at the connecting points of the two streams. In addition, we propose a channel-attention feature discriminator and prior loss, which encourages the generator to focus on more important channels for deblurring among the deep feature prior during training. Experimental results show that our method achieves state-of-the-art performance both qualitatively and quantitatively.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Jung_Deep_Feature_Prior_Guided_Face_Deblurring_WACV_2022_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, Ajou University, South Korea + Department of Artificial Intelligence, Ajou University, South Korea; Department of Artificial Intelligence, Ajou University, South Korea; Department of Electrical and Computer Engineering, Ajou University, South Korea + Department of Artificial Intelligence, Ajou University, South Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Jung_Deep_Feature_Prior_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3225070,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11782693113670203234&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "ajou.ac.kr;ajou.ac.kr;ajou.ac.kr",
        "email": "ajou.ac.kr;ajou.ac.kr;ajou.ac.kr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0;0+0",
        "aff_unique_norm": "Ajou University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ajou.ac.kr",
        "aff_unique_abbr": "Ajou",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "5c458474bd",
        "title": "Deep Online Fused Video Stabilization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Shi_Deep_Online_Fused_Video_Stabilization_WACV_2022_paper.html",
        "author": "Zhenmei Shi; Fuhao Shi; Wei-Sheng Lai; Chia-Kai Liang; Yingyu Liang",
        "abstract": "We present a deep neural network (DNN) that uses both sensor data (gyroscope) and image content (optical flow) to stabilize videos through unsupervised learning. The network fuses optical flow with real/virtual camera pose histories into a joint motion representation. Next, the LSTM cell infers the new virtual camera pose, which is used to generate a warping grid that stabilizes the video frames. We adopt a relative motion representation as well as a multi-stage training strategy to optimize our model without any supervision. To the best of our knowledge, this is the first DNN solution that adopts both sensor data and image content for video stabilization. We validate the proposed framework through ablation studies and demonstrate that the proposed method outperforms the state-of-art alternative solutions via quantitative evaluations and a user study. Check out our video results, code and dataset at our website.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Shi_Deep_Online_Fused_Video_Stabilization_WACV_2022_paper.pdf",
        "aff": "University of Wisconsin Madison; Google; Google; Google; University of Wisconsin Madison",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Shi_Deep_Online_Fused_WACV_2022_supplemental.pdf",
        "arxiv": "2102.01279",
        "pdf_size": 7267881,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1951360539811180532&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Wisconsin-Madison;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.wisc.edu;https://www.google.com",
        "aff_unique_abbr": "UW-Madison;Google",
        "aff_campus_unique_index": "0;1;1;1;0",
        "aff_campus_unique": "Madison;Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0b1854bbde",
        "title": "Deep Optimization Prior for THz Model Parameter Estimation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wong_Deep_Optimization_Prior_for_THz_Model_Parameter_Estimation_WACV_2022_paper.html",
        "author": "Tak Ming Wong; Hartmut Bauermeister; Matthias Kahl; Peter Haring Bol\u00edvar; Michael M\u00f6ller; Andreas Kolb",
        "abstract": "In this paper, we propose a deep optimization prior approach with application to the estimation of material-related model parameters from terahertz (THz) data that is acquired using a Frequency Modulated Continuous Wave (FMCW) THz scanning system. A stable estimation of the THz model parameters for low SNR and shot noise configurations is essential to achieve acquisition times required for applications in, e.g., quality control. Conceptually, our deep optimization prior approach estimates the desired THz model parameters by optimizing for the weights of a neural network. While such a technique was shown to improve the reconstruction quality for convex objectives in the seminal work of Ulyanov et. al., our paper demonstrates that deep priors also allow to find better local optima in the non-convex energy landscape of the nonlinear inverse problem arising from THz imaging. We verify this claim numerically on various THz parameter estimation problems for synthetic and real data under low SNR and shot noise conditions. While the low SNR scenario not even requires regularization, the impact of shot noise is significantly reduced by total variation (TV) regularization. We compare our approach with existing optimization techniques that require sophisticated physically motivated initialization, and with a 1D single-pixel reparametrization method.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wong_Deep_Optimization_Prior_for_THz_Model_Parameter_Estimation_WACV_2022_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Wong_Deep_Optimization_Prior_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2598913,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5822103853383216951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "28609ba9bc",
        "title": "Deep Photo Scan: Semi-Supervised Learning for Dealing With the Real-World Degradation in Smartphone Photo Scanning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ho_Deep_Photo_Scan_Semi-Supervised_Learning_for_Dealing_With_the_Real-World_WACV_2022_paper.html",
        "author": "Man M. Ho; Jinjia Zhou",
        "abstract": "Physical photographs now can be conveniently scanned by smartphones and stored forever as digital images, yet the scanned photos are not restored well. One solution is to train a supervised deep neural network on many digital images and their smartphone-scanned versions. However, it requires a high labor cost, leading to limited training data. Previous works create training pairs by simulating degradation using low-level image processing techniques. Their synthetic images are then formed with perfectly scanned photos in latent space. Even so, the real-world degradation in smartphone photo scanning remains unsolved since it is more complicated due to lens defocus, low-cost cameras, losing details via printing. Besides, locally structural misalignment still occurs in data due to distorted shapes captured in a 3-D world, reducing restoration performance and the reliability of the quantitative evaluation. To address these problems, we propose a semi-supervised Deep Photo Scan (DPScan). First, we present a way of producing real-world degradation and provide the DIV2K-SCAN dataset for smartphone-scanned photo restoration. Also, Local Alignment is proposed to reduce the minor misalignment remaining in data. Second, we simulate many different variants of the real-world degradation using low-level image transformation to gain a generalization in smartphone-scanned image properties, then train a degradation network to generalize all styles of degradation and provide pseudo-scanned photos for unscanned images as if they were scanned by a smartphone. Finally, we propose a Semi-Supervised Learning that allows our restoration network to be trained on both scanned and unscanned images, diversifying training image content. As a result, the proposed DPScan quantitatively and qualitatively outperforms its baseline architecture, state-of-the-art academic research, and industrial products in smartphone photo scanning.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ho_Deep_Photo_Scan_Semi-Supervised_Learning_for_Dealing_With_the_Real-World_WACV_2022_paper.pdf",
        "aff": "Hosei University; Hosei University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ho_Deep_Photo_Scan_WACV_2022_supplemental.pdf",
        "arxiv": "2102.06120",
        "pdf_size": 10139562,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13054526556796608607&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "stu.hosei.ac.jp;hosei.ac.jp",
        "email": "stu.hosei.ac.jp;hosei.ac.jp",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hosei University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hosei.ac.jp",
        "aff_unique_abbr": "Hosei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "950b6414d9",
        "title": "Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Li_Deep_Two-Stream_Video_Inference_for_Human_Body_Pose_and_Shape_WACV_2022_paper.html",
        "author": "Ziwen Li; Bo Xu; Han Huang; Cheng Lu; Yandong Guo",
        "abstract": "Several video-based 3D pose and shape estimation algorithms have been proposed to resolve the temporal inconsistency of single-image-based counterparts. However it still remains chanllenging to have stable and accurate reconstruction. In this paper, we propose a new method Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation (DTS-VIBE), to generate 3D human pose and mesh from RGB videos. We reformulate the task as a multi-modality problem that fuses RGB and optical flow for more reliable estimation. In order to fully utilize both sensory modalities (RGB or optical flow), we train a two-stream temporal network based on transformer to predict SMPL parameters. The supplementary modality, optical flow, helps to maintain temporal consistency by leveraging motion knowlege between two consecutive frames. The proposed algorithm is extensively evaluated on the Human3.6 and 3DPW datasets. The experimental results show that it outperforms other state-of-the-art methods by a significant margin.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Li_Deep_Two-Stream_Video_Inference_for_Human_Body_Pose_and_Shape_WACV_2022_paper.pdf",
        "aff": "OPPO Research Institute; OPPO Research Institute; OPPO Research Institute; Xmotors; OPPO Research Institute",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.11680",
        "pdf_size": 7433716,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=424424058908697805&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "live.com; ; ; ;live.com",
        "email": "live.com; ; ; ;live.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "OPPO Research Institute;Xmotors",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.oppo.com/en;",
        "aff_unique_abbr": "OPPO RI;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "5a8744998d",
        "title": "DeepPatent: Large Scale Patent Drawing Recognition and Retrieval",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kucer_DeepPatent_Large_Scale_Patent_Drawing_Recognition_and_Retrieval_WACV_2022_paper.html",
        "author": "Michal Kucer; Diane Oyen; Juan Castorena; Jian Wu",
        "abstract": "We tackle the problem of analyzing and retrieving technical drawings. First, we introduce DeepPatent, a new large-scale dataset for recognition and retrieval of design patent drawings. The dataset provides more than 350,000 design patent drawings for the purpose of image retrieval. Unlike existing datasets, DeepPatent provides fine-grained image retrieval associations within the collection of drawings and does not rely on cross-domain associations for supervision. We develop a baseline deep learning models, named PatentNet, based on best practices for training retrieval models for static images. We demonstrate the superior performance of PatentNet when trained on our fine-grained associations of DeepPatent against other deep learning approaches and classic computer vision descriptors, such as histogram of oriented gradients (HOG), on DeepPatent. With the introduction of this new dataset, and benchmark algorithms, we demonstrate that the analysis and retrieval of line drawings remains an open challenge in computer vision; and that patent drawing retrieval provides a concrete testbench to spur research.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kucer_DeepPatent_Large_Scale_Patent_Drawing_Recognition_and_Retrieval_WACV_2022_paper.pdf",
        "aff": "Los Alamos National Laboratory; Los Alamos National Laboratory; Los Alamos National Laboratory; Old Dominion University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Kucer_DeepPatent_Large_Scale_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2224967,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11656150719228537121&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "lanl.gov;lanl.gov;lanl.gov;cs.odu.edu",
        "email": "lanl.gov;lanl.gov;lanl.gov;cs.odu.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Los Alamos National Laboratory;Old Dominion University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.lanl.gov;https://www.odu.edu",
        "aff_unique_abbr": "LANL;ODU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e6b2722c8b",
        "title": "Densely-Packed Object Detection via Hard Negative-Aware Anchor Attention",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Cho_Densely-Packed_Object_Detection_via_Hard_Negative-Aware_Anchor_Attention_WACV_2022_paper.html",
        "author": "Sungmin Cho; Jinwook Paeng; Junseok Kwon",
        "abstract": "In this paper, we propose a novel densely-packed object detection method based on advanced weighted Hausdorff distance (AWHD) and hard negative-aware anchor (HNAA) attention. Densely-packed object detection is more challenging than conventional object detection due to the high object density and small-size objects. To overcome these challenges, the proposed AWHD improves the conventional weighted Hausdorff distance and obtains an accurate center area map. Using the precise center area map, the proposed HNAA attention determines the relative importance of each anchor and imposes a penalty on hard negative anchors. Experimental results demonstrate that our proposed method based on the AWHD and HNAA attention produces accurate densely-packed object detection results and comparably outperforms other state-of-the-art detection methods. The code is available at here.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Cho_Densely-Packed_Object_Detection_via_Hard_Negative-Aware_Anchor_Attention_WACV_2022_paper.pdf",
        "aff": "School of Computer Science and Engineering, Chung-Ang University, Seoul, Korea; School of Computer Science and Engineering, Chung-Ang University, Seoul, Korea; School of Computer Science and Engineering, Chung-Ang University, Seoul, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8434658,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=801602417321359627&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "naver.com;naver.com;cau.ac.kr",
        "email": "naver.com;naver.com;cau.ac.kr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Chung-Ang University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.cau.ac.kr",
        "aff_unique_abbr": "CAU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "247d2b59e1",
        "title": "Detail Preserving Residual Feature Pyramid Modules for Optical Flow",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Long_Detail_Preserving_Residual_Feature_Pyramid_Modules_for_Optical_Flow_WACV_2022_paper.html",
        "author": "Libo Long; Jochen Lang",
        "abstract": "Feature pyramids and iterative refinement have recently led to great progress in optical flow estimation. However, downsampling in feature pyramids can cause blending of foreground objects with the background, which will mislead subsequent decisions in the iterative processing. The results are missing details especially in the flow of thin and of small structures. We propose a novel Residual Feature Pyramid Module (RFPM) which retains important details in the feature map without changing the overall iterative refinement design of the optical flow estimation. RFPM incorporates a residual structure between multiple feature pyramids into a downsampling module that corrects the blending of objects across boundaries. We demonstrate how to integrate our module with two state-of-the-art iterative refinement architectures. Results show that our RFPM visibly reduces flow errors and improves state-of-art performance in the clean pass of Sintel, and is one of the top-performing methods in KITTI. According to the particular modular structure of RFPM, we introduce a special transfer learning approach that can dramatically decrease the training time compared to a typical full optical flow training schedule on multiple datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Long_Detail_Preserving_Residual_Feature_Pyramid_Modules_for_Optical_Flow_WACV_2022_paper.pdf",
        "aff": "EECS, University of Ottawa; EECS, University of Ottawa",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2107.10990",
        "pdf_size": 3746377,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=966474560813810280&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "uottawa.ca;uottawa.ca",
        "email": "uottawa.ca;uottawa.ca",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Ottawa",
        "aff_unique_dep": "EECS",
        "aff_unique_url": "https://www.uottawa.ca",
        "aff_unique_abbr": "U Ottawa",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ottawa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "53d9aa7dcc",
        "title": "Detecting Tear Gas Canisters With Limited Training Data",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/DCruz_Detecting_Tear_Gas_Canisters_With_Limited_Training_Data_WACV_2022_paper.html",
        "author": "Ashwin D'Cruz; Christopher Tegho; Sean Greaves; Lachlan Kermode",
        "abstract": "Human rights investigations often require triaging large volumes of open source data in order to find moments within image, or video that are relevant to a given investigation and warrant further inspection. Searching for images of tear gas usage online manually is laborious and time-consuming. In this paper, we focus on object detection models to facilitate discovery and identification of tear gas canisters for human rights monitors. For CNN based object detection to work, a large amount of training data is required, and prior to our work, a dataset of tear gas canisters did not exist. To achieve our objective, we benchmark methods for training object detectors using limited labelled data: we fine-tune different object detection models on the limited labelled data and compare performance to a few shot detector and augmentation strategies using synthetic data. We provide a dataset for evaluating and training tear gas canister detectors and show how such detectors can be deployed for a real world application such as investigating human rights violations. Our experiments show that fine-tuning state of the art detectors perform as well as the few shot detector, and including synthetic data can improve results.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/DCruz_Detecting_Tear_Gas_Canisters_With_Limited_Training_Data_WACV_2022_paper.pdf",
        "aff": "Forensic Architecture; Forensic Architecture; Forensic Architecture; Forensic Architecture + London, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1335828,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:nShovOqhWj4J:scholar.google.com/&scioq=Detecting+Tear+Gas+Canisters+With+Limited+Training+Data&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;gmail.com;btinternet.com;forensic-architecture.org",
        "email": "gmail.com;gmail.com;btinternet.com;forensic-architecture.org",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Forensic Architecture;University of London",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://forensic-architecture.org;https://www.london.ac.uk",
        "aff_unique_abbr": ";UoL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "41d5011d6d",
        "title": "Detection and Localization of Facial Expression Manipulations",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Mazaheri_Detection_and_Localization_of_Facial_Expression_Manipulations_WACV_2022_paper.html",
        "author": "Ghazal Mazaheri; Amit K. Roy-Chowdhury",
        "abstract": "Concerns regarding the wide-spread use of forged images and videos in social media necessitate precise detection of such fraud. Facial manipulations can be created by Identity swap (DeepFake) or Expression swap. Contrary to the identity swap, which can easily be detected with novel deepfake detection methods, expression swap detection has not yet been addressed extensively. The importance of facial expressions in inter-person communication is known. Consequently, it is important to develop methods that can detect and localize manipulations in facial expressions. To this end, we present a novel framework to exploit the underlying feature representations of facial expressions learned from expression recognition models to identify the manipulated features. Using discriminative feature maps extracted from a facial expression recognition framework, our manipulation detector is able to localize the manipulated regions of input images and videos. On the Face2Face dataset, (abundant expression manipulation), and NeuralTextures dataset (facial expressions manipulation corresponding to the mouth regions), our method achieves higher accuracy for both classification and localization of manipulations compared to state-of-the-art methods. Furthermore, we demonstrate that our method performs at-par with the state-of-the-art methods in cases where the expression is not manipulated, but rather the identity is changed, leading to a generalized approach for facial manipulation detection.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Mazaheri_Detection_and_Localization_of_Facial_Expression_Manipulations_WACV_2022_paper.pdf",
        "aff": "Video Computing Group, University of California, Riverside; Video Computing Group, University of California, Riverside",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1077617,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15625089762043567771&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ucr.edu;ece.ucr.edu",
        "email": "ucr.edu;ece.ucr.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Riverside",
        "aff_unique_dep": "Video Computing Group",
        "aff_unique_url": "https://www.ucr.edu",
        "aff_unique_abbr": "UCR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Riverside",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fec6a04a6c",
        "title": "Digital and Physical-World Attacks on Remote Pulse Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Speth_Digital_and_Physical-World_Attacks_on_Remote_Pulse_Detection_WACV_2022_paper.html",
        "author": "Jeremy Speth; Nathan Vance; Patrick Flynn; Kevin W. Bowyer; Adam Czajka",
        "abstract": "Remote photoplethysmography (rPPG) is a technique for estimating blood volume changes from reflected light without the need for a contact sensor. We present the first examples of presentation attacks in the digital and physical domains on rPPG from face video. Digital attacks are easily performed by adding imperceptible periodic noise to the input videos. Physical attacks are performed with illumination from visible spectrum LEDs placed in close proximity to the face, while still being difficult to perceive with the human eye. We also show that our attacks extend beyond medical applications, since the method can effectively generate a strong periodic pulse on 3D-printed face masks, which presents difficulties for pulse-based face presentation attack detection (PAD). The paper concludes with ideas for using this work to improve robustness of rPPG methods and pulse-based face PAD.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Speth_Digital_and_Physical-World_Attacks_on_Remote_Pulse_Detection_WACV_2022_paper.pdf",
        "aff": "University of Notre Dame; University of Notre Dame; University of Notre Dame; University of Notre Dame; University of Notre Dame",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.11525",
        "pdf_size": 6018074,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15468138145699692433&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nd.edu;nd.edu;nd.edu;nd.edu;nd.edu",
        "email": "nd.edu;nd.edu;nd.edu;nd.edu;nd.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Notre Dame",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nd.edu",
        "aff_unique_abbr": "Notre Dame",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6bb71a2936",
        "title": "Discovering Underground Maps From Fashion",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Mall_Discovering_Underground_Maps_From_Fashion_WACV_2022_paper.html",
        "author": "Utkarsh Mall; Kavita Bala; Tamara Berg; Kristen Grauman",
        "abstract": "The fashion sense--meaning the clothing styles people wear--in a geographical region can reveal information about that region. For example, it can reflect the kind of activities people do there, or the type of crowds that frequently visit the region (e.g., tourist hot spot, student neighborhood, business center). We propose a method to create underground neighborhood maps of cities by analyzing how people dress. Using publicly available images from across a city, our method automatically segments the map into neighborhoods with a similar fashion sense. Our approach further allows discovering insights about a city, such as detecting distinct neighborhoods (what is the most unique region of NYC?) and answering analogy questions between cities (what is the \"Downtown LA\" of Bogota?). We also present two new underground map benchmarks derived from non-image data for 37 cities worldwide. Our method shows promising results on both these benchmarks as well as experiments with human judges.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Mall_Discovering_Underground_Maps_From_Fashion_WACV_2022_paper.pdf",
        "aff": "Cornell University + Facebook AI Research; Cornell University; Facebook; University of Texas at Austin + Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Mall_Discovering_Underground_Maps_WACV_2022_supplemental.zip",
        "arxiv": "2012.02897",
        "pdf_size": 5930531,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7642597780505042845&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.cornell.edu;cs.cornell.edu;fb.com;cs.utexas.edu",
        "email": "cs.cornell.edu;cs.cornell.edu;fb.com;cs.utexas.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;1;2+1",
        "aff_unique_norm": "Cornell University;Meta;University of Texas at Austin",
        "aff_unique_dep": ";Facebook AI Research;",
        "aff_unique_url": "https://www.cornell.edu;https://research.facebook.com;https://www.utexas.edu",
        "aff_unique_abbr": "Cornell;FAIR;UT Austin",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9050a512f9",
        "title": "Discrete Neural Representations for Explainable Anomaly Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Szymanowicz_Discrete_Neural_Representations_for_Explainable_Anomaly_Detection_WACV_2022_paper.html",
        "author": "Stanislaw Szymanowicz; James Charles; Roberto Cipolla",
        "abstract": "The aim of this work is to detect and automatically generate high-level explanations of anomalous events in video. Understanding the cause of an anomalous event is crucial as the required response is dependant on its nature and severity. Recent works typically use object or action classifier to detect and provide labels for anomalous events. However, this constrains detection systems to a finite set of known classes and prevents generalisation to unknown objects or behaviours. Here we show how to robustly detect anomalies without the use of object or action classifiers yet still recover the high level reason behind the event. We make the following contributions: (1) a method using saliency maps to decouple the explanation of anomalous events from object and action classifiers, (2) show how to improve the quality of saliency maps using a novel neural architecture for learning discrete representations of video by predicting future frames and (3) beat the state-of-the-art anomaly explanation methods by 60% on a subset of the public benchmark X-MAN dataset.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Szymanowicz_Discrete_Neural_Representations_for_Explainable_Anomaly_Detection_WACV_2022_paper.pdf",
        "aff": "University of Cambridge; University of Cambridge; University of Cambridge",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2112.05585",
        "pdf_size": 1218249,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4168895099208444356&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "269535248a",
        "title": "Disentangled Representation With Dual-Stage Feature Learning for Face Anti-Spoofing",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wang_Disentangled_Representation_With_Dual-Stage_Feature_Learning_for_Face_Anti-Spoofing_WACV_2022_paper.html",
        "author": "Yu-Chun Wang; Chien-Yi Wang; Shang-Hong Lai",
        "abstract": "As face recognition is widely used in diverse security-critical applications, the study of face anti-spoofing (FAS) has attracted more and more attention. Several FAS methods have achieved promising performances if the attack types in the testing data are the same as training data, while the performance significantly degrades for unseen attack types. It is essential to learn more generalized and discriminative features to prevent overfitting to pre-defined spoof attack types. This paper proposes a novel dual-stage disentangled representation learning method that can efficiently untangle spoof-related features from irrelevant ones. Unlike previous FAS disentanglement works with one-stage architecture, we found that the dual-stage training design can improve the training stability and effectively encode the features to detect unseen attack types. Our experiments show that the proposed method provides superior accuracy than the state-of-the-art methods on several cross-type FAS benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wang_Disentangled_Representation_With_Dual-Stage_Feature_Learning_for_Face_Anti-Spoofing_WACV_2022_paper.pdf",
        "aff": "National Tsing Hua University; Microsoft AI R&D Center; National Tsing Hua University+Microsoft AI R&D Center",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Wang_Disentangled_Representation_With_WACV_2022_supplemental.pdf",
        "arxiv": "2110.09157",
        "pdf_size": 8375047,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5293227705198004898&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gapp.nthu.edu.tw;microsoft.com;microsoft.com",
        "email": "gapp.nthu.edu.tw;microsoft.com;microsoft.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "National Tsing Hua University;Microsoft",
        "aff_unique_dep": ";Microsoft AI R&D Center",
        "aff_unique_url": "https://www.nthu.edu.tw;https://www.microsoft.com",
        "aff_unique_abbr": "NTHU;Microsoft",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan;",
        "aff_country_unique_index": "0;1;0+1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "8ad2652c4f",
        "title": "Distance-Based Hyperspherical Classification for Multi-Source Open-Set Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bucci_Distance-Based_Hyperspherical_Classification_for_Multi-Source_Open-Set_Domain_Adaptation_WACV_2022_paper.html",
        "author": "Silvia Bucci; Francesco Cappio Borlino; Barbara Caputo; Tatiana Tommasi",
        "abstract": "Vision systems trained in closed-world scenarios fail when presented with new environmental conditions, new data distributions, and novel classes at deployment time. How to move towards open-world learning is a long-standing research question. The existing solutions mainly focus on specific aspects of the problem (single domain Open-Set, multi-domain Closed-Set), or propose complex strategies which combine several losses and manually tuned hyperparameters. In this work, we tackle multi-source Open-Set domain adaptation by introducing HyMOS: a straightforward model that exploits the power of contrastive learning and the properties of its hyperspherical feature space to correctly predict known labels on the target, while rejecting samples belonging to any unknown class. HyMOS includes style transfer among the instance transformations of contrastive learning to get domain invariance while avoiding the risk of negative-transfer. A self-paced threshold is defined on the basis of the observed data distribution and updates online during training, allowing to handle the known-unknown separation. We validate our method over three challenging datasets. The obtained results show that HyMOS outperforms several competitors, defining the new state-of-the-art. Our code is available at https://github.com/silvia1993/HyMOS.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bucci_Distance-Based_Hyperspherical_Classification_for_Multi-Source_Open-Set_Domain_Adaptation_WACV_2022_paper.pdf",
        "aff": "Politecnico di Torino, Italy; Italian Institute of Technology; Politecnico di Torino, Italy; Politecnico di Torino, Italy",
        "project": "",
        "github": "https://github.com/silvia1993/HyMOS",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Bucci_Distance-Based_Hyperspherical_Classification_WACV_2022_supplemental.pdf",
        "arxiv": "2107.02067",
        "pdf_size": 2400650,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10600160965607685693&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "polito.it;polito.it;polito.it;polito.it",
        "email": "polito.it;polito.it;polito.it;polito.it",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Politecnico di Torino;Italian Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.polito.it;https://www.iit.it",
        "aff_unique_abbr": "Polito;IIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "3d6e659215",
        "title": "Does Data Repair Lead to Fair Models? Curating Contextually Fair Data To Reduce Model Bias",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Agarwal_Does_Data_Repair_Lead_to_Fair_Models_Curating_Contextually_Fair_WACV_2022_paper.html",
        "author": "Sharat Agarwal; Sumanyu Muku; Saket Anand; Chetan Arora",
        "abstract": "Contextual information is a valuable cue for Deep Neural Networks (DNNs) to learn better representations and improve accuracy. However, co-occurrence bias in the training dataset may hamper a DNN model's generalizability to unseen scenarios in the real world. For example, in COCO [??], many object categories have a much higher co-occurrence with men compared to women, which can bias a DNN's prediction in favor of men. Recent works have focused on task-specific training strategies to handle bias in such scenarios, but fixing the available data is often ignored. In this paper, we propose a novel and more generic solution to address the contextual bias in the datasets by selecting a subset of the samples, which is fair in terms of the co-occurrence with various classes for a protected attribute. We introduce a data repair algorithm using the coefficient of variation(c_v), which can curate fair and contextually balanced data for a protected class(es). This helps in training a fair model irrespective of the task, architecture or training methodology. Our proposed solution is simple, effective and can even be used in an active learning setting where the data labels are not present or being generated incrementally. We demonstrate the effectiveness of our algorithm for the task of object detection and multi-label image classification across different datasets. Through a series of experiments, we validate that curating contextually fair data helps make model predictions fair by balancing the true positive rate for the protected class across groups without compromising on the model's overall performance.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Agarwal_Does_Data_Repair_Lead_to_Fair_Models_Curating_Contextually_Fair_WACV_2022_paper.pdf",
        "aff": "IIIT Delhi, India; Indian Institute of Technology Delhi, India; IIIT Delhi, India; Indian Institute of Technology Delhi, India",
        "project": "",
        "github": "https://github.com/sumanyumuku98/contextual-bias",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Agarwal_Does_Data_Repair_WACV_2022_supplemental.pdf",
        "arxiv": "2110.10389",
        "pdf_size": 3887922,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14138059640030393060&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "iiitd.ac.in;iitd.ac.in;iiitd.ac.in;cse.iitd.ac.in",
        "email": "iiitd.ac.in;iitd.ac.in;iiitd.ac.in;cse.iitd.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "IIIT Delhi;Indian Institute of Technology Delhi",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iiitdelhi.ac.in;https://www.iitdelhi.ac.in",
        "aff_unique_abbr": "IIITD;IIT Delhi",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Delhi",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "bad29aa44b",
        "title": "Domain Generalization Through Audio-Visual Relative Norm Alignment in First Person Action Recognition",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Planamente_Domain_Generalization_Through_Audio-Visual_Relative_Norm_Alignment_in_First_Person_WACV_2022_paper.html",
        "author": "Mirco Planamente; Chiara Plizzari; Emanuele Alberti; Barbara Caputo",
        "abstract": "First person action recognition is becoming an increasingly researched area thanks to the rising popularity of wearable cameras. This is bringing to light cross-domain issues that are yet to be addressed in this context. Indeed, the information extracted from learned representations suffers from an intrinsic \"environmental bias\". This strongly affects the ability to generalize to unseen scenarios, limiting the application of current methods to real settings where labeled data are not available during training. In this work, we introduce the first domain generalization approach for egocentric activity recognition, by proposing a new audio-visual loss, called Relative Norm Alignment loss. It re-balances the contributions from the two modalities during training, over different domains, by aligning their feature norm representations. Our approach leads to strong results in domain generalization on both EPIC-Kitchens-55 and EPIC-Kitchens-100, as demonstrated by extensive experiments, and can be extended to work also on domain adaptation settings with competitive results.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Planamente_Domain_Generalization_Through_Audio-Visual_Relative_Norm_Alignment_in_First_Person_WACV_2022_paper.pdf",
        "aff": "Politecnico di Torino+Istituto Italiano di Tecnologia+CINI Consortium; Politecnico di Torino+Istituto Italiano di Tecnologia+CINI Consortium; Politecnico di Torino; Politecnico di Torino+Istituto Italiano di Tecnologia+CINI Consortium",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Planamente_Domain_Generalization_Through_WACV_2022_supplemental.pdf",
        "arxiv": "2110.10101",
        "pdf_size": 1817354,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8696161722799040158&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "polito.it;polito.it;polito.it;polito.it",
        "email": "polito.it;polito.it;polito.it;polito.it",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;0+1+2;0;0+1+2",
        "aff_unique_norm": "Politecnico di Torino;Istituto Italiano di Tecnologia;CINI Consortium",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.polito.it;https://www.iit.it;http://www.cini.it",
        "aff_unique_abbr": "Polito;IIT;CINI",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0+0+0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "0cc3447eaf",
        "title": "Dual-Head Contrastive Domain Adaptation for Video Action Recognition",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/da_Costa_Dual-Head_Contrastive_Domain_Adaptation_for_Video_Action_Recognition_WACV_2022_paper.html",
        "author": "Victor G. Turrisi da Costa; Giacomo Zara; Paolo Rota; Thiago Oliveira-Santos; Nicu Sebe; Vittorio Murino; Elisa Ricci",
        "abstract": "Unsupervised domain adaptation (UDA) methods have become very popular in computer vision. However, while several techniques have been proposed for images, much less attention has been devoted to videos. This paper introduces a novel UDA approach for action recognition from videos, inspired by recent literature on contrastive learning. In particular, we propose a novel two-headed deep architecture that simultaneously adopts cross-entropy and contrastive losses from different network branches to robustly learn a target classifier. Moreover, this work introduces a novel large-scale UDA dataset, Mixamo->Kinetics, which, to the best of our knowledge, is the first dataset that considers the domain shift arising when transferring knowledge from synthetic to real video sequences. Our extensive experimental evaluation conducted on three publicly available benchmarks and on our new Mixamo->Kinetics dataset demonstrate the effectiveness of our approach, which outperforms the current state-of-the-art methods. Code is available at https://github.com/vturrisi/CO2A.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/da_Costa_Dual-Head_Contrastive_Domain_Adaptation_for_Video_Action_Recognition_WACV_2022_paper.pdf",
        "aff": "University of Trento; University of Trento; University of Trento; Universidade Federal do Esp\u00edrito Santo; University of Trento; University of Verona+Huawei Technologies, Ireland Research Center; University of Trento+Fondazione Bruno Kessler",
        "project": "",
        "github": "https://github.com/vturrisi/CO2A",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/da_Costa_Dual-Head_Contrastive_Domain_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1466656,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13415120868499469839&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "unitn.it;unitn.it;unitn.it;inf.ufes.br;unitn.it;iit.it;unitn.it",
        "email": "unitn.it;unitn.it;unitn.it;inf.ufes.br;unitn.it;iit.it;unitn.it",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0;2+3;0+4",
        "aff_unique_norm": "University of Trento;Universidade Federal do Esp\u00edrito Santo;University of Verona;Huawei;Fondazione Bruno Kessler",
        "aff_unique_dep": ";;;Ireland Research Center;",
        "aff_unique_url": "https://www.unitn.it;https://www.ufes.br;https://www.univr.it;https://www.huawei.com/ie/en/;https://www.fbk.eu",
        "aff_unique_abbr": "UniTN;UFES;UniVR;Huawei;FBK",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;0+2;0+0",
        "aff_country_unique": "Italy;Brazil;Ireland"
    },
    {
        "id": "2d5b5f049f",
        "title": "Dynamic CNNs Using Uncertainty To Overcome Domain Generalization for Surgical Instrument Localization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Philipp_Dynamic_CNNs_Using_Uncertainty_To_Overcome_Domain_Generalization_for_Surgical_WACV_2022_paper.html",
        "author": "Markus Philipp; Anna Alperovich; Marielena Gutt-Will; Andrea Mathis; Stefan Saur; Andreas Raabe; Franziska Mathis-Ullrich",
        "abstract": "Due to the limited amount of available annotated data in the medical field, domain generalization for applications in computer-assisted surgery is essential. Our work addresses this problem for the task of surgical instrument tip localization in neurosurgery, which is a classical step towards computer-assisted surgery. We propose an uncertainty-based CNN approach that dynamically selects the most relevant data source by incorporating its own uncertainty into the inference. In addition, the estimated uncertainty can visualize and easily explain the network's decision. Quantitative and qualitative evaluations show that our method outperforms state of the art approaches for large domain shifts and results are on-par for in-domain applications. Further increasing domain shifts by testing on different surgical disciplines, eye and laparoscopic surgeries, proves the generalization capabilities of the proposed method.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Philipp_Dynamic_CNNs_Using_Uncertainty_To_Overcome_Domain_Generalization_for_Surgical_WACV_2022_paper.pdf",
        "aff": "Carl Zeiss Meditec AG, DE+Karlsruhe Institute of Technology, DE; Carl Zeiss AG, DE; Inselspital Bern, CH; Inselspital Bern, CH; Carl Zeiss Meditec AG, DE; Inselspital Bern, CH; Karlsruhe Institute of Technology, DE",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Philipp_Dynamic_CNNs_Using_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3483262,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16129691107163928583&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "zeiss.com;kit.edu;insel.ch;insel.ch;zeiss.com;insel.ch;kit.edu",
        "email": "zeiss.com;kit.edu;insel.ch;insel.ch;zeiss.com;insel.ch;kit.edu",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3;3;0;3;1",
        "aff_unique_norm": "Carl Zeiss Meditec AG;Karlsruhe Institute of Technology;Carl Zeiss AG;Inselspital Bern",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.zeiss.com/meditec;https://www.kit.edu;https://www.zeiss.com;https://www.inselspital.bern.ch",
        "aff_unique_abbr": "Zeiss Meditec;KIT;Zeiss;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;1;1;0;1;0",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "id": "5a031f0f46",
        "title": "Dynamic Iterative Refinement for Efficient 3D Hand Pose Estimation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yang_Dynamic_Iterative_Refinement_for_Efficient_3D_Hand_Pose_Estimation_WACV_2022_paper.html",
        "author": "John Yang; Yash Bhalgat; Simyung Chang; Fatih Porikli; Nojun Kwak",
        "abstract": "While hand pose estimation is a critical component of most interactive extended reality and gesture recognition systems, contemporary approaches are not optimized for computational and memory efficiency. In this paper, we propose a tiny deep neural network of which partial layers are recursively exploited for refining its previous estimations. During its iterative refinements, we employ learned gating criteria to decide whether to exit from the weight-sharing loop, allowing per-sample adaptation in our model. Our network is trained to be aware of the uncertainty in its current predictions to efficiently gate at each iteration, estimating variances after each loop for its keypoint estimates. Additionally, we investigate the effectiveness of end-to-end and progressive training protocols for our recursive structure on maximizing the model capacity. With the proposed setting, our method consistently outperforms state-of-the-art 2D/3D hand pose estimation approaches in terms of both accuracy and efficiency for two widely used benchmarks (e.g., up to 4.9x reduction in GFLOPs and 12.5x fewer parameters than the current SOTA, ACE-Net, while achieving 5.1% AUC improvement on the FPHA dataset).",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yang_Dynamic_Iterative_Refinement_for_Efficient_3D_Hand_Pose_Estimation_WACV_2022_paper.pdf",
        "aff": "Seoul National University; Qualcomm AI Research, Qualcomm Technologies, Inc.+Qualcomm AI Research, Qualcomm Korea YH; Qualcomm AI Research, Qualcomm Korea YH; Qualcomm AI Research, Qualcomm Technologies, Inc.; Seoul National University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yang_Dynamic_Iterative_Refinement_WACV_2022_supplemental.pdf",
        "arxiv": "2111.06500",
        "pdf_size": 3983509,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15764212574429906381&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "snu.ac.kr;qti.qualcomm.com;qti.qualcomm.com;qti.qualcomm.com;snu.ac.kr",
        "email": "snu.ac.kr;qti.qualcomm.com;qti.qualcomm.com;qti.qualcomm.com;snu.ac.kr",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;2;1;0",
        "aff_unique_norm": "Seoul National University;Qualcomm Technologies, Inc.;Qualcomm Korea YH",
        "aff_unique_dep": ";Qualcomm AI Research;Qualcomm AI Research",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.qualcomm.com/research;https://www.qualcomm.com/research",
        "aff_unique_abbr": "SNU;QTI;QKR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "c2b0cd26bf",
        "title": "EZCrop: Energy-Zoned Channels for Robust Output Pruning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lin_EZCrop_Energy-Zoned_Channels_for_Robust_Output_Pruning_WACV_2022_paper.html",
        "author": "Rui Lin; Jie Ran; Dongpeng Wang; King Hung Chiu; Ngai Wong",
        "abstract": "Recent results have revealed an interesting observation in a trained convolutional neural network (CNN), namely, the rank of a feature map channel matrix remains surprisingly constant despite the input images. This has led to an effective rank-based channel pruning algorithm, yet the constant rank phenomenon remains mysterious and unexplained. This work aims at demystifying and interpreting such rank behavior from a frequency-domain perspective, which as a bonus suggests an extremely efficient Fast Fourier Transform (FFT)-based metric for measuring channel importance without explicitly computing its rank. We achieve remarkable CNN channel pruning based on this analytically sound and computationally efficient metric, and adopt it for repetitive pruning to demonstrate robustness via our scheme named Energy-Zoned Channels for Robust Output Pruning (EZCrop), which shows consistently better results than other state-of-the-art channel pruning methods. The codes and Appendix are publicly available at: https://github.com/ruilin0212/EZCrop.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lin_EZCrop_Energy-Zoned_Channels_for_Robust_Output_Pruning_WACV_2022_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "https://github.com/ruilin0212/EZCrop",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Lin_EZCrop_Energy-Zoned_Channels_WACV_2022_supplemental.zip",
        "arxiv": "2105.03679",
        "pdf_size": 1850428,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6553095420409464778&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ecb0044ff9",
        "title": "EdgeConv With Attention Module for Monocular Depth Estimation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lee_EdgeConv_With_Attention_Module_for_Monocular_Depth_Estimation_WACV_2022_paper.html",
        "author": "Minhyeok Lee; Sangwon Hwang; Chaewon Park; Sangyoun Lee",
        "abstract": "Monocular depth estimation is an especially important task in robotics and autonomous driving, where 3D structural information is essential. However, extreme lighting conditions and complex surface objects make it difficult to predict depth in a single image. Therefore, to generate accurate depth maps, it is important for the model to learn structural information about the scene. We propose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module (EAM) to solve the difficulty of monocular depth estimation. The proposed modules extract structural information by learning the relationship between image patches close to each other in space using edge convolution. Our method is evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen split, achieving state-of-the-art performance. We prove that the proposed model predicts depth robustly in challenging scenes through various comparative experiments.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lee_EdgeConv_With_Attention_Module_for_Monocular_Depth_Estimation_WACV_2022_paper.pdf",
        "aff": "Yonsei University; Yonsei University; Yonsei University; Yonsei University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2106.08615",
        "pdf_size": 1852164,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14757381790489054307&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "email": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Yonsei University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yonsei.ac.kr",
        "aff_unique_abbr": "Yonsei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2148879859",
        "title": "Efficient Counterfactual Debiasing for Visual Question Answering",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kolling_Efficient_Counterfactual_Debiasing_for_Visual_Question_Answering_WACV_2022_paper.html",
        "author": "Camila Kolling; Martin More; Nathan Gavenski; Eduardo Pooch; Ot\u00e1vio Parraga; Rodrigo C. Barros",
        "abstract": "Despite the success of neural architectures for Visual Question Answering (VQA), several recent studies have shown that VQA models are mostly driven by superficial correlations that are learned by exploiting undesired priors within training datasets. They often lack sufficient image grounding or tend to overly-rely on textual information, failing to capture knowledge from the images. This affects their generalization to test sets with slight changes in the distribution of facts. To address such an issue, some bias mitigation methods have relied on new training procedures that are capable of synthesizing counterfactual samples by masking critical objects within the images, and words within the questions, while also changing the corresponding ground truth. We propose a novel model-agnostic counterfactual training procedure, namely Efficient Counterfactual Debiasing (ECV), in which we introduce a new negative answer-assignment mechanism that exploits the probability distribution of the answers based on their frequencies, as well as an improved counterfactual sample synthesizer. Our experiments demonstrate that ECV is a simple, computationally-efficient counterfactual sample-synthesizer training procedure that establishes itself as the new state-of-the-art for unbiased VQA.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kolling_Efficient_Counterfactual_Debiasing_for_Visual_Question_Answering_WACV_2022_paper.pdf",
        "aff": "Kunumi, Brazil+MALTA Research Group, PUCRS, Brazil+Teia Labs, Brazil; MALTA Research Group, PUCRS, Brazil+Teia Labs, Brazil; MALTA Research Group, PUCRS, Brazil; MALTA Research Group, PUCRS, Brazil; MALTA Research Group, PUCRS, Brazil+Teia Labs, Brazil; MALTA Research Group, PUCRS, Brazil+Teia Labs, Brazil",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4776721,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=994552487697291714&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "pucrs.br; ; ; ; ;pucrs.br",
        "email": "pucrs.br; ; ; ; ;pucrs.br",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;1+2;1;1;1+2;1+2",
        "aff_unique_norm": "Kunumi;PUCRS;Teia Labs",
        "aff_unique_dep": ";MALTA Research Group;",
        "aff_unique_url": ";https://www.pucrs.br;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0;0;0;0+0;0+0",
        "aff_country_unique": "Brazil"
    },
    {
        "id": "296b391987",
        "title": "EllipsoidNet: Ellipsoid Representation for Point Cloud Classification and Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lyu_EllipsoidNet_Ellipsoid_Representation_for_Point_Cloud_Classification_and_Segmentation_WACV_2022_paper.html",
        "author": "Yecheng Lyu; Xinming Huang; Ziming Zhang",
        "abstract": "Point cloud patterns are hard to learn because of the implicit local geometry features among the orderless points. In recent years, point cloud representation in 2D space has attracted increasing research interest since it exposes the local geometry features in a 2D space. By projecting those points to a 2D feature map, the relationship between points is inherited in the context between pixels, which are further extracted by a 2D convolutional neural network. However, existing 2D representing methods are either accuracy limited or time-consuming. In this paper, we propose a novel 2D representation method that projects a point cloud onto an ellipsoid surface space, where local patterns are well exposed in ellipsoid-level and point-level. Additionally, a novel convolutional neural network named EllipsoidNet is proposed to utilize those features for point cloud classification and segmentation applications. The proposed methods are evaluated in ModelNet40 and ShapeNet benchmarks, where the advantages are clearly shown over existing 2D representation methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lyu_EllipsoidNet_Ellipsoid_Representation_for_Point_Cloud_Classification_and_Segmentation_WACV_2022_paper.pdf",
        "aff": "Worcester Polytechnic Institute + Volvo Car Technology USA; Worcester Polytechnic Institute; Worcester Polytechnic Institute",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2103.02517",
        "pdf_size": 1931595,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2347575134530789799&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "volvocars.com;wpi.edu;wpi.edu",
        "email": "volvocars.com;wpi.edu;wpi.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Worcester Polytechnic Institute;Volvo Car Corporation",
        "aff_unique_dep": ";Car Technology",
        "aff_unique_url": "https://www.wpi.edu;https://www.volvocars.com/us",
        "aff_unique_abbr": "WPI;VCT USA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e2726f9ac2",
        "title": "Enhanced Correlation Matching Based Video Frame Interpolation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lee_Enhanced_Correlation_Matching_Based_Video_Frame_Interpolation_WACV_2022_paper.html",
        "author": "Sungho Lee; Narae Choi; Woong Il Choi",
        "abstract": "We propose a novel DNN based framework called the Enhanced Correlation Matching based Video Frame Interpolation Network to support high resolution like 4K, which has a large scale of motion and occlusion. Considering the extensibility of the network model according to resolution, the proposed scheme employs the recurrent pyramid architecture that shares the parameters among each pyramid layer for the optical flow estimation. In the proposed flow estimation, the optical flows are recursively refined by tracing the location with maximum correlation. The forward warping based correlation matching enables to improve the accuracy of flow update by excluding incorrectly warped features around the occlusion area. Based on the final bi-directional flows, the intermediate frame at arbitrary temporal position is synthesized using the warping and blending network and it is further improved by refinement network. Experiment results demonstrate that the proposed scheme outperforms the previous works at 4K video data and low-resolution benchmark datasets as well in terms of objective and subjective quality with the smallest number of model parameters.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lee_Enhanced_Correlation_Matching_Based_Video_Frame_Interpolation_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Lee_Enhanced_Correlation_Matching_WACV_2022_supplemental.zip",
        "arxiv": "2111.08869",
        "pdf_size": 5807648,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16694607504069480743&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "20e1ce13e3",
        "title": "Enhancing Few-Shot Image Classification With Unlabelled Examples",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bateni_Enhancing_Few-Shot_Image_Classification_With_Unlabelled_Examples_WACV_2022_paper.html",
        "author": "Peyman Bateni; Jarred Barber; Jan-Willem van de Meent; Frank Wood",
        "abstract": "We develop a transductive meta-learning method that uses unlabelled instances to improve few-shot image classification performance. Our approach combines a regularized Mahalanobis-distance-based soft k-means clustering procedure with a modified state of the art neural adaptive feature extractor to achieve improved test-time classification accuracy using unlabelled data. We evaluate our method on transductive few-shot learning tasks, in which the goal is to jointly predict labels for query (test) examples given a set of support (training) examples. We achieve state of the art performance on the Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. All trained models and code have been made publicly available at github.com/plai-group/simple-cnaps.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bateni_Enhancing_Few-Shot_Image_Classification_With_Unlabelled_Examples_WACV_2022_paper.pdf",
        "aff": "University of British Columbia1+Beam AI6; Amazon2; Northeastern University3; University of British Columbia1+Inverted AI4+MILA5",
        "project": "",
        "github": "github.com/plai-group/simple-cnaps",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Bateni_Enhancing_Few-Shot_Image_WACV_2022_supplemental.pdf",
        "arxiv": "2006.12245",
        "pdf_size": 2343125,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10332831401837417302&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.ubc.ca;gmail.com;northeastern.edu;cs.ubc.ca",
        "email": "cs.ubc.ca;gmail.com;northeastern.edu;cs.ubc.ca",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3;0+4+5",
        "aff_unique_norm": "University of British Columbia;Beam AI;Amazon;Northeastern University;Inverted AI;Mila",
        "aff_unique_dep": ";;Amazon;;;",
        "aff_unique_url": "https://www.ubc.ca;;https://www.amazon.com;https://www.northeastern.edu;;https://mila.quebec",
        "aff_unique_abbr": "UBC;;Amazon;NEU;Inverted AI;MILA",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;2;2;0+0",
        "aff_country_unique": "Canada;;United States"
    },
    {
        "id": "b88c8cffa7",
        "title": "Equine Pain Behavior Classification via Self-Supervised Disentangled Pose Representation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Rashid_Equine_Pain_Behavior_Classification_via_Self-Supervised_Disentangled_Pose_Representation_WACV_2022_paper.html",
        "author": "Maheen Rashid; Sofia Broom\u00e9; Katrina Ask; Elin Hernlund; Pia Haubro Andersen; Hedvig Kjellstr\u00f6m; Yong Jae Lee",
        "abstract": "Timely detection of horse pain is important for equine welfare. Horses express pain through their facial and body behavior, but may hide signs of pain from unfamiliar human observers. In addition, collecting visual data with detailed annotation of horse behavior and pain state is both cumbersome and not scalable. Consequently, a pragmatic equine pain classification system would use video of the unobserved horse and weak labels. This paper proposes such a method for equine pain classification by using multi-view surveillance video footage of unobserved horses with induced orthopaedic pain, with temporally sparse video level pain labels. To ensure that pain is learned from horse body language alone, we first train a self-supervised generative model to disentangle horse pose from its appearance and background before using the disentangled horse pose latent representation for pain classification. To make best use of the pain labels, we develop a novel loss that formulates pain classification as a multi-instance learning problem. Our method achieves pain classification accuracy better than human expert performance with 60% accuracy. The learned latent horse pose representation is shown to be viewpoint covariant, and disentangled from horse appearance. Qualitative analysis of pain classified segments shows correspondence between the pain symptoms identified by our model, and equine pain scales used in veterinary practice.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Rashid_Equine_Pain_Behavior_Classification_via_Self-Supervised_Disentangled_Pose_Representation_WACV_2022_paper.pdf",
        "aff": "UC Davis, USA+Univrses AB, Sweden; KTH Royal Institute of Technology, Sweden; SLU, Sweden; SLU, Sweden; SLU, Sweden; KTH Royal Institute of Technology, Sweden+Silo AI, Sweden; UC Davis, USA+UW Madison, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Rashid_Equine_Pain_Behavior_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1834834,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7220646207324344137&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "univrses.com;kth.se;slu.se;slu.se;slu.se;kth.se;cs.wisc.edu",
        "email": "univrses.com;kth.se;slu.se;slu.se;slu.se;kth.se;cs.wisc.edu",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3;3;3;2+4;0+5",
        "aff_unique_norm": "University of California, Davis;Univrses AB;KTH Royal Institute of Technology;Swedish University of Agricultural Sciences;Silo AI;University of Wisconsin-Madison",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.ucdavis.edu;;https://www.kth.se;https://www.slu.se;https://silo.ai;https://www.wisc.edu",
        "aff_unique_abbr": "UC Davis;;KTH;SLU;;UW-Madison",
        "aff_campus_unique_index": "0;;0+2",
        "aff_campus_unique": "Davis;;Madison",
        "aff_country_unique_index": "0+1;1;1;1;1;1+1;0+0",
        "aff_country_unique": "United States;Sweden"
    },
    {
        "id": "9302abfa5a",
        "title": "Estimating Image Depth in the Comics Domain",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bhattacharjee_Estimating_Image_Depth_in_the_Comics_Domain_WACV_2022_paper.html",
        "author": "Deblina Bhattacharjee; Martin Everaert; Mathieu Salzmann; Sabine S\u00fcsstrunk",
        "abstract": "Estimating the depth of comics images is challenging as such images a) are monocular; b) lack ground-truth depth annotations; c) differ across different artistic styles; d) are sparse and noisy. We thus, use an off-the-shelf unsupervised image to image translation method to translate the comics images to natural ones and then use an attention-guided monocular depth estimator to predict their depth. This lets us leverage the depth annotations of existing natural images to train the depth estimator. Furthermore, our model learns to distinguish between text and images in the comics panels to reduce text-based artefacts in the depth estimates. Our method consistently outperforms the existing state-of-the-art approaches across all metrics on both the DCM and eBDtheque images. Finally, we introduce a dataset to evaluate depth prediction on comics.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bhattacharjee_Estimating_Image_Depth_in_the_Comics_Domain_WACV_2022_paper.pdf",
        "aff": "School of Computer and Communication Sciences, EPFL, Switzerland; School of Computer and Communication Sciences, EPFL, Switzerland; School of Computer and Communication Sciences, EPFL, Switzerland; School of Computer and Communication Sciences, EPFL, Switzerland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Bhattacharjee_Estimating_Image_Depth_WACV_2022_supplemental.pdf",
        "arxiv": "2110.03575",
        "pdf_size": 1242786,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11060397269822331977&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "School of Computer and Communication Sciences",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "ee8040309e",
        "title": "Evaluating and Mitigating Bias in Image Classifiers: A Causal Perspective Using Counterfactuals",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Dash_Evaluating_and_Mitigating_Bias_in_Image_Classifiers_A_Causal_Perspective_WACV_2022_paper.html",
        "author": "Saloni Dash; Vineeth N Balasubramanian; Amit Sharma",
        "abstract": "Counterfactual examples for an input---perturbations that change specific features but not others---have been shown to be useful for evaluating bias of machine learning models, e.g., against specific demographic groups. However, generating counterfactual examples for images is non-trivial due to the underlying causal structure on the various features of an image. To be meaningful, generated perturbations need to satisfy constraints implied by the causal model. We present a method for generating counterfactuals by incorporating a structural causal model (SCM) in an improved variant of Adversarially Learned Inference (ALI), that generates counterfactuals in accordance with the causal relationships between attributes of an image. Based on the generated counterfactuals, we show how to explain a pre-trained machine learning classifier, evaluate its bias, and mitigate the bias using a counterfactual regularizer. On the Morpho-MNIST dataset, our method generates counterfactuals comparable in quality to prior work on SCM-based counterfactuals. Our method also works on the more complex CelebA faces dataset. Generated counterfactuals are indistinguishable from reconstructed images in a human evaluation experiment and we use them to evaluate a standard classifier trained on CelebA data. We show that the classifier is biased w.r.t. skin and hair color, and how counterfactual regularization can remove those biases.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Dash_Evaluating_and_Mitigating_Bias_in_Image_Classifiers_A_Causal_Perspective_WACV_2022_paper.pdf",
        "aff": "Microsoft Research India, Bangalore, Karnataka, India; Indian Institute of Technology Hyderabad, Telangana, India; Microsoft Research India, Bangalore, Karnataka, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Dash_Evaluating_and_Mitigating_WACV_2022_supplemental.pdf",
        "arxiv": "2009.08270",
        "pdf_size": 2536533,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16666112155159328342&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "microsoft.com;iith.ac.in;microsoft.com",
        "email": "microsoft.com;iith.ac.in;microsoft.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Microsoft;Indian Institute of Technology Hyderabad",
        "aff_unique_dep": "Microsoft Research India;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-india;https://www.iith.ac.in",
        "aff_unique_abbr": "MSRI;IIT Hyderabad",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Bangalore;Hyderabad",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "fc76d8d1dd",
        "title": "Evaluating the Robustness of Semantic Segmentation for Autonomous Driving Against Real-World Adversarial Patch Attacks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Nesti_Evaluating_the_Robustness_of_Semantic_Segmentation_for_Autonomous_Driving_Against_WACV_2022_paper.html",
        "author": "Federico Nesti; Giulio Rossolini; Saasha Nair; Alessandro Biondi; Giorgio Buttazzo",
        "abstract": "Deep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a real-world scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scene-specific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Nesti_Evaluating_the_Robustness_of_Semantic_Segmentation_for_Autonomous_Driving_Against_WACV_2022_paper.pdf",
        "aff": "Department of Excellence in Robotics & AI, Scuola Superiore Sant\u2019Anna; Department of Excellence in Robotics & AI, Scuola Superiore Sant\u2019Anna; Department of Excellence in Robotics & AI, Scuola Superiore Sant\u2019Anna; Department of Excellence in Robotics & AI, Scuola Superiore Sant\u2019Anna; Department of Excellence in Robotics & AI, Scuola Superiore Sant\u2019Anna",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Nesti_Evaluating_the_Robustness_WACV_2022_supplemental.pdf",
        "arxiv": "2108.06179",
        "pdf_size": 8902510,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1564761331632398678&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "santannapisa.it;santannapisa.it;santannapisa.it;santannapisa.it;santannapisa.it",
        "email": "santannapisa.it;santannapisa.it;santannapisa.it;santannapisa.it;santannapisa.it",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Scuola Superiore Sant\u2019Anna",
        "aff_unique_dep": "Department of Excellence in Robotics & AI",
        "aff_unique_url": "https://www.sssup.it",
        "aff_unique_abbr": "SSSUP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "d191970f6d",
        "title": "Evaluation of Correctness in Unsupervised Many-to-Many Image Translation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bashkirova_Evaluation_of_Correctness_in_Unsupervised_Many-to-Many_Image_Translation_WACV_2022_paper.html",
        "author": "Dina Bashkirova; Ben Usman; Kate Saenko",
        "abstract": "Given an input image from a source domain and a guidance image from a target domain, unsupervised many-to-many image-to-image (UMMI2I) translation methods seek to generate a plausible example from the target domain that preserves domain-invariant information of the input source image and inherits the domain-specific information from the guidance image. For example, when translating female faces to male faces, the generated male face should have the same expression, pose and hair color as the input female image, and the same facial hairstyle and other male-specific attributes as the guidance male image. Current state-of-the art UMMI2I methods generate visually pleasing images, but, since for most pairs of real datasets we do not know which attributes are domain-specific and which are domain-invariant, the semantic correctness of existing approaches has not been quantitatively evaluated yet. In this paper, we propose a set of benchmarks and metrics for the evaluation of semantic correctness of these methods. We provide an extensive study of existing state-of-the-art UMMI2I translation methods, showing that all methods, to different degrees, fail to infer which attributes are domain-specific and which are domain-invariant from data, and mostly rely on inductive biases hard-coded into their architectures. Our code can be found at https://github.com/dbash/umi2i_correctness.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bashkirova_Evaluation_of_Correctness_in_Unsupervised_Many-to-Many_Image_Translation_WACV_2022_paper.pdf",
        "aff": "Boston University; Boston University; Boston University+MIT-IBM Watson AI Lab",
        "project": "",
        "github": "https://github.com/dbash/umi2i_correctness",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Bashkirova_Evaluation_of_Correctness_WACV_2022_supplemental.pdf",
        "arxiv": "2103.15727",
        "pdf_size": 3142783,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5683504151439918731&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "bu.edu;bu.edu;bu.edu",
        "email": "bu.edu;bu.edu;bu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Boston University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";IBM Watson AI Lab",
        "aff_unique_url": "https://www.bu.edu;https://www.mitibmwatsonailab.org",
        "aff_unique_abbr": "BU;MIT-IBM AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d64e8cb73e",
        "title": "Event-Based Kilohertz Eye Tracking Using Coded Differential Lighting",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Stoffregen_Event-Based_Kilohertz_Eye_Tracking_Using_Coded_Differential_Lighting_WACV_2022_paper.html",
        "author": "Timo Stoffregen; Hossein Daraei; Clare Robinson; Alexander Fix",
        "abstract": "Pixels in an event camera operate asynchronously and independently, reporting changes in intensity as events - tuples of (x,y) position, polarity s and timestamp t at microsecond resolution. Event cameras operate at low power ( 5mW) and respond to changes in the scene with a latency on the order of microseconds. These properties make event cameras an exciting candidate for eye tracking sensors on mobile platforms such as AR/VR headsets, since these systems have hard real-time and power constraints. One proven method for eye tracking and gaze estimation is corneal glint detection. We exploit the fact that corneal glint tracking only requires a sparse set of pixels in the image, by making use of the natural sparsity of event cameras, which only detect changes in the scene. To enhance this effect, we design an illumination scheme, Coded Differential Lighting, which enhances specular reflections, suppresses all other events, and solves the light-to-glint correspondence. This is the first purely event-based corneal glint detection and tracking algorithm, which operates on standard hardware at kHz sampling rate.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Stoffregen_Event-Based_Kilohertz_Eye_Tracking_Using_Coded_Differential_Lighting_WACV_2022_paper.pdf",
        "aff": "Facebook; Facebook; Facebook; Facebook",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Stoffregen_Event-Based_Kilohertz_Eye_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 7619149,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17197639413108279319&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "fb.com;fb.com;fb.com;fb.com",
        "email": "fb.com;fb.com;fb.com;fb.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Meta",
        "aff_unique_dep": "Facebook, Inc.",
        "aff_unique_url": "https://www.facebook.com",
        "aff_unique_abbr": "FB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f3578c9b87",
        "title": "Extracting Vignetting and Grain Filter Effects From Photos",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Abdelhamed_Extracting_Vignetting_and_Grain_Filter_Effects_From_Photos_WACV_2022_paper.html",
        "author": "Abdelrahman Abdelhamed; Jonghwa Yim; Abhijith Punnappurath; Michael S. Brown; Jihwan Choe; Kihwan Kim",
        "abstract": "Most smartphones support the use of real-time camera filters to impart visual effects to captured images. Currently, such filters come preinstalled on-device or need to be downloaded and installed before use (e.g., Instagram filters). Recent work [24] proposed a method to extract a camera filter directly from an example photo that has already had a filter applied. The work in [24] focused only on the color and tonal aspects of the underlying filter. In this paper, we introduce a method to extract two spatially varying effects commonly used by on-device camera filters---namely, image vignetting and image grain. Specifically, we show how to extract the parameters for vignetting and image grain present in an example image and replicate these effects as an on-device filter. We use lightweight CNNs to estimate the filter parameters and employ efficient techniques---isotropic Gaussian filters and simplex noise---for regenerating the filters. Our design achieves a reasonable trade-off between efficiency and realism. We show that our method can extract vignetting and image grain filters from stylized photos and replicate the filters on captured images more faithfully, as compared to color and style transfer methods. Our method is significantly efficient and has been already deployed to millions of flagship smartphones.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Abdelhamed_Extracting_Vignetting_and_Grain_Filter_Effects_From_Photos_WACV_2022_paper.pdf",
        "aff": "Samsung AI Center \u2013 Toronto; Samsung Electronics + NCSOFT; Samsung AI Center \u2013 Toronto; Samsung AI Center \u2013 Toronto; Samsung Electronics; Samsung Electronics",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Abdelhamed_Extracting_Vignetting_and_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5164689,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3065315540205928990&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "samsung.com;gmail.com;samsung.com;samsung.com;samsung.com;samsung.com",
        "email": "samsung.com;gmail.com;samsung.com;samsung.com;samsung.com;samsung.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0;0;0",
        "aff_unique_norm": "Samsung;NCSOFT Corporation",
        "aff_unique_dep": "AI Center;",
        "aff_unique_url": "https://www.samsung.com/global/innovation/ai-research/;https://www.ncsoft.com",
        "aff_unique_abbr": "Samsung AI;NCSOFT",
        "aff_campus_unique_index": "0;;0;0",
        "aff_campus_unique": "Toronto;",
        "aff_country_unique_index": "0;1+1;0;0;1;1",
        "aff_country_unique": "Canada;South Korea"
    },
    {
        "id": "f5823cc278",
        "title": "Extraction of Positional Player Data From Broadcast Soccer Videos",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Theiner_Extraction_of_Positional_Player_Data_From_Broadcast_Soccer_Videos_WACV_2022_paper.html",
        "author": "Jonas Theiner; Wolfgang Gritz; Eric M\u00fcller-Budack; Robert Rein; Daniel Memmert; Ralph Ewerth",
        "abstract": "Computer-aided support and analysis are becoming increasingly important in the modern world of sports. The scouting of potential prospective players, performance as well as match analysis, and the monitoring of training programs rely more and more on data-driven technologies to ensure success. Therefore, many approaches require large amounts of data, which are, however, not easy to obtain in general. In this paper, we propose a pipeline for the fully-automated extraction of positional data from broadcast video recordings of soccer matches. In contrast to previous work, the system integrates all necessary sub-tasks like sports field registration, player detection, or team assignment that are crucial for player position estimation. The quality of the modules and the entire system is interdependent. A comprehensive experimental evaluation is presented for the individual modules as well as the entire pipeline to identify the influence of errors to subsequent modules and the overall result. In this context, we propose novel evaluation metrics to compare the output with ground-truth positional data.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Theiner_Extraction_of_Positional_Player_Data_From_Broadcast_Soccer_Videos_WACV_2022_paper.pdf",
        "aff": "L3S Research Center, Leibniz University Hannover, Hannover, Germany; L3S Research Center, Leibniz University Hannover, Hannover, Germany; TIB \u2013 Leibniz Information Centre for Science and Technology, Hannover, Germany; Institute of Exercise and Sport Informatics, German Sport University Cologne; Institute of Exercise and Sport Informatics, German Sport University Cologne; L3S Research Center, Leibniz University Hannover, Hannover, Germany+TIB \u2013 Leibniz Information Centre for Science and Technology, Hannover, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9413699,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14857685051447043890&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "l3s.de;l3s.de; ;dshs-koeln.de;dshs-koeln.de; ",
        "email": "l3s.de;l3s.de; ;dshs-koeln.de;dshs-koeln.de; ",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;2;0+1",
        "aff_unique_norm": "Leibniz University Hannover;Leibniz Information Centre for Science and Technology;German Sport University Cologne",
        "aff_unique_dep": "L3S Research Center;;Institute of Exercise and Sport Informatics",
        "aff_unique_url": "https://www.uni-hannover.de;https://www.tib.eu;https://www.dshs.de",
        "aff_unique_abbr": "LUH;TIB;",
        "aff_campus_unique_index": "0;0;0;0+0",
        "aff_campus_unique": "Hannover;",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "4d657c95e1",
        "title": "Extractive Knowledge Distillation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kobayashi_Extractive_Knowledge_Distillation_WACV_2022_paper.html",
        "author": "Takumi Kobayashi",
        "abstract": "Knowledge distillation (KD) transfers knowledge of a teacher model to improve performance of a student model which is usually equipped with lower capacity. In the KD framework, however, it is unclear what kind of knowledge is effective and how it is transferred. This paper analyzes a KD process to explore the key factors. In a KD formulation, softmax temperature entangles three main components of student and teacher probabilities and a weight for KD, making it hard to analyze contributions of those factors separately. We disentangle those components so as to further analyze especially the temperature and improve the components respectively. Based on the analysis about temperature and uniformity of the teacher probability, we propose a method, called extractive distillation, for extracting effective knowledge from the teacher model. The extractive KD touches only teacher knowledge, thus being applicable to various KD methods. In the experiments on image classification tasks using Cifar-100 and TinyImageNet datasets, we demonstrate that the proposed method outperforms the other KD methods and analyze feature representation to show its effectiveness in the framework of transfer learning.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kobayashi_Extractive_Knowledge_Distillation_WACV_2022_paper.pdf",
        "aff": "National Institute of Advanced Industrial Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1577252,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13843862102870667033&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "aist.go.jp",
        "email": "aist.go.jp",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "National Institute of Advanced Industrial Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.aist.go.jp",
        "aff_unique_abbr": "AIST",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "f4e4298720",
        "title": "F-CAM: Full Resolution Class Activation Maps via Guided Parametric Upscaling",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Belharbi_F-CAM_Full_Resolution_Class_Activation_Maps_via_Guided_Parametric_Upscaling_WACV_2022_paper.html",
        "author": "Soufiane Belharbi; Aydin Sarraf; Marco Pedersoli; Ismail Ben Ayed; Luke McCaffrey; Eric Granger",
        "abstract": "Class Activation Mapping (CAM) methods have recently gained much attention for weakly-supervised object localization (WSOL) tasks. They allow for CNN visualization and interpretation without training on fully annotated image datasets. CAM methods are typically integrated within off-the-shelf CNN backbones, such as ResNet50. Due to convolution and pooling operations, these backbones yield low resolution CAMs with a down-scaling factor of up to 32, contributing to inaccurate localizations. Interpolation is required to restore full size CAMs, yet it does not consider the statistical properties of objects, such as color and texture, leading to activations with inconsistent boundaries, and inaccurate localizations. As an alternative, we introduce a generic method for parametric upscaling of CAMs that allows constructing accurate full resolution CAMs (F-CAMs). In particular, we propose a trainable decoding architecture that can be connected to any CNN classifier to produce highly accurate CAM localizations. Given an original low resolution CAM, foreground and background pixels are randomly sampled to fine-tune the decoder. Additional priors such as image statistics and size constraints are also considered to expand and refine object boundaries. Extensive experiments, over three CNN backbones and six WSOL baselines on the CUB-200-2011 and OpenImages datasets, indicate that our F-CAM method yields a significant improvement in CAM localization accuracy. F-CAM performance is competitive with state-of-art WSOL methods, yet it requires fewer computations during inference.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Belharbi_F-CAM_Full_Resolution_Class_Activation_Maps_via_Guided_Parametric_Upscaling_WACV_2022_paper.pdf",
        "aff": "LIVIA, Dept. of Systems Engineering, \u00c9cole de technologie sup\u00e9rieure, Montreal, Canada; Ericsson, Global AI Accelerator, Montreal, Canada; LIVIA, Dept. of Systems Engineering, \u00c9cole de technologie sup\u00e9rieure, Montreal, Canada; LIVIA, Dept. of Systems Engineering, \u00c9cole de technologie sup\u00e9rieure, Montreal, Canada; Goodman Cancer Research Centre, Dept. of Oncology, McGill University, Montreal, Canada; LIVIA, Dept. of Systems Engineering, \u00c9cole de technologie sup\u00e9rieure, Montreal, Canada",
        "project": "",
        "github": "https://github.com/sbelharbi/fcam-wsol",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Belharbi_F-CAM_Full_Resolution_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3463442,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14510292417856771557&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ens.etsmtl.ca;ericsson.com;etsmtl.ca;etsmtl.ca;mcgill.ca;etsmtl.ca",
        "email": "ens.etsmtl.ca;ericsson.com;etsmtl.ca;etsmtl.ca;mcgill.ca;etsmtl.ca",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;2;0",
        "aff_unique_norm": "\u00c9cole de technologie sup\u00e9rieure;Ericsson;McGill University",
        "aff_unique_dep": "Dept. of Systems Engineering;Global AI Accelerator;Dept. of Oncology",
        "aff_unique_url": "https://www.etsmtl.ca;https://www.ericsson.com;https://www.mcgill.ca",
        "aff_unique_abbr": "ETS;;McGill",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "0b6929eaa8",
        "title": "FASSST: Fast Attention Based Single-Stage Segmentation Net for Real-Time Instance Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Cheng_FASSST_Fast_Attention_Based_Single-Stage_Segmentation_Net_for_Real-Time_Instance_WACV_2022_paper.html",
        "author": "Yuan Cheng; Rui Lin; Peining Zhen; Tianshu Hou; Chiu Wa Ng; Hai-Bao Chen; Hao Yu; Ngai Wong",
        "abstract": "Real-time instance segmentation is crucial in various AI applications. This work designs a network named Fast Attention based Single-Stage Segmentation NeT (FASSST) that performs instance segmentation with video-grade speed. Using an instance attention module (IAM), FASSST quickly locates target instances and segments with region of interest (ROI) feature fusion (RFF) aggregating ROI features from pyramid mask layers. The module employs an efficient single-stage feature regression, straight from features to instance coordinates and class probabilities. Experiments on COCO and CityScapes datasets show that FASSST achieves state-of-the-art performance under competitive accuracy: real-time inference of 47.5FPS on a GTX1080Ti GPU and 5.3FPS on a Jetson Xavier NX board with only 71.6GFLOPs.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Cheng_FASSST_Fast_Attention_Based_Single-Stage_Segmentation_Net_for_Real-Time_Instance_WACV_2022_paper.pdf",
        "aff": "Shanghai Jiao Tong University*; The University of Hong Kong\u2020; Shanghai Jiao Tong University*; Shanghai Jiao Tong University*; The University of Hong Kong\u2020; Shanghai Jiao Tong University*; Southern University of Science and Technology\u2021; The University of Hong Kong\u2020",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6310569,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3959722177434729078&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "sjtu.edu.cn; ; ; ; ; ; ; ",
        "email": "sjtu.edu.cn; ; ; ; ; ; ; ",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;1;0;2;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;University of Hong Kong;Southern University of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.hku.hk;https://www.sustech.edu.cn",
        "aff_unique_abbr": "SJTU;HKU;SUSTech",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "b1df50ee0e",
        "title": "FLUID: Few-Shot Self-Supervised Image Deraining",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Nandan_FLUID_Few-Shot_Self-Supervised_Image_Deraining_WACV_2022_paper.html",
        "author": "Shyam Nandan Rai; Rohit Saluja; Chetan Arora; Vineeth N Balasubramanian; Anbumani Subramanian; C.V. Jawahar",
        "abstract": "Self-supervised methods have shown promising results in denoising and dehazing tasks, where the collection of the paired dataset is challenging and expensive. However, we find that these methods fail to remove the rain streaks when applied for image deraining tasks. The method's poor performance is due to the explicit assumptions: (i) the distribution of noise or haze is uniform and (ii) the value of a noisy or hazy pixel is independent of its neighbors. The rainy pixels are non-uniformly distributed, and it is not necessarily dependant on its neighboring pixels. Hence, we conclude that the self-supervised method needs to have some prior knowledge about rain distribution to perform the deraining task. To provide this knowledge, we hypothesize a network trained with minimal supervision to estimate the likelihood of rainy pixels. This leads us to our proposed method called FLUID: Few Shot Self-Supervised Image Deraining. We perform extensive experiments and comparisons with existing image deraining and few-shot image-to-image translation methods on Rain 100L and DDN-SIRR datasets containing real and synthetic rainy images. In addition, we use the Rainy Cityscapes dataset to show that our method trained in a few-shot setting can improve semantic segmentation and object detection in rainy conditions. Our approach obtains a mIoU gain of 51.20 over the current best-performing deraining method.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Nandan_FLUID_Few-Shot_Self-Supervised_Image_Deraining_WACV_2022_paper.pdf",
        "aff": "CVIT - IIIT Hyderabad, India; CVIT - IIIT Hyderabad, India; IIT Delhi, India; IIT Hyderabad, India; CVIT - IIIT Hyderabad, India; CVIT - IIIT Hyderabad, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Nandan_FLUID_Few-Shot_Self-Supervised_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6745453,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17546993295307118350&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;research.iiit.ac.in;cse.iitd.ac.in;iith.ac.in;iiit.ac.in;iiit.ac.in",
        "email": "gmail.com;research.iiit.ac.in;cse.iitd.ac.in;iith.ac.in;iiit.ac.in;iiit.ac.in",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad;Indian Institute of Technology Delhi;Indian Institute of Technology Hyderabad",
        "aff_unique_dep": "Center for Visual Information Technology;;",
        "aff_unique_url": "https://www.iiit Hyderabad.ac.in;https://www.iitd.ac.in;https://www.iith.ac.in",
        "aff_unique_abbr": "IIIT Hyderabad;IITD;IIT Hyderabad",
        "aff_campus_unique_index": "0;0;1;0;0;0",
        "aff_campus_unique": "Hyderabad;Delhi",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "514e46038b",
        "title": "FT-DeepNets: Fault-Tolerant Convolutional Neural Networks With Kernel-Based Duplication",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Baek_FT-DeepNets_Fault-Tolerant_Convolutional_Neural_Networks_With_Kernel-Based_Duplication_WACV_2022_paper.html",
        "author": "Iljoo Baek; Wei Chen; Zhihao Zhu; Soheil Samii; Raj Rajkumar",
        "abstract": "Deep neural network (deepnet) applications play a crucial role in safety-critical systems such as autonomous vehicles (AVs). An AV must drive safely towards its destination, avoiding obstacles, and respond quickly when the vehicle must stop. Any transient errors in software calculations or hardware memory in these deepnet applications can potentially lead to dramatically incorrect results. Therefore, assessing and mitigating any transient errors and providing robust results are important for safety-critical systems. Previous research on this subject focused on detecting errors and then recovering from the errors by re-running the network. Other approaches were based on the extent of full network duplication such as the ensemble learning-based approach to boost system fault-tolerance by leveraging each model's advantages. However, it is hard to detect errors in a deep neural network, and the computational overhead of full redundancy can be substantial. We first study the impact of the error types and locations in deepnets. We next focus on selecting which part should be duplicated using multiple ranking methods to measure the order of importance among neurons. We find that the duplication overhead for computation and memory is a trade-off between algorithmic performance and robustness. To achieve higher robustness with less system overhead, we present two error protection mechanisms that only duplicate parts of the network from critical neurons. Finally, we substantiate the practical feasibility of our approach and evaluate the improvement in the accuracy of a deepnet in the presence of errors. We demonstrate these results using a case study with real-world applications on an Nvidia GeForce RTX 2070Ti GPU and an Nvidia Xavier embedded platform used by automotive OEMs.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Baek_FT-DeepNets_Fault-Tolerant_Convolutional_Neural_Networks_With_Kernel-Based_Duplication_WACV_2022_paper.pdf",
        "aff": "Carnegie Mellon University; Purdue University; Carnegie Mellon University; Motional / Link \u00a8oping University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1429349,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3696838715315850506&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "andrew.cmu.edu;purdue.edu;gmail.com;motional.com / soheil.samii;andrew.cmu.edu",
        "email": "andrew.cmu.edu;purdue.edu;gmail.com;motional.com / soheil.samii;andrew.cmu.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "Carnegie Mellon University;Purdue University;Link\u00f6ping University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.purdue.edu;https://www.liu.se",
        "aff_unique_abbr": "CMU;Purdue;LiU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United States;Sweden"
    },
    {
        "id": "c4844580b7",
        "title": "Face Verification With Challenging Imposters and Diversified Demographics",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Popescu_Face_Verification_With_Challenging_Imposters_and_Diversified_Demographics_WACV_2022_paper.html",
        "author": "Adrian Popescu; Liviu-Daniel \u0218tefan; J\u00e9r\u00f4me Deshayes-Chossart; Bogdan Ionescu",
        "abstract": "Face verification aims to distinguish between genuine and imposter pairs of faces, which include the same or different identities, respectively. The performance reported in recent years gives the impression that the task is practically solved. Here, we revisit the problem and argue that existing evaluation datasets were built using two oversimplifying design choices. First, the usual identity selection to form imposter pairs is not challenging enough because, in practice, verification is needed to detect challenging imposters. Second, the underlying demographics of existing datasets are often insufficient to account for the wide diversity of facial characteristics of people from across the world. To mitigate these limitations, we introduce the FaVCI2D dataset. Imposter pairs are challenging because they include visually similar faces selected from a large pool of demographically diversified identities. The dataset also includes metadata related to gender, country and age to facilitate fine-grained analysis of results. FaVCI2D is generated from freely distributable resources and is compliant with data protection regulations. Experiments with state-of-the-art deep models that provide nearly 100% performance on existing datasets show a significant performance drop for FaVCI2D, confirming our starting hypothesis. Equally important, we analyze legal and ethical challenges which appeared in recent years and hindered the development of face analysis research. We introduce a series of design choices which address these challenges and make the dataset constitution and usage more sustainable and fairer.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Popescu_Face_Verification_With_Challenging_Imposters_and_Diversified_Demographics_WACV_2022_paper.pdf",
        "aff": "Universit\u00b4e Paris-Saclay, CEA, List, F-91120, Palaiseau, France; University Politehnica of Bucharest, Romania; Universit\u00b4e Paris-Saclay, CEA, List, F-91120, Palaiseau, France; University Politehnica of Bucharest, Romania",
        "project": "",
        "github": "https://github.com/AIMultimediaLab/FaVCI2D-Face-Verification-with-Challenging-Imposters-and-Diversified-Demographics",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Popescu_Face_Verification_With_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 506148,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9966327437451254828&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cea.fr;upb.ro;cea.fr;upb.ro",
        "email": "cea.fr;upb.ro;cea.fr;upb.ro",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Universit\u00e9 Paris-Saclay;University Politehnica of Bucharest",
        "aff_unique_dep": "CEA List;",
        "aff_unique_url": "https://www.universite-paris-saclay.fr;https://www.upb.ro",
        "aff_unique_abbr": "UPS;UPB",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Palaiseau;",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "France;Romania"
    },
    {
        "id": "4014aa805e",
        "title": "Facial Attribute Transformers for Precise and Robust Makeup Transfer",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wan_Facial_Attribute_Transformers_for_Precise_and_Robust_Makeup_Transfer_WACV_2022_paper.html",
        "author": "Zhaoyi Wan; Haoran Chen; Jie An; Wentao Jiang; Cong Yao; Jiebo Luo",
        "abstract": "In this paper, we address the problem of makeup transfer, which aims at transplanting the makeup from the reference face to the source face while preserving the identity of the source. Existing makeup transfer methods have made notable progress in generating realistic makeup faces, but do not perform well in terms of color fidelity and spatial transformation. To tackle these issues, we propose a novel Facial Attribute Transformer (FAT) and its variant Spatial FAT for high-quality makeup transfer. Drawing inspirations from the Transformer in NLP, FAT is able to model the semantic correspondences and interactions between the source face and reference face, and then precisely estimate and transfer the facial attributes. To further facilitate shape deformation and transformation of facial parts, we also integrate thin plate splines (TPS) into FAT, thus creating Spatial FAT, which is the first method that can transfer geometric attributes in addition to color and texture. Extensive qualitative and quantitative experiments demonstrate the effectiveness and superiority of our proposed FATs in the following aspects: (1) ensuring high-fidelity color transfer; (2) allowing for geometric transformation of facial parts; (3) handling facial variations (such as poses and shadows) and (4) supporting high-resolution face generation.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wan_Facial_Attribute_Transformers_for_Precise_and_Robust_Makeup_Transfer_WACV_2022_paper.pdf",
        "aff": "University of Rochester; Megvii; University of Rochester; Beihang University; Megvii; University of Rochester",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Wan_Facial_Attribute_Transformers_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1250606,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4501105631407221969&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "wanzy.me; ;cs.rochester.edu; ; ;cs.rochester.edu",
        "email": "wanzy.me; ;cs.rochester.edu; ; ;cs.rochester.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2;1;0",
        "aff_unique_norm": "University of Rochester;Megvii Technology;Beihang University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.rochester.edu;https://www.megvii.com;http://www.buaa.edu.cn/",
        "aff_unique_abbr": "U of R;Megvii;BUAA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "03b81c0c94",
        "title": "Fair Visual Recognition in Limited Data Regime Using Self-Supervision and Self-Distillation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Mazumder_Fair_Visual_Recognition_in_Limited_Data_Regime_Using_Self-Supervision_and_WACV_2022_paper.html",
        "author": "Pratik Mazumder; Pravendra Singh; Vinay P. Namboodiri",
        "abstract": "Deep learning models generally learn the biases present in the training data. Researchers have proposed several approaches to mitigate such biases and make the model fair. Bias mitigation techniques assume that a sufficiently large number of training examples are present. However, we observe that if the training data is limited, then the effectiveness of bias mitigation methods is severely degraded. In this paper, we propose a novel approach to address this problem. Specifically, we adapt self-supervision and self-distillation to reduce the impact of biases on the model in this setting. Self-supervision and self-distillation are not used for bias mitigation. However, through this work, we demonstrate for the first time that these techniques are very effective in bias mitigation. We empirically show that our approach can significantly reduce the biases learned by the model. Further, we experimentally demonstrate that our approach is complementary to other bias mitigation strategies. Our approach significantly improves their performance and further reduces the model biases in the limited data regime. Specifically, on the L-CIFAR-10S skewed dataset, our approach significantly reduces the bias score of the baseline model by 78.22% and outperforms it in terms of accuracy by a significant absolute margin of 8.89%. It also significantly reduces the bias score for the state-of-the-art domain independent bias mitigation method by 59.26% and improves its performance by a significant absolute margin of 7.08%.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Mazumder_Fair_Visual_Recognition_in_Limited_Data_Regime_Using_Self-Supervision_and_WACV_2022_paper.pdf",
        "aff": "IIT Kanpur, India; IIT Roorkee, India; IIT Kanpur, India+University of Bath, United Kingdom",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Mazumder_Fair_Visual_Recognition_WACV_2022_supplemental.pdf",
        "arxiv": "2107.00067",
        "pdf_size": 841428,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12007634035556190571&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cse.iitk.ac.in;cs.iitr.ac.in;bath.ac.uk",
        "email": "cse.iitk.ac.in;cs.iitr.ac.in;bath.ac.uk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;Indian Institute of Technology Roorkee;University of Bath",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.iitk.ac.in;https://www.iitr.ac.in;https://www.bath.ac.uk",
        "aff_unique_abbr": "IITK;IIT Roorkee;Bath",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "India;United Kingdom"
    },
    {
        "id": "4f70c4e61b",
        "title": "Fair and Accurate Age Prediction Using Distribution Aware Data Curation and Augmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Cao_Fair_and_Accurate_Age_Prediction_Using_Distribution_Aware_Data_Curation_WACV_2022_paper.html",
        "author": "Yushi Cao; David Berend; Palina Tolmach; Guy Amit; Moshe Levy; Yang Liu; Asaf Shabtai; Yuval Elovici",
        "abstract": "Deep learning-based facial recognition systems have experienced increased media attention due to exhibiting unfair behavior. Large enterprises, such as IBM, shut down their facial recognition and age prediction systems as a consequence. Age prediction is an especially difficult application with the issue of fairness remaining an open research problem (e.g. predicting age for different ethnicity equally accurate). One of the main causes of unfair behavior in age prediction methods lies in the distribution and diversity of the training data. In this work, we present two novel approaches for dataset curation and data augmentation in order to increase fairness through balanced feature curation and increase diversity through distribution aware augmentation. To achieve this, we introduce out-of-distribution detection to the facial recognition domain which is used to select the data most relevant to the deep neural network's (DNN) task when balancing the data among age, ethnicity, and gender. Our approach shows promising results. Our best-trained DNN model outperformed all academic and industrial baselines in terms of fairness by up to 4.92 times and also enhanced the DNN's ability to generalise outperforming Amazon AWS and Microsoft Azure public cloud systems by 31.88% and 10.95%, respectively.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Cao_Fair_and_Accurate_Age_Prediction_Using_Distribution_Aware_Data_Curation_WACV_2022_paper.pdf",
        "aff": "Nanyang Technological University; Nanyang Technological University; Nanyang Technological University; Ben-Gurion University of the Negev; Ben-Gurion University of the Negev; Nanyang Technological University; Ben-Gurion University of the Negev; Ben-Gurion University of the Negev",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Cao_Fair_and_Accurate_WACV_2022_supplemental.pdf",
        "arxiv": "2009.05283",
        "pdf_size": 3598906,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7549841237106539915&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "e.ntu.edu.sg;e.ntu.edu.sg;e.ntu.edu.sg;post.bgu.ac.il;post.bgu.ac.il;ntu.edu.sg;bgu.ac.il;bgu.ac.il",
        "email": "e.ntu.edu.sg;e.ntu.edu.sg;e.ntu.edu.sg;post.bgu.ac.il;post.bgu.ac.il;ntu.edu.sg;bgu.ac.il;bgu.ac.il",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1;0;1;1",
        "aff_unique_norm": "Nanyang Technological University;Ben-Gurion University of the Negev",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.bgu.ac.il",
        "aff_unique_abbr": "NTU;BGU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;1;0;1;1",
        "aff_country_unique": "Singapore;Israel"
    },
    {
        "id": "95a2b605fb",
        "title": "FalCon: Fine-Grained Feature Map Sparsity Computing With Decomposed Convolutions for Inference Optimization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Xu_FalCon_Fine-Grained_Feature_Map_Sparsity_Computing_With_Decomposed_Convolutions_for_WACV_2022_paper.html",
        "author": "Zirui Xu; Fuxun Yu; Chenxi Liu; Zhe Wu; Hongcheng Wang; Xiang Chen",
        "abstract": "Many works focus on the model's static parameter optimization (e.g., filters and weights) for CNN inference acceleration. Compared to parameter sparsity, feature map sparsity is per-input related which has better adaptability. The practical sparsity patterns are non-structural and randomly located on feature maps with non-identical shapes. However, the existing feature map sparsity works take computing efficiency as the primary goal, thereby they can only remove structural sparsity and fail to match the above characteristics. In this paper, we develop a novel sparsity computing scheme called FalCon, which can well adapt to the practical sparsity patterns while still maintaining efficient computing. Specifically, we first propose a decomposed convolution design that enables a fine-grained computing unit for sparsity. Additionally, a decomposed convolution computing optimization paradigm is proposed to convert the sparse computing units to practical acceleration. Extensive experiments show that FalCon achieves at most 67.30% theoretical computation reduction with a neglected accuracy drop while accelerating CNN inference by 37%.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Xu_FalCon_Fine-Grained_Feature_Map_Sparsity_Computing_With_Decomposed_Convolutions_for_WACV_2022_paper.pdf",
        "aff": "George Mason University; George Mason University; George Mason University; Comcast Applied AI Research; Comcast Applied AI Research; George Mason University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 536236,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9525384205289562914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmu.edu;gmu.edu;gmu.edu;comcast.com;comcast.com;gmu.edu",
        "email": "gmu.edu;gmu.edu;gmu.edu;comcast.com;comcast.com;gmu.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1;0",
        "aff_unique_norm": "George Mason University;Comcast",
        "aff_unique_dep": ";Applied AI Research",
        "aff_unique_url": "https://www.gmu.edu;https://www.comcast.com",
        "aff_unique_abbr": "GMU;Comcast",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "107be7e62c",
        "title": "Fast Nonlinear Image Unblending",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Horita_Fast_Nonlinear_Image_Unblending_WACV_2022_paper.html",
        "author": "Daichi Horita; Kiyoharu Aizawa; Ryohei Suzuki; Taizan Yonetsuji; Huachun Zhu",
        "abstract": "Nonlinear color blending, which is advanced blending indicated by blend modes such as overlay and multiply, is extensively employed by digital creators to produce attractive visual effects. To enjoy such flexible editing modalities on existing bitmap images like photographs, however, creators need a fast nonlinear blending algorithm that decomposes an image into a set of semi-transparent layers. To address this issue, we propose a neural-network-based method for nonlinear decomposition of an input image into linear and nonlinear alpha layers that can be separately modified for editing purposes, based on the specified color palettes and blend modes. Experiments show that our proposed method achieves an inference speed 370 times faster than the state-of-the-art method of nonlinear image unblending, which uses computationally intensive iterative optimization. Furthermore, our reconstruction quality is higher or comparable than other methods, including linear blending models. In addition, we provide examples that apply our method to image editing with nonlinear blend modes. Our code will be made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Horita_Fast_Nonlinear_Image_Unblending_WACV_2022_paper.pdf",
        "aff": "The University of Tokyo; The University of Tokyo; Preferred Networks, Inc.; Preferred Networks, Inc.; Preferred Networks, Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2105286,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8383616875457935511&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;1",
        "aff_unique_norm": "University of Tokyo;Preferred Networks, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.preferred-networks.com",
        "aff_unique_abbr": "UTokyo;PFN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "d91e659c12",
        "title": "Fast and Efficient Restoration of Extremely Dark Light Fields",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lamba_Fast_and_Efficient_Restoration_of_Extremely_Dark_Light_Fields_WACV_2022_paper.html",
        "author": "Mohit Lamba; Kaushik Mitra",
        "abstract": "The ability of Light Field (LF) cameras to capture the 3D geometry of a scene in a single photographic exposure has become central to several applications ranging from passive depth estimation to autonomous driving. But these applications cannot rely on LF captured in low-light conditions due to excessive noise and poor image photometry. The existing low-light enhancement techniques are inappropriate for mitigating this problem as they do not leverage LF's multi-view perspective and give blurry restorations. The recent L3Fnet algorithm alleviates this problem reasonably, but its enormous time and memory complexity make it unaffordable for real-world applications. Thus, we propose a three-stage network that is simultaneously much faster and more accurate. We are more accurate because the three stages compute three complementary features: global, local, and view specific features, which are then fused by our RNN inspired feedforward network to restore LF views. We are faster because we restore multiple views simultaneously and so require less number of forward passes. Besides these advantages, our network is flexible enough to restore a m xm LF during inference even if trained for a smaller n xn (n<m) LF without any finetuning. Extensive experiments on real low-light LF demonstrate that compared to state-of-the-art, our model can achieve up to 1 dB higher restoration PSNR, with 9 xspeedup, 23% smaller model size and about 5 xlower floating-point operations.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lamba_Fast_and_Efficient_Restoration_of_Extremely_Dark_Light_Fields_WACV_2022_paper.pdf",
        "aff": "Indian Institute of Technology Madras, Department of Electrical Engineering; Indian Institute of Technology Madras, Department of Electrical Engineering",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Lamba_Fast_and_Efficient_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 7815275,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11540641277313617435&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "smail.iitm.ac.in;ee.iitm.ac.in",
        "email": "smail.iitm.ac.in;ee.iitm.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Madras",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.iitm.ac.in",
        "aff_unique_abbr": "IIT Madras",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madras",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "bd008747bd",
        "title": "Fast and Explicit Neural View Synthesis",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Guo_Fast_and_Explicit_Neural_View_Synthesis_WACV_2022_paper.html",
        "author": "Pengsheng Guo; Miguel Angel Bautista; Alex Colburn; Liang Yang; Daniel Ulbricht; Joshua M. Susskind; Qi Shan",
        "abstract": "We study the problem of novel view synthesis from sparse source observations of a scene comprised of 3D objects. We propose a simple yet effective approach that is neither continuous nor implicit, challenging recent trends on view synthesis. Our approach explicitly encodes observations into a volumetric representation that enables amortized rendering. We demonstrate that although continuous radiance field representations have gained a lot of attention due to their expressive power, our simple approach obtains comparable or even better novel view reconstruction quality comparing with state-of-the-art baselines while increasing rendering speed by over 400x. Our model is trained in a category-agnostic manner and does not require scene-specific optimization. Therefore, it is able to generalize novel view synthesis to object categories not seen during training. In addition, we show that with our simple formulation, we can use view synthesis as a self-supervision signal for efficient learning of 3D geometry without explicit 3D supervision.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Guo_Fast_and_Explicit_Neural_View_Synthesis_WACV_2022_paper.pdf",
        "aff": "Apple; Apple; Apple; Apple; Apple; Apple; Apple",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Guo_Fast_and_Explicit_WACV_2022_supplemental.pdf",
        "arxiv": "2107.05775",
        "pdf_size": 9127980,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13100963751625942177&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com",
        "email": "apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Apple",
        "aff_unique_dep": "Apple Inc.",
        "aff_unique_url": "https://www.apple.com",
        "aff_unique_abbr": "Apple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9c0f3065fb",
        "title": "Fast-CLOCs: Fast Camera-LiDAR Object Candidates Fusion for 3D Object Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Pang_Fast-CLOCs_Fast_Camera-LiDAR_Object_Candidates_Fusion_for_3D_Object_Detection_WACV_2022_paper.html",
        "author": "Su Pang; Daniel Morris; Hayder Radha",
        "abstract": "When compared to single modality approaches, fusion-based object detection methods often require more complex models to integrate heterogeneous sensor data, and use more GPU memory and computational resources. This is particularly true for camera-LiDAR based multimodal fusion, which may require three separate deep-learning networks and/or processing pipelines that are designated for the visual data, LiDAR data, and for some form of a fusion framework. In this paper, we propose Fast Camera-LiDAR Object Candidates (Fast-CLOCs) fusion network that can run high-accuracy fusion-based 3D object detection in near real-time. Fast-CLOCs operates on the output candidates before Non-Maximum Suppression (NMS) of any 3D detector, and adds a lightweight 3D detector-cued 2D image detector (3D-Q-2D) to extract visual features from the image domain to improve 3D detections significantly. The 3D detection candidates are shared with the proposed 3D-Q-2D image detector as proposals to reduce the network complexity drastically. The superior experimental results of our Fast-CLOCs on the challenging KITTI and nuScenes datasets illustrate that our Fast-CLOCs outperforms state-of-the-art fusion-based 3D object detection approaches.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Pang_Fast-CLOCs_Fast_Camera-LiDAR_Object_Candidates_Fusion_for_3D_Object_Detection_WACV_2022_paper.pdf",
        "aff": "Michigan State University; Michigan State University; Michigan State University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Pang_Fast-CLOCs_Fast_Camera-LiDAR_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2952084,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18173749784388094443&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "msu.edu;msu.edu;msu.edu",
        "email": "msu.edu;msu.edu;msu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7ad9c854ca",
        "title": "FastAno: Fast Anomaly Detection via Spatio-Temporal Patch Transformation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Park_FastAno_Fast_Anomaly_Detection_via_Spatio-Temporal_Patch_Transformation_WACV_2022_paper.html",
        "author": "Chaewon Park; MyeongAh Cho; Minhyeok Lee; Sangyoun Lee",
        "abstract": "Video anomaly detection has gained significant attention due to the increasing requirements of automatic monitoring for surveillance videos. Especially, the prediction based approach is one of the most studied methods to detect anomalies by predicting frames that include abnormal events in the test set after learning with the normal frames of the training set. However, a lot of prediction networks are computationally expensive owing to the use of pre-trained optical flow networks, or fail to detect abnormal situations because of their strong generative ability to predict even the anomalies. To address these shortcomings, we propose spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids in order to enhance the learning of normal features. Additionally, the proposed patch transformation is used only during the training phase, allowing our model to detect abnormal frames at fast speed during inference. Our model is evaluated on three anomaly detection benchmarks, achieving competitive accuracy and surpassing all the previous works in terms of speed.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Park_FastAno_Fast_Anomaly_Detection_via_Spatio-Temporal_Patch_Transformation_WACV_2022_paper.pdf",
        "aff": "Yonsei University, Seoul, Republic of Korea; Yonsei University, Seoul, Republic of Korea; Yonsei University, Seoul, Republic of Korea; Yonsei University, Seoul, Republic of Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2106.08613",
        "pdf_size": 1698357,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2278635238009411001&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "email": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Yonsei University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yonsei.ac.kr",
        "aff_unique_abbr": "Yonsei",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "8d6e86ca88",
        "title": "Federated Multi-Target Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yao_Federated_Multi-Target_Domain_Adaptation_WACV_2022_paper.html",
        "author": "Chun-Han Yao; Boqing Gong; Hang Qi; Yin Cui; Yukun Zhu; Ming-Hsuan Yang",
        "abstract": "Federated learning methods enable us to train machine learning models on distributed user data while preserving its privacy. However, it is not always feasible to obtain high-quality supervisory signals from users, especially for vision tasks. Unlike typical federated settings with labeled client data, we consider a more practical scenario where the distributed client data is unlabeled, and a centralized labeled dataset is available on the server. We further take the server-client and inter-client domain shifts into account and pose a domain adaptation problem with one source (centralized server data) and multiple targets (distributed client data). Within this new Federated Multi-Target Domain Adaptation (FMTDA) task, we analyze the model performance of existing domain adaptation methods and propose an effective DualAdapt method to address the new challenges. Extensive experimental results on image classification and semantic segmentation tasks demonstrate that our method achieves high accuracy, incurs minimal communication cost, and requires low computational resources on client devices.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yao_Federated_Multi-Target_Domain_Adaptation_WACV_2022_paper.pdf",
        "aff": "UC Merced; Google; Google; Google; Google; UC Merced+Google+Yonsei University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yao_Federated_Multi-Target_Domain_WACV_2022_supplemental.pdf",
        "arxiv": "2108.07792",
        "pdf_size": 2400179,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7842317320174556496&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;0+1+2",
        "aff_unique_norm": "University of California, Merced;Google;Yonsei University",
        "aff_unique_dep": ";Google;",
        "aff_unique_url": "https://www.ucmerced.edu;https://www.google.com;https://www.yonsei.ac.kr",
        "aff_unique_abbr": "UCM;Google;Yonsei",
        "aff_campus_unique_index": "0;1;1;1;1;0+1",
        "aff_campus_unique": "Merced;Mountain View;",
        "aff_country_unique_index": "0;0;0;0;0;0+0+1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "d22e2e6945",
        "title": "Few-Shot Object Detection by Attending to Per-Sample-Prototype",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lee_Few-Shot_Object_Detection_by_Attending_to_Per-Sample-Prototype_WACV_2022_paper.html",
        "author": "Hojun Lee; Myunggi Lee; Nojun Kwak",
        "abstract": "Few-shot object detection aims to detect instances of specific categories in a query image with only a handful of support samples. Although this takes less effort than obtaining enough annotated images for supervised object detection, it results in a far inferior performance compared to the conventional object detection methods. In this paper, we propose a meta-learning-based approach that considers the unique characteristics of each support sample. Rather than simply averaging the information of the support samples to generate a single prototype per category, our method can better utilize the information of each support sample by treating each support sample as an individual prototype. Specifically, we introduce two types of attention mechanisms for aggregating the query and support feature maps. The first is to refine the information of few-shot samples by extracting shared information between the support samples through attention. Second, each support sample is used as a class code to leverage the information by comparing similarities between each support feature and query features. Our proposed method is complementary to the previous methods, making it easy to plug and play for further improvement. We have evaluated our method on PASCAL VOC and COCO benchmarks, and the results verify the effectiveness of our method. In particular, the advantages of our method is maximized when there is more diversity among support data.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lee_Few-Shot_Object_Detection_by_Attending_to_Per-Sample-Prototype_WACV_2022_paper.pdf",
        "aff": "Seoul National University1; Seoul National University1+NA VER WEBTOON2; Seoul National University1",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2109.07734",
        "pdf_size": 4113159,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10181632872899095587&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "snu.ac.kr;webtoonscorp.com;snu.ac.kr",
        "email": "snu.ac.kr;webtoonscorp.com;snu.ac.kr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Seoul National University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.snu.ac.kr;",
        "aff_unique_abbr": "SNU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "fe0628b591",
        "title": "Few-Shot Open-Set Recognition of Hyperspectral Images With Outlier Calibration Network",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Pal_Few-Shot_Open-Set_Recognition_of_Hyperspectral_Images_With_Outlier_Calibration_Network_WACV_2022_paper.html",
        "author": "Debabrata Pal; Valay Bundele; Renuka Sharma; Biplab Banerjee; Yogananda Jeppu",
        "abstract": "We tackle the few-shot open-set recognition (FSOSR) problem in the context of remote sensing hyperspectral image (HSI) classification. Prior research on OSR mainly considers an empirical threshold on the class prediction scores to reject the outlier samples. Further, recent endeavors in few-shot HSI classification fail to recognize outliers due to the `closed-set' nature of the problem and the fact that the entire class distributions are unknown during training. To this end, we propose to optimize a novel outlier calibration network (OCN) together with a feature extraction module during the meta-training phase. The feature extractor is equipped with a novel residual 3D convolutional block attention network (R3CBAM) for enhanced spectral-spatial feature learning from HSI. Our method rejects the outliers based on OCN prediction scores barring the need for manual thresholding. Finally, we propose to augment the query set with synthesized support set features during the similarity learning stage in order to combat the data scarcity issue of few-shot learning. The superiority of the proposed model is showcased on four benchmark HSI datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Pal_Few-Shot_Open-Set_Recognition_of_Hyperspectral_Images_With_Outlier_Calibration_Network_WACV_2022_paper.pdf",
        "aff": "Indian Institute of Technology, Bombay + Honeywell Technology Solutions, India; Indian Institute of Technology, Bombay; Indian Institute of Technology, Bombay; Indian Institute of Technology, Bombay; Honeywell Technology Solutions, India",
        "project": "",
        "github": "https://github.com/DebabrataPal7/OCN",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1218433,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15722394022829871559&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0;1",
        "aff_unique_norm": "Indian Institute of Technology Bombay;Honeywell Technology Solutions",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.honeywell.com",
        "aff_unique_abbr": "IIT Bombay;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Bombay;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "ee9eee97c3",
        "title": "Few-Shot Weakly-Supervised Object Detection via Directional Statistics",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Shaban_Few-Shot_Weakly-Supervised_Object_Detection_via_Directional_Statistics_WACV_2022_paper.html",
        "author": "Amirreza Shaban; Amir Rahimi; Thalaiyasingam Ajanthan; Byron Boots; Richard Hartley",
        "abstract": "Detecting novel objects from few examples has become an emerging topic in computer vision recently. However, current methods need fully annotated training images to learn new object categories which limits their applicability in real world scenarios such as field robotics. In this work, we propose a probabilistic multiple-instance learning approach for few-shot Common Object Localization (COL) and few-shot Weakly Supervised Object Detection (WSOD). In these tasks, only image-level labels, which are much cheaper to acquire, are available. We find that operating on features extracted from the last layer of a pre-trained Faster-RCNN is more effective compared to previous episodic learning based few-shot COL methods. Our model simultaneously learns the distribution of the novel objects and localizes them via expectation-maximization steps. As a probabilistic model, we employ von Mises-Fisher (vMF) distribution which captures the semantic information better than Gaussian distribution when applied to the pre-trained embedding space. When the novel objects are localized, we utilize them to learn a linear appearance model to detect novel classes in new images. Our extensive experiments show that the proposed method, despite being simple, outperforms strong baselines in few-shot COL and WSOD, as well as large-scale WSOD tasks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Shaban_Few-Shot_Weakly-Supervised_Object_Detection_via_Directional_Statistics_WACV_2022_paper.pdf",
        "aff": "University of Washington; ANU & ACRV; ANU & ACRV; University of Washington; ANU & ACRV",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Shaban_Few-Shot_Weakly-Supervised_Object_WACV_2022_supplemental.pdf",
        "arxiv": "2103.14162",
        "pdf_size": 6529350,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1649299971303094454&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uw.edu;anu.edu.au; ; ; ",
        "email": "uw.edu;anu.edu.au; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "University of Washington;Australian National University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.anu.edu.au",
        "aff_unique_abbr": "UW;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;1",
        "aff_country_unique": "United States;Australia"
    },
    {
        "id": "13644b66dc",
        "title": "ForeSI: Success-Aware Visual Navigation Agent",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Moghaddam_ForeSI_Success-Aware_Visual_Navigation_Agent_WACV_2022_paper.html",
        "author": "Mahdi Kazemi Moghaddam; Ehsan Abbasnejad; Qi Wu; Javen Qinfeng Shi; Anton Van Den Hengel",
        "abstract": "In this work, we present a method to improve the efficiency and robustness of the previous model-free Reinforcement Learning (RL) algorithms for the task of object-goal visual navigation. Despite achieving state-of-the-art results, one of the major drawbacks of those approaches is the lack of a forward model that informs the agent about the potential consequences of its actions, i.e., being model-free. In this work, we augment the model-free RL with such a forward model that can predict a representation of a future state, from the beginning of a navigation episode, if the episode were to be successful. Furthermore, in order for efficient training, we develop an algorithm to integrate a replay buffer into the model-free RL that alternates between training the policy and the forward model. We call our agent ForeSI; ForeSI is trained to imagine a future latent state that leads to success. By explicitly imagining such a state, during the navigation, our agent is able to take better actions leading to two main advantages: first, in the absence of an object detector, ForeSI presents a more robust policy, i.e., it leads to about 5% absolute improvement on the Success Rate (SR); second, when combined with an off-the-shelf object detector to help better distinguish the target object, our method leads to about 3% absolute improvement on the SR and about 2% absolute improvement on Success weighted by inverse Path Length (SPL), i.e., presents higher efficiency.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Moghaddam_ForeSI_Success-Aware_Visual_Navigation_Agent_WACV_2022_paper.pdf",
        "aff": "The Australian Institute for Machine Learning; The University of Adelaide; The University of Adelaide; The University of Adelaide; The University of Adelaide",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Moghaddam_ForeSI_Success-Aware_Visual_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2461386,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14398829282616280715&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "Australian Institute for Machine Learning;University of Adelaide",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.aiml.com.au;https://www.adelaide.edu.au",
        "aff_unique_abbr": "AIML;Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "9028df9cd0",
        "title": "Forgery Detection by Internal Positional Learning of Demosaicing Traces",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bammey_Forgery_Detection_by_Internal_Positional_Learning_of_Demosaicing_Traces_WACV_2022_paper.html",
        "author": "Quentin Bammey; Rafael Grompone von Gioi; Jean-Michel Morel",
        "abstract": "We propose 4Point (Forensics with Positional Internal Training), an unsupervised neural network trained to assess the consistency of the image colour mosaic to find forgeries. Positional learning trains the model to learn the modulo-2 position of pixels, leveraging the translation-invariance of CNN to replicate the underlying mosaic and its potential inconsistencies. Internal learning on a single potentially forged image improves adaption and robustness to varied post-processing and counter-forensics measures. This solution beats existing mosaic detection methods, is more robust to various post-processing and counter-forensic artefacts such as JPEG compression, and can exploit traces to which state-of-the-art generic neural networks are blind. Check qbammey.github.io/4point for the code.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bammey_Forgery_Detection_by_Internal_Positional_Learning_of_Demosaicing_Traces_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4104846,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4054496472631792252&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0b7d135158",
        "title": "From Node To Graph: Joint Reasoning on Visual-Semantic Relational Graph for Zero-Shot Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Nie_From_Node_To_Graph_Joint_Reasoning_on_Visual-Semantic_Relational_Graph_WACV_2022_paper.html",
        "author": "Hui Nie; Ruiping Wang; Xilin Chen",
        "abstract": "Zero-Shot Detection (ZSD), which aims at localizing and recognizing unseen objects in a complicated scene, usually leverages the visual and semantic information of individual objects alone. However, scene understanding of human exceeds recognizing individual objects separately: the contextual information among multiple objects such as visual relational information (e.g. visually similar objects) and semantic relational information (e.g. co-occurrences) is helpful for understanding of visual scene. In this paper, we verify that contextual information plays a more important role in ZSD than in traditional object detection. To make full use of such information, we propose a new end-to-end ZSD method GRaph Aligning Network (GRAN) based on graph modeling and reasoning which simultaneously considers visual and semantic information of multiple objects instead of individual objects. Specifically, we formulate a Visual Relational Graph (VRG) and a Semantic Relational Graph (SRG), where the nodes are the objects in the image and the semantic representations of classes respectively and the edges are the relevance between nodes in each graph. To characterize mutual effect between two modalities, the two graphs are further merged into a heterogeneous Visual-Semantic Relational Graph (VSRG), where modal translators are designed for the two subgraphs to enable modal information to transform into a common space for communication, and message passing among nodes is enforced to refine their representations. Comprehensive experiments on MSCOCO dataset demonstrate the advantage of our method over state-of-the-arts, and qualitative analysis suggests the validity of using contextual information.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Nie_From_Node_To_Graph_Joint_Reasoning_on_Visual-Semantic_Relational_Graph_WACV_2022_paper.pdf",
        "aff": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China + Beijing Academy of Artificial Intelligence, Beijing, 100084, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Nie_From_Node_To_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3250299,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9687841477002495403&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1+2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Institute of Computing Technology;;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn;https://www.baaic.cn",
        "aff_unique_abbr": "CAS;UCAS;BAAI",
        "aff_campus_unique_index": "0+0;0+0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "5e83a09334",
        "title": "Fully Convolutional Cross-Scale-Flows for Image-Based Defect Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Rudolph_Fully_Convolutional_Cross-Scale-Flows_for_Image-Based_Defect_Detection_WACV_2022_paper.html",
        "author": "Marco Rudolph; Tom Wehrbein; Bodo Rosenhahn; Bastian Wandt",
        "abstract": "In industrial manufacturing processes, errors frequently occur at unpredictable times and in unknown manifestations. We tackle this problem, known as automatic defect detection, without requiring any image samples of defective parts. Recent works model the distribution of defect-free image data, using either strong statistical priors or overly simplified data representations. In contrast, our approach handles fine-grained representations incorporating the global and local image context while estimating flexibly the density. To this end, we propose a novel fully convolutional cross-scale normalizing flow (CS-Flow) that jointly processes multiple feature maps of different scales. Using normalizing flows to assign meaningful likelihoods to input samples allows for an efficient defect detection on image-level. Moreover, due to the preserved spatial arrangement the latent space of the normalizing flow is interpretable, i. e. it is applicable to localize defective regions in the image. Our work sets a new state-of-the-art in image-level defect detection on the benchmark datasets Magnetic Tile Defects and MVTec AD showing a 100% AUROC on 4 out of 15 classes.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Rudolph_Fully_Convolutional_Cross-Scale-Flows_for_Image-Based_Defect_Detection_WACV_2022_paper.pdf",
        "aff": "Leibniz University Hannover, Germany; Leibniz University Hannover, Germany; Leibniz University Hannover, Germany; University of British Columbia, Canada",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Rudolph_Fully_Convolutional_Cross-Scale-Flows_WACV_2022_supplemental.pdf",
        "arxiv": "2110.02855",
        "pdf_size": 2475718,
        "gs_citation": 315,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12556792937572177489&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "tnt.uni-hannover.de; ; ; ",
        "email": "tnt.uni-hannover.de; ; ; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Leibniz University Hannover;University of British Columbia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.leibniz.uni-hannover.de;https://www.ubc.ca",
        "aff_unique_abbr": "LUH;UBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Germany;Canada"
    },
    {
        "id": "edb41b6aa1",
        "title": "Fusion Point Pruning for Optimized 2D Object Detection With Radar-Camera Fusion",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Stacker_Fusion_Point_Pruning_for_Optimized_2D_Object_Detection_With_Radar-Camera_WACV_2022_paper.html",
        "author": "Lukas St\u00e4cker; Philipp Heidenreich; Jason Rambach; Didier Stricker",
        "abstract": "Object detection is one of the most important perception tasks for advanced driver assistant systems and autonomous driving. Due to its complementary features and moderate cost, radar-camera fusion is of particular interest in the automotive industry but comes with the challenge of how to optimally fuse the heterogeneous data sources. To solve this for 2D object detection, we propose two new techniques to project the radar detections onto the image plane, exploiting additional uncertainty information. We also introduce a new technique called fusion point pruning, which automatically finds the best fusion points of radar and image features in the neural network architecture. These new approaches combined surpass the state of the art in 2D object detection performance for radar-camera fusion models, evaluated with the nuScenes dataset. We further find that the utilization of radar-camera fusion is especially beneficial for night scenes.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Stacker_Fusion_Point_Pruning_for_Optimized_2D_Object_Detection_With_Radar-Camera_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5436313,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4951230904484700178&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0d3d0f8602",
        "title": "GANs Spatial Control via Inference-Time Adaptive Normalization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Jakoel_GANs_Spatial_Control_via_Inference-Time_Adaptive_Normalization_WACV_2022_paper.html",
        "author": "Karin Jakoel; Liron Efraim; Tamar Rott Shaham",
        "abstract": "We introduce a new approach for spatial control over the generation process of Generative Adversarial Networks (GANs). Our approach includes modifying the normalization scheme of a pre-trained GAN at test time, so as to act differently at different image regions, according to guidance from the user. This enables to achieve different generation effects at different locations across the image. In contrast to previous works that require either fine-tuning the model's parameters or training an additional network, our approach uses the pre-trained GAN as is, without any further modifications or training phase. Our method is thus completely generic and can be easily incorporated into common GAN models. We prove our technique to be useful for solving a line of image manipulation tasks, allowing different generation effects across the image, while preserving the GAN's high visual quality.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Jakoel_GANs_Spatial_Control_via_Inference-Time_Adaptive_Normalization_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Jakoel_GANs_Spatial_Control_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2674669,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10765674241433325817&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "074cc93246",
        "title": "Generalized Clustering and Multi-Manifold Learning With Geometric Structure Preservation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wu_Generalized_Clustering_and_Multi-Manifold_Learning_With_Geometric_Structure_Preservation_WACV_2022_paper.html",
        "author": "Lirong Wu; Zicheng Liu; Jun Xia; Zelin Zang; Siyuan Li; Stan Z. Li",
        "abstract": "Though manifold-based clustering has become a popular research topic, we observe that one important factor has been omitted by these works, namely that the defined clustering loss may corrupt the local and global structure of the latent space. In this paper, we propose a novel Generalized Clustering and Multi-manifold Learning (GCML) framework with geometric structure preservation for generalized data, i.e., not limited to 2-D image data and has a wide range of applications in speech, text, and biology domains. In the proposed framework, manifold clustering is done in the latent space guided by a clustering loss. To overcome the problem that the clustering-oriented loss may deteriorate the geometric structure of the latent space, an isometric loss is proposed for preserving intra-manifold structure locally and a ranking loss for inter-manifold structure globally. Extensive experimental results have shown that GCML exhibits superior performance to counterparts in terms of qualitative visualizations and quantitative metrics, which demonstrates the effectiveness of preserving geometric structure.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wu_Generalized_Clustering_and_Multi-Manifold_Learning_With_Geometric_Structure_Preservation_WACV_2022_paper.pdf",
        "aff": "Zhejiang University+Westlake University; Westlake University; Westlake University; Westlake University; Westlake University; Westlake University",
        "project": "",
        "github": "https://github.com/LirongWu/GCML",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Wu_Generalized_Clustering_and_WACV_2022_supplemental.pdf",
        "arxiv": "2009.09590",
        "pdf_size": 5992882,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13378035497256748757&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn",
        "email": "westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;1;1;1",
        "aff_unique_norm": "Zhejiang University;Westlake University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.westlake.edu.cn",
        "aff_unique_abbr": "ZJU;WU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "1805298958",
        "title": "Generalized Facial Manipulation Detection With Edge Region Feature Extraction",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kim_Generalized_Facial_Manipulation_Detection_With_Edge_Region_Feature_Extraction_WACV_2022_paper.html",
        "author": "Dong-Keon Kim; Kwang-Su Kim",
        "abstract": "This paper presents a generalized and robust face manipulation detection method based on the edge region features appearing in images. Most contemporary face synthesis processes include color awkwardness reduction but damage the natural fingerprint in the edge region. In addition, these color correction processes do not proceed in the non-face background region. We also observe that the synthesis process does not consider the natural properties of the image appearing in the time domain. Considering these observations, we propose a facial forensic framework that utilizes pixel-level color features appearing in the edge region of the whole image. Furthermore, our framework includes a 3D-CNN classification model that interprets the extracted color features spatially and temporally. Unlike other existing studies, we conduct authenticity determination by considering all features extracted from multiple frames within one video. Through extensive experiments, including real-world scenarios to evaluate generalized detection ability, we show that our framework outperforms state-of-the-art facial manipulation detection technologies in terms of accuracy and robustness.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kim_Generalized_Facial_Manipulation_Detection_With_Edge_Region_Feature_Extraction_WACV_2022_paper.pdf",
        "aff": "Sungkyunkwan University; Sungkyunkwan University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Kim_Generalized_Facial_Manipulation_WACV_2022_supplemental.zip",
        "arxiv": "2102.01381",
        "pdf_size": 2476790,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9655631892450443938&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "skku.edu;skku.edu",
        "email": "skku.edu;skku.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Sungkyunkwan University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.skku.edu",
        "aff_unique_abbr": "SKKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "8fb45292cc",
        "title": "Generating and Controlling Diversity in Image Search",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tanjim_Generating_and_Controlling_Diversity_in_Image_Search_WACV_2022_paper.html",
        "author": "Md. Mehrab Tanjim; Ritwik Sinha; Krishna Kumar Singh; Sridhar Mahadevan; David Arbour; Moumita Sinha; Garrison W. Cottrell",
        "abstract": "In our society, generations of systemic biases have led to some professions being more common among certain genders and races. This bias is also reflected in image search on stock image repositories and search engines, e.g., a query like \"male Asian administrative assistant\" may produce limited results. The pursuit of a utopian world demands providing content users with an opportunity to present any profession with diverse racial and gender characteristics. The limited choice of existing content for certain combinations of profession, race, and gender presents a challenge to content providers. Current research dealing with bias in search mostly focuses on re-ranking algorithms. However, these methods cannot create new content or change the overall distribution of protected attributes in photos. To remedy these problems, we propose a new task of high-fidelity image generation by controlling multiple attributes from imbalanced datasets. Our proposed task poses new sets of challenges for the state-of-the-art Generative Adversarial Networks (GANs). In this paper, we also propose a new training framework to better address the challenges. We evaluate our framework rigorously on a real-world dataset and perform user studies that show our model is preferable to the alternatives.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tanjim_Generating_and_Controlling_Diversity_in_Image_Search_WACV_2022_paper.pdf",
        "aff": "UC San Diego; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Adobe Applied ML; UC San Diego",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Tanjim_Generating_and_Controlling_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 10190978,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15832334780413083565&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "eng.ucsd.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;eng.ucsd.edu",
        "email": "eng.ucsd.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;eng.ucsd.edu",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;1;0",
        "aff_unique_norm": "University of California, San Diego;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ucsd.edu;https://research.adobe.com",
        "aff_unique_abbr": "UCSD;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "23afb2d2e4",
        "title": "Generative Adversarial Attack on Ensemble Clustering",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kumar_Generative_Adversarial_Attack_on_Ensemble_Clustering_WACV_2022_paper.html",
        "author": "Chetan Kumar; Deepak Kumar; Ming Shao",
        "abstract": "Adversarial attack on learning tasks has attracted substantial attention in recent years; however, most existing works focus on supervised learning. Recently, research has shown that unsupervised learning, such as clustering, tends to be vulnerable due to adversarial attack. In this paper, we focus on a clustering algorithm widely used in the real-world environment, namely, ensemble clustering (EC). EC algorithms usually leverage basic partition (BP) and ensemble techniques to improve the clustering performance collaboratively. Each BP may stem from one trial of clustering, feature segment, or part of data stored on the cloud. We have observed that the attack tends to be less perceivable when only a few BPs are compromised. To explore plausible attack strategies, we propose a novel generative adversarial attack (GA2) model for EC, titled GA2EC. First, we show that not all BPs are equally important, and some of them are more vulnerable under adversarial attack. Second, we develop a generative adversarial model to mimic the attack on EC. In particular, the generative model will simulate behaviors of both clean BPs and perturbed key BPs, and their derived graphs, and thus can launch effective attacks with less attention. We have conducted extensive experiments on eleven clustering benchmarks and have demonstrated that our approach is effective in attacking EC under both transductive and inductive settings.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kumar_Generative_Adversarial_Attack_on_Ensemble_Clustering_WACV_2022_paper.pdf",
        "aff": "University of Massachusetts Dartmouth, Dartmouth, MA, USA; University of Massachusetts Dartmouth, Dartmouth, MA, USA; University of Massachusetts Dartmouth, Dartmouth, MA, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3967460,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1827058781737939160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "umassd.edu;umassd.edu;umassd.edu",
        "email": "umassd.edu;umassd.edu;umassd.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Massachusetts Dartmouth",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umassd.edu",
        "aff_unique_abbr": "UMass Dartmouth",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Dartmouth",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "66bd36d680",
        "title": "Generative Adversarial Graph Convolutional Networks for Human Action Synthesis",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Degardin_Generative_Adversarial_Graph_Convolutional_Networks_for_Human_Action_Synthesis_WACV_2022_paper.html",
        "author": "Bruno Degardin; Jo\u00e3o Neves; Vasco Lopes; Jo\u00e3o Brito; Ehsan Yaghoubi; Hugo Proen\u00e7a",
        "abstract": "Synthesising the spatial and temporal dynamics of the human body skeleton remains a challenging task, not only in terms of the quality of the generated shapes, but also of their diversity, particularly to synthesise realistic body movements of a specific action (action conditioning). In this paper, we propose Kinetic-GAN, a novel architecture that leverages the benefits of Generative Adversarial Networks and Graph Convolutional Networks to synthesise the kinetics of the human body. The proposed adversarial architecture can condition up to 120 different actions over local and global body movements while improving sample quality and diversity through latent space disentanglement and stochastic variations. Our experiments were carried out in three well-known datasets, where Kinetic-GAN notably surpasses the state-of-the-art methods in terms of distribution quality metrics while having the ability to synthesise more than one order of magnitude regarding the number of different actions. Our code and models are publicly available at https://github.com/DegardinBruno/Kinetic-GAN.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Degardin_Generative_Adversarial_Graph_Convolutional_Networks_for_Human_Action_Synthesis_WACV_2022_paper.pdf",
        "aff": "IT - Instituto de Telecomunicac\u00b8 \u02dcoes+Universidade da Beira Interior+DeepNeuronic; NOV A LINCS+Universidade da Beira Interior+DeepNeuronic; NOV A LINCS+Universidade da Beira Interior+DeepNeuronic; DeepNeuronic; C4-Cloud Computing Competence Center; IT - Instituto de Telecomunicac\u00b8 \u02dcoes+Universidade da Beira Interior",
        "project": "",
        "github": "https://github.com/DegardinBruno/Kinetic-GAN",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Degardin_Generative_Adversarial_Graph_WACV_2022_supplemental.pdf",
        "arxiv": "2110.11191",
        "pdf_size": 6459792,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15804306674417356191&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ubi.pt; ; ; ; ; ",
        "email": "ubi.pt; ; ; ; ; ",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;3+1+2;3+1+2;2;4;0+1",
        "aff_unique_norm": "Instituto de Telecomunica\u00e7\u00f5es;Universidade da Beira Interior;DeepNeuronic;NOV A LINCS;Cloud Computing Competence Center",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.it.pt;https://www.ubi.pt;;;",
        "aff_unique_abbr": "IT;UBI;;;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "Portugal;"
    },
    {
        "id": "6c8fffad23",
        "title": "Geometrically Adaptive Dictionary Attack on Face Recognition",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Byun_Geometrically_Adaptive_Dictionary_Attack_on_Face_Recognition_WACV_2022_paper.html",
        "author": "Junyoung Byun; Hyojun Go; Changick Kim",
        "abstract": "CNN-based face recognition models have brought remarkable performance improvement, but they are vulnerable to adversarial perturbations. Recent studies have shown that adversaries can fool the models even if they can only access the models' hard-label output. However, since many queries are needed to find imperceptible adversarial noise, reducing the number of queries is crucial for these attacks. In this paper, we point out two limitations of existing decision-based black-box attacks. We observe that they waste queries for background noise optimization, and they do not take advantage of adversarial perturbations generated for other images. We exploit 3D face alignment to overcome these limitations and propose a general strategy for query-efficient black-box attacks on face recognition named Geometrically Adaptive Dictionary Attack (GADA). Our core idea is to create an adversarial perturbation in the UV texture map and project it onto the face in the image. It greatly improves query efficiency by limiting the perturbation search space to the facial area and effectively recycling previous perturbations. We apply the GADA strategy to two existing attack methods and show overwhelming performance improvement in the experiments on the LFW and CPLFW datasets. Furthermore, we also present a novel attack strategy that can circumvent query similarity-based stateful detection that identifies the process of query-based black-box attacks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Byun_Geometrically_Adaptive_Dictionary_Attack_on_Face_Recognition_WACV_2022_paper.pdf",
        "aff": "Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST)",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Byun_Geometrically_Adaptive_Dictionary_WACV_2022_supplemental.pdf",
        "arxiv": "2111.04371",
        "pdf_size": 2693821,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3320338603724069705&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "d1228374d1",
        "title": "Geometry-Aware Hierarchical Bayesian Learning on Manifolds",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Fan_Geometry-Aware_Hierarchical_Bayesian_Learning_on_Manifolds_WACV_2022_paper.html",
        "author": "Yonghui Fan; Yalin Wang",
        "abstract": "Bayesian learning with Gaussian processes demonstrates encouraging regression and classification performance in solving computer vision tasks. However, Bayesian methods on 3D manifold-valued vision data, such as meshes and point clouds, are seldom studied. One of the primary challenges is how to effectively and efficiently aggregate geometric features from inputs. In this paper, we propose a hierarchical Bayesian learning model to address this challenge. We implicitly introduce the geometry-awareness and the intra-kernel convolution to the kernel so that the prior becomes geometry sensitive without using any hand-crafted feature descriptors. We implement a hierarchical feature aggregation architecture by concatenating multiple Gaussian processes together. Furthermore, we incorporate the feature learning of neural networks with the feature aggregation of Bayesian models to investigate the feasibility of jointly learning inferences on manifolds. Experimental results not only show that our method outperforms existing Bayesian methods on manifolds but also demonstrate the prospect of coupling neural networks with Bayesian learning methods",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Fan_Geometry-Aware_Hierarchical_Bayesian_Learning_on_Manifolds_WACV_2022_paper.pdf",
        "aff": "Arizona State University; Arizona State University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Fan_Geometry-Aware_Hierarchical_Bayesian_WACV_2022_supplemental.pdf",
        "arxiv": "2111.00184",
        "pdf_size": 2059173,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13692826422836527847&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "asu.edu;asu.edu",
        "email": "asu.edu;asu.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ebc83280b5",
        "title": "Geometry-Inspired Top-K Adversarial Perturbations",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tursynbek_Geometry-Inspired_Top-K_Adversarial_Perturbations_WACV_2022_paper.html",
        "author": "Nurislam Tursynbek; Aleksandr Petiushko; Ivan Oseledets",
        "abstract": "The brittleness of deep image classifiers to small adver-sarial input perturbations has been extensively studied inthe last several years. However, the main objective of ex-isting perturbations is primarily limited to change the cor-rectly predicted Top-1class by an incorrect one, which doesnot intend to change the Top-kprediction. In many digi-tal real-world scenarios Top-kprediction is more relevant.In this work, we propose a fast and accurate method ofcomputing Top-kadversarial examples as a simple multi-objective optimization. We demonstrate its efficacy andperformance by comparing it to other adversarial examplecrafting techniques. Moreover, based on this method, wepropose Top-kUniversal Adversarial Perturbations, image-agnostic tiny perturbations that cause the true class to beabsent among the Top-kprediction for the majority of nat-ural images. We experimentally show that our approachoutperforms baseline methods and even improves existingtechniques of finding Universal Adversarial Perturbations.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tursynbek_Geometry-Inspired_Top-K_Adversarial_Perturbations_WACV_2022_paper.pdf",
        "aff": "Skolkovo Institute of Science and Technology; Huawei + Lomonosov Moscow State University; Skolkovo Institute of Science and Technology + Institute of Numerical Mathematics, Russian Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2616459,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13786672744424549974&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;huawei.com;skoltech.ru",
        "email": "gmail.com;huawei.com;skoltech.ru",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;0+3",
        "aff_unique_norm": "Skolkovo Institute of Science and Technology;Huawei;Lomonosov Moscow State University;Russian Academy of Sciences",
        "aff_unique_dep": ";Huawei Technologies Co., Ltd.;;Institute of Numerical Mathematics",
        "aff_unique_url": "https://www.skoltech.ru;https://www.huawei.com;https://www.msu.ru;https://www.ras.ru",
        "aff_unique_abbr": "Skoltech;Huawei;MSU;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Moscow",
        "aff_country_unique_index": "0;1+0;0+0",
        "aff_country_unique": "Russian Federation;China"
    },
    {
        "id": "c9af70fb74",
        "title": "Global Assists Local: Effective Aerial Representations for Field of View Constrained Image Geo-Localization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Rodrigues_Global_Assists_Local_Effective_Aerial_Representations_for_Field_of_View_WACV_2022_paper.html",
        "author": "Royston Rodrigues; Masahiro Tani",
        "abstract": "When we humans recognize places from images, we not only infer about the objects that are available but even think about landmarks that might be surrounding it. Current place recognition approaches lack the ability to go beyond objects that are available in the image and hence miss out on understanding the scene completely. In this paper, we take a step towards holistic scene understanding. We address the problem of image geo-localization by retrieving corresponding aerial views from a large database of geotagged aerial imagery. One of the main challenges in tackling this problem is the limited Field of View (FoV) nature of query images which needs to be matched to aerial views which contain 360degFoV details. State-of-the-art method DSM-Net [17] tackles this challenge by matching aerial images locally within fixed FoV sectors. We show that local matching limits complete scene understanding and is inadequate when partial buildings are visible in query images or when local sectors of aerial images are covered by dense trees. Our approach considers both local and global properties of aerial images and hence is robust to such conditions. Experiments on standard benchmarks demonstrates that the proposed approach improves top-1% image recall rate on the CVACT [9] data-set from 57.08% to 77.19% and from 61.20% to 75.21% on the CVUSA [25] data-set for 70degFoV. We also achieve state-of-the art results for 90degFoV on both CVACT [9] and CVUSA [25] data-sets demonstrating the effectiveness of our proposed method.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Rodrigues_Global_Assists_Local_Effective_Aerial_Representations_for_Field_of_View_WACV_2022_paper.pdf",
        "aff": "Biometrics Research Laboratories, NEC Corporation, Japan; Biometrics Research Laboratories, NEC Corporation, Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7321690,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18217892430464933475&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "nec.com;nec.com",
        "email": "nec.com;nec.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "NEC Corporation",
        "aff_unique_dep": "Biometrics Research Laboratories",
        "aff_unique_url": "https://www.nec.com",
        "aff_unique_abbr": "NEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "ca37775ee3",
        "title": "GraDual: Graph-Based Dual-Modal Representation for Image-Text Matching",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Long_GraDual_Graph-Based_Dual-Modal_Representation_for_Image-Text_Matching_WACV_2022_paper.html",
        "author": "Siqu Long; Soyeon Caren Han; Xiaojun Wan; Josiah Poon",
        "abstract": "Image-text retrieval task is a challenging task. It aims to measure the visual-semantic correspondence between an image and a text caption. This is tough mainly because the image lacks semantic context information as in its corresponding text caption, and the text representation is very limited to fully describe the details of an image. In this paper, we introduce Graph-based Dual-modal Representations (GraDual), including Vision-Integrated Text Embedding (VITE) and Context-Integrated Visual Embedding (CIVE), for image-text retrieval. The GraDual improves the coverage of each modality by exploiting textual context semantics for the image representation, and using visual features as a guidance for the text representation. To be specific, we design: 1) a dual-modal graph representation mechanism to solve the lack of coverage issue for each modality. 2) an intermediate graph embedding integration strategy to enhance the important pattern across other modality global features. 3) a dual-modal driven cross-modal matching network to generate a filtered representation of another modality. Extensive experiments on two benchmark datasets, MS-COCO and Flickr30K, demonstrates the superiority of the proposed GraDual in comparison to state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Long_GraDual_Graph-Based_Dual-Modal_Representation_for_Image-Text_Matching_WACV_2022_paper.pdf",
        "aff": "School of Computer Science, The University of Sydney + Wangxuan Institute of Computer Technology, Peking University; School of Computer Science, The University of Sydney; Wangxuan Institute of Computer Technology, Peking University; School of Computer Science, The University of Sydney",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Long_GraDual_Graph-Based_Dual-Modal_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1921037,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18407088947693325496&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "sydney.edu.au;sydney.edu.au;pku.edu.cn;sydney.edu.au",
        "email": "sydney.edu.au;sydney.edu.au;pku.edu.cn;sydney.edu.au",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;1;0",
        "aff_unique_norm": "University of Sydney;Peking University",
        "aff_unique_dep": "School of Computer Science;Wangxuan Institute of Computer Technology",
        "aff_unique_url": "https://www.sydney.edu.au;http://www.pku.edu.cn",
        "aff_unique_abbr": "USYD;PKU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Sydney;",
        "aff_country_unique_index": "0+1;0;1;0",
        "aff_country_unique": "Australia;China"
    },
    {
        "id": "7d16657e48",
        "title": "GraN-GAN: Piecewise Gradient Normalization for Generative Adversarial Networks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bhaskara_GraN-GAN_Piecewise_Gradient_Normalization_for_Generative_Adversarial_Networks_WACV_2022_paper.html",
        "author": "Vineeth S. Bhaskara; Tristan Aumentado-Armstrong; Allan D. Jepson; Alex Levinshtein",
        "abstract": "Modern generative adversarial networks (GANs) predominantly use piecewise linear activation functions in discriminators (or critics), including ReLU and LeakyReLU. Such models learn piecewise linear mappings, where each piece handles a subset of the input space, and the gradients per subset are piecewise constant. Under such a class of discriminator (or critic) functions, we present Gradient Normalization (GraN), a novel input-dependent normalization method, which guarantees a piecewise K-Lipschitz constraint in the input space. In contrast to spectral normalization, GraN does not constrain processing at the individual network layers, and, unlike gradient penalties, strictly enforces a piecewise Lipschitz constraint almost everywhere. Empirically, we demonstrate improved image generation performance across multiple datasets (incl. CIFAR-10/100, STL-10, LSUN bedrooms, and CelebA), GAN loss functions, and metrics. Further, we analyze altering the often untuned Lipschitz constant K in several standard GANs, not only attaining significant performance gains, but also finding connections between K and training dynamics, particularly in low-gradient loss plateaus, with the common Adam optimizer.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bhaskara_GraN-GAN_Piecewise_Gradient_Normalization_for_Generative_Adversarial_Networks_WACV_2022_paper.pdf",
        "aff": "Samsung AI Centre Toronto; Samsung AI Centre Toronto + University of Toronto + Vector Institute for AI; Samsung AI Centre Toronto; Samsung AI Centre Toronto",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Bhaskara_GraN-GAN_Piecewise_Gradient_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2707781,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15448247165704650392&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "samsung.com;partner.samsung.com;samsung.com;samsung.com",
        "email": "samsung.com;partner.samsung.com;samsung.com;samsung.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1+2;0;0",
        "aff_unique_norm": "Samsung;University of Toronto;Vector Institute for AI",
        "aff_unique_dep": "AI Centre;;",
        "aff_unique_url": "https://www.samsung.com/global/innovation/ai-research-centers/;https://www.utoronto.ca;https://vectorinstitute.ai/",
        "aff_unique_abbr": "Samsung AI;U of T;Vector AI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Toronto;",
        "aff_country_unique_index": "0;0+0+0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "a85250c5ad",
        "title": "HERS Superpixels: Deep Affinity Learning for Hierarchical Entropy Rate Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Peng_HERS_Superpixels_Deep_Affinity_Learning_for_Hierarchical_Entropy_Rate_Segmentation_WACV_2022_paper.html",
        "author": "Hankui Peng; Angelica I. Aviles-Rivero; Carola-Bibiane Sch\u00f6nlieb",
        "abstract": "Superpixels serve as a powerful preprocessing tool in many computer vision tasks. By using superpixel representation, the number of image primitives can be largely reduced by orders of magnitudes. The majority of superpixel methods use handcrafted features, which usually do not translate well into strong adherence to object boundaries. A few recent superpixel methods have introduced deep learning into the superpixel segmentation process. However, none of these methods is able to produce superpixels in near real-time, which is crucial to the applicability of a superpixel method in practice. In this work, we propose a two-stage graph-based framework for superpixel segmentation. In the first stage, we introduce an efficient Deep Affinity Learning (DAL) network that learns pairwise pixel affinities by aggregating multi-scale information. In the second stage, we propose a highly efficient superpixel method called Hierarchical Entropy Rate Segmentation (HERS). Using the learned affinities from the first stage, HERS builds a hierarchical tree structure that can produce any number of highly adaptive superpixels instantaneously. We demonstrate, through visual and numerical experiments, the effectiveness and efficiency of our method compared to various state-of-the-art superpixel methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Peng_HERS_Superpixels_Deep_Affinity_Learning_for_Hierarchical_Entropy_Rate_Segmentation_WACV_2022_paper.pdf",
        "aff": "DAMTP, University of Cambridge; DAMTP, University of Cambridge; DAMTP, University of Cambridge",
        "project": "",
        "github": "https://github.com/hankuipeng/DAL-HERS",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Peng_HERS_Superpixels_Deep_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2786301,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11517856491283998552&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Applied Mathematics and Theoretical Physics",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "95dbcc391e",
        "title": "HHP-Net: A Light Heteroscedastic Neural Network for Head Pose Estimation With Uncertainty",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Cantarini_HHP-Net_A_Light_Heteroscedastic_Neural_Network_for_Head_Pose_Estimation_WACV_2022_paper.html",
        "author": "Giorgio Cantarini; Federico Figari Tomenotti; Nicoletta Noceti; Francesca Odone",
        "abstract": "In this paper we introduce a novel method to estimate the head pose of people in single images starting from a small set of head keypoints. To this purpose, we propose a regression model that exploits keypoints computed automatically by 2D pose estimation algorithms and outputs the head pose represented by yaw, pitch, and roll. Our model is simple to implement and more efficient with respect to the state of the art -- faster in inference and smaller in terms of memory occupancy -- with comparable accuracy. Our method also provides a measure of the heteroscedastic uncertainties associated with the three angles, through an appropriately designed loss function; we show there is a correlation between error and uncertainty values, thus this extra source of information may be used in subsequent computational steps. As an example application, we address social interaction analysis in images: we propose an algorithm for a quantitative estimation of the level of interaction between people, starting from their head poses and reasoning on their mutual positions.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Cantarini_HHP-Net_A_Light_Heteroscedastic_Neural_Network_for_Head_Pose_Estimation_WACV_2022_paper.pdf",
        "aff": "MaLGa-DIBRIS, Universit `a degli Studi di Genova; MaLGa-DIBRIS, Universit `a degli Studi di Genova + IMA VIS srl; MaLGa-DIBRIS, Universit `a degli Studi di Genova; MaLGa-DIBRIS, Universit `a degli Studi di Genova",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Cantarini_HHP-Net_A_Light_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6074605,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3576923650767820&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "imavis.com;edu.unige.it; ;unige.it",
        "email": "imavis.com;edu.unige.it; ;unige.it",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Universit\u00e0 degli Studi di Genova;IMA VIS srl",
        "aff_unique_dep": "MaLGa-DIBRIS;",
        "aff_unique_url": "https://www.unige.it;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "aa5f31ed4f",
        "title": "Hessian-Aware Pruning and Optimal Neural Implant",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yu_Hessian-Aware_Pruning_and_Optimal_Neural_Implant_WACV_2022_paper.html",
        "author": "Shixing Yu; Zhewei Yao; Amir Gholami; Zhen Dong; Sehoon Kim; Michael W. Mahoney; Kurt Keutzer",
        "abstract": "Pruning is an effective method to reduce the memory footprint and FLOPs associated with neural network models. However, existing structured pruning methods often result in significant accuracy degradation for moderate pruning levels. To address this problem, we introduce a new Hessian Aware Pruning (HAP) method coupled with a Neural Implant approach that uses second-order sensitivity as a metric for structured pruning. The basic idea is to prune insensitive components and to use a Neural Implant for moderately sensitive components, instead of completely pruning them. For the latter approach, the moderately sensitive components are replaced with a low-rank implant that is smaller and less computationally expensive than the original component. We use the relative Hessian trace to measure sensitivity, as opposed to the magnitude-based sensitivity metric commonly used in the literature. We test HAP for both computer vision tasks and natural language tasks, and we achieve new state-of-the-art results. Specifically,HAP achieves less than 0.1%/0.5% degradation on PreResNet29/ResNet50(CIFAR-10/ImageNet) with more than 70%/50% of parameters pruned. Meanwhile, HAP also achieves significantly better performance (up to 0.8% with 60% of parameters pruned) as compared to gradient-based method for head pruning on transformer-based models.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yu_Hessian-Aware_Pruning_and_Optimal_Neural_Implant_WACV_2022_paper.pdf",
        "aff": "Peking University; University of California, Berkeley; University of California, Berkeley+ICSI; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley+ICSI; University of California, Berkeley",
        "project": "",
        "github": "https://github.com/yaozhewei/HAP",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yu_Hessian-Aware_Pruning_and_WACV_2022_supplemental.pdf",
        "arxiv": "2101.08940",
        "pdf_size": 8754565,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16691174490930066939&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "pku.edu.cn;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "pku.edu.cn;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1+2;1;1;1+2;1",
        "aff_unique_norm": "Peking University;University of California, Berkeley;International Computer Science Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.berkeley.edu;https://www.icsi.berkeley.edu/",
        "aff_unique_abbr": "Peking U;UC Berkeley;ICSI",
        "aff_campus_unique_index": "1;1;1;1;1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1;1+1;1;1;1+1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "1e470cce16",
        "title": "HierMatch: Leveraging Label Hierarchies for Improving Semi-Supervised Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Garg_HierMatch_Leveraging_Label_Hierarchies_for_Improving_Semi-Supervised_Learning_WACV_2022_paper.html",
        "author": "Ashima Garg; Shaurya Bagga; Yashvardhan Singh; Saket Anand",
        "abstract": "Semi-supervised learning approaches have emerged as an active area of research to combat the challenge of obtaining large amounts of annotated data. Towards the goal of improving the performance of semi-supervised learning methods, we propose a novel framework, HIERMATCH, a semi-supervised approach that leverages hierarchical information to reduce labeling costs and performs as well as a vanilla semi-supervised learning method. Hierarchical information is often available as prior knowledge in the form of coarse labels (e.g., woodpeckers) for images with fine-grained labels (e.g., downy woodpeckers or golden-fronted woodpeckers). However, the use of supervision using coarse-category labels to improve semi-supervised techniques has not been explored. In the absence of fine-grained labels, HIERMATCH exploits the label hierarchy and uses coarse class labels as a weak supervisory signal. Additionally, HIERMATCH is a generic-approach to improve any semi-supervised learning framework, we demonstrate this using our results on recent state-of-the-art techniques MixMatch and FixMatch. We evaluate the efficacy of HIERMATCH on two benchmark datasets, namely CIFAR-100 and NABirds. HIERMATCH can reduce the usage of fine-grained labels by 50% on CIFAR-100 with only a marginal drop of 0.59% in top-1 accuracy as compared to MixMatch.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Garg_HierMatch_Leveraging_Label_Hierarchies_for_Improving_Semi-Supervised_Learning_WACV_2022_paper.pdf",
        "aff": "IIIT-Delhi, India; IIIT-Delhi, India; IIIT-Delhi, India; IIIT-Delhi, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Garg_HierMatch_Leveraging_Label_WACV_2022_supplemental.pdf",
        "arxiv": "2111.00164",
        "pdf_size": 663049,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10890490579400234444&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in;iiitd.ac.in",
        "email": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in;iiitd.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "IIIT-Delhi",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iiitdelhi.ac.in",
        "aff_unique_abbr": "IIIT-D",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "92730e067c",
        "title": "Hierarchical Modeling for Task Recognition and Action Segmentation in Weakly-Labeled Instructional Videos",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ghoddoosian_Hierarchical_Modeling_for_Task_Recognition_and_Action_Segmentation_in_Weakly-Labeled_WACV_2022_paper.html",
        "author": "Reza Ghoddoosian; Saif Sayed; Vassilis Athitsos",
        "abstract": "This paper focuses on task recognition and action segmentation in weakly-labeled instructional videos, where only the ordered sequence of video-level actions is available during training. We propose a two-stream framework, which exploits semantic and temporal hierarchies to recognize top-level tasks in instructional videos. Further, we present a novel top-down weakly-supervised action segmentation approach, where the predicted task is used to constrain the inference of fine-grained action sequences. Experimental results on the popular Breakfast and Cooking 2 datasets show that our two-stream hierarchical task modeling significantly outperforms existing methods in top-level task recognition for all datasets and metrics. Additionally, using our task recognition framework in the proposed top-down action segmentation approach consistently improves the state of the art, while also reducing segmentation inference time by 80-90 percent.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ghoddoosian_Hierarchical_Modeling_for_Task_Recognition_and_Action_Segmentation_in_Weakly-Labeled_WACV_2022_paper.pdf",
        "aff": "Vision-Learning-Mining Lab, University of Texas at Arlington; Vision-Learning-Mining Lab, University of Texas at Arlington; Vision-Learning-Mining Lab, University of Texas at Arlington",
        "project": "",
        "github": "https://github.com/rezaghoddoosian/Hierarchical-Task-Modeling",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ghoddoosian_Hierarchical_Modeling_for_WACV_2022_supplemental.pdf",
        "arxiv": "2110.05697",
        "pdf_size": 2585611,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9468334943418232983&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "mavs.uta.edu;mavs.uta.edu;uta.edu",
        "email": "mavs.uta.edu;mavs.uta.edu;uta.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Arlington",
        "aff_unique_dep": "Vision-Learning-Mining Lab",
        "aff_unique_url": "https://www.uta.edu",
        "aff_unique_abbr": "UTA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Arlington",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5a6764240e",
        "title": "Hierarchical Proxy-Based Loss for Deep Metric Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yang_Hierarchical_Proxy-Based_Loss_for_Deep_Metric_Learning_WACV_2022_paper.html",
        "author": "Zhibo Yang; Muhammet Bastan; Xinliang Zhu; Douglas Gray; Dimitris Samaras",
        "abstract": "Proxy-based metric learning losses are superior to pair-based losses due to their fast convergence and low training complexity. However, existing proxy-based losses focus on learning class-discriminative features while overlooking the commonalities shared across classes which are potentially useful in describing and matching samples. Moreover, they ignore the implicit hierarchy of categories in real-world datasets, where similar subordinate classes can be grouped together. In this paper, we present a framework that leverages this implicit hierarchy by imposing a hierarchical structure on the proxies and can be used with any existing proxy-based loss. This allows our model to capture both class-discriminative features and class-shared characteristics without breaking the implicit data hierarchy. We evaluate our method on five established image retrieval datasets such as In-Shop and SOP. Results demonstrate that our hierarchical proxy-based loss framework improves the performance of existing proxy-based losses, especially on large datasets which exhibit strong hierarchical structure.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yang_Hierarchical_Proxy-Based_Loss_for_Deep_Metric_Learning_WACV_2022_paper.pdf",
        "aff": "Stony Brook University; Visual Search & AR, Amazon; Visual Search & AR, Amazon; Visual Search & AR, Amazon; Stony Brook University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yang_Hierarchical_Proxy-Based_Loss_WACV_2022_supplemental.pdf",
        "arxiv": "2103.13538",
        "pdf_size": 1346740,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4962305766190462729&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Stony Brook University;Amazon",
        "aff_unique_dep": ";Visual Search & AR",
        "aff_unique_url": "https://www.stonybrook.edu;https://www.amazon.com",
        "aff_unique_abbr": "SBU;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c66d98f9af",
        "title": "Hierarchically Decoupled Spatial-Temporal Contrast for Self-Supervised Video Representation Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zhang_Hierarchically_Decoupled_Spatial-Temporal_Contrast_for_Self-Supervised_Video_Representation_Learning_WACV_2022_paper.html",
        "author": "Zehua Zhang; David Crandall",
        "abstract": "We present a novel technique for self-supervised video representation learning by: (a) decoupling the learning objective into two contrastive subtasks respectively emphasizing spatial and temporal features, and (b) performing it hierarchically to encourage multi-scale understanding. Motivated by their effectiveness in supervised learning, we first introduce spatial-temporal feature learning decoupling and hierarchical learning to the context of unsupervised video learning. We show by experiments that augmentations can be manipulated as regularization to guide the network to learn desired semantics in contrastive learning, and we propose a way for the model to separately capture spatial and temporal features at multiple scales. We also introduce an approach to overcome the problem of divergent levels of instance invariance at different hierarchies by modeling the invariance as loss weights for objective re-weighting. Experiments on downstream action recognition benchmarks on UCF101 and HMDB51 show that our proposed Hierarchically Decoupled Spatial-Temporal Contrast (HDC) makes substantial improvements over directly learning spatial-temporal features as a whole and achieves competitive performance when compared with other state-of-the-art unsupervised methods. Code will be made available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zhang_Hierarchically_Decoupled_Spatial-Temporal_Contrast_for_Self-Supervised_Video_Representation_Learning_WACV_2022_paper.pdf",
        "aff": "Indiana University Bloomington; Indiana University Bloomington",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2011.11261",
        "pdf_size": 1303707,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17896472428691193793&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "indiana.edu;indiana.edu",
        "email": "indiana.edu;indiana.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indiana University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.indiana.edu",
        "aff_unique_abbr": "IU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bloomington",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c21342b84a",
        "title": "High Dynamic Range Imaging of Dynamic Scenes With Saturation Compensation but Without Explicit Motion Compensation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chung_High_Dynamic_Range_Imaging_of_Dynamic_Scenes_With_Saturation_Compensation_WACV_2022_paper.html",
        "author": "Haesoo Chung; Nam Ik Cho",
        "abstract": "High dynamic range (HDR) imaging is a highly challenging task since a large amount of information is lost due to the limitations of camera sensors. For HDR imaging, some methods capture multiple low dynamic range (LDR) images with altering exposures to aggregate more information. However, these approaches introduce ghosting artifacts when significant inter-frame motions are present. Moreover, although multi-exposure images are given, we have little information in severely over-exposed areas. Most existing methods focus on motion compensation, i.e., alignment of multiple LDR shots to reduce the ghosting artifacts, but they still produce unsatisfying results. These methods also rather overlook the need to restore the saturated areas. In this paper, we generate well-aligned multi-exposure features by reformulating a motion alignment problem into a simple brightness adjustment problem. In addition, we propose a coarse-to-fine merging strategy with explicit saturation compensation. The saturated areas are reconstructed with similar well-exposed content using adaptive contextual attention. We demonstrate that our method outperforms the state-of-the-art methods regarding qualitative and quantitative evaluations.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chung_High_Dynamic_Range_Imaging_of_Dynamic_Scenes_With_Saturation_Compensation_WACV_2022_paper.pdf",
        "aff": "Department of ECE, INMC, Seoul National University, Korea; Department of ECE, INMC, Seoul National University, Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Chung_High_Dynamic_Range_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 8440972,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5267403207618341904&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ispl.snu.ac.kr;snu.ac.kr",
        "email": "ispl.snu.ac.kr;snu.ac.kr",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "8cbb120c87",
        "title": "Hole-Robust Wireframe Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kong_Hole-Robust_Wireframe_Detection_WACV_2022_paper.html",
        "author": "Naejin Kong; Kiwoong Park; Harshith Goka",
        "abstract": "\"Wireframe\" is a line segment based representation designed to well capture large-scale visual properties of regular, structural shaped man-made scenes surrounding us. Unlike the wireframes, conventional edges or line segments focus on all visible edges and lines without particularly distinguishing which of them are more salient to man-made structural information. Existing wireframe detection models rely on supervising the annotated data but do not explicitly pay attention to understand how to compose the structural shapes of the scene. In addition, we often face that many foreground objects occluding the background scene interfere with proper inference of the full scene structure behind them. To resolve these problems, we first time in the field, propose new conditional data generation and training that help the model understand how to ignore occlusion indicated by holes, such as foreground object regions masked out on the image. In addition, we first time combine GAN in the model to let the model better predict underlying scene structure even beyond large holes. We also introduce pseudo labeling to further enlarge the model capacity to overcome small-scale labeled data. We show qualitatively and quantitatively that our approach significantly outperforms previous works unable to handle holes, as well as improves ordinary detection without holes given.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kong_Hole-Robust_Wireframe_Detection_WACV_2022_paper.pdf",
        "aff": "Samsung Research AI Center; Samsung Research AI Center; Samsung Research AI Center",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Kong_Hole-Robust_Wireframe_Detection_WACV_2022_supplemental.pdf",
        "arxiv": "2111.15064",
        "pdf_size": 8592081,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18132447872000409665&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "samsung.com;samsung.com;samsung.com",
        "email": "samsung.com;samsung.com;samsung.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Samsung",
        "aff_unique_dep": "AI Center",
        "aff_unique_url": "https://www.samsung.com/global/research/",
        "aff_unique_abbr": "SRAIC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "0eb5d3c890",
        "title": "How Good Is Your Explanation? Algorithmic Stability Measures To Assess the Quality of Explanations for Deep Neural Networks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Fel_How_Good_Is_Your_Explanation_Algorithmic_Stability_Measures_To_Assess_WACV_2022_paper.html",
        "author": "Thomas Fel; David Vigouroux; R\u00e9mi Cad\u00e8ne; Thomas Serre",
        "abstract": "A plethora of methods have been proposed to explain how deep neural networks reach their decisions but comparatively, little effort has been made to ensure that the explanations produced by these methods are objectively relevant. While several desirable properties for trustworthy explanations have been formulated, objective measures have been harder to derive. Here, we propose two new measures to evaluate explanations borrowed from the field of algorithmic stability: mean generalizability MeGe and relative consistency ReCo. We conduct extensive experiments on different network architectures, common explainability methods, and several image datasets to demonstrate the benefits of the proposed measures. In comparison to ours, popular fidelity measures are not sufficient to guarantee trustworthy explanations. Finally, we found that 1-Lipschitz networks produce explanations with higher MeGe and ReCo than common neural networks while reaching similar accuracy. This suggests that 1-Lipschitz networks are a relevant direction towards predictors that are more explainable and trustworthy.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Fel_How_Good_Is_Your_Explanation_Algorithmic_Stability_Measures_To_Assess_WACV_2022_paper.pdf",
        "aff": "Carney Institute for Brain Science, Brown University + Artificial and Natural Intelligence Toulouse Institute, Universit \u00b4e de Toulouse, France + IRT Saint-Exupery; IRT Saint-Exupery; Carney Institute for Brain Science, Brown University; Carney Institute for Brain Science, Brown University + Artificial and Natural Intelligence Toulouse Institute, Universit \u00b4e de Toulouse, France",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Fel_How_Good_Is_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3966999,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18396669282138639903&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 38,
        "aff_domain": "brown.edu; ; ; ",
        "email": "brown.edu; ; ; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;2;0;0+1",
        "aff_unique_norm": "Brown University;Universit\u00e9 de Toulouse;IRT Saint-Exupery",
        "aff_unique_dep": "Carney Institute for Brain Science;Artificial and Natural Intelligence Toulouse Institute;",
        "aff_unique_url": "https://www.brown.edu;https://www.univ-toulouse.fr;",
        "aff_unique_abbr": "Brown;UT;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Toulouse",
        "aff_country_unique_index": "0+1+1;1;0;0+1",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "8b8f833972",
        "title": "How and What To Learn: Taxonomizing Self-Supervised Learning for 3D Action Recognition",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tanfous_How_and_What_To_Learn_Taxonomizing_Self-Supervised_Learning_for_3D_WACV_2022_paper.html",
        "author": "Amor Ben Tanfous; Aimen Zerroug; Drew Linsley; Thomas Serre",
        "abstract": "There are two competing standards for self-supervised learning in action recognition from 3D skeletons. Su et al., 2020 used an auto-encoder architecture and an image reconstruction objective function to achieve state-of-the-art performance on the NTU60 C-View benchmark. Rao et al., 2020 used Contrastive learning in the latent space to achieve state-of-the-art performance on the NTU60 C-Sub benchmark. Here, we reconcile these disparate approaches by developing a taxonomy of self-supervised learning for action recognition. We observe that leading approaches generally use one of two types of objective functions: those that seek to reconstruct the input from a latent representation (\"Attractive\" learning) versus those that also try to maximize the representations distinctiveness (\"Contrastive\" learning). Independently, leading approaches also differ in how they implement these objective functions: there are those that optimize representations in the decoder output space and those which optimize representations in the network's latent space (encoder output). We find that combining these approaches leads to larger gains in performance and tolerance to transformation than is achievable by any individual method, leading to state-of-the-art performance on three standard action recognition datasets. We include links to our code and data.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tanfous_How_and_What_To_Learn_Taxonomizing_Self-Supervised_Learning_for_3D_WACV_2022_paper.pdf",
        "aff": "Artificial and Natural Intelligence Toulouse Institute, Toulouse University, France+Carney Institute for Brain Science, Dpt. of Cognitive Linguistic Psychological Sciences Brown University, Providence, RI 02912; Artificial and Natural Intelligence Toulouse Institute, Toulouse University, France+Carney Institute for Brain Science, Dpt. of Cognitive Linguistic Psychological Sciences Brown University, Providence, RI 02912; Carney Institute for Brain Science, Dpt. of Cognitive Linguistic Psychological Sciences Brown University, Providence, RI 02912; Carney Institute for Brain Science, Dpt. of Cognitive Linguistic Psychological Sciences Brown University, Providence, RI 02912",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Tanfous_How_and_What_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1317457,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3883421233557862438&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "brown.edu;brown.edu;brown.edu;brown.edu",
        "email": "brown.edu;brown.edu;brown.edu;brown.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;1;1",
        "aff_unique_norm": "Toulouse University;Brown University",
        "aff_unique_dep": "Artificial and Natural Intelligence Toulouse Institute;Department of Cognitive Linguistic Psychological Sciences",
        "aff_unique_url": "https://www.univ-toulouse.fr;https://www.brown.edu",
        "aff_unique_abbr": "Toulouse University;Brown",
        "aff_campus_unique_index": "0+1;0+1;1;1",
        "aff_campus_unique": "Toulouse;Providence",
        "aff_country_unique_index": "0+1;0+1;1;1",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "f207497773",
        "title": "Human-Aided Saliency Maps Improve Generalization of Deep Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Boyd_Human-Aided_Saliency_Maps_Improve_Generalization_of_Deep_Learning_WACV_2022_paper.html",
        "author": "Aidan Boyd; Kevin W. Bowyer; Adam Czajka",
        "abstract": "Deep learning has driven remarkable accuracy increases in many computer vision problems. One ongoing challenge is how to achieve the greatest accuracy in cases where training data is limited. A second ongoing challenge is that trained models oftentimes do not generalize well even to new data that is subjectively similar to the training set. We address these challenges in a novel way, with the first-ever (to our knowledge) exploration of encoding human judgement about salient regions of images into the training data. We compare the accuracy and generalization of a state-of-the-art deep learning algorithm for a difficult problem in biometric presentation attack detection when trained on (a) original images with typical data augmentations, and (b) the same original images transformed to encode human judgement about salient image regions. The latter approach results in models that achieve higher accuracy and better generalization, decreasing the error of the LivDet-Iris 2020 winner from 29.78% to 16.37%, and achieving impressive generalization in a leave-one-attack-type-out evaluation scenario. This work opens a new area of study for how to embed human intelligence into training strategies for deep learning to achieve high accuracy and generalization in cases of limited training data.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Boyd_Human-Aided_Saliency_Maps_Improve_Generalization_of_Deep_Learning_WACV_2022_paper.pdf",
        "aff": "University of Notre Dame, Notre Dame, IN 46556; University of Notre Dame, Notre Dame, IN 46556; University of Notre Dame, Notre Dame, IN 46556",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Boyd_Human-Aided_Saliency_Maps_WACV_2022_supplemental.zip",
        "arxiv": "2105.03492",
        "pdf_size": 6417333,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11075094779693940913&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nd.edu;nd.edu;nd.edu",
        "email": "nd.edu;nd.edu;nd.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Notre Dame",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nd.edu",
        "aff_unique_abbr": "Notre Dame",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Notre Dame",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a11baa181c",
        "title": "HybVIO: Pushing the Limits of Real-Time Visual-Inertial Odometry",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Seiskari_HybVIO_Pushing_the_Limits_of_Real-Time_Visual-Inertial_Odometry_WACV_2022_paper.html",
        "author": "Otto Seiskari; Pekka Rantalankila; Juho Kannala; Jerry Ylilammi; Esa Rahtu; Arno Solin",
        "abstract": "We present HybVIO, a novel hybrid approach for combining filtering-based visual-inertial odometry (VIO) with optimization-based SLAM. The core of our method is highly robust, independent VIO with improved IMU bias modeling, outlier rejection, stationarity detection, and feature track selection, which is adjustable to run on embedded hardware. Long-term consistency is achieved with a loosely-coupled SLAM module. In academic benchmarks, our solution yields excellent performance in all categories, especially in the real-time use case, where we outperform the current state-of-the-art. We also demonstrate the feasibility of VIO for vehicular tracking on consumer-grade hardware using a custom dataset, and show good performance in comparison to current commercial VISLAM alternatives.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Seiskari_HybVIO_Pushing_the_Limits_of_Real-Time_Visual-Inertial_Odometry_WACV_2022_paper.pdf",
        "aff": "Spectacular AI; Spectacular AI; Spectacular AI + Aalto University; Spectacular AI; Spectacular AI + Tampere University; Spectacular AI + Aalto University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Seiskari_HybVIO_Pushing_the_WACV_2022_supplemental.pdf",
        "arxiv": "2106.11857",
        "pdf_size": 1338165,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17219961282760425163&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "spectacularai.com;spectacularai.com;spectacularai.com;spectacularai.com;tuni.fi;spectacularai.com",
        "email": "spectacularai.com;spectacularai.com;spectacularai.com;spectacularai.com;tuni.fi;spectacularai.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;0;0+2;0+1",
        "aff_unique_norm": "Spectacular AI;Aalto University;Tampere University",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.aalto.fi;https://www.tuni.fi",
        "aff_unique_abbr": ";Aalto;Tuni",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";Finland"
    },
    {
        "id": "bfe5bd05ed",
        "title": "Hyper-Convolution Networks for Biomedical Image Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ma_Hyper-Convolution_Networks_for_Biomedical_Image_Segmentation_WACV_2022_paper.html",
        "author": "Tianyu Ma; Adrian V. Dalca; Mert R. Sabuncu",
        "abstract": "The convolution operation is a central building block of neural network architectures widely used in computer vision. The size of the convolution kernels determines both the expressiveness of convolutional neural networks (CNN), as well as the number of learnable parameters. Increasing the network capacity to capture rich pixel relationships requires increasing the number of learnable parameters, often leading to overfitting and/or lack of robustness. In this paper, we propose a powerful novel building block, the hyper-convolution, which implicitly represents the convolution kernel as a function of kernel coordinates. Hyper-convolutions enable decoupling the kernel size, and hence its receptive field, from the number of learnable parameters. In our experiments, focused on challenging biomedical image segmentation tasks, we demonstrate that replacing regular convolutions with hyper-convolutions leads to more efficient architectures that achieve improved accuracy. Our analysis also shows that learned hyper-convolutions are naturally regularized, which can offer better generalization performance. We believe that hyper-convolutions can be a powerful building block in future neural network architectures solving computer vision tasks. We provide all of our code here: https://github.com/tym002/Hyper-Convolution",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ma_Hyper-Convolution_Networks_for_Biomedical_Image_Segmentation_WACV_2022_paper.pdf",
        "aff": "Cornell University+Cornell Tech; Massachusetts Institute of Technology+Massachusetts General Hospital+Harvard Medical School; Cornell University+Cornell Tech",
        "project": "",
        "github": "https://github.com/tym002/Hyper-Convolution",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ma_Hyper-Convolution_Networks_for_WACV_2022_supplemental.pdf",
        "arxiv": "2105.10559",
        "pdf_size": 2183098,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2556944454602678235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cornell.edu;mit.edu;cornell.edu",
        "email": "cornell.edu;mit.edu;cornell.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;1+2+3;0+0",
        "aff_unique_norm": "Cornell University;Massachusetts Institute of Technology;Massachusetts General Hospital;Harvard University",
        "aff_unique_dep": ";;;Medical School",
        "aff_unique_url": "https://www.cornell.edu;https://web.mit.edu;https://www.massgeneral.org;https://hms.harvard.edu",
        "aff_unique_abbr": "Cornell;MIT;MGH;HMS",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";New York City;Boston",
        "aff_country_unique_index": "0+0;0+0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d797543c92",
        "title": "Hyperspectral Image Super-Resolution With RGB Image Super-Resolution as an Auxiliary Task",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Li_Hyperspectral_Image_Super-Resolution_With_RGB_Image_Super-Resolution_as_an_Auxiliary_WACV_2022_paper.html",
        "author": "Ke Li; Dengxin Dai; Luc Van Gool",
        "abstract": "This work studies Hyperspectral image (HSI) super-resolution (SR). HSI SR is characterized by high-dimensional data and a limited amount of training exam-ples. This raises challenges for training deep neural net-works that are known to be data hungry. This work ad-dresses this issue with two contributions. First, we observethat HSI SR and RGB image SR are correlated and developa novel multi-tasking network to train them jointly so thatthe auxiliary task RGB image SR can provide additionalsupervision and regulate the network training. Second,we extend the network to a semi-supervised setting so thatit can learn from datasets containing only low-resolutionHSIs. With these contributions, our method is able to learnhyperspectral image super-resolution from heterogeneousdatasets and lifts the requirement for having a large amountof HD HSI training samples. Extensive experiments onthree standard datasets show that our method outperformsexisting methods significantly and underpin the relevance ofour contributions.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Li_Hyperspectral_Image_Super-Resolution_With_RGB_Image_Super-Resolution_as_an_Auxiliary_WACV_2022_paper.pdf",
        "aff": "CVL, ETH Zurich; MPI for Informatics; CVL, ETH Zurich + PSI, KU Leuven",
        "project": "",
        "github": "https://github.com/kli8996/HSISR.git",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8866171,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16784424966564094376&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "vision.ee.ethz.ch;mpi-inf.mpg.de;vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch;mpi-inf.mpg.de;vision.ee.ethz.ch",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "ETH Zurich;Max Planck Institute for Informatics;KU Leuven",
        "aff_unique_dep": "Computer Vision Laboratory;Informatics;PSI",
        "aff_unique_url": "https://www.ethz.ch;https://www.mpi-inf.mpg.de;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;MPII;KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0+2",
        "aff_country_unique": "Switzerland;Germany;Belgium"
    },
    {
        "id": "1e80f96163",
        "title": "Identifying Wrongly Predicted Samples: A Method for Active Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Aljundi_Identifying_Wrongly_Predicted_Samples_A_Method_for_Active_Learning_WACV_2022_paper.html",
        "author": "Rahaf Aljundi; Nikolay Chumerin; Daniel Olmeda Reino",
        "abstract": "While unlabelled data can be largely available and even abundant, the annotation process can be quite expensive and limiting. Under the assumption that some samples are more important for a given task than others, active learning targets the problem of identifying the most informative samples that one should acquire annotations for. In this work we propose a simple sample selection criterion that moves beyond the conventional reliance on model uncertainty as proxy to leverage new labels. By first accepting the model prediction and then judging its effect on the generalization error, we can better identify wrongly predicted samples. We also present a very efficient approximation to our criterion, providing a similarity-based interpretation. In addition to evaluating our method on the standard benchmarks of active learning, we consider the challenging yet realistic imbalanced data scenario. We show state-of-the-art results, especially on the imbalanced setting, and achieve better rates at identifying wrongly predicted samples than existing active learning methods. Our method is simple, model agnostic and relies on the current model status without the need for re-training from scratch.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Aljundi_Identifying_Wrongly_Predicted_Samples_A_Method_for_Active_Learning_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Aljundi_Identifying_Wrongly_Predicted_WACV_2022_supplemental.pdf",
        "arxiv": "2010.06890",
        "pdf_size": 966464,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11737317699851334264&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ae1a6b785c",
        "title": "ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Rukhovich_ImVoxelNet_Image_to_Voxels_Projection_for_Monocular_and_Multi-View_General-Purpose_WACV_2022_paper.html",
        "author": "Danila Rukhovich; Anna Vorontsova; Anton Konushin",
        "abstract": "In this paper, we introduce the task of multi-view RGB-based 3D object detection as an end-to-end optimization problem. To address this problem, we propose ImVoxelNet, a novel fully convolutional method of 3D object detection based on posed monocular or multi-view RGB images. The number of monocular images in each multi-view input can variate during training and inference; actually, this number might be unique for each multi-view input. ImVoxelNet successfully handles both indoor and outdoor scenes, which makes it general-purpose. Specifically, it achieves state-of-the-art results in car detection on KITTI (monocular) and nuScenes (multi-view) benchmarks among all methods that accept RGB images. Moreover, it surpasses existing RGB-based 3D object detection methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark for multi-view 3D object detection.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Rukhovich_ImVoxelNet_Image_to_Voxels_Projection_for_Monocular_and_Multi-View_General-Purpose_WACV_2022_paper.pdf",
        "aff": "Samsung AI Center Moscow + Lomonosov Moscow State University; Samsung AI Center Moscow + Lomonosov Moscow State University; Samsung AI Center Moscow + Lomonosov Moscow State University",
        "project": "",
        "github": "https://github.com/saic-vul/imvoxelnet",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Rukhovich_ImVoxelNet_Image_to_WACV_2022_supplemental.pdf",
        "arxiv": "2106.01178",
        "pdf_size": 8534988,
        "gs_citation": 236,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14766840776929327982&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "samsung.com;samsung.com;samsung.com",
        "email": "samsung.com;samsung.com;samsung.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Samsung;Lomonosov Moscow State University",
        "aff_unique_dep": "AI Center;",
        "aff_unique_url": "https://www.samsung.com/global/innovation/ai-research/;https://www.msu.ru",
        "aff_unique_abbr": "Samsung AI;MSU",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Moscow",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "id": "e3bef166fa",
        "title": "Image Restoration by Deep Projected GSURE",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Abu-Hussein_Image_Restoration_by_Deep_Projected_GSURE_WACV_2022_paper.html",
        "author": "Shady Abu-Hussein; Tom Tirer; Se Young Chun; Yonina C. Eldar; Raja Giryes",
        "abstract": "Ill-posed inverse problems appear in many image processing applications, such as deblurring and super-resolution. In recent years, solutions that are based on deep Convolutional Neural Networks (CNNs) have shown great promise. Yet, most of these techniques, which train CNNs using external data, are restricted to the observation models that have been used in the training phase. A recent alternative that does not have this drawback relies on learning the target image using internal learning. One such prominent example is the Deep Image Prior (DIP) technique that trains a network directly on the input image with the least-squares loss. In this paper, we propose a new image restoration framework that is based on minimizing a loss function that includes a \"projected-version\" of the Generalized Stein Unbiased Risk Estimator (GSURE) and parameterization of the latent image by a CNN. We demonstrate two ways to use our framework. In the first one, where no explicit prior is used, we show that the proposed approach outperforms other internal learning methods, such as DIP. In the second one, we show that our GSURE-based loss leads to improved performance when used within a plug-and-play priors scheme.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Abu-Hussein_Image_Restoration_by_Deep_Projected_GSURE_WACV_2022_paper.pdf",
        "aff": "EE, Tel Aviv University; EE, Tel Aviv University; ECE, INMC, Seoul National Univ.; Weizmann Institute of Science; EE, Tel Aviv University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7782638,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8147671007352980233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "mail.tau.ac.il;mail.tau.ac.il;snu.ac.kr;weizmann.ac.il;tauex.tau.ac.il",
        "email": "mail.tau.ac.il;mail.tau.ac.il;snu.ac.kr;weizmann.ac.il;tauex.tau.ac.il",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Tel Aviv University;Seoul National University;Weizmann Institute of Science",
        "aff_unique_dep": "Electrical Engineering;Electrical and Computer Engineering;",
        "aff_unique_url": "https://www.tau.ac.il;https://www.snu.ac.kr;https://www.weizmann.org.il",
        "aff_unique_abbr": "TAU;SNU;Weizmann",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seoul",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "Israel;South Korea"
    },
    {
        "id": "5c058db6e5",
        "title": "Image-Adaptive Hint Generation via Vision Transformer for Outpainting",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kong_Image-Adaptive_Hint_Generation_via_Vision_Transformer_for_Outpainting_WACV_2022_paper.html",
        "author": "Daehyeon Kong; Kyeongbo Kong; Kyunghun Kim; Sung-Jun Min; Suk-Ju Kang",
        "abstract": "Image outpainting has recently received considerable attention because it can be useful in tasks such as image retargeting and panorama image generation. In general, the problem of extending an image beyond its given boundaries is still ill-posed. Conventional methods predominantly attempt image outpainting by using complex network structures. Some recent studies have tried to decrease the problem complexity through the conversion techniques from outpainting to inpainting. Although these methodologies work well in simple cases, their performance reduces considerably for asymmetrical images. This paper proposes a novel hint-based outpainting methodology that can adaptively select the most plausible patches as hints from a given image to reduce the difficulty of outpainting. To estimate high-quality hints, inspired by patch-based image inpainting methods, we utilize Vision Transformer that also considers self-attention for each patch. The estimated hints are attached on both boundaries of the input image and the inside missing regions are predicted by using an inpainting network. After finishing the prediction, the output image is obtained by removing the hints. Experiments show that our image-adaptive hint framework, when employed in representative inpainting networks, can consistently improve its performance compared to the other conversion techniques from outpainting to inpainting on SUN and Beach benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kong_Image-Adaptive_Hint_Generation_via_Vision_Transformer_for_Outpainting_WACV_2022_paper.pdf",
        "aff": "Sogang University; Pukyong National University; Sogang University; Sogang University; Sogang University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Kong_Image-Adaptive_Hint_Generation_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 8813043,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11095491910861792588&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "sogang.ac.kr;pknu.ac.kr;sogang.ac.kr;sogang.ac.kr;sogang.ac.kr",
        "email": "sogang.ac.kr;pknu.ac.kr;sogang.ac.kr;sogang.ac.kr;sogang.ac.kr",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Sogang University;Pukyong National University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sogang.ac.kr;https://www.pukyong.ac.kr",
        "aff_unique_abbr": "Sogang;PKNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "93ffd757e8",
        "title": "Improve Image Captioning by Estimating the Gazing Patterns From the Caption",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Alahmadi_Improve_Image_Captioning_by_Estimating_the_Gazing_Patterns_From_the_WACV_2022_paper.html",
        "author": "Rehab Alahmadi; James Hahn",
        "abstract": "Recently, there has been much interest in developing image captioning models. State-of-the-art models reached a good performance in producing human-like descriptions from image features that are extracted from neural network models such as CNN and R-CNN. However, none of the previous methods have encapsulated explicit features that reflect a human perception of the images such as gazing patterns without the use of the eye-tracking systems. In this paper, we hypothesize that the nouns (i.e. entities) and their orders in the image description reflect human gazing patterns and perception. To this end, we estimate the sequence of the gazed objects from the words in the captions and then train a pointer network to learn to produce such sequence automatically given a set of objects in new images. We incorporate the suggested sequence by pointer network in existing image caption models and investigate its performance. Our experiments show a significant increase in the performance of the image captioning models when the sequence of the gazed objects are utilized as additional features (up to 13 points improvement in CIDEr score when combined with Neural Image Caption model).",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Alahmadi_Improve_Image_Captioning_by_Estimating_the_Gazing_Patterns_From_the_WACV_2022_paper.pdf",
        "aff": "Department of Computer Science, The George Washington University + Department of Information Technology, King Saud University; Department of Computer Science, The George Washington University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1498661,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14275113053000813557&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gwu.edu;gwu.edu",
        "email": "gwu.edu;gwu.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "George Washington University;King Saud University",
        "aff_unique_dep": "Department of Computer Science;Department of Information Technology",
        "aff_unique_url": "https://www.gwu.edu;https://www.ksu.edu.sa",
        "aff_unique_abbr": "GWU;KSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "United States;Saudi Arabia"
    },
    {
        "id": "bace801046",
        "title": "Improving Fractal Pre-Training",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Anderson_Improving_Fractal_Pre-Training_WACV_2022_paper.html",
        "author": "Connor Anderson; Ryan Farrell",
        "abstract": "The deep neural networks used in modern computer vision systems require enormous image datasets to train them. These carefully-curated datasets typically have a million or more images, across a thousand or more distinct categories. The process of creating and curating such a dataset is a monumental undertaking, demanding extensive effort and labelling expense and necessitating careful navigation of technical and social issues such as label accuracy, copyright ownership, and content bias. What if we had a way to harness the power of large image datasets but with few or none of the major issues and concerns currently faced? This paper extends the recent work of Kataoka et al. [2020], proposing an improved pre-training dataset based on dynamically-generated fractal images. Challenging issues with large-scale image datasets become points of elegance for fractal pre-training: perfect label accuracy at zero cost; no need to store/transmit large image archives; no privacy/demographic bias/concerns of inappropriate content, as no humans are pictured; limitless supply and diversity of images; and the images are free/open-source. Perhaps surprisingly, avoiding these difficulties imposes only a small penalty in performance. Leveraging a newly-proposed pre-training task---multi-instance prediction---our experiments demonstrate that fine-tuning a network pre-trained using fractals attains 92.7-98.1% of the accuracy of an ImageNet pre-trained network. Our code is publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Anderson_Improving_Fractal_Pre-Training_WACV_2022_paper.pdf",
        "aff": "Brigham Young University; Brigham Young University",
        "project": "catalys1.github.io/fractal-pretraining/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Anderson_Improving_Fractal_Pre-Training_WACV_2022_supplemental.pdf",
        "arxiv": "2110.03091",
        "pdf_size": 2186428,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7615309810625460994&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "byu.edu;cs.byu.edu",
        "email": "byu.edu;cs.byu.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Brigham Young University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.byu.edu",
        "aff_unique_abbr": "BYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2b2ba3827a",
        "title": "Improving Model Generalization by Agreement of Learned Representations From Data Augmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Atienza_Improving_Model_Generalization_by_Agreement_of_Learned_Representations_From_Data_WACV_2022_paper.html",
        "author": "Rowel Atienza",
        "abstract": "Data augmentation reduces the generalization error by forcing a model to learn invariant representations given different transformations of the input image. In computer vision, on top of the standard image processing functions, data augmentation techniques based on regional dropout such as CutOut, MixUp, and CutMix and policy-based selection such as AutoAugment demonstrated state-of-the-art (SOTA) results. With an increasing number of data augmentation algorithms being proposed, the focus is always on optimizing the input-output mapping while not realizing that there might be an untapped value in the transformed images with the same label. We hypothesize that by forcing the representations of two transformations to agree, we can further reduce the model generalization error. We call our proposed method Agreement Maximization or simply AgMax. With this simple constraint applied during training, empirical results show that data augmentation algorithms can further improve the classification accuracy of ResNet50 on ImageNet by up to 1.5%, WideResNet40-2 on CIFAR10 by up to 0.7%, WideResNet40-2 on CIFAR100 by up to 1.6%, and LeNet5 on Speech Commands Dataset by up to 1.4%. Experimental results further show that unlike other regularization terms such as label smoothing, AgMax can take advantage of the data augmentation to consistently improve model generalization by a significant margin. On downstream tasks such as object detection and segmentation on PascalVOC and COCO, AgMax pre-trained models outperforms other data augmentation methods by as much as 1.0mAP (box) and 0.5mAP (mask). Code is available at https://github.com/roatienza/agmax.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Atienza_Improving_Model_Generalization_by_Agreement_of_Learned_Representations_From_Data_WACV_2022_paper.pdf",
        "aff": "University of the Philippines Electrical and Electronics Engineering Institute, Diliman, 1101 Quezon City, Philippines",
        "project": "",
        "github": "https://github.com/roatienza/agmax",
        "supp": "",
        "arxiv": "2110.10536",
        "pdf_size": 4020573,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10449247233453352827&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "eee.upd.edu.ph",
        "email": "eee.upd.edu.ph",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of the Philippines",
        "aff_unique_dep": "Electrical and Electronics Engineering Institute",
        "aff_unique_url": "https://www.up.edu.ph",
        "aff_unique_abbr": "UP",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Diliman",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Philippines"
    },
    {
        "id": "9d172c706d",
        "title": "Improving Object Detection by Label Assignment Distillation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Nguyen_Improving_Object_Detection_by_Label_Assignment_Distillation_WACV_2022_paper.html",
        "author": "Chuong H. Nguyen; Thuy C. Nguyen; Tuan N. Tang; Nam L.H. Phan",
        "abstract": "Label assignment in object detection aims to assign targets, foreground or background, to sampled regions in an image. Unlike labeling for image classification, this problem is not well defined due to the object's bounding box. In this paper, we investigate the problem from a perspective of distillation, hence we call Label Assignment Distillation (LAD). Our initial motivation is very simple, we use a teacher network to generate labels for the student. This can be achieved in two ways: either using the teacher's prediction as the direct targets (soft label), or through the hard labels dynamically assigned by the teacher (LAD). Our experiments reveal that: (i) LAD is more effective than soft-label, but they are complementary. (ii) Using LAD, a smaller teacher can also improve a larger student significantly, while soft-label can't. We then introduce Co-learning LAD, in which two networks simultaneously learn from scratch and the role of teacher and student are dynamically interchanged. Using PAA-ResNet50 as a teacher, our LAD techniques can improve detectors PAA-ResNet101 and PAA-ResNeXt101 to 46 AP and 47.5 AP on the COCO test-dev set. With a stronger teacher PAA-SwinB, we improve the students PAA-ResNet50 to 43.7 AP by only 1x schedule training and standard setting, and PAA-ResNet101 to 47.9 AP, significantly surpassing the current methods. Our source code is released at https://git.io/JrDZo.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Nguyen_Improving_Object_Detection_by_Label_Assignment_Distillation_WACV_2022_paper.pdf",
        "aff": "CyberCore AI, Ho Chi Minh, Viet Nam; CyberCore AI, Ho Chi Minh, Viet Nam; CyberCore AI, Ho Chi Minh, Viet Nam; CyberCore AI, Ho Chi Minh, Viet Nam",
        "project": "https://git.io/JrDZo",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Nguyen_Improving_Object_Detection_WACV_2022_supplemental.pdf",
        "arxiv": "2108.10520",
        "pdf_size": 796652,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8884306980263647674&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cybercore.co.jp;cybercore.co.jp;cybercore.co.jp;cybercore.co.jp",
        "email": "cybercore.co.jp;cybercore.co.jp;cybercore.co.jp;cybercore.co.jp",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "CyberCore AI",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Ho Chi Minh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Vietnam"
    },
    {
        "id": "561aae7670",
        "title": "Improving Single-Image Defocus Deblurring: How Dual-Pixel Images Help Through Multi-Task Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Abuolaim_Improving_Single-Image_Defocus_Deblurring_How_Dual-Pixel_Images_Help_Through_Multi-Task_WACV_2022_paper.html",
        "author": "Abdullah Abuolaim; Mahmoud Afifi; Michael S. Brown",
        "abstract": "Many camera sensors use a dual-pixel (DP) design that operates as a rudimentary light field providing two sub-aperture views of a scene in a single capture. The DP sensor was developed to improve how cameras perform autofocus. Since the DP sensor's introduction, researchers have found additional uses for the DP data, such as depth estimation, reflection removal, and defocus deblurring. We are interested in the latter task of defocus deblurring. In particular, we propose a single-image deblurring network that incorporates the two sub-aperture views into a multi-task framework. Specifically, we show that jointly learning to predict the two DP views from a single blurry input image improves the network's ability to learn to deblur the image. Our experiments show this multi-task strategy achieves +1dB PSNR improvement over state-of-the-art defocus deblurring methods. In addition, our multi-task framework allows accurate DP-view synthesis (e.g.,  39dB PSNR) from the single input image. These high-quality DP views can be used for other DP-based applications, such as reflection removal. As part of this effort, we have captured a new dataset of 7,059 high-quality images to support our training for the DP-view synthesis task.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Abuolaim_Improving_Single-Image_Defocus_Deblurring_How_Dual-Pixel_Images_Help_Through_Multi-Task_WACV_2022_paper.pdf",
        "aff": "York University; York University; York University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Abuolaim_Improving_Single-Image_Defocus_WACV_2022_supplemental.zip",
        "arxiv": "2108.05251",
        "pdf_size": 6001977,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8101884381057866161&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "eecs.yorku.ca;eecs.yorku.ca;eecs.yorku.ca",
        "email": "eecs.yorku.ca;eecs.yorku.ca;eecs.yorku.ca",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yorku.ca",
        "aff_unique_abbr": "York U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "7e7c922adf",
        "title": "In-Field Phenotyping Based on Crop Leaf and Plant Instance Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Weyler_In-Field_Phenotyping_Based_on_Crop_Leaf_and_Plant_Instance_Segmentation_WACV_2022_paper.html",
        "author": "Jan Weyler; Federico Magistri; Peter Seitz; Jens Behley; Cyrill Stachniss",
        "abstract": "A detailed analysis of a plant's phenotype in real field conditions is critical for plant scientists and breeders to understand plant function. In contrast to traditional phenotyping performed manually, vision-based systems have the potential for an objective and automated assessment with high spatial and temporal resolution. One of such systems' objectives is to detect and segment individual leaves of each plant since this information correlates to the growth stage and provides phenotypic traits, such as leaf count, coverage, and size. In this paper, we propose a vision-based approach that performs instance segmentation of individual crop leaves and associates each with its corresponding crop plant in real fields. This enables us to compute relevant basic phenotypic traits on a per-plant level. We employ a convolutional neural network and operate directly on drone imagery. The network generates two different representations of the input image that we utilize to cluster individual crop leaf and plant instances. We propose a novel method to compute clustering regions based on our network's predictions that achieves high accuracy. Furthermore, we compare to other state-of-the-art approaches and show that our system achieves superior performance. The source code of our approach is available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Weyler_In-Field_Phenotyping_Based_on_Crop_Leaf_and_Plant_Instance_Segmentation_WACV_2022_paper.pdf",
        "aff": "University of Bonn, Germany; University of Bonn, Germany; Robert Bosch GmbH, Germany; University of Bonn, Germany; University of Bonn, Germany",
        "project": "",
        "github": "https://github.com/PRBonn/leaf-plant-instance-segmentation",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Weyler_In-Field_Phenotyping_Based_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2915437,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6426293311637711515&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Bonn;Robert Bosch GmbH",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-bonn.de;https://www.bosch.com",
        "aff_unique_abbr": "UBonn;Bosch",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "5858297f0e",
        "title": "Inferring the Class Conditional Response Map for Weakly Supervised Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sun_Inferring_the_Class_Conditional_Response_Map_for_Weakly_Supervised_Semantic_WACV_2022_paper.html",
        "author": "Weixuan Sun; Jing Zhang; Nick Barnes",
        "abstract": "Image-level weakly supervised semantic segmentation (WSSS) relies on class activation maps (CAMs) for pseudo labels generation. As CAMs only highlight the most discriminative regions of objects, the generated pseudo labels are usually unsatisfactory to serve directly as supervision. To solve this, most existing approaches follow a multi-training pipeline to refine CAMs for better pseudo-labels, which includes: 1) re-training the classification model to generate CAMs; 2) post-processing CAMs to obtain pseudo labels; and 3) training a semantic segmentation model with the obtained pseudo labels. However, this multi-training pipeline requires complicated adjustment and additional time. To address this, we propose a class-conditional inference strategy and an activation aware mask refinement loss function to generate better pseudo labels without re-training the classifier. The class conditional inference-time approach is presented to separately and iteratively reveal the classification network's hidden object activation to generate more complete response maps. Further, our activation aware mask refinement loss function introduces a novel way to exploit saliency maps during segmentation training and refine the foreground object masks without suppressing background objects. Our method achieves superior WSSS results without requiring re-training of the classifier.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sun_Inferring_the_Class_Conditional_Response_Map_for_Weakly_Supervised_Semantic_WACV_2022_paper.pdf",
        "aff": "Australian National University; Australian National University; Australian National University",
        "project": "",
        "github": "https://github.com/weixuansun/InferCam",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Sun_Inferring_the_Class_WACV_2022_supplemental.pdf",
        "arxiv": "2110.14309",
        "pdf_size": 1638299,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2984783976227695367&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "c9d80b2963",
        "title": "InfographicVQA",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Mathew_InfographicVQA_WACV_2022_paper.html",
        "author": "Minesh Mathew; Viraj Bagal; Rub\u00e8n Tito; Dimosthenis Karatzas; Ernest Valveny; C.V. Jawahar",
        "abstract": "Infographics communicate information using a combination of textual, graphical and visual elements. This work explores the automatic understanding of infographic images by using a Visual Question Answering technique. To this end, we present InfographicVQA, a new dataset comprising a diverse collection of infographics and question-answer annotations. The questions require methods that jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with an emphasis on questions that require elementary reasoning and basic arithmetic skills. For VQA on the dataset, we evaluate two Transformer-based strong baselines. Both the baselines yield unsatisfactory results compared to near perfect human performance on the dataset. The results suggest that VQA on infographics--images that are designed to communicate information quickly and clearly to human brain--is ideal for benchmarking machine understanding of complex document images. The dataset is available for download at docvqa.org",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Mathew_InfographicVQA_WACV_2022_paper.pdf",
        "aff": "CVIT, IIIT Hyderabad, India+IISER Pune, India; Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain; CVIT, IIIT Hyderabad, India",
        "project": "docvqa.org",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Mathew_InfographicVQA_WACV_2022_supplemental.pdf",
        "arxiv": "2104.12756",
        "pdf_size": 5464539,
        "gs_citation": 311,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18210396866769101896&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "research.iiit.ac.in;students.iiserpune.ac.in;cvc.uab.cat; ; ; ",
        "email": "research.iiit.ac.in;students.iiserpune.ac.in;cvc.uab.cat; ; ; ",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;2;2;2;0",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad;Indian Institute of Science Education and Research;Universitat Aut\u00f2noma de Barcelona",
        "aff_unique_dep": ";;Computer Vision Center",
        "aff_unique_url": "https://iiit Hyderabad.ac.in;https://www.iiserpune.ac.in;https://www.uab.cat",
        "aff_unique_abbr": "IIIT Hyderabad;IISER Pune;UAB",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "Hyderabad;Pune;",
        "aff_country_unique_index": "0+0;1;1;1;1;0",
        "aff_country_unique": "India;Spain"
    },
    {
        "id": "210a8b8dea",
        "title": "Information Bottlenecked Variational Autoencoder for Disentangled 3D Facial Expression Modelling",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sun_Information_Bottlenecked_Variational_Autoencoder_for_Disentangled_3D_Facial_Expression_Modelling_WACV_2022_paper.html",
        "author": "Hao Sun; Nick Pears; Yajie Gu",
        "abstract": "Learning a disentangled representation is essential to build 3D face models that accurately capture identity and expression. We propose a novel variational autoencoder (VAE) framework to disentangle identity and expression from 3D input faces that have a wide variety of expressions. Specifically, we design a system that has two decoders: one for neutral-expression faces (i.e. identity-only faces) and one for the original (expressive) input faces respectively. Crucially, we have an additional mutual-information regulariser applied on the identity part to solve the issue of imbalanced information over the expressive input faces and the reconstructed neutral faces. Our evaluations on two public datasets (CoMA and BU-3DFE) show that this model achieves competitive results on the 3D face reconstruction task and state-of-the-art results on identity-expression disentanglement. We also show that by updating to a conditional VAE, we have a system that generates different levels of expressions from semantically meaningful variables.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sun_Information_Bottlenecked_Variational_Autoencoder_for_Disentangled_3D_Facial_Expression_Modelling_WACV_2022_paper.pdf",
        "aff": "Department of Computer Science, University of York, UK; Department of Computer Science, University of York, UK; Department of Computer Science, University of York, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1700469,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16871293279489099311&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "york.ac.uk;york.ac.uk;york.ac.uk",
        "email": "york.ac.uk;york.ac.uk;york.ac.uk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of York",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.york.ac.uk",
        "aff_unique_abbr": "York",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "601ea30a47",
        "title": "Inpaint2Learn: A Self-Supervised Framework for Affordance Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zhang_Inpaint2Learn_A_Self-Supervised_Framework_for_Affordance_Learning_WACV_2022_paper.html",
        "author": "Lingzhi Zhang; Weiyu Du; Shenghao Zhou; Jiancong Wang; Jianbo Shi",
        "abstract": "Perceiving affordances -- the opportunities of interaction in a scene, is a fundamental ability of humans. It is an equally important skill for AI agents and robots to better understand and interact with the world. However, labeling affordances in the environment is not a trivial task. To address this issue, we propose a task-agnostic framework, named Inpaint2Learn, that generates affordance labels in a fully automatic manner and opens the door for affordance learning in the wild. To demonstrate its effectiveness, we apply it to three different tasks: human affordance prediction, Location2Object and 6D object pose hallucination. Our experiments and user studies show that our models, trained with the Inpaint2Learn scaffold, are able to generate diverse and visually plausible results in all three scenarios.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zhang_Inpaint2Learn_A_Self-Supervised_Framework_for_Affordance_Learning_WACV_2022_paper.pdf",
        "aff": "University of Pennsylvania; University of Pennsylvania + Nuro, Inc.; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Zhang_Inpaint2Learn_A_Self-Supervised_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1919805,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8581254061608744816&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "University of Pennsylvania;Nuro, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;https://www.nuro.ai",
        "aff_unique_abbr": "UPenn;Nuro",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7bfeac12eb",
        "title": "Intelligent Camera Selection Decisions for Target Tracking in a Camera Network",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sharma_Intelligent_Camera_Selection_Decisions_for_Target_Tracking_in_a_Camera_WACV_2022_paper.html",
        "author": "Anil Sharma; Saket Anand; Sanjit K Kaul",
        "abstract": "Camera Selection Decisions (CSD) are highly useful for several applications in a multi-camera network. For example, CSD benefit multi-camera target tracking by reducing the number of candidate cameras to look for the target's next location. The correct candidate cameras, decreases the number of false Re-ID queries as well as the computation time. Also, in multi-camera trajectory forecasting (MCTF) to predict where a person will re-appear in the camera network along with the transition time. These applications require a large amount of annotated data for training. In this paper, we use state-representation learning with a reinforcement learning based policy to effectively and efficiently make camera selection decisions. We further demonstrate that by using learned state representations, as opposed to hand-crafted state variables, we are able to achieve state-of-the-art results on camera selection, while reducing the training time for the RL policy. Along with this, we use a reward function that helps to reduce the amount of supervision in training the policy in a semi-supervised way. We report our results on four datasets: NLPR-MCT, DukeMTMC, CityFlow, and WNMF dataset. We show that an RL policy reduces unnecessary Re-ID queries and therefore the false alarms, scales well to larger camera networks, and is target-agnostic.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sharma_Intelligent_Camera_Selection_Decisions_for_Target_Tracking_in_a_Camera_WACV_2022_paper.pdf",
        "aff": "Indraprastha Institute of Information Technology (IIIT), Delhi; Indraprastha Institute of Information Technology (IIIT), Delhi; Indraprastha Institute of Information Technology (IIIT), Delhi",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1141829,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9624558903742816522&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in",
        "email": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indraprastha Institute of Information Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iiitdelhi.ac.in",
        "aff_unique_abbr": "IIIT Delhi",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Delhi",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "87f086e579",
        "title": "Interpretable Semantic Photo Geolocation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Theiner_Interpretable_Semantic_Photo_Geolocation_WACV_2022_paper.html",
        "author": "Jonas Theiner; Eric M\u00fcller-Budack; Ralph Ewerth",
        "abstract": "Planet-scale photo geolocalization is the complex task of estimating the location depicted in an image solely based on its visual content. Due to the success of convolutional neural networks (CNNs), current approaches achieve super-human performance. However, previous work has exclusively focused on optimizing geolocalization accuracy. Due to the black-box property of deep learning systems, their predictions are difficult to validate for humans. State-of-the-art methods treat the task as a classification problem, where the choice of the classes, that is the partitioning of the world map, is crucial for the performance. In this paper, we present two contributions to improve the interpretability of a geolocalization model: (1) We propose a novel semantic partitioning method which intuitively leads to an improved understanding of the predictions, while achieving state-of-the-art results for geolocational accuracy on benchmark test sets; (2) We introduce a metric to assess the importance of semantic visual concepts for a certain prediction to provide additional interpretable information, which allows for a large-scale analysis of already trained models. Source code and dataset are publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Theiner_Interpretable_Semantic_Photo_Geolocation_WACV_2022_paper.pdf",
        "aff": "L3S Research Center, Leibniz University Hannover, Hannover, Germany; TIB \u2013 Leibniz Information Centre for Science and Technology, Hannover, Germany; L3S Research Center, Leibniz University Hannover, Hannover, Germany+TIB \u2013 Leibniz Information Centre for Science and Technology, Hannover, Germany",
        "project": "",
        "github": "https://github.com/jtheiner/semantic_geo_partitioning",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5473125,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4497802832065277987&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "l3s.de;tib.eu;tib.eu",
        "email": "l3s.de;tib.eu;tib.eu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "Leibniz University Hannover;Leibniz Information Centre for Science and Technology",
        "aff_unique_dep": "L3S Research Center;",
        "aff_unique_url": "https://www.uni-hannover.de;https://www.tib.eu",
        "aff_unique_abbr": "LUH;TIB",
        "aff_campus_unique_index": "0;0;0+0",
        "aff_campus_unique": "Hannover",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "1b3fc68c94",
        "title": "Is an Image Worth Five Sentences? A New Look Into Semantics for Image-Text Matching",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Biten_Is_an_Image_Worth_Five_Sentences_A_New_Look_Into_WACV_2022_paper.html",
        "author": "Ali Furkan Biten; Andr\u00e9s Mafla; Llu\u00eds G\u00f3mez; Dimosthenis Karatzas",
        "abstract": "The task of image-text matching aims to map representations from different modalities into a common joint visual-textual embedding. However, the most widely used datasets for this task, MSCOCO and Flickr30K, are actually image captioning datasets that offer a very limited set of relationships between images and sentences in their ground-truth annotations. This limited ground truth information forces us to use evaluation metrics based on binary relevance: given a sentence query we consider only one image as relevant. However, many other relevant images or captions may be present in the dataset. In this work, we propose two metrics that evaluate the degree of semantic relevance of retrieved items, independently of their annotated binary relevance. Additionally, we incorporate a novel strategy that uses an image captioning metric, CIDEr, to define a Semantic Adaptive Margin (SAM) to be optimized in a standard triplet loss. By incorporating our formulation to existing models, a large improvement is obtained in scenarios where available training data is limited. We also demonstrate that the performance on the annotated image-caption pairs is maintained while improving on other non-annotated relevant items when employing the full training set. The code for our new metric can be found at github.com/furkanbiten/ncs_metric and the model implementation at github.com/andrespmd/semantic_adaptive_margin.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Biten_Is_an_Image_Worth_Five_Sentences_A_New_Look_Into_WACV_2022_paper.pdf",
        "aff": "Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain",
        "project": "",
        "github": "github.com/furkanbiten/ncs_metric; github.com/andrespmd/semantic_adaptive_margin",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Biten_Is_an_Image_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2284752,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4427942065223026313&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cvc.uab.es;cvc.uab.es;cvc.uab.es;cvc.uab.es",
        "email": "cvc.uab.es;cvc.uab.es;cvc.uab.es;cvc.uab.es",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Universitat Aut\u00f2noma de Barcelona",
        "aff_unique_dep": "Computer Vision Center",
        "aff_unique_url": "https://www.uab.cat",
        "aff_unique_abbr": "UAB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "abb6fded36",
        "title": "Joint Classification and Trajectory Regression of Online Handwriting Using a Multi-Task Learning Approach",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ott_Joint_Classification_and_Trajectory_Regression_of_Online_Handwriting_Using_a_WACV_2022_paper.html",
        "author": "Felix Ott; David R\u00fcgamer; Lucas Heublein; Bernd Bischl; Christopher Mutschler",
        "abstract": "Multivariate Time Series (MTS) classification is important in various applications such as signature verification, person identification, and motion recognition. In deep learning these classification tasks are usually learned using the cross-entropy loss. A related yet different task is predicting trajectories observed as MTS. Important use cases include handwriting reconstruction, shape analysis, and human pose estimation. The goal is to align an arbitrary dimensional time series with its ground truth as accurately as possible while reducing the error in the prediction with a distance loss and the variance with a similarity loss. Although learning both losses with Multi-Task Learning (MTL) helps to improve trajectory alignment, learning often remains difficult as both tasks are contradictory. We propose a novel neural network architecture for MTL that notably improves the MTS classification and trajectory regression performance in online handwriting (OnHW) recognition. We achieve this by jointly learning the cross-entropy loss in combination with distance and similarity losses. On an OnHW task of handwritten characters with multivariate inertial and visual data inputs we are able to achieve crucial improvements (lower error with less variance) of trajectory prediction while still improving the character classification accuracy in comparison to models trained on the individual tasks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ott_Joint_Classification_and_Trajectory_Regression_of_Online_Handwriting_Using_a_WACV_2022_paper.pdf",
        "aff": "Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS + LMU Munich, Munich, Germany; LMU Munich, Munich, Germany; Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS + LMU Munich, Munich, Germany; LMU Munich, Munich, Germany; Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ott_Joint_Classification_and_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2562178,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6230733925682859196&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "iis.fraunhofer.de;stat.uni-muenchen.de;iis.fraunhofer.de;stat.uni-muenchen.de;iis.fraunhofer.de",
        "email": "iis.fraunhofer.de;stat.uni-muenchen.de;iis.fraunhofer.de;stat.uni-muenchen.de;iis.fraunhofer.de",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0+1;1;0",
        "aff_unique_norm": "Fraunhofer Institute for Integrated Circuits;Ludwig Maximilian University of Munich",
        "aff_unique_dep": "Integrated Circuits;",
        "aff_unique_url": "https://www.iis.fraunhofer.de/;https://www.lmu.de",
        "aff_unique_abbr": "Fraunhofer IIS;LMU",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Munich",
        "aff_country_unique_index": "0+0;0;0+0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "b8604fd148",
        "title": "Knowledge Capture and Replay for Continual Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gopalakrishnan_Knowledge_Capture_and_Replay_for_Continual_Learning_WACV_2022_paper.html",
        "author": "Saisubramaniam Gopalakrishnan; Pranshu Ranjan Singh; Haytham Fayek; Savitha Ramasamy; ArulMurugan Ambikapathi",
        "abstract": "Deep neural networks model data for a task or a sequence of tasks, where the knowledge extracted from the data is encoded in the parameters and representations of the network. Extraction and utilization of these representations is vital when data is no longer available in the future, especially in a continual learning scenario. We introduce 'flashcards', which are visual representations that 'capture' the encoded knowledge of a network as a recursive function of some predefined random image patterns. In a continual learning scenario, flashcards help to prevent catastrophic forgetting by consolidating the knowledge of all the previous tasks. Flashcards are required to be constructed only before learning the subsequent task, hence, they are independent of the number of tasks trained before, making them task agnostic. We demonstrate the efficacy of flashcards in capturing learned knowledge representation (as an alternative to the original data), and empirically validate on a variety of continual learning tasks: reconstruction, denoising, and task-incremental classification, using several heterogeneous (varying background and complexity) benchmark datasets. Experimental evidence indicates that: (i) flashcards as a replay strategy is 'task agnostic', (ii) performs better than generative replay, and (iii) is on par with episodic replay without additional memory overhead.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gopalakrishnan_Knowledge_Capture_and_Replay_for_Continual_Learning_WACV_2022_paper.pdf",
        "aff": "Institute for Infocomm Research (I2R), A*STAR, Singapore+CNRS@CREATE LTD, 1 Create Way, 08-01 CREATE Tower, Singapore 138602; Institute for Infocomm Research (I2R), A*STAR, Singapore+CNRS@CREATE LTD, 1 Create Way, 08-01 CREATE Tower, Singapore 138602; RMIT University, Australia; Institute for Infocomm Research (I2R), A*STAR, Singapore+Artificial Intelligence, Analytics And Informatics (AI3), A*STAR, Singapore+CNRS@CREATE LTD, 1 Create Way, 08-01 CREATE Tower, Singapore 138602; Institute for Infocomm Research (I2R), A*STAR, Singapore+Artificial Intelligence, Analytics And Informatics (AI3), A*STAR, Singapore+CNRS@CREATE LTD, 1 Create Way, 08-01 CREATE Tower, Singapore 138602",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Gopalakrishnan_Knowledge_Capture_and_WACV_2022_supplemental.pdf",
        "arxiv": "2012.06789",
        "pdf_size": 5508798,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=794717470280670598&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "i2r.a-star.edu.sg;i2r.a-star.edu.sg;ieee.org;i2r.a-star.edu.sg;i2r.a-star.edu.sg",
        "email": "i2r.a-star.edu.sg;i2r.a-star.edu.sg;ieee.org;i2r.a-star.edu.sg;i2r.a-star.edu.sg",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2;0+3+1;0+3+1",
        "aff_unique_norm": "Institute for Infocomm Research;CREATE LTD;RMIT University;A*STAR",
        "aff_unique_dep": ";CNRS;;Artificial Intelligence, Analytics And Informatics (AI3)",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg;;https://www.rmit.edu.au;https://www.a-star.edu.sg",
        "aff_unique_abbr": "I2R;;RMIT;A*STAR",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;1;0+0+0;0+0+0",
        "aff_country_unique": "Singapore;Australia"
    },
    {
        "id": "6239303fee",
        "title": "Knowledge-Augmented Contrastive Learning for Abnormality Classification and Localization in Chest X-Rays With Radiomics Using a Feedback Loop",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Han_Knowledge-Augmented_Contrastive_Learning_for_Abnormality_Classification_and_Localization_in_Chest_WACV_2022_paper.html",
        "author": "Yan Han; Chongyan Chen; Ahmed Tewfik; Benjamin Glicksberg; Ying Ding; Yifan Peng; Zhangyang Wang",
        "abstract": "Accurate classification and localization of abnormalities in chest X-rays play an important role in clinical diagnosis and treatment planning. Building a highly accurate predictive model for these tasks usually requires a large number of manually annotated labels and pixel regions (bounding boxes) of abnormalities. However, it is expensive to acquire such annotations, especially the bounding boxes. Recently, contrastive learning has shown strong promise in leveraging unlabeled natural images to produce highly generalizable and discriminative features. However, extending its power to the medical image domain is under-explored and highly non-trivial, since medical images are much less amendable to data augmentations. In contrast, their prior knowledge, as well as radiomic features, is often crucial. To bridge this gap, we propose an end-to-end semi-supervised knowledge-augmented contrastive learning framework, that simultaneously performs disease classification and localization tasks. The key knob of our framework is a unique positive sampling approach tailored for the medical images, by seamlessly integrating radiomic features as a knowledge augmentation. Specifically, we first apply an image encoder to classify the chest X-rays and to generate the image features. We next leverage Grad-CAM to highlight the crucial (abnormal) regions for chest X-rays (even when unannotated), from which we extract radiomic features. The radiomic features are then passed through another dedicated encoder to act as the positive sample for the image features generated from the same chest X-ray. In this way, our framework constitutes a feedback loop for image and radiomic features to mutually reinforce each other. Their contrasting yields knowledge-augmented representations that are both robust and interpretable. Extensive experiments on the NIH Chest X-ray dataset demonstrate that our approach outperforms existing baselines in both classification and localization tasks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Han_Knowledge-Augmented_Contrastive_Learning_for_Abnormality_Classification_and_Localization_in_Chest_WACV_2022_paper.pdf",
        "aff": "The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; Icahn School of Medicine at Mount Sinai; The University of Texas at Austin; Weill Cornell Medicine; The University of Texas at Austin",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5087233,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5297100435285558189&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "utexas.edu;utexas.edu;utexas.edu;icahn.mssm.edu;utexas.edu;med.cornell.edu;utexas.edu",
        "email": "utexas.edu;utexas.edu;utexas.edu;icahn.mssm.edu;utexas.edu;med.cornell.edu;utexas.edu",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0;2;0",
        "aff_unique_norm": "University of Texas at Austin;Icahn School of Medicine at Mount Sinai;Weill Cornell Medicine",
        "aff_unique_dep": ";School of Medicine;",
        "aff_unique_url": "https://www.utexas.edu;https://icahn.mssm.edu;https://weill.cornell.edu",
        "aff_unique_abbr": "UT Austin;ISMMS;WCM",
        "aff_campus_unique_index": "0;0;0;1;0;0",
        "aff_campus_unique": "Austin;New York;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1d5a64dc43",
        "title": "LEAD: Self-Supervised Landmark Estimation by Aligning Distributions of Feature Similarity",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Karmali_LEAD_Self-Supervised_Landmark_Estimation_by_Aligning_Distributions_of_Feature_Similarity_WACV_2022_paper.html",
        "author": "Tejan Karmali; Abhinav Atrishi; Sai Sree Harsha; Susmit Agrawal; Varun Jampani; R. Venkatesh Babu",
        "abstract": "In this work, we introduce LEAD, an approach to discover landmarks from an unannotated collection of category-specific images. Existing works in self-supervised landmark detection are based on learning dense (pixel-level) feature representations from an image, which are further used to learn landmarks in a semi-supervised manner. While there have been advances in self-supervised learning of image features for instance-level tasks like classification, these methods do not ensure dense equivariant representations. The property of equivariance is of interest for dense prediction tasks like landmark estimation. In this work, we introduce an approach to enhance the learning of dense equivariant representations in a self-supervised fashion. We follow a two-stage training approach: first, we train a network using the BYOL objective which operates at an instance level. The correspondences obtained through this network are further used to train a dense and compact representation of the image using a lightweight network. We show that having such a prior in the feature extractor helps in landmark detection, even under drastically limited number of annotations while also improving generalization across scale variations.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Karmali_LEAD_Self-Supervised_Landmark_Estimation_by_Aligning_Distributions_of_Feature_Similarity_WACV_2022_paper.pdf",
        "aff": "Indian Institute of Science, Bengaluru; Indian Institute of Science, Bengaluru; Indian Institute of Science, Bengaluru; Indian Institute of Science, Bengaluru; Google Research; Indian Institute of Science, Bengaluru",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Karmali_LEAD_Self-Supervised_Landmark_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3547268,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=460890874741986566&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "Indian Institute of Science;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.iisc.ac.in;https://research.google",
        "aff_unique_abbr": "IISc;Google Research",
        "aff_campus_unique_index": "0;0;0;0;1;0",
        "aff_campus_unique": "Bengaluru;Mountain View",
        "aff_country_unique_index": "0;0;0;0;1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "a12af96bc0",
        "title": "Lane-Level Street Map Extraction From Aerial Imagery",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/He_Lane-Level_Street_Map_Extraction_From_Aerial_Imagery_WACV_2022_paper.html",
        "author": "Songtao He; Hari Balakrishnan",
        "abstract": "Digital maps with lane-level details are the foundation of many applications. However, creating and maintaining digital maps especially maps with lane-level details, are labor-intensive and expensive. In this work, we propose a mapping pipeline to extract lane-level street maps from aerial imagery automatically. Our mapping pipeline first extracts lanes at non-intersection areas, then it enumerates all the possible turning lanes at intersections, validates the connectivity of them, and extracts the valid turning lanes to complete the map. We evaluate the accuracy of our mapping pipeline on a dataset consisting of four U.S. cities, demonstrating the effectiveness of our proposed mapping pipeline and the potential of scalable mapping solutions based on aerial imagery.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/He_Lane-Level_Street_Map_Extraction_From_Aerial_Imagery_WACV_2022_paper.pdf",
        "aff": "MIT CSAIL; MIT CSAIL",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/He_Lane-Level_Street_Map_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3836611,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12729078431967134305&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "mit.edu;csail.mit.edu",
        "email": "mit.edu;csail.mit.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT CSAIL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c1c7782258",
        "title": "Late-Resizing: A Simple but Effective Sketch Extraction Strategy for Improving Generalization of Line-Art Colorization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kim_Late-Resizing_A_Simple_but_Effective_Sketch_Extraction_Strategy_for_Improving_WACV_2022_paper.html",
        "author": "Dohyun Kim; Dajung Je; Kwangjin Lee; Moohyun Kim; Han Kim",
        "abstract": "Automatic line-art colorization is a demanding research field owing to its expensive and labor-intensive workload. Learning-based approaches have lately emerged to improve the quality of colorization. To handle the lack of paired data in line art and color images, sketch extraction has been widely adopted. This study primarily focuses on the resizing process applied within the sketch extraction procedure, which is essential for normalizing input sketches of various sizes to the target size of the colorization model. We first analyze the inherent risk in a conventional resizing strategy, i.e., early-resizing, which places the resizing step before the line detection process to ensure the practicality. Although the strategy is extensively used, it involves an often overlooked risk of significantly degrading the generalization of the colorization model. Thus, we propose a late-resizing strategy in which resizing is applied after the line detection step. The proposed late-resizing strategy has three advantages: prevention of a quality degradation in the color image, augmentation for downsizing artifacts, and alleviation of look-ahead bias. In conclusion, we present both quantitative and qualitative evaluations on representative learning-based line-art colorization methods, which verify the effectiveness of the proposed method in the generalization of the colorization model.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kim_Late-Resizing_A_Simple_but_Effective_Sketch_Extraction_Strategy_for_Improving_WACV_2022_paper.pdf",
        "aff": "Naver Webtoon Corporation, Seoul, Korea; Naver Webtoon Corporation, Seoul, Korea; Naver Webtoon Corporation, Seoul, Korea; Naver Webtoon Corporation, Seoul, Korea; Naver Webtoon Corporation, Seoul, Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Kim_Late-Resizing_A_Simple_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6222907,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6684873654725318559&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "webtoonscorp.com;webtoonscorp.com;webtoonscorp.com;webtoonscorp.com;webtoonscorp.com",
        "email": "webtoonscorp.com;webtoonscorp.com;webtoonscorp.com;webtoonscorp.com;webtoonscorp.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Naver Webtoon Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.webtoons.com",
        "aff_unique_abbr": "Naver Webtoon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "4be38bbc8f",
        "title": "Latent Reweighting, an Almost Free Improvement for GANs",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Issenhuth_Latent_Reweighting_an_Almost_Free_Improvement_for_GANs_WACV_2022_paper.html",
        "author": "Thibaut Issenhuth; Ugo Tanielian; David Picard; J\u00e9r\u00e9mie Mary",
        "abstract": "Standard formulations of GANs, where a continuous function deforms a connected latent space, have been shown to be misspecified when fitting different classes of images. In particular, the generator will necessarily sample some low-quality images in between the classes. Rather than modifying the architecture, a line of works aims at improving the sampling quality from pre-trained generators at the expense of increased computational cost. Building on this, we introduce an additional network to predict latent importance weights and two associated sampling methods to avoid the poorest samples. This idea has several advantages: 1) it provides a way to inject disconnectedness into any GAN architecture, 2) since the rejection happens in the latent space, it avoids going through both the generator and the discriminator, saving computation time, 3) this importance weights formulation provides a principled way to reduce the Wasserstein's distance to the target distribution. We demonstrate the effectiveness of our method on several datasets, both synthetic and high-dimensional.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Issenhuth_Latent_Reweighting_an_Almost_Free_Improvement_for_GANs_WACV_2022_paper.pdf",
        "aff": "Criteo AI Lab, Paris, France + LIGM, Ecole des Ponts, Marne-la-vall\u00e9e, France; Criteo AI Lab, Paris, France; LIGM, Ecole des Ponts, Marne-la-vall\u00e9e, France; Criteo AI Lab, Paris, France",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Issenhuth_Latent_Reweighting_an_WACV_2022_supplemental.pdf",
        "arxiv": "2110.09803",
        "pdf_size": 4201964,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2026689760519987520&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "criteo.com;criteo.com;enpc.fr;criteo.com",
        "email": "criteo.com;criteo.com;enpc.fr;criteo.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;1;0",
        "aff_unique_norm": "Criteo;Ecole des Ponts ParisTech",
        "aff_unique_dep": "Criteo AI Lab;LIGM",
        "aff_unique_url": "https://www.criteo.com;https://www.ponts.fr",
        "aff_unique_abbr": "Criteo;ENPC",
        "aff_campus_unique_index": "0+1;0;1;0",
        "aff_campus_unique": "Paris;Marne-la-vall\u00e9e",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "967c4552fa",
        "title": "Latent to Latent: A Learned Mapper for Identity Preserving Editing of Multiple Face Attributes in StyleGAN-Generated Images",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Khodadadeh_Latent_to_Latent_A_Learned_Mapper_for_Identity_Preserving_Editing_WACV_2022_paper.html",
        "author": "Siavash Khodadadeh; Shabnam Ghadar; Saeid Motiian; Wei-An Lin; Ladislau B\u00f6l\u00f6ni; Ratheesh Kalarot",
        "abstract": "Several recent papers introduced techniques to adjust the attributes of human faces generated by unconditional GANs such as StyleGAN. Despite efforts to disentangle the attributes, a request to change one attribute often triggers unwanted changes to other attributes as well. More importantly, in some cases, a human observer would not recognize the edited face to belong to the same person. We propose an approach where a neural network takes as input the latent encoding of a face and the desired attribute changes and outputs the latent space encoding of the edited image. The network is trained offline using unsupervised data, with training labels generated by an off-the-shelf attribute classifier. The desired attribute changes and conservation laws, such as identity maintenance, are encoded in the training loss. The number of attributes the mapper can simultaneously modify is only limited by the attributes available to the classifier -- we trained a network that handles 35 attributes, more than any previous approach. As no optimization is performed at deployment time, the computation time is negligible, allowing real-time attribute editing. Qualitative and quantitative comparisons with the current state-of-the-art show our method is better at conserving the identity of the face and restricting changes to the requested attributes.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Khodadadeh_Latent_to_Latent_A_Learned_Mapper_for_Identity_Preserving_Editing_WACV_2022_paper.pdf",
        "aff": "Adobe Inc. San Jose, CA 95110; Adobe Inc. San Jose, CA 95110; Adobe Inc. San Jose, CA 95110; Adobe Inc. San Jose, CA 95110; Department of Computer Science University of Central Florida Orlando, FL 32816; Adobe Inc. San Jose, CA 95110",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Khodadadeh_Latent_to_Latent_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6056077,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1982083452674969217&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "adobe.com;adobe.com;adobe.com;adobe.com;ucf.edu;adobe.com",
        "email": "adobe.com;adobe.com;adobe.com;adobe.com;ucf.edu;adobe.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "Adobe;University of Central Florida",
        "aff_unique_dep": "Adobe Inc.;Department of Computer Science",
        "aff_unique_url": "https://www.adobe.com;https://www.ucf.edu",
        "aff_unique_abbr": "Adobe;UCF",
        "aff_campus_unique_index": "0;0;0;0;1;0",
        "aff_campus_unique": "San Jose;Orlando",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "36ac7c4499",
        "title": "Leaky Gated Cross-Attention for Weakly Supervised Multi-Modal Temporal Action Localization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lee_Leaky_Gated_Cross-Attention_for_Weakly_Supervised_Multi-Modal_Temporal_Action_Localization_WACV_2022_paper.html",
        "author": "Jun-Tae Lee; Sungrack Yun; Mihir Jain",
        "abstract": "As multiple modalities sometimes have a weak complementary relationship, multi-modal fusion is not always beneficial for weakly supervised action localization. Hence, to attain the adaptive multi-modal fusion, we propose a leaky gated cross-attention mechanism. In our work, we take the multi-stage cross-attention as the baseline fusion module to obtain multi-modal features. Then, for the stages of each modality, we design gates to decide the dependency on the other modality. For each input frame, if two modalities have a strong complementary relationship, the gate selects the cross-attended feature, otherwise the non-attended feature. Also, the proposed gate allows the non-selected feature to escape through it with a small intensity, we call it leaky gate. This leaky feature makes effective regularization of the selected major feature. Therefore, our leaky gating makes cross-attention more adaptable and robust even when the modalities have a weak complementary relationship. The proposed leaky gated cross-attention provides a modality fusion module that is generally compatible with various temporal action localization methods. To show its effectiveness, we do extensive experimental analysis and apply the proposed method to boost the performance of the state-of-the-art methods on two benchmark datasets (ActivityNet1.2 and THUMOS14).",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lee_Leaky_Gated_Cross-Attention_for_Weakly_Supervised_Multi-Modal_Temporal_Action_Localization_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2744278,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10153068292500696302&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "684d7d10e4",
        "title": "Learnable Adaptive Cosine Estimator (LACE) for Image Classification",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Peeples_Learnable_Adaptive_Cosine_Estimator_LACE_for_Image_Classification_WACV_2022_paper.html",
        "author": "Joshua Peeples; Connor H. McCurley; Sarah Walker; Dylan Stewart; Alina Zare",
        "abstract": "In this work, we propose a new loss to improve feature discriminability and classification performance. Motivated by the adaptive cosine/coherence estimator (ACE), our proposed method incorporates angular information that is inherently learned by artificial neural networks. Our learnable ACE (LACE) transforms the data into a new \"whitened\" space that improves the inter-class separability and intra-class compactness. We compare our LACE to alternative state-of-the art softmax-based and feature regularization approaches. Our results show that the proposed method can serve as a viable alternative to cross entropy and angular softmax approaches. Our code is publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Peeples_Learnable_Adaptive_Cosine_Estimator_LACE_for_Image_Classification_WACV_2022_paper.pdf",
        "aff": "Department of Electrical & Computer Engineering, University of Florida; Department of Electrical & Computer Engineering, University of Florida; Department of Electrical & Computer Engineering, University of Florida; Department of Electrical & Computer Engineering, University of Florida; Department of Electrical & Computer Engineering, University of Florida",
        "project": "",
        "github": "https://github.com/GatorSense/LACE",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Peeples_Learnable_Adaptive_Cosine_WACV_2022_supplemental.pdf",
        "arxiv": "2110.05324",
        "pdf_size": 4660785,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15340462926883510386&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ufl.edu;ufl.edu;ufl.edu;ufl.edu;ufl.edu",
        "email": "ufl.edu;ufl.edu;ufl.edu;ufl.edu;ufl.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "Department of Electrical & Computer Engineering",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0c46ca0bea",
        "title": "Learnable Multi-Level Frequency Decomposition and Hierarchical Attention Mechanism for Generalized Face Presentation Attack Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Fang_Learnable_Multi-Level_Frequency_Decomposition_and_Hierarchical_Attention_Mechanism_for_Generalized_WACV_2022_paper.html",
        "author": "Meiling Fang; Naser Damer; Florian Kirchbuchner; Arjan Kuijper",
        "abstract": "With the increased deployment of face recognition systems in our daily lives, face presentation attack detection (PAD) is attracting much attention and playing a key role in securing face recognition systems. Despite the great performance achieved by the hand-crafted and deep-learning-based methods in intra-dataset evaluations, the performance drops when dealing with unseen scenarios. In this work, we propose a dual-stream convolution neural networks (CNNs) framework. One stream adapts four learnable frequency filters to learn features in the frequency domain, which are less influenced by variations in sensors/illuminations. The other stream leverages the RGB images to complement the features of the frequency domain. Moreover, we propose a hierarchical attention module integration to join the information from the two streams at different stages by considering the nature of deep features in different layers of the CNN. The proposed method is evaluated in the intra-dataset and cross-dataset setups, and the results demonstrate that our proposed approach enhances the generalizability in most experimental setups in comparison to state-of-the-art, including the methods designed explicitly for domain adaption/shift problems. We successfully prove the design of our proposed PAD solution in a step-wise ablation study that involves our proposed learnable frequency decomposition, our hierarchical attention module design, and the used loss function. Training codes and pre-trained models are publicly released.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Fang_Learnable_Multi-Level_Frequency_Decomposition_and_Hierarchical_Attention_Mechanism_for_Generalized_WACV_2022_paper.pdf",
        "aff": "Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany+Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany+Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany+Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany+Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany",
        "project": "",
        "github": "https://github.com/meilfang/LMFD-PAD",
        "supp": "",
        "arxiv": "2109.07950",
        "pdf_size": 4208158,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2555718315601090620&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "igd.fraunhofer.de; ; ;",
        "email": "igd.fraunhofer.de; ; ;",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Fraunhofer Institute for Computer Graphics Research IGD;Technische Universit\u00e4t Darmstadt",
        "aff_unique_dep": ";Mathematical and Applied Visual Computing",
        "aff_unique_url": "https://www.igd.fraunhofer.de;https://www.tu-darmstadt.de",
        "aff_unique_abbr": "IGD;TU Darmstadt",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Darmstadt",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "8bb3073fa7",
        "title": "Learned Event-Based Visual Perception for Improved Space Object Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Salvatore_Learned_Event-Based_Visual_Perception_for_Improved_Space_Object_Detection_WACV_2022_paper.html",
        "author": "Nikolaus Salvatore; Justin Fletcher",
        "abstract": "The detection of dim artificial Earth satellites using ground-based electro-optical sensors, particularly in the presence of background light, is technologically challenging. This perceptual task is foundational to our understanding of the space environment, and grows in importance as the number, variety, and dynamism of space objects increases. We present a hybrid image- and event-based architecture that leverages dynamic vision sensing technology to detect resident space objects in geosynchronous Earth orbit. Given the asynchronous, one-dimensional image data supplied by a dynamic vision sensor, our architecture applies conventional image feature extractors to integrated, two-dimensional frames in conjunction with point-cloud feature extractors, such as PointNet, in order to increase detection performance for dim objects in scenes with high background activity. In addition, an end-to-end event-based imaging simulator is developed to both produce data for model training as well as approximate the optimal sensor parameters for event-based sensing in the context of electro-optical telescope imagery. Experimental results confirm that the inclusion of point-cloud feature extractors increases recall for dim objects in the high-background regime.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Salvatore_Learned_Event-Based_Visual_Perception_for_Improved_Space_Object_Detection_WACV_2022_paper.pdf",
        "aff": "Pacific Defense Solutions; United States Space Force",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Salvatore_Learned_Event-Based_Visual_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2405422,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15342983495693069419&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "centauricorp.com;us.af.mil",
        "email": "centauricorp.com;us.af.mil",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Pacific Defense Solutions;United States Space Force",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.spaceforce.mil",
        "aff_unique_abbr": ";USSF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7bdbc40650",
        "title": "Learning Color Representations for Low-Light Image Enhancement",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kim_Learning_Color_Representations_for_Low-Light_Image_Enhancement_WACV_2022_paper.html",
        "author": "Bomi Kim; Sunhyeok Lee; Nahyun Kim; Donggon Jang; Dae-Shik Kim",
        "abstract": "Color conveys important information about the visible world. However, under low-light conditions, both pixel intensity, as well as true color distribution, can be significantly shifted. Moreover, most of such distortions are non-recoverable due to inverse problems. In the present study, we utilized recent advancements in learning-based methods for low-light image enhancement. However, while most \"deep learning\" methods aim to restore high-level and object-oriented visual information, we hypothesized that learning-based methods can also be used for restoring color-based information. To address this question, we propose a novel color representation learning method for low-light image enhancement. More specifically, we used a channel-aware residual network and a differentiable intensity histogram to capture color features. Experimental results using synthetic and natural datasets suggest that the proposed learning scheme achieves state-of-the-art performance. We conclude from our study that inter-channel dependency and color distribution matching are crucial factors for learning color representations under low-light conditions.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kim_Learning_Color_Representations_for_Low-Light_Image_Enhancement_WACV_2022_paper.pdf",
        "aff": "Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Kim_Learning_Color_Representations_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2284300,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=853370373005822507&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "4f9c5ba26b",
        "title": "Learning Foreground-Background Segmentation From Improved Layered GANs",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yang_Learning_Foreground-Background_Segmentation_From_Improved_Layered_GANs_WACV_2022_paper.html",
        "author": "Yu Yang; Hakan Bilen; Qiran Zou; Wing Yin Cheung; Xiangyang Ji",
        "abstract": "Deep learning approaches heavily rely on high-quality human supervision which is nonetheless expensive, time-consuming, and error-prone, especially for image segmentation task. In this paper, we propose a method to automatically synthesize paired photo-realistic images and segmentation masks for the use of training a foreground-background segmentation network. In particular, we learn a generative adversarial network that decomposes an image into foreground and background layers, and avoid trivial decompositions by maximizing mutual information between generated images and latent variables. The improved layered GANs can synthesize higher quality datasets from which segmentation networks of higher performance can be learned. Moreover, the segmentation networks are employed to stabilize the training of layered GANs in return, which are further alternately trained with Layered GANs. Experiments on a variety of single-object datasets show that our method achieves competitive generation quality and segmentation performance compared to related methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yang_Learning_Foreground-Background_Segmentation_From_Improved_Layered_GANs_WACV_2022_paper.pdf",
        "aff": "Tsinghua University, BNRist; The University of Edinburgh; Tsinghua University, BNRist; Tsinghua University, BNRist; Tsinghua University, BNRist",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yang_Learning_Foreground-Background_Segmentation_WACV_2022_supplemental.pdf",
        "arxiv": "2104.00483",
        "pdf_size": 2056535,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16630841641472241291&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mails.tsinghua.edu.cn;ed.ac.uk;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;ed.ac.uk;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Tsinghua University;University of Edinburgh",
        "aff_unique_dep": "BNRist;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.ed.ac.uk",
        "aff_unique_abbr": "THU;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "296ced5b44",
        "title": "Learning From the CNN-Based Compressed Domain",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wang_Learning_From_the_CNN-Based_Compressed_Domain_WACV_2022_paper.html",
        "author": "Zhenzhen Wang; Minghai Qin; Yen-Kuang Chen",
        "abstract": "Images are transmitted or stored in their compressed form and most of the AI tasks are performed from the reconstructed domain. Convolutional neural network (CNN)-based image compression and reconstruction is growing rapidly and it achieves or surpasses the state-of-the-art heuristic image compression methods, such as JPEG or BPG. A major limitation of the application of CNN-based image compression is on the computation complexity during compression and reconstruction. Therefore, learning from the compressed domain is desirable to avoid the computation and latency caused by reconstruction. In this paper, we show that learning from the compressed domain can achieve comparative or even better accuracy than from the reconstructed domain. At a high compression rate of 0.098 bpp, for example, the proposed compression-learning system has over 3% absolute accuracy boost over the traditional compression-reconstruction-learning flow. The improvement is achieved by optimizing the compression-learning system targeting original-sized instead of standardized (e.g., 224x224) images, which is crucial in practice since real-world images into the system have different sizes. We also propose an efficient model-free entropy estimation method and a criterion to learn from a selected subset of features in the compressed domain to further reduce the transmission and computation cost without accuracy degradation.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wang_Learning_From_the_CNN-Based_Compressed_Domain_WACV_2022_paper.pdf",
        "aff": "Computing Technology Lab, DAMO Academy1 + Nanyang Technological University2; Computing Technology Lab, DAMO Academy1; Computing Technology Lab, DAMO Academy1",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1080567,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6691216780524294892&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "e.ntu.edu.sg;gmail.com;ieee.org",
        "email": "e.ntu.edu.sg;gmail.com;ieee.org",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "DAMO Academy;Nanyang Technological University",
        "aff_unique_dep": "Computing Technology Lab;",
        "aff_unique_url": "https://www.damo.ac.cn;https://www.ntu.edu.sg",
        "aff_unique_abbr": "DAMO;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "bd25a88f55",
        "title": "Learning Maritime Obstacle Detection From Weak Annotations by Scaffolding",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zust_Learning_Maritime_Obstacle_Detection_From_Weak_Annotations_by_Scaffolding_WACV_2022_paper.html",
        "author": "Lojze \u017dust; Matej Kristan",
        "abstract": "Coastal water autonomous boats rely on robust perception methods for obstacle detection and timely collision avoidance. The current state-of-the-art is based on deep segmentation networks trained on large datasets. Per-pixel ground truth labeling of such datasets, however, is labor-intensive and expensive. We observe that far less information is required for practical obstacle avoidance -- the location of water edge on static obstacles like shore and approximate location and bounds of dynamic obstacles in the water is sufficient to plan a reaction. We propose a new scaffolding learning regime (SLR) that allows training obstacle detection segmentation networks only from such weak annotations, thus significantly reducing the cost of ground-truth labeling. Experiments show that maritime obstacle segmentation networks trained using SLR substantially outperform the same networks trained with dense ground truth labels, despite a significant reduction in labelling effort. Thus accuracy is not sacrificed for labelling simplicity but is in fact improved, which is a remarkable result.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zust_Learning_Maritime_Obstacle_Detection_From_Weak_Annotations_by_Scaffolding_WACV_2022_paper.pdf",
        "aff": "University of Ljubljana, Faculty of Computer and Information Science; University of Ljubljana, Faculty of Computer and Information Science",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3981391,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3674414446597496485&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "fri.uni-lj.si;fri.uni-lj.si",
        "email": "fri.uni-lj.si;fri.uni-lj.si",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Ljubljana",
        "aff_unique_dep": "Faculty of Computer and Information Science",
        "aff_unique_url": "https://www.fri.uni-lj.si",
        "aff_unique_abbr": "UL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Slovenia"
    },
    {
        "id": "d08cbfa1d7",
        "title": "Learning Temporal Video Procedure Segmentation From an Automatically Collected Large Dataset",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ji_Learning_Temporal_Video_Procedure_Segmentation_From_an_Automatically_Collected_Large_WACV_2022_paper.html",
        "author": "Lei Ji; Chenfei Wu; Daisy Zhou; Kun Yan; Edward Cui; Xilin Chen; Nan Duan",
        "abstract": "Temporal Video Segmentation (TVS) is a fundamental video understanding task and has been widely researched in recent years. There are two subtasks of TVS: Video Action Segmentation (VAS) and Video Procedure Segmentation (VPS): VAS aims to recognize what actions happen inside the video while VPS aims to segment the video into a sequence of video clips as a procedure. The VAS task inevitably relies on pre-defined action labels and is thus hard to scale to various open-domain videos. To overcome this limitation, the VPS task tries to divide a video into several category-independent procedure segments. However, the existing dataset for the VPS task is small (2k videos) and lacks diversity (only cooking domain). To tackle these problems, we collect a large and diverse dataset called TIPS, specifically for the VPS task. TIPS contains 63k videos including more than 300k procedure segments from instructional videos on YouTube, which covers plenty of how-to areas such as cooking, health, beauty, parenting, gardening, etc. We then propose a multi-modal Transformer with Gaussian Boundary Detection (MT-GBD) model for VPS, with the backbone of the Transformer and Convolution. Furthermore, we propose a new EIOU metric for the VPS task, which helps better evaluate VPS quality in a more comprehensive way. Experimental results show the effectiveness of our proposed model and metric.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ji_Learning_Temporal_Video_Procedure_Segmentation_From_an_Automatically_Collected_Large_WACV_2022_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1770280,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10409275857146029249&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b350b4ab8c",
        "title": "Learning To Generate the Unknowns as a Remedy to the Open-Set Domain Shift",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Baktashmotlagh_Learning_To_Generate_the_Unknowns_as_a_Remedy_to_the_WACV_2022_paper.html",
        "author": "Mahsa Baktashmotlagh; Tianle Chen; Mathieu Salzmann",
        "abstract": "In many situations, the data one has access to at test time follows a different distribution from the training data. Over the years, this problem has been tackled by closed-set domain adaptation techniques. Recently, open-set domain adaptation has emerged to address the more realistic scenario where additional unknown classes are present in the target data. In this setting, existing techniques focus on the challenging task of isolating the unknown target samples, so as to avoid the negative transfer resulting from aligning the source feature distributions with the broader target one that encompasses the additional unknown classes. Here, we propose a simpler and more effective solution consisting of complementing the source data distribution and making it comparable to the target one by enabling the model to generate source samples corresponding to the unknown target classes. We formulate this as a general module that can be incorporated into any existing closed-set approach and show that this strategy allows us to outperform the state-of-the-art on open-set domain adaptation benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Baktashmotlagh_Learning_To_Generate_the_Unknowns_as_a_Remedy_to_the_WACV_2022_paper.pdf",
        "aff": "University of Queensland; University of Queensland; EPFL",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5795025,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8257996704453084274&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uq.edu.au;uq.edu.au;epfl.ch",
        "email": "uq.edu.au;uq.edu.au;epfl.ch",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Queensland;EPFL",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uq.edu.au;https://www.epfl.ch",
        "aff_unique_abbr": "UQ;EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Australia;Switzerland"
    },
    {
        "id": "bf21c74b13",
        "title": "Learning To Reconstruct 3D Non-Cuboid Room Layout From a Single RGB Image",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yang_Learning_To_Reconstruct_3D_Non-Cuboid_Room_Layout_From_a_Single_WACV_2022_paper.html",
        "author": "Cheng Yang; Jia Zheng; Xili Dai; Rui Tang; Yi Ma; Xiaojun Yuan",
        "abstract": "Single-image room layout reconstruction aims to reconstruct the enclosed 3D structure of a room from a single image. Most previous work relies on the cuboid shape prior. This paper considers a more general indoor assumption, i.e., the room layout consists of a single ceiling, a single floor, and several vertical walls. To this end, we first employ Convolutional Neural Networks to detect planes and vertical lines between adjacent walls. Meanwhile, estimating the 3D parameters for each plane. Then, a simple yet effective geometric reasoning method is adopted to achieve room layout reconstruction. Furthermore, we optimize the 3D plane parameters to reconstruct a geometrically consistent room layout between planes and lines. The experimental results on public datasets validate the effectiveness and efficiency of our method.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yang_Learning_To_Reconstruct_3D_Non-Cuboid_Room_Layout_From_a_Single_WACV_2022_paper.pdf",
        "aff": "University of Electronic Science and Technology of China+Hikvision Research Institute; Manycore Tech Inc. (Kujiale); University of Electronic Science and Technology of China+University of California, Berkeley; Manycore Tech Inc. (Kujiale); University of California, Berkeley; University of Electronic Science and Technology of China",
        "project": "",
        "github": "https://github.com/CYang0515/NonCuboidRoom",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yang_Learning_To_Reconstruct_WACV_2022_supplemental.pdf",
        "arxiv": "2104.07986",
        "pdf_size": 5072568,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15728976535412702006&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+3;2;3;0",
        "aff_unique_norm": "University of Electronic Science and Technology of China;Hikvision Research Institute;Manycore Tech Inc.;University of California, Berkeley",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.uestc.edu.cn;https://www.hikvision.com/cn/;;https://www.berkeley.edu",
        "aff_unique_abbr": "UESTC;Hikvision;MTI;UC Berkeley",
        "aff_campus_unique_index": ";1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0+0;0;0+1;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "e075bd5b1c",
        "title": "Learning With Label Noise for Image Retrieval by Selecting Interactions",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ibrahimi_Learning_With_Label_Noise_for_Image_Retrieval_by_Selecting_Interactions_WACV_2022_paper.html",
        "author": "Sarah Ibrahimi; Arnaud Sors; Rafael Sampaio de Rezende; St\u00e9phane Clinchant",
        "abstract": "Learning with noisy labels is an active research area for image classification. However, the effect of noisy labels on image retrieval has been less studied. In this work, we propose a noise-resistant method for image retrieval named Teacher-based Selection of Interactions, T-SINT, which identifies noisy interactions, i.e. elements in the distance matrix, and selects correct positive and negative interactions to be considered in the retrieval loss by using a teacher-based training setup which contributes to the stability. As a result, it consistently outperforms state-of-the-art methods on high noise rates across benchmark datasets with synthetic noise and more realistic noise.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ibrahimi_Learning_With_Label_Noise_for_Image_Retrieval_by_Selecting_Interactions_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ibrahimi_Learning_With_Label_WACV_2022_supplemental.pdf",
        "arxiv": "2112.10453",
        "pdf_size": 3155155,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10306310381625881038&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a4b094d611",
        "title": "Learning to Weight Filter Groups for Robust Classification",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yuan_Learning_to_Weight_Filter_Groups_for_Robust_Classification_WACV_2022_paper.html",
        "author": "Siyang Yuan; Yitong Li; Dong Wang; Ke Bai; Lawrence Carin; David Carlson",
        "abstract": "In many real-world tasks, a canonical \"big data\" problem is created by combining data from several individual groups or domains. Because test data will likely come from a new group of data, we want to utilize the grouped structure of our training data to enforce generalization between groups of data, not just individual samples. This can be viewed as a multiple-domain generalization problem. Specifically, the goal is to encourage generalization between previously seen labeled source data from multiple domains and unlabeled target domain data. To address this challenge, we introduce Domain-Specific Filter Group (DSFG), where each training domain has a unique filter group and each test data point is predicted by a weighted sum over the outputs of different domain filters. A separate neural network learns to estimate the appropriate filter group weights through a meta-learning strategy. Empirically, experiments on three benchmark datasets demonstrate improved performance compared to current state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yuan_Learning_to_Weight_Filter_Groups_for_Robust_Classification_WACV_2022_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yuan_Learning_to_Weight_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 880058,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13097905743626404486&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "82651c0d9a",
        "title": "Less Can Be More: Sound Source Localization With a Classification Model",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Senocak_Less_Can_Be_More_Sound_Source_Localization_With_a_Classification_WACV_2022_paper.html",
        "author": "Arda Senocak; Hyeonggon Ryu; Junsik Kim; In So Kweon",
        "abstract": "In this paper, we tackle sound localization as a natural outcome of the audio-visual video classification problem. Differently from the existing sound localization approaches, we do not use any explicit sub-modules or training mechanisms but use simple cross-modal attention on top of the representations learned by a classification loss. Our key contribution is to show that a simple audio-visual classification model has the ability to localize sound sources accurately and to give on par performance with state-of-the-art methods by proving that indeed \"less is more\". Furthermore, we propose potential applications that can be built based on our model. First, we introduce informative moment selection to enhance the localization task learning in the existing approaches compare to mid-frame usage. Then, we introduce a pseudo bounding box generation procedure that can significantly boost the performance of the existing methods in semi-supervised settings or be used for large-scale automatic annotation with minimal effort from any video dataset.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Senocak_Less_Can_Be_More_Sound_Source_Localization_With_a_Classification_WACV_2022_paper.pdf",
        "aff": "KAIST; KAIST; KAIST+Harvard University; KAIST",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Senocak_Less_Can_Be_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 10272543,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4561198879533109934&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;harvard.edu;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;harvard.edu;kaist.ac.kr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Harvard University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.harvard.edu",
        "aff_unique_abbr": "KAIST;Harvard",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "39bec08488",
        "title": "Let There Be a Clock on the Beach: Reducing Object Hallucination in Image Captioning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Biten_Let_There_Be_a_Clock_on_the_Beach_Reducing_Object_WACV_2022_paper.html",
        "author": "Ali Furkan Biten; Llu\u00eds G\u00f3mez; Dimosthenis Karatzas",
        "abstract": "Explaining an image with missing or non-existent objects is known as object bias (hallucination) in image captioning. This behaviour is quite common in the state-of-the-art captioning models which is not desirable by humans. To decrease the object hallucination in captioning, we propose three simple yet efficient training augmentation method for sentences which requires no new training data or increase in the model size. By extensive analysis, we show that the proposed methods can significantly diminish our models' object bias on hallucination metrics. Moreover, we experimentally demonstrate that our methods decrease the dependency on the visual features. All of our code, configuration files and model weights is available at https://github.com/furkanbiten/object-bias.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Biten_Let_There_Be_a_Clock_on_the_Beach_Reducing_Object_WACV_2022_paper.pdf",
        "aff": "Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain; Computer Vision Center, UAB, Spain",
        "project": "",
        "github": "https://github.com/furkanbiten/object-bias",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Biten_Let_There_Be_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1298334,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4189949903004613817&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cvc.uab.es;cvc.uab.es;cvc.uab.es",
        "email": "cvc.uab.es;cvc.uab.es;cvc.uab.es",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universitat Aut\u00f2noma de Barcelona",
        "aff_unique_dep": "Computer Vision Center",
        "aff_unique_url": "https://www.uab.cat",
        "aff_unique_abbr": "UAB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "f186525d8a",
        "title": "Leveraging Test-Time Consensus Prediction for Robustness Against Unseen Noise",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sarkar_Leveraging_Test-Time_Consensus_Prediction_for_Robustness_Against_Unseen_Noise_WACV_2022_paper.html",
        "author": "Anindya Sarkar; Anirban Sarkar; Vineeth N Balasubramanian",
        "abstract": "We propose a method to improve DNN robustness against unseen noisy corruptions, such as Gaussian noise, Shot Noise, Impulse Noise, Speckle noise with different levels of severity by leveraging ensemble technique through a consensus based prediction method using self-supervised learning at inference time. We also propose to enhance the model training by considering other aspects of the issue i.e. noise in data and better representation learning which shows even better generalization performance with the consensus based prediction strategy. We report results of each noisy corruption on the standard CIFAR10-C and ImageNet-C benchmark which shows significant boost in performance over previous methods. We also introduce results for MNIST-C and TinyImagenet-C to show usefulness of our method across datasets of different complexities to provide robustness against unseen noise. We show results with different architectures to validate our method against other baseline methods, and also conduct experiments to show the usefulness of each part of our method.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sarkar_Leveraging_Test-Time_Consensus_Prediction_for_Robustness_Against_Unseen_Noise_WACV_2022_paper.pdf",
        "aff": "Indian Institute of Technology, Hyderabad; Indian Institute of Technology, Hyderabad; Indian Institute of Technology, Hyderabad",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Sarkar_Leveraging_Test-Time_Consensus_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 7213941,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3457983965127110405&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;iith.ac.in;iith.ac.in",
        "email": "gmail.com;iith.ac.in;iith.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Hyderabad",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iith.ac.in",
        "aff_unique_abbr": "IIT Hyderabad",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "f3f150f648",
        "title": "Lightweight Monocular Depth With a Novel Neural Architecture Search Method",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Huynh_Lightweight_Monocular_Depth_With_a_Novel_Neural_Architecture_Search_Method_WACV_2022_paper.html",
        "author": "Lam Huynh; Phong Nguyen; Ji\u0159\u00ed Matas; Esa Rahtu; Janne Heikkil\u00e4",
        "abstract": "This paper presents a novel neural architecture search method, called LiDNAS, for generating lightweight monocular depth estimation models. Unlike previous neural architecture search (NAS) approaches, where finding optimized networks is computationally highly demanding, the introduced novel Assisted Tabu Search leads to efficient architecture exploration. Moreover, we construct the search space on a pre-defined backbone network to balance layer diversity and search space size. The LiDNAS method outperforms the state-of-the-art NAS approach, proposed for disparity and depth estimation, in terms of search efficiency and output model performance. The LiDNAS optimized models achieve result superior to compact depth estimation state-of-the-art on NYU-Depth-v2, KITTI, and ScanNet, while being 7%-500% more compact in size, i.e the number of model parameters.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Huynh_Lightweight_Monocular_Depth_With_a_Novel_Neural_Architecture_Search_Method_WACV_2022_paper.pdf",
        "aff": "University of Oulu; Czech Technical University in Prague; Tampere University; University of Oulu; University of Oulu",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Huynh_Lightweight_Monocular_Depth_WACV_2022_supplemental.zip",
        "arxiv": "2108.11105",
        "pdf_size": 8563291,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2023429487211652849&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "University of Oulu;Czech Technical University;Tampere University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.oulu.fi;https://www.ctu.cz;https://www.tuni.fi",
        "aff_unique_abbr": "UOulu;CTU;Tuni",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Prague",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "Finland;Czech Republic"
    },
    {
        "id": "5f04be0213",
        "title": "Low-Cost Multispectral Scene Analysis With Modality Distillation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zhang_Low-Cost_Multispectral_Scene_Analysis_With_Modality_Distillation_WACV_2022_paper.html",
        "author": "Heng Zhang; Elisa Fromont; S\u00e9bastien Lef\u00e8vre; Bruno Avignon",
        "abstract": "Despite its robust performance under various illumination conditions, multispectral scene analysis has not been widely deployed due to two strong practical limitations: 1) thermal cameras, especially high-resolution ones are much more expensive than conventional visible cameras; 2) the most commonly adopted multispectral architectures, two-stream neural networks, nearly double the inference time of a regular mono-spectral model which makes them impractical in embedded environments. In this work, we aim to tackle these two limitations by proposing a novel knowledge distillation framework named Modality Distillation (MD). The proposed framework distils the knowledge from a high thermal resolution two-stream network with feature-level fusion to a low thermal resolution one-stream network with image-level fusion. We show on different multispectral scene analysis benchmarks that our method can effectively allow the use of low-resolution thermal sensors with more compact one-stream networks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zhang_Low-Cost_Multispectral_Scene_Analysis_With_Modality_Distillation_WACV_2022_paper.pdf",
        "aff": "Univ Rennes 1, IRISA, France; Univ Rennes 1, IRISA, France; Univ Bretagne Sud, IRISA, France; ATERMES company, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7645774,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6286969066729436126&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "irisa.fr;irisa.fr;irisa.fr;atermes.fr",
        "email": "irisa.fr;irisa.fr;irisa.fr;atermes.fr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University Rennes 1;University of Bretagne Sud;ATERMES",
        "aff_unique_dep": "IRISA;IRISA;",
        "aff_unique_url": "https://www.univ-rennes1.fr;https://www.univ-ubs.fr;",
        "aff_unique_abbr": "Univ Rennes 1;UBS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "b53221f7eb",
        "title": "LwPosr: Lightweight Efficient Fine Grained Head Pose Estimation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Dhingra_LwPosr_Lightweight_Efficient_Fine_Grained_Head_Pose_Estimation_WACV_2022_paper.html",
        "author": "Naina Dhingra",
        "abstract": "This paper presents a lightweight network for head pose estimation (HPE) task. While previous approaches rely on convolutional neural networks, the proposed network LwPosr uses mixture of depthwise separable convolutional (DSC) and transformer encoder layers which are structured in two streams and three stages to provide fine-grained regression for predicting head poses. The quantitative and qualitative demonstration is provided to show that the proposed network is able to learn head poses efficiently while using less parameter space. Extensive ablations are conducted using three open-source datasets namely 300W-LP, AFLW2000, and BIWI datasets. To our knowledge, (1) LwPosr is the lightest network proposed for estimating head poses compared to both keypoints-based and keypoints-free approaches; (2) it sets a benchmark for both overperforming the previous lightweight network on mean absolute error and on reducing number of parameters; (3) it is first of its kind to use mixture of DSCs and transformer encoders for HPE. This approach is suitable for mobile devices which require lightweight networks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Dhingra_LwPosr_Lightweight_Efficient_Fine_Grained_Head_Pose_Estimation_WACV_2022_paper.pdf",
        "aff": "ETH Zurich",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Dhingra_LwPosr_Lightweight_Efficient_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2384814,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10953798480945601248&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ethz.ch",
        "email": "ethz.ch",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "d17cf14a26",
        "title": "M3DETR: Multi-Representation, Multi-Scale, Mutual-Relation 3D Object Detection With Transformers",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Guan_M3DETR_Multi-Representation_Multi-Scale_Mutual-Relation_3D_Object_Detection_With_Transformers_WACV_2022_paper.html",
        "author": "Tianrui Guan; Jun Wang; Shiyi Lan; Rohan Chandra; Zuxuan Wu; Larry Davis; Dinesh Manocha",
        "abstract": "We present a novel architecture for 3D object detection, M3DETR, which combines different point cloud representations (raw, voxels, bird-eye view) with different feature scales based on multi-scale feature pyramids. M3DETR is the first approach that unifies multiple point cloud representations, feature scales, as well as models mutual relationships between point clouds simultaneously using transformers. We perform extensive ablation experiments that highlight the benefits of fusing representation and scale, and modeling the relationships. Our method achieves state-of-the-art performance on the KITTI 3D object detection dataset and Waymo Open Dataset. Results show that M3DETR improves the baseline significantly by 1.48% mAP for all classes on Waymo Open Dataset. In particular, our approach ranks 1st on the well-known KITTI 3D Detection Benchmark for both car and cyclist classes, and ranks 1st on Waymo Open Dataset with single frame point cloud input. Our code is available at: https://github.com/rayguan97/M3DETR.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Guan_M3DETR_Multi-Representation_Multi-Scale_Mutual-Relation_3D_Object_Detection_With_Transformers_WACV_2022_paper.pdf",
        "aff": "University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park+University of Maryland, College Park; University of Maryland, College Park; Fudan University; University of Maryland, College Park; University of Maryland, College Park",
        "project": "",
        "github": "https://github.com/rayguan97/M3DETR",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Guan_M3DETR_Multi-Representation_Multi-Scale_WACV_2022_supplemental.pdf",
        "arxiv": "2104.11896",
        "pdf_size": 4747223,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5625187739363736470&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.umd.edu;umiacs.umd.edu;cs.umd.edu;cs.umd.edu;fudan.edu.cn;umiacs.umd.edu;umd.edu",
        "email": "cs.umd.edu;umiacs.umd.edu;cs.umd.edu;cs.umd.edu;fudan.edu.cn;umiacs.umd.edu;umd.edu",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+0;0;1;0;0",
        "aff_unique_norm": "University of Maryland;Fudan University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://www.fudan.edu.cn",
        "aff_unique_abbr": "UMD;Fudan",
        "aff_campus_unique_index": "0;0;0+0;0;0;0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0;0;0+0;0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "56be2738da",
        "title": "MAPS: Multimodal Attention for Product Similarity",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Das_MAPS_Multimodal_Attention_for_Product_Similarity_WACV_2022_paper.html",
        "author": "Nilotpal Das; Aniket Joshi; Promod Yenigalla; Gourav Agrwal",
        "abstract": "Learning to identify similar products in the e-commerce domain has widespread applications such as ensuring consistent grouping of the products in the catalog, avoiding duplicates in the search results, etc. Here, we address the problem of learning product similarity for highly challenging real-world data from the Amazon catalog. We define it as a metric learning problem, where similar products are projected close to each other and dissimilar ones are projected further apart. To this end, we propose a scalable end-to-end multimodal framework for product representation learning in a weakly supervised setting using raw data from the catalog. This includes product images as well as textual attributes like product title and category information. The model uses the image as the primary source of information, while the title helps the model focus on relevant regions in the image by ignoring the background clutter. To validate our approach, we created multimodal datasets covering three broad product categories, where we achieve up to 10% improvement in precision compared to state-of-the-art multimodal benchmark. Along with this, we also incorporate several effective heuristics for training data generation, which further complements the overall training. Additionally, we demonstrate that incorporating the product title makes the model scale effectively across multiple product categories.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Das_MAPS_Multimodal_Attention_for_Product_Similarity_WACV_2022_paper.pdf",
        "aff": "Retail Business Services; Amazon; Amazon; Amazon",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3034252,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9294592122446056878&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Retail Business Services;Amazon",
        "aff_unique_dep": ";Amazon.com, Inc.",
        "aff_unique_url": ";https://www.amazon.com",
        "aff_unique_abbr": ";Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "072f1aa8c2",
        "title": "MEGAN: Memory Enhanced Graph Attention Network for Space-Time Video Super-Resolution",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/You_MEGAN_Memory_Enhanced_Graph_Attention_Network_for_Space-Time_Video_Super-Resolution_WACV_2022_paper.html",
        "author": "Chenyu You; Lianyi Han; Aosong Feng; Ruihan Zhao; Hui Tang; Wei Fan",
        "abstract": "Space-time video super-resolution (STVSR) aims to construct a high space-time resolution video sequence from the corresponding low-frame-rate, low-resolution video sequence. Inspired by the recent success to consider spatial-temporal information for space-time super-resolution, our main goal in this work is to take full considerations of spatial and temporal correlations within the video sequences of fast dynamic events. To this end, we propose a novel one-stage memory enhanced graph attention network (MEGAN) for space-time video super-resolution. Specifically, we build a novel long-range memory graph aggregation (LMGA) module to dynamically capture correlations along the channel dimensions of the feature maps and adaptively aggregate channel features to enhance the feature representations. We introduce a non-local residual block, which enables each channel-wise feature to attend global spatial hierarchical features. In addition, we adopt a progressive fusion module to further enhance the representation ability by extensively exploiting spatio-temporal correlations from multiple frames. Experiment results demonstrate that our method achieves better results compared with the state-of-the-art methods quantitatively and visually.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/You_MEGAN_Memory_Enhanced_Graph_Attention_Network_for_Space-Time_Video_Super-Resolution_WACV_2022_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/You_MEGAN_Memory_Enhanced_WACV_2022_supplemental.pdf",
        "arxiv": "2110.15327",
        "pdf_size": 1808602,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10648283550156740137&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9486376743",
        "title": "METGAN: Generative Tumour Inpainting and Modality Synthesis in Light Sheet Microscopy",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Horvath_METGAN_Generative_Tumour_Inpainting_and_Modality_Synthesis_in_Light_Sheet_WACV_2022_paper.html",
        "author": "Izabela Horvath; Johannes Paetzold; Oliver Schoppe; Rami Al-Maskari; Ivan Ezhov; Suprosanna Shit; Hongwei Li; Ali Ert\u00fcrk; Bjoern Menze",
        "abstract": "Novel multimodal imaging methods are capable of generating extensive, super high resolution datasets for preclinical research. Yet, a massive lack of annotations prevents the broad use of deep learning to analyze such data. In this paper, we introduce a novel generative method which leverages real anatomical information to generate realistic image-label pairs of tumours. We construct a dual pathway generator, for the anatomical image and label, trained in a cycle-consistent setup, constrained by an independent, pretrained segmentor. Our method performs two concurrent tasks: domain adaptation and semantic synthesis, which, to our knowledge, has not been done before. The generated images yield significant quantitative improvement compared to existing methods that specialize in either of these tasks. To validate the quality of synthesis, we train segmentation networks on a dataset augmented with the synthetic data, substantially improving the segmentation over the baseline.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Horvath_METGAN_Generative_Tumour_Inpainting_and_Modality_Synthesis_in_Light_Sheet_WACV_2022_paper.pdf",
        "aff": "1+2; 1+2; 1+2; 1+2; 2+3; 2+3; 2+4; 1+5; 4",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Horvath_METGAN_Generative_Tumour_WACV_2022_supplemental.pdf",
        "arxiv": "2104.10993",
        "pdf_size": 7658490,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18351057609625304179&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tum.de;tum.de;tum.de;tum.de;tum.de;tum.de;tum.de;helmholtz-muenchen.de;uzh.ch",
        "email": "tum.de;tum.de;tum.de;tum.de;tum.de;tum.de;tum.de;helmholtz-muenchen.de;uzh.ch",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": ";;;;;;;",
        "aff_unique_norm": "",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": ";;;;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";;;;;;;",
        "aff_country_unique": ""
    },
    {
        "id": "b00e27b0ec",
        "title": "MM-ViT: Multi-Modal Video Transformer for Compressed Video Action Recognition",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chen_MM-ViT_Multi-Modal_Video_Transformer_for_Compressed_Video_Action_Recognition_WACV_2022_paper.html",
        "author": "Jiawei Chen; Chiu Man Ho",
        "abstract": "This paper presents a pure transformer-based approach, dubbed the Multi-Modal Video Transformer (MM-ViT), for video action recognition. Different from other schemes which solely utilize the decoded RGB frames, MM-ViT operates exclusively in the compressed video domain and exploits all readily available modalities, i.e., I-frames, motion vectors, residuals and audio waveform. In order to handle the large number of spatiotemporal tokens extracted from multiple modalities, we develop several scalable model variants which factorize self-attention across the space, time and modality dimensions. In addition, to further explore the rich inter-modal interactions and their effects, we develop and compare three distinct cross-modal attention mechanisms that can be seamlessly integrated into the transformer building block. Extensive experiments on three public action recognition benchmarks (UCF-101,Something-Something-v2, Kinetics-600) demonstrate that MM-ViT outperforms the state-of-the-art video transformers in both efficiency and accuracy, and performs better or equally well to the state-of-the-art CNN counterparts with computationally-heavy optical flow",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chen_MM-ViT_Multi-Modal_Video_Transformer_for_Compressed_Video_Action_Recognition_WACV_2022_paper.pdf",
        "aff": "OPPO US Research Center; OPPO US Research Center",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1061472,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16726000344747584349&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "oppo.com;oppo.com",
        "email": "oppo.com;oppo.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "OPPO",
        "aff_unique_dep": "US Research Center",
        "aff_unique_url": "https://www.oppo.com",
        "aff_unique_abbr": "OPPO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4eb4f13561",
        "title": "MTGLS: Multi-Task Gaze Estimation With Limited Supervision",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ghosh_MTGLS_Multi-Task_Gaze_Estimation_With_Limited_Supervision_WACV_2022_paper.html",
        "author": "Shreya Ghosh; Munawar Hayat; Abhinav Dhall; Jarrod Knibbe",
        "abstract": "Robust gaze estimation is a challenging task, even for deep CNNs, due to the non-availability of large-scale labeled data. Moreover, gaze annotation is a time-consuming process and requires specialized hardware setups. We propose MTGLS: a Multi-Task Gaze estimation framework with Limited Supervision, which leverages abundantly available non-annotated facial image data. MTGLS distills knowledge from off-the-shelf facial image analysis models, and learns strong feature representations of human eyes, guided by three complementary auxiliary signals: (a) the line of sight of the pupil (i.e. pseudo-gaze) defined by the localized facial landmarks, (b) the head-pose given by Euler angles, and (c) the orientation of the eye patch (left/right eye). To overcome inherent noise in the supervisory signals, MTGLS further incorporates a noise distribution modelling approach. Our experimental results show that MTGLS learns highly generalized representations which consistently perform well on a range of datasets. Our proposed framework outperforms the unsupervised state-of-the-art on CAVE (by approx. 6.43%) and even supervised state-of-the-art methods on Gaze360 (by approx. 6.59%) datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ghosh_MTGLS_Multi-Task_Gaze_Estimation_With_Limited_Supervision_WACV_2022_paper.pdf",
        "aff": "Monash University; Monash University; Monash University + Indian Institute of Technology Ropar; University of Melbourne",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.12100",
        "pdf_size": 7469565,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11007353235294686722&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "monash.edu;monash.edu;monash.edu;unimelb.edu.au",
        "email": "monash.edu;monash.edu;monash.edu;unimelb.edu.au",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;2",
        "aff_unique_norm": "Monash University;Indian Institute of Technology Ropar;University of Melbourne",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.monash.edu;https://www.iitrpr.ac.in;https://www.unimelb.edu.au",
        "aff_unique_abbr": "Monash;IIT Ropar;UniMelb",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Ropar",
        "aff_country_unique_index": "0;0;0+1;0",
        "aff_country_unique": "Australia;India"
    },
    {
        "id": "71da75931c",
        "title": "MUGL: Large Scale Multi Person Conditional Action Generation With Locomotion",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Maheshwari_MUGL_Large_Scale_Multi_Person_Conditional_Action_Generation_With_Locomotion_WACV_2022_paper.html",
        "author": "Shubh Maheshwari; Debtanu Gupta; Ravi Kiran Sarvadevabhatla",
        "abstract": "We introduce MUGL, a novel deep neural model for large-scale, diverse generation of single and multi-person pose-based action sequences with locomotion. Our controllable approach enables variable-length generations customizable by action category, across more than 100 categories. To enable intra/inter-category diversity, we model the latent generative space using a Conditional Gaussian Mixture Variational Autoencoder. To enable realistic generation of actions involving locomotion, we decouple local pose and global trajectory components of the action sequence. We incorporate duration-aware feature representations to enable variable-length sequence generation. We use a hybrid pose sequence representation with 3D pose sequences sourced from videos and 3D Kinect-based sequences of NTU-RGBD-120. To enable principled comparison of generation quality, we employ suitably modified strong baselines during evaluation. Although smaller and simpler compared to baselines, MUGL provides better quality generations, paving the way for practical and controllable large-scale human action generation.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Maheshwari_MUGL_Large_Scale_Multi_Person_Conditional_Action_Generation_With_Locomotion_WACV_2022_paper.pdf",
        "aff": "Centre for Visual Information Technology; Centre for Visual Information Technology; Centre for Visual Information Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.11460",
        "pdf_size": 3760170,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17858532303847362245&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;research.iiit.ac.in;iiit.ac.in",
        "email": "gmail.com;research.iiit.ac.in;iiit.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Centre for Visual Information Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "de921a3087",
        "title": "MaskSplit: Self-Supervised Meta-Learning for Few-Shot Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Amac_MaskSplit_Self-Supervised_Meta-Learning_for_Few-Shot_Semantic_Segmentation_WACV_2022_paper.html",
        "author": "Mustafa Sercan Amac; Ahmet Sencan; Bugra Baran; Nazli Ikizler-Cinbis; Ramazan Gokberk Cinbis",
        "abstract": "Just like other few-shot learning problems, few-shot segmentation aims to minimize the need for manual annotation, which is particularly costly in segmentation tasks. Even though the few-shot setting reduces this cost for novel test classes, there is still a need to annotate the training data. To alleviate this need, we propose a self-supervised training approach for learning few-shot segmentation models. We first use unsupervised saliency estimation to obtain pseudo-masks on images. We then train a simple prototype based model over different splits of pseudo masks and augmentations of images. Our extensive experiments show that the proposed approach achieves promising results, highlighting the potential of self-supervised training. To the best of our knowledge this is the first work that addresses unsupervised few-shot segmentation problem on natural images.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Amac_MaskSplit_Self-Supervised_Meta-Learning_for_Few-Shot_Semantic_Segmentation_WACV_2022_paper.pdf",
        "aff": "MonolithAI; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey; Department of Computer Engineering, Hacettepe University, Ankara, Turkey; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.12207",
        "pdf_size": 7219287,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2955176363832314115&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com;metu.edu.tr;metu.edu.tr;cs.hacettepe.edu.tr;metu.edu.tr",
        "email": "gmail.com;metu.edu.tr;metu.edu.tr;cs.hacettepe.edu.tr;metu.edu.tr",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "MonolithAI;Middle East Technical University;Hacettepe University",
        "aff_unique_dep": ";Department of Computer Engineering;Department of Computer Engineering",
        "aff_unique_url": ";https://www.metu.edu.tr;https://www.hacettepe.edu.tr",
        "aff_unique_abbr": "MonolithAI;METU;Hacettepe",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Ankara",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "United States;T\u00fcrkiye"
    },
    {
        "id": "3f7e7295b8",
        "title": "Masking Modalities for Cross-Modal Video Retrieval",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gabeur_Masking_Modalities_for_Cross-Modal_Video_Retrieval_WACV_2022_paper.html",
        "author": "Valentin Gabeur; Arsha Nagrani; Chen Sun; Karteek Alahari; Cordelia Schmid",
        "abstract": "Pre-training on large scale unlabelled datasets has shown impressive performance improvements in the fields of computer vision and natural language processing. Given the advent of large-scale instructional video datasets, a common strategy for pre-training video encoders is to use the accompanying speech as weak supervision. However, as speech is used to supervise the pre-training, it is never seen by the video encoder, which does not learn to process that modality. We address this drawback of current pre-training methods, which fail to exploit the rich cues in spoken language. Our proposal is to pre-train a video encoder using all the available video modalities as supervision, namely, appearance, sound, and transcribed speech. We mask an entire modality in the input and predict it using the other two modalities. This encourages each modality to collaborate with the others, and our video encoder learns to process appearance and audio as well as speech. We show the superior performance of our modality masking pre-training approach for video retrieval on the How2R, YouCook2 and Condensed Movies datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gabeur_Masking_Modalities_for_Cross-Modal_Video_Retrieval_WACV_2022_paper.pdf",
        "aff": "Inria\u2217; Google Research; Google Research; Inria\u2217; Google Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2111.01300",
        "pdf_size": 7909782,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3322423660357518162&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "INRIA;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.inria.fr;https://research.google",
        "aff_unique_abbr": "Inria;Google Research",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;1;0;1",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "d676383a83",
        "title": "Matching and Recovering 3D People From Multiple Views",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Perez-Yus_Matching_and_Recovering_3D_People_From_Multiple_Views_WACV_2022_paper.html",
        "author": "Alejandro Perez-Yus; Antonio Agudo",
        "abstract": "This paper introduces an approach to simultaneously match and recover 3D people from multiple calibrated cameras. To this end, we present an affinity measure between 2D detections across different views that enforces an uncertainty geometric consistency. This similarity is then exploited by a novel multi-view matching algorithm to cluster the detections, being robust against partial observations as well as bad detections and without assuming any prior about the number of people in the scene. After that, the multi-view correspondences are used in order to efficiently infer the 3D pose of each body by means of a 3D pictorial structure model in combination with physico-geometric constraints. Our algorithm is thoroughly evaluated on challenging scenarios where several human bodies are performing different activities which involve complex motions, producing large occlusions in some views and noisy observations. We outperform state-of-the-art results in terms of matching and 3D reconstruction.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Perez-Yus_Matching_and_Recovering_3D_People_From_Multiple_Views_WACV_2022_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Perez-Yus_Matching_and_Recovering_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 4763130,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11155291376412370263&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "31ae34e45d",
        "title": "Maximizing Cosine Similarity Between Spatial Features for Unsupervised Domain Adaptation in Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chung_Maximizing_Cosine_Similarity_Between_Spatial_Features_for_Unsupervised_Domain_Adaptation_WACV_2022_paper.html",
        "author": "Inseop Chung; Daesik Kim; Nojun Kwak",
        "abstract": "We propose a novel method that tackles the problem of unsupervised domain adaptation for semantic segmentation by maximizing the cosine similarity between the source and the target domain at the feature level. A segmentation network mainly consists of two parts, a feature extractor and a classification head. We expect that if we can make the two domains have small domain gap at the feature level, they would also have small domain discrepancy at the classification head. Our method computes a cosine similarity matrix between the source feature map and the target feature map, then we maximize the elements exceeding a threshold to guide the target features to have high similarity with the most similar source feature. Moreover, we use a class-wise source feature dictionary which stores the latest features of the source domain to prevent the unmatching problem when computing the cosine similarity matrix and be able to compare a target feature with various source features from various images. Through extensive experiments, we verify that our method gains performance on two unsupervised domain adaptation tasks (GTA5->Cityscaspes and SYNTHIA->Cityscapes).",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chung_Maximizing_Cosine_Similarity_Between_Spatial_Features_for_Unsupervised_Domain_Adaptation_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Chung_Maximizing_Cosine_Similarity_WACV_2022_supplemental.pdf",
        "arxiv": "2102.13002",
        "pdf_size": 2341303,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5356746603005698913&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "efc5fd5ecc",
        "title": "Measuring Hidden Bias Within Face Recognition via Racial Phenotypes",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yucer_Measuring_Hidden_Bias_Within_Face_Recognition_via_Racial_Phenotypes_WACV_2022_paper.html",
        "author": "Seyma Yucer; Furkan Tektas; Noura Al Moubayed; Toby P. Breckon",
        "abstract": "Recent work reports disparate performance for intersectional racial groups across face recognition tasks: face verification and identification. However, the definition of racial groups has a significant impact on the underlying findings of such racial bias analysis. Previous studies define these groups based on either demographic information (e.g. African, Asian etc.) or skin tone (e.g. lighter or darker skins). The use of such either sensitive or broad and loosely defined group definitions has disadvantages for both bias investigation and the design of subsequent counter-bias solutions. By contrast, this study introduces an alternative racial bias analysis methodology via the use of facial phenotype attributes for face recognition. We use the set of observable characteristics of an individual face where a race-related facial phenotype is hence specific to the human face and correlated to the racial profile of the subject. We propose categorical test cases to investigate the individual influence of those attributes on bias within face recognition tasks. We compare our phenotype-based grouping methodology with previous grouping strategies and show that phenotype-based groupings uncover hidden bias without exposing any potentially protected attributes. Furthermore, we contribute corresponding phenotype attribute category labels for face recognition tasks: RFW for face verification and VGGFace2 (test set) for face identification.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yucer_Measuring_Hidden_Bias_Within_Face_Recognition_via_Racial_Phenotypes_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yucer_Measuring_Hidden_Bias_WACV_2022_supplemental.pdf",
        "arxiv": "2110.09839",
        "pdf_size": 496167,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8931314772522996489&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d08799311c",
        "title": "Measuring Representation of Race, Gender, and Age in Children's Books: Face Detection and Feature Classification in Illustrated Images",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Szasz_Measuring_Representation_of_Race_Gender_and_Age_in_Childrens_Books_WACV_2022_paper.html",
        "author": "Teodora Szasz; Emileigh Harrison; Ping-Jung Liu; Ping-Chang Lin; Hakizumwami Birali Runesha; Anjali Adukia",
        "abstract": "Images in children's books convey messages about society and the roles that people play in it. Understanding these messages requires systematic measurement of who is represented. Computer vision face detection tools can provide such measurements; however, state-of-the-art face detection models were trained with photographs, and 80% of images in children's books are illustrated; thus existing methods both misclassify and miss classifying many faces. In this paper, we introduce a new approach to analyze images using AI tools, resulting in data that can assess representation of race, gender, and age in both illustrations and photographs in children's books. We make four primary contributions to the fields of deep learning and social sciences: (1) We curate an original face detection data set (IllusFace 1.0) by manually labeling 5,403 illustrated faces with bounding boxes. (2) We train two AutoML-based face detection models for illustrations: (i) using IllusFace 1.0 (FDAI); (ii) using iCartoon, a publicly available data set (FDAI_iC), each optimized for illustrated images, detecting 2.5 times more faces in our testing data than the established face detector using Google Vision (FDGV). (3) We curate a data set of the race, gender, and age of 980 faces manually labeled by three different raters (CBFeatures 1.0). (4) We train an AutoML feature classification model (FCA) using CBFeatures 1.0. We compare FCA with the performance of another AutoML model that we trained on UTKFace, a public data set (FCA_UTK) and of an established model using FairFace (FCF). Finally, we examine distributions of character identities over the last century across the models. We find that FCA is 34% more accurate than FCF in its race predictions. These contributions provide tools to educators, caregivers, and curriculum developers to assess the representation contained in children's content.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Szasz_Measuring_Representation_of_Race_Gender_and_Age_in_Childrens_Books_WACV_2022_paper.pdf",
        "aff": "University of Chicago; University of Chicago; University of Chicago; University of Chicago; University of Chicago; University of Chicago+NBER",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 866250,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15179424750640764975&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "uchicago.edu;uchicago.edu;uchicago.edu;uchicago.edu;uchicago.edu;uchicago.edu",
        "email": "uchicago.edu;uchicago.edu;uchicago.edu;uchicago.edu;uchicago.edu;uchicago.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0+1",
        "aff_unique_norm": "University of Chicago;National Bureau of Economic Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uchicago.edu;https://www.nber.org",
        "aff_unique_abbr": "UChicago;NBER",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dbdc33f775",
        "title": "Mending Neural Implicit Modeling for 3D Vehicle Reconstruction in the Wild",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Duggal_Mending_Neural_Implicit_Modeling_for_3D_Vehicle_Reconstruction_in_the_WACV_2022_paper.html",
        "author": "Shivam Duggal; Zihao Wang; Wei-Chiu Ma; Sivabalan Manivasagam; Justin Liang; Shenlong Wang; Raquel Urtasun",
        "abstract": "Reconstructing high-quality 3D objects from sparse, partial observations from a single view is of crucial importance for various applications in computer vision, robotics, and graphics. While recent neural implicit modeling methods show promising results on synthetic or dense data, they perform poorly on sparse and noisy real-world data. We discover that the limitations of a popular neural implicit model are due to lack of robust shape priors and lack of proper regularization. In this work, we demonstrate high-quality in-the-wild shape reconstruction using: (i) a deep encoder as a robust-initializer of the shape latent-code; (ii) regularized test-time optimization of the latent-code; (iii) a deep discriminator as a learned high-dimensional shape prior; (iv) a novel curriculum learning strategy that allows the model to learn shape priors on synthetic data and smoothly transfer them to sparse real-world data. Our approach better captures the global structure, performs well on occluded and sparse observations, and registers well with the ground-truth shape. We demonstrate superior performance over state-of-the-art 3D object reconstruction methods on two real-world datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Duggal_Mending_Neural_Implicit_Modeling_for_3D_Vehicle_Reconstruction_in_the_WACV_2022_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Duggal_Mending_Neural_Implicit_WACV_2022_supplemental.pdf",
        "arxiv": "2101.06860",
        "pdf_size": 3482989,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6004037165729691864&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ba79fe0bbf",
        "title": "Mesh Convolutional Autoencoder for Semi-Regular Meshes of Different Sizes",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hahner_Mesh_Convolutional_Autoencoder_for_Semi-Regular_Meshes_of_Different_Sizes_WACV_2022_paper.html",
        "author": "Sara Hahner; Jochen Garcke",
        "abstract": "The analysis of deforming 3D surface meshes is accelerated by autoencoders since the low-dimensional embeddings can be used to visualize underlying dynamics. But, state-of-the-art mesh convolutional autoencoders require a fixed connectivity of all input meshes handled by the autoencoder. This is due to either the use of spectral convolutional layers or mesh dependent pooling operations. Therefore, the types of datasets that one can study are limited and the learned knowledge cannot be transferred to other datasets that exhibit similar behavior. To address this, we transform the discretization of the surfaces to semi-regular meshes that have a locally regular connectivity and whose meshing is hierarchical. This allows us to apply the same spatial convolutional filters to the local neighborhoods and to define a pooling operator that can be applied to every semi-regular mesh. We apply the same mesh autoencoder to different datasets and our reconstruction error is more than 50% lower than the error from state-of-the-art models, which have to be trained for every mesh separately. Additionally, we visualize the underlying dynamics of unseen mesh sequences with an autoencoder trained on different classes of meshes.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hahner_Mesh_Convolutional_Autoencoder_for_Semi-Regular_Meshes_of_Different_Sizes_WACV_2022_paper.pdf",
        "aff": "Fraunhofer Center for Machine Learning and SCAI, Sankt Augustin, Germany+Institut f \u00a8ur Numerische Simulation, Universit \u00a8at Bonn, Germany; Fraunhofer Center for Machine Learning and SCAI, Sankt Augustin, Germany+Institut f \u00a8ur Numerische Simulation, Universit \u00a8at Bonn, Germany",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hahner_Mesh_Convolutional_Autoencoder_WACV_2022_supplemental.pdf",
        "arxiv": "2110.09401",
        "pdf_size": 4666224,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10600748366526502335&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "scai.fraunhofer.de; ",
        "email": "scai.fraunhofer.de; ",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Fraunhofer Center for Machine Learning and SCAI;Universit\u00e4t Bonn",
        "aff_unique_dep": ";Institut f\u00fcr Numerische Simulation",
        "aff_unique_url": "https://www.fraunhofer.de/en/research/centers/fraunhofer-center-for-machine-learning.html;https://www.uni-bonn.de",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Sankt Augustin;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "60b9e1dcc3",
        "title": "Meta Approach to Data Augmentation Optimization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hataya_Meta_Approach_to_Data_Augmentation_Optimization_WACV_2022_paper.html",
        "author": "Ryuichiro Hataya; Jan Zdenek; Kazuki Yoshizoe; Hideki Nakayama",
        "abstract": "Data augmentation policies drastically improve the performance of image recognition tasks, especially when the policies are optimized for the target data and tasks. In this paper, we propose to optimize image recognition models and data augmentation policies simultaneously to improve the performance using gradient descent. Unlike prior methods, our approach avoids using proxy tasks or reducing search space, and can directly improve the validation performance. Our method achieves efficient and scalable training by approximating the gradient of policies by implicit gradient with Neumann series approximation. We demonstrate that our approach can improve the performance of various image classification tasks, including fine-grained image recognition, without using dataset-specific hyperparameter tuning.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hataya_Meta_Approach_to_Data_Augmentation_Optimization_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hataya_Meta_Approach_to_WACV_2022_supplemental.pdf",
        "arxiv": "2006.07965",
        "pdf_size": 1804851,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=974149259930954216&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "addf760153",
        "title": "Meta-Learning for Multi-Label Few-Shot Classification",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Simon_Meta-Learning_for_Multi-Label_Few-Shot_Classification_WACV_2022_paper.html",
        "author": "Christian Simon; Piotr Koniusz; Mehrtash Harandi",
        "abstract": "Even with the luxury of having abundant data, multi-label classification is widely known to be a challenging task to address. This work targets the problem of multi-label meta-learning, where a model learns to predict multiple labels within a query (e.g., an image) by just observing a few supporting examples. In doing so, we first propose a benchmark for Few-Shot Learning (FSL) with multiple labels per sample. Next, we discuss and extend several solutions specifically designed to address the conventional and single-label FSL, to work in the multi-label regime. Lastly, we introduce a neural module to estimate the label count of a given sample by exploiting the relational inference. We will show empirically the benefit of the label count module, the label propagation algorithm, and the extensions of conventional FSL methods on three challenging datasets, namely MS-COCO, iMaterialist, and Open MIC. Overall, our thorough experiments suggest that the proposed label-propagation algorithm in conjunction with the neural label count module (NLC) shall be considered as the method of choice.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Simon_Meta-Learning_for_Multi-Label_Few-Shot_Classification_WACV_2022_paper.pdf",
        "aff": "The Australian National University+Data61-CSIRO; Monash University+Data61-CSIRO; Monash University+Data61-CSIRO",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.13494",
        "pdf_size": 3487770,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13637726164359789885&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "anu.edu.au;monash.edu;data61.csiro.au",
        "email": "anu.edu.au;monash.edu;data61.csiro.au",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1;2+1",
        "aff_unique_norm": "Australian National University;Commonwealth Scientific and Industrial Research Organisation;Monash University",
        "aff_unique_dep": ";Data61;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.csiro.au;https://www.monash.edu",
        "aff_unique_abbr": "ANU;CSIRO;Monash",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "a2a9ef4704",
        "title": "Meta-Meta Classification for One-Shot Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chowdhury_Meta-Meta_Classification_for_One-Shot_Learning_WACV_2022_paper.html",
        "author": "Arkabandhu Chowdhury; Dipak Chaudhari; Swarat Chaudhuri; Chris Jermaine",
        "abstract": "We present a new approach, called meta-meta classification, to learning in small-data settings. In this approach, one uses a large set of learning problems to design an ensemble of learners, where each learner has high bias and low variance and is skilled at solving a specific type of learning problem. The meta-meta classifier learns how to examine a given learning problem and combine the various learners to solve the problem. The meta-meta learning approach is especially suited to solving few-shot learning tasks, as it is easier to learn to classify a new learning problem with little data than it is to apply a learning algorithm to a small data set. We evaluate the approach on a one-shot, one-class-versus-all classification task and show that it is able to outperform traditional meta-learning as well as ensembling approaches.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chowdhury_Meta-Meta_Classification_for_One-Shot_Learning_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Chowdhury_Meta-Meta_Classification_for_WACV_2022_supplemental.zip",
        "arxiv": "2004.08083",
        "pdf_size": 1203927,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15000378444991341360&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "904dd84069",
        "title": "Meta-UDA: Unsupervised Domain Adaptive Thermal Object Detection Using Meta-Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/VS_Meta-UDA_Unsupervised_Domain_Adaptive_Thermal_Object_Detection_Using_Meta-Learning_WACV_2022_paper.html",
        "author": "Vibashan VS; Domenick Poster; Suya You; Shuowen Hu; Vishal M. Patel",
        "abstract": "Object detectors trained on large-scale RGB datasets are being extensively employed in real-world applications. However, these RGB-trained models suffer a performance drop under adverse illumination and lighting conditions. Infrared (IR) cameras are robust under such conditions and can be helpful in real-world applications. Though thermal cameras are widely used for military applications and increasingly for commercial applications, there is a lack of robust algorithms to robustly exploit the thermal imagery due to the limited availability of labeled thermal data. In this work, we aim to enhance the object detection performance in the thermal domain by leveraging the labeled visible domain data in an Unsupervised Domain Adaptation (UDA) setting. We propose an algorithm agnostic meta-learning framework to improve existing UDA methods instead of proposing a new UDA strategy. We achieve this by meta-learning the initial condition of the detector, which facilitates the adaptation process with fine updates without overfitting or getting stuck at local optima. However, meta-learning the initial condition for the detection scenario is computationally heavy due to long and intractable computation graphs. Therefore, we propose an online meta-learning paradigm which performs online updates resulting in a short and tractable computation graph. To this end, we demonstrate the superiority of our method over many baselines in the UDA setting, producing a state-of-the-art thermal detector for the KAIST and DSIAC datasets. Source code will be made publicly available after the review process.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/VS_Meta-UDA_Unsupervised_Domain_Adaptive_Thermal_Object_Detection_Using_Meta-Learning_WACV_2022_paper.pdf",
        "aff": "Johns Hopkins University, MD, USA; West Virginia University, WV, USA; U.S. Army CCDC Army Research Laboratory, USA; U.S. Army CCDC Army Research Laboratory, USA; Johns Hopkins University, MD, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 657806,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18097142826845183452&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "jhu.edu;mix.wvu.edu;mail.mil;mail.mil;jhu.edu",
        "email": "jhu.edu;mix.wvu.edu;mail.mil;mail.mil;jhu.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;0",
        "aff_unique_norm": "Johns Hopkins University;West Virginia University;U.S. Army Research Laboratory",
        "aff_unique_dep": ";;Army Research Laboratory",
        "aff_unique_url": "https://www.jhu.edu;https://www.wvu.edu;https://www.arl.army.mil",
        "aff_unique_abbr": "JHU;WVU;ARL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "06e0240472",
        "title": "MisConv: Convolutional Neural Networks for Missing Data",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Przewiezlikowski_MisConv_Convolutional_Neural_Networks_for_Missing_Data_WACV_2022_paper.html",
        "author": "Marcin Przewi\u0119\u017alikowski; Marek \u015amieja; \u0141ukasz Struski; Jacek Tabor",
        "abstract": "Processing of missing data by modern neural networks, such as CNNs, remains a fundamental, yet unsolved challenge, which naturally arises in many practical applications, like image inpainting or autonomous vehicles and robots. While imputation-based techniques are still one of the most popular solutions, they frequently introduce unreliable information to the data and do not take into account the uncertainty of estimation, which may be destructive for a machine learning model. In this paper, we present MisConv, a general mechanism, for adapting various CNN architectures to process incomplete images. By modeling the distribution of missing values by the Mixture of Factor Analyzers, we cover the spectrum of possible replacements and find an analytical formula for the expected value of convolution operator applied to the incomplete image. The whole framework is realized by matrix operations, which makes MisConv extremely efficient in practice. Experiments performed on various image processing tasks demonstrate that MisConv achieves superior or comparable performance to the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Przewiezlikowski_MisConv_Convolutional_Neural_Networks_for_Missing_Data_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Przewiezlikowski_MisConv_Convolutional_Neural_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 4398652,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6873806722217615859&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6822ab8562",
        "title": "Mixed-Dual-Head Meets Box Priors: A Robust Framework for Semi-Supervised Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chen_Mixed-Dual-Head_Meets_Box_Priors_A_Robust_Framework_for_Semi-Supervised_Segmentation_WACV_2022_paper.html",
        "author": "Chenshu Chen; Tao Liu; Wenming Tan; Shiliang Pu",
        "abstract": "As it is costly to densely annotate large scale datasets for supervised semantic segmentation, extensive semi-supervised methods have been proposed. However, the accuracy, stability and flexibility of existing methods are still far from satisfactory. In this paper, we propose an effective and flexible framework for semi-supervised semantic segmentation using a small set of fully labeled images and a set of weakly labeled images with bounding box labels. In our framework, position and class priors are designed to guide the annotation network to predict accurate pseudo masks for weakly labeled images, which are used to train the segmentation network. We also propose a mixed-dual-head training method to reduce the interference of label noise while enabling the training process more stable. Experiments on PASCAL VOC 2012 show that our method achieves state-of-the-art performance and can achieve competitive results even with very few fully labeled images. Furthermore, the performance can be further boosted with extra weakly labeled images from COCO dataset.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chen_Mixed-Dual-Head_Meets_Box_Priors_A_Robust_Framework_for_Semi-Supervised_Segmentation_WACV_2022_paper.pdf",
        "aff": "Hikvision Research Institute; Hikvision Research Institute; Hikvision Research Institute; Hikvision Research Institute",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Chen_Mixed-Dual-Head_Meets_Box_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 8119195,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:BLtEYgfAZA0J:scholar.google.com/&scioq=Mixed-Dual-Head+Meets+Box+Priors:+A+Robust+Framework+for+Semi-Supervised+Segmentation&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff_domain": "hikvision.com;hikvision.com;hikvision.com;hikvision.com",
        "email": "hikvision.com;hikvision.com;hikvision.com;hikvision.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Hikvision Research Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hikvision.com/cn/",
        "aff_unique_abbr": "Hikvision",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "d5f65a4b1a",
        "title": "MoESR: Blind Super-Resolution Using Kernel-Aware Mixture of Experts",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Emad_MoESR_Blind_Super-Resolution_Using_Kernel-Aware_Mixture_of_Experts_WACV_2022_paper.html",
        "author": "Mohammad Emad; Maurice Peemen; Henk Corporaal",
        "abstract": "Modern deep learning super-resolution approaches have achieved remarkable performance where the low-resolution (LR) input is a degraded high-resolution (HR) image by a fixed known kernel i.e. kernel-specific super-resolution (SR). However, real images often vary in their degradation kernels, thus a single kernel-specific SR approach does not often produce accurate HR results. Recently, degradation-aware networks are introduced to generate blind SR results for unknown kernel conditions. They can restore images for multiple blur kernels, however they have to compromise in quality compared to their kernel-specific counterparts. To address this issue, we propose a novel blind SR method called Mixture of Experts Super-Resolution (MoESR), which uses different experts for different degradation kernels. A broad space of degradation kernels is covered by kernel-specific SR networks (experts). We present an accurate kernel prediction method (gating mechanism) by evaluating the sharpness of images generated by experts. Based on the predicted kernel our most suited expert network is selected for the input image. Finally, we fine-tune the selected network on the test image itself to leverage the advantage of internal learning. Our experimental results on standard synthetic datasets and real images demonstrate that MoESR outperforms state-of-the-art methods both quantitatively and qualitatively. Especially for the challenging x4 SR task, our PSNR improvement of 0.93 dB on the DIV2KRK dataset is substantial.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Emad_MoESR_Blind_Super-Resolution_Using_Kernel-Aware_Mixture_of_Experts_WACV_2022_paper.pdf",
        "aff": "Eindhoven University of Technology, Netherlands; Thermo Fisher Scientific, Netherlands; Eindhoven University of Technology, Netherlands",
        "project": "",
        "github": "https://github.com/memad73/MoESR",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Emad_MoESR_Blind_Super-Resolution_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 9564809,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7951740550830449824&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff_domain": "tue.nl;thermofisher.com;tue.nl",
        "email": "tue.nl;thermofisher.com;tue.nl",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Eindhoven University of Technology;Thermo Fisher Scientific",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tue.nl;https://www.thermofisher.com",
        "aff_unique_abbr": "TU/e;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2b444e16f9",
        "title": "Mobile Based Human Identification Using Forehead Creases: Application and Assessment Under COVID-19 Masked Face Scenarios",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bharadwaj_Mobile_Based_Human_Identification_Using_Forehead_Creases_Application_and_Assessment_WACV_2022_paper.html",
        "author": "Rohit Bharadwaj; Gaurav Jaswal; Aditya Nigam; Kamlesh Tiwari",
        "abstract": "In the COVID-19 situation, face masks have become an essential part of our daily life. As mask occludes most prominent facial characteristics, it brings new challenges to the existing facial recognition systems. This paper presents an idea to consider forehead creases (under surprise facial expression) as a new biometric modality to authenticate mask-wearing faces. The forehead biometrics utilizes the creases and textural skin patterns appearing due to voluntary contraction of the forehead region as features. The proposed framework is an efficient and generalizable deep learning framework for forehead recognition. Face-selfie images are collected using smartphone's frontal camera in an unconstrained environment with various indoor/outdoor realistic environments. Acquired forehead images are first subjected to a segmentation model that results in rectangular Region Of Interest (ROI's). A set of convolutional feature maps are subsequently obtained using a backbone network. The primary embeddings are enriched using a dual attention network (DANet) to induce discriminative feature learning. The attention-empowered embeddings are then optimized using Large Margin Cosine Loss (LMCL) followed by Focal Loss to update weights for inducting robust training and better feature discriminating capabilities. Our system is end-to-end and few-shot; thus, it is very efficient in memory requirements and recognition rate. Besides, we present a forehead image dataset that has been recorded in two sessions from 247 subjects containing a total of 4,964 selfie-face mask images. To the best of our knowledge, this is the first to date mobile-based forehead dataset and is being made available along with the mobile application in the public domain. The proposed system has achieved high performance results in both closed-set, i.e., CRR of 99.08% and EER of 0.44% and open-set matching, i.e., CRR: 97.84%, EER: 12.40% which justifies the significance of using forehead as a biometric modality.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bharadwaj_Mobile_Based_Human_Identification_Using_Forehead_Creases_Application_and_Assessment_WACV_2022_paper.pdf",
        "aff": "Dept. of CSIS, Birla Institute of Technology and Science Pilani, Jhunjhunu-333031, Rajasthan, INDIA; Dept. of EE, Indian Institute of Technology Delhi, Hauz Khas-110016, New Delhi, INDIA; SCEE, Indian Institute of Technology Mandi, Mandi-175005, Himachal Pradesh, INDIA; Dept. of CSIS, Birla Institute of Technology and Science Pilani, Jhunjhunu-333031, Rajasthan, INDIA",
        "project": "http://ktiwari.in/projects/foreheadcreases/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4202395,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14533849026281743751&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "pilani.bits-pilani.ac.in;pilani.bits-pilani.ac.in;ee.iitd.ac.in;iitmandi.ac.in",
        "email": "pilani.bits-pilani.ac.in;pilani.bits-pilani.ac.in;ee.iitd.ac.in;iitmandi.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Birla Institute of Technology and Science;Indian Institute of Technology Delhi;Indian Institute of Technology Mandi",
        "aff_unique_dep": "Dept. of CSIS;Dept. of EE;SCEE",
        "aff_unique_url": "https://www.bits-pilani.ac.in;https://www.iitd.ac.in;https://www.iitmandi.ac.in",
        "aff_unique_abbr": "BITS Pilani;IIT Delhi;IIT Mandi",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Pilani;New Delhi;Mandi",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "962c186e13",
        "title": "MobileStereoNet: Towards Lightweight Deep Networks for Stereo Matching",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Shamsafar_MobileStereoNet_Towards_Lightweight_Deep_Networks_for_Stereo_Matching_WACV_2022_paper.html",
        "author": "Faranak Shamsafar; Samuel Woerz; Rafia Rahim; Andreas Zell",
        "abstract": "Recent methods in stereo matching have continuously improved the accuracy using deep models. This gain, however, is attained with a high increase in computation cost, such that the network may not fit even on a moderate GPU. This issue raises problems when the model needs to be deployed on resource-limited devices. For this, we propose two light models for stereo vision with reduced complexity and without sacrificing accuracy. Depending on the dimension of cost volume, we design a 2D and a 3D model with encoder-decoders built from 2D and 3D convolutions, respectively. To this end, we leverage 2D MobileNet blocks and extend them to 3D for stereo vision application. Besides, a new cost volume is proposed to boost the accuracy of the 2D model, making it performing close to 3D networks. Experiments show that the proposed 2D/3D networks effectively reduce the computational expense (27%/95% and 72%/38% fewer parameters/operations in 2D and 3D models, respectively) while upholding the accuracy. Code: https://github.com/cogsys-tuebingen/mobilestereonet.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Shamsafar_MobileStereoNet_Towards_Lightweight_Deep_Networks_for_Stereo_Matching_WACV_2022_paper.pdf",
        "aff": "WSI Institute for Computer Science, University of Tuebingen, Germany; WSI Institute for Computer Science, University of Tuebingen, Germany; WSI Institute for Computer Science, University of Tuebingen, Germany; WSI Institute for Computer Science, University of Tuebingen, Germany",
        "project": "",
        "github": "https://github.com/cogsys-tuebingen/mobilestereonet",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Shamsafar_MobileStereoNet_Towards_Lightweight_WACV_2022_supplemental.pdf",
        "arxiv": "2108.09770",
        "pdf_size": 9142542,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7642881110425311344&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de",
        "email": "uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Tuebingen",
        "aff_unique_dep": "WSI Institute for Computer Science",
        "aff_unique_url": "https://www.uni-tuebingen.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "f41bf84650",
        "title": "Model Compression Using Optimal Transport",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lohit_Model_Compression_Using_Optimal_Transport_WACV_2022_paper.html",
        "author": "Suhas Lohit; Michael Jones",
        "abstract": "Model compression methods are important to allow for easier deployment of deep learning models in compute, memory and energy-constrained environments such as mobile phones. Knowledge distillation is a class of model compression algorithms where knowledge from a large teacher network is transferred to a smaller student network thereby improving the student's performance. In this paper, we show how optimal transport-based loss functions can be used for training a student network which encourages learning student network parameters that help bring the distribution of student features closer to that of the teacher features. We present image classification results on CIFAR-100, SVHN and ImageNet and show that the proposed optimal transport loss functions perform comparably to or better than other loss functions.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lohit_Model_Compression_Using_Optimal_Transport_WACV_2022_paper.pdf",
        "aff": "Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Lohit_Model_Compression_Using_WACV_2022_supplemental.pdf",
        "arxiv": "2012.03907",
        "pdf_size": 667806,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17569253593263634469&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "merl.com;merl.com",
        "email": "merl.com;merl.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Mitsubishi Electric Research Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.merl.com",
        "aff_unique_abbr": "MERL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7e914f3378",
        "title": "Modeling Aleatoric Uncertainty for Camouflaged Object Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Liu_Modeling_Aleatoric_Uncertainty_for_Camouflaged_Object_Detection_WACV_2022_paper.html",
        "author": "Jiawei Liu; Jing Zhang; Nick Barnes",
        "abstract": "Aleatoric uncertainty captures noise within the observations. For camouflaged object detection, due to similar appearance of the camouflaged foreground and the background, it's difficult to obtain highly accurate annotations, especially annotations around object boundaries. We argue that training directly with the noisy camouflage map may lead to a model of poor generalization ability. In this paper, we introduce an explicitly aleatoric uncertainty estimation technique to represent predictive uncertainty due to noisy labeling. Specifically, we present a confidence-aware camouflaged object detection (COD) framework using dynamic supervision to produce both an accurate camouflage map and a reliable aleatoric uncertainty. Different from existing techniques that produce deterministic prediction following the point estimation pipeline, our framework formalises aleatoric uncertainty as probability distribution over model output and the input image. We claim that, once trained, our confidence estimation network can evaluate the pixel-wise accuracy of the prediction without relying on the ground truth camouflage map. Extensive results illustrate the superior performance of the proposed model in explaining the camouflage prediction. Our codes are available at https://github.com/Carlisle-Liu/OCENet",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Liu_Modeling_Aleatoric_Uncertainty_for_Camouflaged_Object_Detection_WACV_2022_paper.pdf",
        "aff": "Australian National University; Australian National University; Australian National University",
        "project": "",
        "github": "https://github.com/Carlisle-Liu/OCENet",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10008437,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12813868889911912409&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "73e914e0b3",
        "title": "Modeling Dynamic Target Deformation in Camera Calibration",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hagemann_Modeling_Dynamic_Target_Deformation_in_Camera_Calibration_WACV_2022_paper.html",
        "author": "Annika Hagemann; Moritz Knorr; Christoph Stiller",
        "abstract": "Most approaches to camera calibration rely on calibration targets of well-known geometry. During data acquisition, calibration target and camera system are typically moved w.r.t. each other, to allow image coverage and perspective versatility. We show that moving the target can lead to small temporary deformations of the target, which can introduce significant errors into the calibration result. While static inaccuracies of calibration targets have been addressed in previous works, to our knowledge, none of the existing approaches can capture time-varying, dynamic deformations. To achieve high-accuracy calibrations despite moving the target, we propose a way to explicitly model dynamic target deformations in camera calibration. This is achieved by using a low-dimensional deformation model with only few parameters per image, which can be optimized jointly with target poses and intrinsics. We demonstrate the effectiveness of modeling dynamic deformations using different calibration targets and show its significance in a structure-from-motion application.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hagemann_Modeling_Dynamic_Target_Deformation_in_Camera_Calibration_WACV_2022_paper.pdf",
        "aff": "Robert Bosch GmbH; Robert Bosch GmbH; Institute of Measurement & Control Systems, Karlsruhe Institute of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hagemann_Modeling_Dynamic_Target_WACV_2022_supplemental.pdf",
        "arxiv": "2110.07322",
        "pdf_size": 2957269,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17909587266224535223&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "de.bosch.com; ; ",
        "email": "de.bosch.com; ; ",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Robert Bosch GmbH;Karlsruhe Institute of Technology",
        "aff_unique_dep": ";Institute of Measurement & Control Systems",
        "aff_unique_url": "https://www.bosch.com;https://www.kit.edu",
        "aff_unique_abbr": "Bosch;KIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "3d612c650b",
        "title": "Monocular Depth Estimation With Adaptive Geometric Attention",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Naderi_Monocular_Depth_Estimation_With_Adaptive_Geometric_Attention_WACV_2022_paper.html",
        "author": "Taher Naderi; Amir Sadovnik; Jason Hayward; Hairong Qi",
        "abstract": "Single image depth estimation is an ill-posed problem. That is, it is not mathematically possible to uniquely estimate the 3rd dimension (or depth) from a single 2D image. Hence, additional constraints need to be incorporated in order to regulate the solution space. In this paper, we explore the idea of constraining the model by taking advantage of the similarity between the RGB image and the corresponding depth map at the geometric edges of the 3D scene for more accurate depth estimation. We propose a general light-weight adaptive geometric attention module that uses the cross-correlation between the encoder and the decoder as a measure of this similarity. More precisely, we use the cosine similarity between the local embedded features in the encoder and the decoder at each spatial point. The proposed module along with the encoder-decoder network is trained in an end-to-end fashion and achieves superior and competitive performance in comparison with other state-of-the-art methods. In addition, adding our module to the base encoder-decoder model adds only an additional 0.03% (or 0.0003) parameters. Therefore, this module can be added to any base encoder-decoder network without changing its structure to address any task at hand.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Naderi_Monocular_Depth_Estimation_With_Adaptive_Geometric_Attention_WACV_2022_paper.pdf",
        "aff": "Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science; Department of Nuclear Engineering; Department of Electrical Engineering and Computer Science",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8781932,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12099500202293199938&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "utk.edu;utk.edu;utk.edu;utk.edu",
        "email": "utk.edu;utk.edu;utk.edu;utk.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Department of Nuclear Engineering",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;Nuclear Engineering",
        "aff_unique_url": "https://web.mit.edu;",
        "aff_unique_abbr": "MIT;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "aa6bf10405",
        "title": "MovingFashion: A Benchmark for the Video-To-Shop Challenge",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Godi_MovingFashion_A_Benchmark_for_the_Video-To-Shop_Challenge_WACV_2022_paper.html",
        "author": "Marco Godi; Christian Joppi; Geri Skenderi; Marco Cristani",
        "abstract": "Retrieving clothes which are worn in social media videos (Instagram, TikTok) is the latest frontier of e-fashion, referred to as \"video-to-shop\" in the computer vision literature. In this paper we present MovingFashion, the first publicly available dataset to cope with this challenge. MovingFashion is composed of 14855 social videos, each one of them associated to e-commerce \"shop\" images where the corresponding clothing items are clearly portrayed. In addition, we present a network for retrieving the shop images in this scenario, dubbed SEAM Match-RCNN. The model is trained by image-to-video domain adaptation, allowing to use video sequences where only their association with a shop image is given, eliminating the need of millions of annotated bounding boxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted sum of few frames (10) of a social video is enough to individuate the correct product within the first 5 retrieved items in a 14K+ shop element gallery with an accuracy of 80%. This provides the best performance on MovingFashion, comparing exhaustively against the related state-of-the-art approaches and alternative baselines.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Godi_MovingFashion_A_Benchmark_for_the_Video-To-Shop_Challenge_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "https://github.com/HumaticsLAB/SEAM-Match-RCNN",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Godi_MovingFashion_A_Benchmark_WACV_2022_supplemental.pdf",
        "arxiv": "2110.02627",
        "pdf_size": 2377398,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14453564602076074348&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "313b2dfe80",
        "title": "Multi-Branch Neural Networks for Video Anomaly Detection in Adverse Lighting and Weather Conditions",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Leroux_Multi-Branch_Neural_Networks_for_Video_Anomaly_Detection_in_Adverse_Lighting_WACV_2022_paper.html",
        "author": "Sam Leroux; Bo Li; Pieter Simoens",
        "abstract": "Automated anomaly detection in surveillance videos has attracted much interest as it provides a scalable alternative to manual monitoring. Most existing approaches achieve good performance on clean benchmark datasets recorded in well-controlled environments. However, detecting anomalies is much more challenging in the real world. Adverse weather conditions like rain or changing brightness levels cause a significant shift in the input data distribution, which in turn can lead to the detector model incorrectly reporting high anomaly scores. Additionally, surveillance cameras are usually deployed in evolving environments such as a city street of which the appearance changes over time because of seasonal changes or roadworks. The anomaly detection model will need to be updated periodically to deal with these issues. In this paper, we introduce a multi-branch model that is equipped with a trainable preprocessing step and multiple identical branches for detecting anomalies during day and night as well as in sunny and rainy conditions. We experimentally validate our approach on a distorted version of the Avenue dataset and provide qualitative results on real-world surveillance camera data. Experimental results show that our method outperforms the existing methods in terms of detection accuracy while being faster and more robust on scenes with varying visibility.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Leroux_Multi-Branch_Neural_Networks_for_Video_Anomaly_Detection_in_Adverse_Lighting_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4809792,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6799158733011151130&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d46833d7ba",
        "title": "Multi-Dimensional Dynamic Model Compression for Efficient Image Super-Resolution",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hou_Multi-Dimensional_Dynamic_Model_Compression_for_Efficient_Image_Super-Resolution_WACV_2022_paper.html",
        "author": "Zejiang Hou; Sun-Yuan Kung",
        "abstract": "Modern single image super-resolution (SR) system based on convolutional neural networks achieves substantial progress. However, most SR deep networks are computationally expensive and require excessively large activation memory footprints, impeding their effective deployment to resource-limited devices. Based on the observation that the activation patterns in SR networks exhibit high input-dependency, we propose Multi-Dimensional Dynamic Model Compression method that can reduce both spatial and channel wise redundancy in an SR deep network for different input images. To reduce the spatial-wise redundancy, we propose to perform convolution on scaled-down feature-maps where the down-scaling factor is made adaptive to different input images. To reduce the channel-wise redundancy, we introduce a low-cost channel saliency predictor for each convolution to dynamically skip the computation of unimportant channels based on the Gumbel-Softmax. To better capture the feature-maps information and facilitate input-adaptive decision, we employ classic image processing metrics, e.g., Spatial Information, to guide the saliency predictors. The proposed method can be readily applied to a variety of SR deep networks and trained end-to-end with standard super-resolution loss, in combination with a sparsity criterion. Experiments on several benchmarks demonstrate that our method can effectively reduce the FLOPs of both lightweight and non-compact SR models with negligible PSNR loss. Moreover, our compressed models achieve competitive PSNR-FLOPs Pareto frontier compared with SOTA NAS-based SR methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hou_Multi-Dimensional_Dynamic_Model_Compression_for_Efficient_Image_Super-Resolution_WACV_2022_paper.pdf",
        "aff": "Princeton University; Princeton University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hou_Multi-Dimensional_Dynamic_Model_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5718650,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11428435767640385513&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7a980c053f",
        "title": "Multi-Domain Incremental Learning for Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Garg_Multi-Domain_Incremental_Learning_for_Semantic_Segmentation_WACV_2022_paper.html",
        "author": "Prachi Garg; Rohit Saluja; Vineeth N Balasubramanian; Chetan Arora; Anbumani Subramanian; C.V. Jawahar",
        "abstract": "Recent efforts in multi-domain learning for semantic segmentation attempt to learn multiple geographical datasets in a universal, joint model. A simple fine-tuning experiment performed sequentially on three popular road scene segmentation datasets demonstrates that existing segmentation frameworks fail at incrementally learning on a series of visually disparate geographical domains. When learning a new domain, the model catastrophically forgets previously learned knowledge. In this work, we pose the problem of multi-domain incremental learning for semantic segmentation. Given a model trained on a particular geographical domain, the goal is to (i) incrementally learn a new geographical domain, (ii) while retaining performance on the old domain, (iii) given that the previous domain's dataset is not accessible. We propose a dynamic architecture that assigns universally shared, domain-invariant parameters to capture homogeneous semantic features present in all domains, while dedicated domain-specific parameters learn the statistics of each domain. Our novel optimization strategy helps achieve a good balance between retention of old knowledge (stability) and acquiring new knowledge (plasticity). We demonstrate the effectiveness of our proposed solution on domain incremental settings pertaining to real-world driving scenes from roads of Germany (Cityscapes), the United States (BDD100k), and India (IDD).",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Garg_Multi-Domain_Incremental_Learning_for_Semantic_Segmentation_WACV_2022_paper.pdf",
        "aff": "CVIT - IIIT Hyderabad, India; CVIT - IIIT Hyderabad, India; IIT Hyderabad, India; IIT Delhi, India; CVIT - IIIT Hyderabad, India; CVIT - IIIT Hyderabad, India",
        "project": "",
        "github": "https://github.com/prachigarg23/MDIL-SS",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Garg_Multi-Domain_Incremental_Learning_WACV_2022_supplemental.pdf",
        "arxiv": "2110.12205",
        "pdf_size": 4192336,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14551163437303823824&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;research.iiit.ac.in;iith.ac.in;cse.iitd.ac.in;iiit.ac.in;iiit.ac.in",
        "email": "gmail.com;research.iiit.ac.in;iith.ac.in;cse.iitd.ac.in;iiit.ac.in;iiit.ac.in",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad;Indian Institute of Technology Hyderabad;Indian Institute of Technology Delhi",
        "aff_unique_dep": "Center for Visual Information Technology;;",
        "aff_unique_url": "https://www.iiit Hyderabad.ac.in;https://www.iith.ac.in;https://www.iitd.ac.in",
        "aff_unique_abbr": "IIIT Hyderabad;IIT Hyderabad;IITD",
        "aff_campus_unique_index": "0;0;0;1;0;0",
        "aff_campus_unique": "Hyderabad;Delhi",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "3b5bc865b0",
        "title": "Multi-Domain Semantic Segmentation With Overlapping Labels",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bevandic_Multi-Domain_Semantic_Segmentation_With_Overlapping_Labels_WACV_2022_paper.html",
        "author": "Petra Bevandi\u0107; Marin Or\u0161i\u0107; Ivan Grubi\u0161i\u0107; Josip \u0160ari\u0107; Sini\u0161a \u0160egvi\u0107",
        "abstract": "Deep supervised models have an unprecedented capacity to absorb large quantities of training data. Hence, training on many datasets becomes a method of choice towards graceful degradation in unusual scenes. Unfortunately, different datasets often use incompatible labels. For instance, the Cityscapes road class subsumes all driving surfaces, while Vistas defines separate classes for road markings, manholes etc. We address this challenge by proposing a principled method for seamless learning on datasets with overlapping classes based on partial labels and probabilistic loss. Our method achieves competitive within-dataset and cross-dataset generalization, as well as ability to learn visual concepts which are not separately labeled in any of the training datasets. Experiments reveal competitive or state-of-the-art performance on two multi-domain dataset collections and on the WildDash 2 benchmark.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bevandic_Multi-Domain_Semantic_Segmentation_With_Overlapping_Labels_WACV_2022_paper.pdf",
        "aff": "University of Zagreb, Faculty of Electrical Engineering and Computing; University of Zagreb, Faculty of Electrical Engineering and Computing; ; ; ",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Bevandic_Multi-Domain_Semantic_Segmentation_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3082367,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7882643142899487654&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "fer.hr;fer.hr;fer.hr;fer.hr;fer.hr",
        "email": "fer.hr;fer.hr;fer.hr;fer.hr;fer.hr",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Zagreb",
        "aff_unique_dep": "Faculty of Electrical Engineering and Computing",
        "aff_unique_url": "https://www.feezagreb.unizg.hr",
        "aff_unique_abbr": "UNIZG FEEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Croatia"
    },
    {
        "id": "b6ef960cf1",
        "title": "Multi-Head Deep Metric Learning Using Global and Local Representations",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ebrahimpour_Multi-Head_Deep_Metric_Learning_Using_Global_and_Local_Representations_WACV_2022_paper.html",
        "author": "Mohammad K. Ebrahimpour; Gang Qian; Allison Beach",
        "abstract": "Deep Metric Learning (DML) aims to learn a data embedding space in which similar data points are grouped together while dissimilar data points are pushed away from each other. Successful DML models often require strong local and global representations, however, effective integration of local and global features in DML model training is a challenge. DML models are often trained with specific loss functions, including pairwise-based and proxy-based losses. The pairwise-based loss functions leverage rich semantic relations among data points, however, they often suffer from slow convergence during DML model training. On the other hand, the proxy-based loss functions often lead to significant speedups in convergence during training, while the rich relations among data points are often not fully explored by the proxy-based losses. In this paper, we propose a novel DML approach to address these challenges. The proposed DML approach makes use of a hybrid loss by integrating the pairwise-based and the proxy-based loss functions to leverage rich data-to-data relations as well as fast convergence. Furthermore, the proposed DML approach utilizes both global and local features to obtain rich representations in DML model training. Finally, We also use the second-order attention for feature enhancement to improve accurate and efficient retrieval. In our experiments, we extensively evaluated the proposed DML approach on four public benchmarks, and the experimental results demonstrate that the proposed method achieved state-of-the-art performance on all benchmarks, often with a large margin.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ebrahimpour_Multi-Head_Deep_Metric_Learning_Using_Global_and_Local_Representations_WACV_2022_paper.pdf",
        "aff": "ObjectVideo Labs, Inc., 8281 Greensboro Dr., Tysons, V A 22102; ObjectVideo Labs, Inc., 8281 Greensboro Dr., Tysons, V A 22102; ObjectVideo Labs, Inc., 8281 Greensboro Dr., Tysons, V A 22102",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2112.14327",
        "pdf_size": 11007195,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7095289929750404061&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;objectvideo.com;objectvideo.com",
        "email": "gmail.com;objectvideo.com;objectvideo.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ObjectVideo Labs, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b705ffee11",
        "title": "Multi-Level Attentive Adversarial Learning With Temporal Dilation for Unsupervised Video Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chen_Multi-Level_Attentive_Adversarial_Learning_With_Temporal_Dilation_for_Unsupervised_Video_WACV_2022_paper.html",
        "author": "Peipeng Chen; Yuan Gao; Andy J. Ma",
        "abstract": "Most existing works on unsupervised video domain adaptation attempt to mitigate the distribution gap across domains in frame and video levels. Such two-level distribution alignment approach may suffer from the problems of insufficient alignment for complex video data and misalignment along the temporal dimension. To address these issues, we develop a novel framework of Multi-level Attentive Adversarial Learning with Temporal Dilation (MA2L-TD). Given frame-level features as input, multi-level temporal features are generated and multiple domain discriminators are individually trained by adversarial learning for them. For better distribution alignment, level-wise attention weights are calculated by the degree of domain confusion in each level. To mitigate the negative effect of misalignment, features are aggregated with the attention mechanism determined by individual domain discriminators. Moreover, temporal dilation is designed for sequential non-repeatability to balance the computational efficiency and the possible number of levels. Extensive experimental results show that our proposed method outperforms the state of the arts on four benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chen_Multi-Level_Attentive_Adversarial_Learning_With_Temporal_Dilation_for_Unsupervised_Video_WACV_2022_paper.pdf",
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University, China+Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China; School of Computer Science and Engineering, Sun Yat-sen University, China+Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China; School of Computer Science and Engineering, Sun Yat-sen University, China+Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
        "project": "",
        "github": "https://github.com/justchenpp/MA2L-TD",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1169850,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14994720251366641244&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "email": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Sun Yat-sen University;Key Laboratory of Machine Intelligence and Advanced Computing",
        "aff_unique_dep": "School of Computer Science and Engineering;Ministry of Education",
        "aff_unique_url": "http://www.sysu.edu.cn;",
        "aff_unique_abbr": "SYSU;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2be7b959b7",
        "title": "Multi-Motion and Appearance Self-Supervised Moving Object Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yang_Multi-Motion_and_Appearance_Self-Supervised_Moving_Object_Detection_WACV_2022_paper.html",
        "author": "Fan Yang; Srikrishna Karanam; Meng Zheng; Terrence Chen; Haibin Ling; Ziyan Wu",
        "abstract": "In this work, we consider the problem of self-supervised Moving Object Detection (MOD) in video, where no ground truth is involved in both training and inference phases. Recently, an adversarial learning framework is proposed to leverage inherent temporal information for MOD. While showing great promising results, it uses single scale temporal information and may meet problems when dealing with a deformable object under multi-scale motion in different parts. Additional challenges can arise from the moving camera, which results in the failure of the motion independence hypothesis and locally independent background motion. To deal with these problems, we propose a Multi-motion and Appearance Self-supervised Network (MASNet) to introduce multi-scale motion information and appearance information of scene for MOD. In particular, a moving object, especially the deformable, usually consists of moving regions at various temporal scales. Introducing multi-scale motion can aggregate these regions to form a more complete detection. Appearance information can serve as another cue for MOD when the motion independence is not reliable and for removing false detection in background caused by locally independent background motion. To encode multi-scale motion and appearance, in MASNet we respectively design a multi-branch flow encoding module and an image inpainter module. The proposed modules and MASNet are extensively evaluated on the DAVIS dataset to demonstrate the effectiveness and superiority to state-of-the-art self-supervised methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yang_Multi-Motion_and_Appearance_Self-Supervised_Moving_Object_Detection_WACV_2022_paper.pdf",
        "aff": "United Imaging Intelligence, Cambridge MA, USA+Temple University, Philadelphia PA, USA; United Imaging Intelligence, Cambridge MA, USA; United Imaging Intelligence, Cambridge MA, USA; United Imaging Intelligence, Cambridge MA, USA; Stony Brook University, Stony Brook NY, USA; United Imaging Intelligence, Cambridge MA, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4506790,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16594662612386745869&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "uii-ai.com;uii-ai.com;uii-ai.com;uii-ai.com;cs.stonybrook.edu;uii-ai.com",
        "email": "uii-ai.com;uii-ai.com;uii-ai.com;uii-ai.com;cs.stonybrook.edu;uii-ai.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0;2;0",
        "aff_unique_norm": "United Imaging Intelligence;Temple University;Stony Brook University",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.temple.edu;https://www.stonybrook.edu",
        "aff_unique_abbr": ";Temple;SBU",
        "aff_campus_unique_index": "0+1;0;0;0;2;0",
        "aff_campus_unique": "Cambridge;Philadelphia;Stony Brook",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4d71926f19",
        "title": "Multi-Scale Patch-Based Representation Learning for Image Anomaly Detection and Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tsai_Multi-Scale_Patch-Based_Representation_Learning_for_Image_Anomaly_Detection_and_Segmentation_WACV_2022_paper.html",
        "author": "Chin-Chia Tsai; Tsung-Hsuan Wu; Shang-Hong Lai",
        "abstract": "Unsupervised representation learning has been proven to be effective for the challenging anomaly detection and segmentation tasks. In this paper, we propose a multi-scale patch-based representation learning method to extract critical and representative information from normal images. By taking the relative feature similarity between patches of different local distances into account, we can achieve better representation learning. Moreover, we propose a refined way to improve the self-supervised learning strategy, thus allowing our model to learn better geometric relationship between neighboring patches. Through sliding patches of different scales all over an image, our model extracts representative features from each patch and compares them with those in the training set of normal images to detect the anomalous regions. Our experimental results on MVTec AD dataset and BTAD dataset demonstrate the proposed method achieves the state-of-the-art accuracy for both anomaly detection and segmentation.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tsai_Multi-Scale_Patch-Based_Representation_Learning_for_Image_Anomaly_Detection_and_Segmentation_WACV_2022_paper.pdf",
        "aff": "Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan + Microsoft AI R&D Center, Taipei, Taiwan",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Tsai_Multi-Scale_Patch-Based_Representation_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3648071,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7347693917730108996&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "m108.nthu.edu.tw;mx.nthu.edu.tw;microsoft.com",
        "email": "m108.nthu.edu.tw;mx.nthu.edu.tw;microsoft.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "National Tsing Hua University;Microsoft",
        "aff_unique_dep": "Department of Computer Science;Microsoft AI R&D Center",
        "aff_unique_url": "https://www.nthu.edu.tw;https://www.microsoft.com",
        "aff_unique_abbr": "NTHU;Microsoft",
        "aff_campus_unique_index": "0;0;0+0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "cf2896f18f",
        "title": "Multi-Stream Dynamic Video Summarization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Elfeki_Multi-Stream_Dynamic_Video_Summarization_WACV_2022_paper.html",
        "author": "Mohamed Elfeki; Liqiang Wang; Ali Borji",
        "abstract": "With vast amounts of video content being uploaded to the Internet every minute, video summarization becomes critical for efficient browsing, searching, and indexing of visual content. Nonetheless, the spread of social and egocentric cameras creates an abundance of sparse scenarios captured by several devices, and ultimately required to be jointly summarized. In this paper, we discuss the problem of summarizing videos recorded independently by several dynamic cameras that intermittently share the field of view. We present a robust framework that (a) identifies a diverse set of important events among moving cameras that often are not capturing the same scene, and (b) selects the most representative view(s) at each event to be included in a universal summary. Due to the lack of an applicable alternative, we collected a new multi-view egocentric dataset, Multi-Ego. Our dataset is recorded simultaneously by three cameras, covering a wide variety of real-life scenarios. The footage is annotated by multiple individuals under various summarization configurations, with a consensus analysis ensuring a reliable ground truth. We conduct extensive experiments on the compiled dataset in addition to three other standard benchmarks that show the robustness and the advantage of our approach in both supervised and unsupervised settings. Additionally, we show that our approach learns collectively from data of varied number-of-views and orthogonal to other summarization methods, deeming it scalable and generic. Our materials will be made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Elfeki_Multi-Stream_Dynamic_Video_Summarization_WACV_2022_paper.pdf",
        "aff": "Microsoft; University of Central Florida; ",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Elfeki_Multi-Stream_Dynamic_Video_WACV_2022_supplemental.pdf",
        "arxiv": "1812.00108",
        "pdf_size": 928947,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9030004839052649643&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "microsoft.com;cs.ucf.edu;gmail.com",
        "email": "microsoft.com;cs.ucf.edu;gmail.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;University of Central Florida",
        "aff_unique_dep": "Microsoft Corporation;",
        "aff_unique_url": "https://www.microsoft.com;https://www.ucf.edu",
        "aff_unique_abbr": "Microsoft;UCF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5c6ec3779a",
        "title": "Multi-Task Classification of Sewer Pipe Defects and Properties Using a Cross-Task Graph Neural Network Decoder",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Haurum_Multi-Task_Classification_of_Sewer_Pipe_Defects_and_Properties_Using_a_WACV_2022_paper.html",
        "author": "Joakim Bruslund Haurum; Meysam Madadi; Sergio Escalera; Thomas B. Moeslund",
        "abstract": "The sewerage infrastructure is one of the most important and expensive infrastructures in modern society. In order to efficiently manage the sewerage infrastructure, automated sewer inspection has to be utilized. However, while sewer defect classification has been investigated for decades, little attention has been given to classifying sewer pipe properties such as water level, pipe material, and pipe shape, which are needed to evaluate the level of sewer pipe deterioration. In this work we classify sewer pipe defects and properties concurrently and present a novel decoder-focused multi-task classification architecture Cross-Task Graph Neural Network (CT-GNN), which refines the disjointed per-task predictions using cross-task information. The CT-GNN architecture extends the traditional disjointed task-heads decoder, by utilizing a cross-task graph and unique class node embeddings. The cross-task graph can either be determined a priori based on the conditional probability between the task classes or determined dynamically using self-attention. CT-GNN can be added to any backbone and trained end-to-end at a small increase in the parameter count. We achieve state-of-the-art performance on all four classification tasks in the Sewer-ML dataset, improving defect classification and water level classification by 5.3 and 8.0 percentage points, respectively. We also outperform the single task methods as well as other multi-task classification approaches while introducing 50 times fewer parameters than previous model-focused approaches. The code and models are available at the project page http://vap.aau.dk/ctgnn.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Haurum_Multi-Task_Classification_of_Sewer_Pipe_Defects_and_Properties_Using_a_WACV_2022_paper.pdf",
        "aff": "Visual Analysis and Perception (VAP) Laboratory, Aalborg University, Denmark; Computer Vision Center, Autonomous University of Barcelona, Spain; Computer Vision Center, Autonomous University of Barcelona, Spain + Dept. of Mathematics and Informatics, Universitat de Barcelona, Spain; Visual Analysis and Perception (VAP) Laboratory, Aalborg University, Denmark",
        "project": "http://vap.aau.dk/ctgnn",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Haurum_Multi-Task_Classification_of_WACV_2022_supplemental.pdf",
        "arxiv": "2111.07846",
        "pdf_size": 1703567,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17639398640200259815&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "create.aau.dk;cvc.uab.es;maia.ub.es;create.aau.dk",
        "email": "create.aau.dk;cvc.uab.es;maia.ub.es;create.aau.dk",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1+2;0",
        "aff_unique_norm": "Aalborg University;Autonomous University of Barcelona;Universitat de Barcelona",
        "aff_unique_dep": "Visual Analysis and Perception (VAP) Laboratory;Computer Vision Center;Dept. of Mathematics and Informatics",
        "aff_unique_url": "https://www.aau.dk;https://www.uab.cat;https://www.ub.edu",
        "aff_unique_abbr": "AAU;;UB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1+1;0",
        "aff_country_unique": "Denmark;Spain"
    },
    {
        "id": "9e4adf04a2",
        "title": "Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Fadadu_Multi-View_Fusion_of_Sensor_Data_for_Improved_Perception_and_Prediction_WACV_2022_paper.html",
        "author": "Sudeep Fadadu; Shreyash Pandey; Darshan Hegde; Yi Shi; Fang-Chieh Chou; Nemanja Djuric; Carlos Vallespi-Gonzalez",
        "abstract": "We present an end-to-end method for object detection and trajectory prediction utilizing multi-view representations of LiDAR returns. Our method builds on a state-of-the-art Bird's-Eye View (BEV) network that fuses voxelized features from a sequence of historical LiDAR data as well as rasterized high-definition map to perform detection and prediction tasks. We extend the BEV network with additional LiDAR Range-View (RV) features that use the raw LiDAR information in its native, non-quantized representation. The RV feature map is projected into BEV and fused with the BEV features computed from LiDAR and high-definition map. The fused features are then further processed to output the final detections and trajectories, within a single end-to-end trainable network. In addition, the RV fusion of LiDAR and camera is performed in a straightforward and computational efficient manner using this framework. The proposed approach improves the state-of-the-art on proprietary large-scale real-world data collected by a fleet of self-driving vehicles, as well as on the public nuScenes data set.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Fadadu_Multi-View_Fusion_of_Sensor_Data_for_Improved_Perception_and_Prediction_WACV_2022_paper.pdf",
        "aff": "Aurora Innovation; Aurora Innovation; Aurora Innovation; Aurora Innovation; Aurora Innovation; Aurora Innovation; Aurora Innovation",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2008.11901",
        "pdf_size": 1983286,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7697625986029250217&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "aurora.tech;aurora.tech;aurora.tech;aurora.tech;aurora.tech;aurora.tech;aurora.tech",
        "email": "aurora.tech;aurora.tech;aurora.tech;aurora.tech;aurora.tech;aurora.tech;aurora.tech",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Aurora Innovation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://aurora.tech",
        "aff_unique_abbr": "Aurora",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "62509faa6c",
        "title": "Multimodal Learning Using Optimal Transport for Sarcasm and Humor Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Pramanick_Multimodal_Learning_Using_Optimal_Transport_for_Sarcasm_and_Humor_Detection_WACV_2022_paper.html",
        "author": "Shraman Pramanick; Aniket Roy; Vishal M. Patel",
        "abstract": "Multimodal learning is an emerging yet challenging research area. In this paper, we deal with multimodal sarcasm and humor detection from conversational videos and image-text pairs. Being a fleeting action, which is dependent across the modalities, sarcasm detection is challenging since large datasets are not available for this task in the literature. Therefore, we primarily focus on resource-constrained training, where the number of training samples is limited. To this end, we propose a novel multimodal learning system, MuLOT (Multimodal Learning using Optimal Transport), which utilizes self-attention to exploit intra-modal correspondence and optimal transport for cross-modal correspondence. Finally, the modalities are combined with multimodal attention fusion to capture the inter-dependencies across modalities. We test our proposed approach for multimodal sarcasm and humor detection on three benchmark datasets - MUStARD (video, audio, text), UR-FUNNY (video, audio, text), MST (image, text) and obtain 2.1%, 1.54%, and 2.34% accuracy improvements over the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Pramanick_Multimodal_Learning_Using_Optimal_Transport_for_Sarcasm_and_Humor_Detection_WACV_2022_paper.pdf",
        "aff": "Johns Hopkins University; Johns Hopkins University; Johns Hopkins University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Pramanick_Multimodal_Learning_Using_WACV_2022_supplemental.pdf",
        "arxiv": "2110.10949",
        "pdf_size": 17931865,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16912214469570508668&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "jhu.edu;jhu.edu;jhu.edu",
        "email": "jhu.edu;jhu.edu;jhu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d095f3aa04",
        "title": "Mutual Learning of Joint and Separate Domain Alignments for Multi-Source Domain Adaptation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Xu_Mutual_Learning_of_Joint_and_Separate_Domain_Alignments_for_Multi-Source_WACV_2022_paper.html",
        "author": "Yuanyuan Xu; Meina Kan; Shiguang Shan; Xilin Chen",
        "abstract": "Multi-Source Domain Adaptation (MSDA) aims at transferring knowledge from multiple labeled source domains to benefit the task in an unlabeled target domain. The challenges of MSDA lie in mitigating domain gaps and combining information from diverse source domains. In most existing methods, the multiple source domains can be jointly or separately aligned to the target domain. In this work, we consider that these two types of methods, i.e. joint and separate domain alignments, are complementary and propose a mutual learning based alignment network (MLAN) to combine their advantages. Specifically, our proposed method is composed of three components, i.e. a joint alignment branch, a separate alignment branch, and a mutual learning objective between them. In the joint alignment branch, the samples from all source domains and the target domain are aligned together, with a single domain alignment goal, while in the separate alignment branch, each source domain is individually aligned to the target domain. Finally, by taking advantage of the complementarity of joint and separate domain alignment mechanisms, mutual learning is used to make the two branches learn collaboratively. Compared with other existing methods, our proposed MLAN integrates information of different domain alignment mechanisms and thus can mine rich knowledge from multiple domains for better performance. The experiments on DomainNet, Office-31, and Digits-five datasets demonstrate the effectiveness of our method.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Xu_Mutual_Learning_of_Joint_and_Separate_Domain_Alignments_for_Multi-Source_WACV_2022_paper.pdf",
        "aff": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China + University of Chinese Academy of Sciences, Beijing 100049, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China + University of Chinese Academy of Sciences, Beijing 100049, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China + University of Chinese Academy of Sciences, Beijing 100049, China + Peng Cheng Laboratory, Shenzhen, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China + University of Chinese Academy of Sciences, Beijing 100049, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Xu_Mutual_Learning_of_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 797905,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=323613542002877986&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1+2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Pengcheng Laboratory",
        "aff_unique_dep": "Institute of Computing Technology;;Peng Cheng Laboratory",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0;0+0;0+0+1;0+0",
        "aff_campus_unique": "Beijing;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "8dd0cdfc1a",
        "title": "NUTA: Non-Uniform Temporal Aggregation for Action Recognition",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Li_NUTA_Non-Uniform_Temporal_Aggregation_for_Action_Recognition_WACV_2022_paper.html",
        "author": "Xinyu Li; Chunhui Liu; Bing Shuai; Yi Zhu; Hao Chen; Joseph Tighe",
        "abstract": "In the world of action recognition research, one primary focus has been on how to construct and train networks to model the spatial-temporal volume of an input video. These methods typically uniformly sample a segment of an input clip (along the temporal dimension). However, not all parts of a video are equally important to determine the action in the clip. In this work, we focus instead on learning where to extract features, so as to focus on the most informative parts of the video. We propose a method called the non-uniform temporal aggregation (NUTA), which aggregates features only from informative temporal segments. We also introduce a synchronization method that allows our NUTA features to be temporally aligned with traditional uniformly sampled video features, so that both local and clip-level features can be combined. Our model has achieved state-of-the-art performance on four widely used large-scale action-recognition datasets (Kinetics400, Kinetics700, Something-something V2 and Charades). In addition, we have created a visualization to illustrate how the proposed NUTA method selects only the most relevant parts of a video clip.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Li_NUTA_Non-Uniform_Temporal_Aggregation_for_Action_Recognition_WACV_2022_paper.pdf",
        "aff": "Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Li_NUTA_Non-Uniform_Temporal_WACV_2022_supplemental.pdf",
        "arxiv": "2012.08041",
        "pdf_size": 4532028,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17578455764963167978&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon Web Services",
        "aff_unique_url": "https://aws.amazon.com",
        "aff_unique_abbr": "AWS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8e1826ec8c",
        "title": "Natural Language Video Moment Localization Through Query-Controlled Temporal Convolution",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zhang_Natural_Language_Video_Moment_Localization_Through_Query-Controlled_Temporal_Convolution_WACV_2022_paper.html",
        "author": "Lingyu Zhang; Richard J. Radke",
        "abstract": "The goal of natural language video moment localization is to locate a short segment of a long, untrimmed video that corresponds to a description presented as natural text. The description may contain several pieces of key information, including subjects/objects, sequential actions, and locations. Here, we propose a novel video moment localization framework based on the convolutional response between multimodal signals, i.e., the video sequence, the text query, and subtitles for the video if they are available. We emphasize the effect of the language sequence as a query about the video content, by converting the query sentence into a boundary detector with a filter kernel size and stride. We convolve the video sequence with the query detector to locate the start and end boundaries of the target video segment. When subtitles are available, we blend the boundary heatmaps from the visual and subtitle branches together using an LSTM to capture asynchronous dependencies across two modalities in the video. We perform extensive experiments on the TVR, Charades-STA, and TACoS benchmark datasets, demonstrating that our model achieves state-of-the-art results on all three.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zhang_Natural_Language_Video_Moment_Localization_Through_Query-Controlled_Temporal_Convolution_WACV_2022_paper.pdf",
        "aff": "Rensselaer Polytechnic Institute; Rensselaer Polytechnic Institute",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3316708,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=207759116389454215&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;ecse.rpi.edu",
        "email": "gmail.com;ecse.rpi.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rpi.edu",
        "aff_unique_abbr": "RPI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "83eeba759f",
        "title": "Network Generalization Prediction for Safety Critical Tasks in Novel Operating Domains",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/OBrien_Network_Generalization_Prediction_for_Safety_Critical_Tasks_in_Novel_Operating_WACV_2022_paper.html",
        "author": "Molly O'Brien; Mike Medoff; Julia Bukowski; Gregory D. Hager",
        "abstract": "It is well known that Neural Network (network) performance often degrades when a network is used in novel operating domains that differ from its training and testing domains. This is a major limitation, as networks are being integrated into safety critical, cyber-physical systems that must work in unconstrained environments, e.g., perception for autonomous vehicles. Training networks that generalize to novel operating domains and that extract robust features is an active area of research, but previous work fails to predict what the network performance will be in novel operating domains. We propose the task Network Generalization Prediction: predicting the expected network performance in novel operating domains. We describe the network performance in terms of an interpretable Context Subspace, and we propose a methodology for selecting the features of the Context Subspace that provide the most information about the network performance. We identify the Context Subspace for a pretrained Faster RCNN network performing pedestrian detection on the Berkeley Deep Drive (BDD) Dataset, and demonstrate Network Generalization Prediction accuracy within 5% of observed performance. We also demonstrate that the Context Subspace from the BDD Dataset is informative for completely unseen datasets, JAAD and Cityscapes, where predictions have a bias of 10% or less.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/OBrien_Network_Generalization_Prediction_for_Safety_Critical_Tasks_in_Novel_Operating_WACV_2022_paper.pdf",
        "aff": "Department of Computer Science, Johns Hopkins University, Baltimore MD 21218; exida LLC, Sellersville PA 18960; Department of Electrical and Computer Engineering, Villanova University, Villanova, PA 19085; Department of Computer Science, Johns Hopkins University, Baltimore MD 21218",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/OBrien_Network_Generalization_Prediction_WACV_2022_supplemental.pdf",
        "arxiv": "2108.07399",
        "pdf_size": 8587624,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7693880591872003771&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "jhu.edu;exida.com;villanova.edu;cs.jhu.edu",
        "email": "jhu.edu;exida.com;villanova.edu;cs.jhu.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Johns Hopkins University;exida LLC;Villanova University",
        "aff_unique_dep": "Department of Computer Science;;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.jhu.edu;;https://www.villanova.edu",
        "aff_unique_abbr": "JHU;;Villanova",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Baltimore;;Villanova",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a0f54ce528",
        "title": "Neural Architecture Search for Efficient Uncalibrated Deep Photometric Stereo",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sarno_Neural_Architecture_Search_for_Efficient_Uncalibrated_Deep_Photometric_Stereo_WACV_2022_paper.html",
        "author": "Francesco Sarno; Suryansh Kumar; Berk Kaya; Zhiwu Huang; Vittorio Ferrari; Luc Van Gool",
        "abstract": "We present an automated machine learning approach for uncalibrated photometric stereo (PS). Our work aims at discovering a light and computationally efficient PS neural network with excellent surface normal accuracy. Unlike previous uncalibrated deep PS networks, which are handcrafted and carefully tuned, we leverage the recent differentiable neural architecture search (NAS) strategy to find uncalibrated PS architecture automatically. We begin by defining a discrete search space for a light calibration network and a normal estimation network, respectively. We then perform a continuous relaxation of this search space, and present a gradient-based optimization strategy to find an efficient light calibration and normal estimation network. Directly applying the NAS methodology to uncalibrated PS is not straightforward as certain mathematical constraints must be satisfied, which we impose explicitly. Moreover, we search for and train the two networks separately to account for the Generalized Bas Relief (GBR) ambiguity. Extensive experiments on the DiLiGenT benchmark show that the automatically searched neural architectures outperform the current state-of-the-art uncalibrated PS methods, while having a lower memory footprint.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sarno_Neural_Architecture_Search_for_Efficient_Uncalibrated_Deep_Photometric_Stereo_WACV_2022_paper.pdf",
        "aff": "Computer Vision Lab, ETH Z\u00fcrich; Computer Vision Lab, ETH Z\u00fcrich; Computer Vision Lab, ETH Z\u00fcrich; Computer Vision Lab, ETH Z\u00fcrich; Google Research; Computer Vision Lab, ETH Z\u00fcrich+KU Leuven",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Sarno_Neural_Architecture_Search_WACV_2022_supplemental.pdf",
        "arxiv": "2110.05621",
        "pdf_size": 1774470,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=533999208725250089&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;0+2",
        "aff_unique_norm": "ETH Zurich;Google;Katholieke Universiteit Leuven",
        "aff_unique_dep": "Computer Vision Lab;Google Research;",
        "aff_unique_url": "https://www.ethz.ch;https://research.google;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;Google Research;KU Leuven",
        "aff_campus_unique_index": "0;0;0;0;1;0",
        "aff_campus_unique": "Z\u00fcrich;Mountain View;",
        "aff_country_unique_index": "0;0;0;0;1;0+2",
        "aff_country_unique": "Switzerland;United States;Belgium"
    },
    {
        "id": "7d461c3a68",
        "title": "Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kaya_Neural_Radiance_Fields_Approach_to_Deep_Multi-View_Photometric_Stereo_WACV_2022_paper.html",
        "author": "Berk Kaya; Suryansh Kumar; Francesco Sarno; Vittorio Ferrari; Luc Van Gool",
        "abstract": "We present a modern solution to the multi-view photometric stereo problem (MVPS). Our work suitably exploits the image formation model in a MVPS experimental setup to recover the dense 3D reconstruction of an object from images. We procure the surface orientation using a photometric stereo (PS) image formation model and blend it with a multi-view neural radiance field representation to recover the object's surface geometry. Contrary to the previous multi-staged framework to MVPS, where the position, iso-depth contours, or orientation measurements are estimated independently and then fused later, our method is simple to implement and realize. Our method performs neural rendering of multi-view images while utilizing surface normals estimated by a deep photometric stereo network. We render the MVPS images by considering the object's surface normals for each 3D sample point along the viewing direction rather than explicitly using the density gradient in the volume space via 3D occupancy information. We optimize the proposed neural radiance field representation for the MVPS setup efficiently using a fully connected deep network to recover the 3D geometry of an object. Extensive evaluation on the DiLiGenT-MV benchmark dataset shows that our method performs better than the approaches that perform only PS or only multi-view stereo (MVS) and provides comparable results against the state-of-the-art multi-stage fusion methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kaya_Neural_Radiance_Fields_Approach_to_Deep_Multi-View_Photometric_Stereo_WACV_2022_paper.pdf",
        "aff": "Computer Vision Lab, ETH Z\u00fcrich; Computer Vision Lab, ETH Z\u00fcrich; Computer Vision Lab, ETH Z\u00fcrich; Google Research; Computer Vision Lab, ETH Z\u00fcrich+KU Leuven",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Kaya_Neural_Radiance_Fields_WACV_2022_supplemental.zip",
        "arxiv": "2110.05594",
        "pdf_size": 1829459,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4342288274035965769&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0+2",
        "aff_unique_norm": "ETH Zurich;Google;Katholieke Universiteit Leuven",
        "aff_unique_dep": "Computer Vision Lab;Google Research;",
        "aff_unique_url": "https://www.ethz.ch;https://research.google;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;Google Research;KU Leuven",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Z\u00fcrich;Mountain View;",
        "aff_country_unique_index": "0;0;0;1;0+2",
        "aff_country_unique": "Switzerland;United States;Belgium"
    },
    {
        "id": "9557ea0216",
        "title": "No-Reference Image Quality Assessment via Transformers, Relative Ranking, and Self-Consistency",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Golestaneh_No-Reference_Image_Quality_Assessment_via_Transformers_Relative_Ranking_and_Self-Consistency_WACV_2022_paper.html",
        "author": "S. Alireza Golestaneh; Saba Dadsetan; Kris M. Kitani",
        "abstract": "The goal of No-Reference Image Quality Assessment (NR-IQA) is to estimate the perceptual image quality in accordance with subjective evaluations, it is a complex and unsolved problem due to the absence of the pristine reference image. In this paper, we propose a novel model to address the NR-IQA task by leveraging a hybrid approach that benefits from Convolutional Neural Networks (CNNs) and self-attention mechanism in Transformers to extract both local and non-local features from the input image. We capture local structure information of the image via CNNs, then to circumvent the locality bias among the extracted CNNs features and obtain a non-local representation of the image, we utilize Transformers on the extracted features where we model them as a sequential input to the Transformer model. Furthermore, to improve the monotonicity correlation between the subjective and objective scores, we utilize the relative distance information among the images within each batch and enforce the relative ranking among them. Last but not least, we observe that the performance of NR-IQA models degrades when we apply equivariant transformations (e.g. horizontal flipping) to the inputs. Therefore, we propose a method that leverages self-consistency as a source of self-supervision to improve the robustness of NR-IQA models. Specifically, we enforce self-consistency between the outputs of our quality assessment model for each image and its transformation (horizontally flipped) to utilize the rich self-supervisory information and reduce the uncertainty of the model. To demonstrate the effectiveness of our work, we evaluate it on seven standard IQA datasets (both synthetic and authentic) and show that our model achieves state-of-the-art results on various datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Golestaneh_No-Reference_Image_Quality_Assessment_via_Transformers_Relative_Ranking_and_Self-Consistency_WACV_2022_paper.pdf",
        "aff": "Carnegie Mellon University*; University of Pittsburgh; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2108.06858",
        "pdf_size": 4271315,
        "gs_citation": 376,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15793710510316537095&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cmu.edu;pitt.edu;andrew.cmu.edu",
        "email": "cmu.edu;pitt.edu;andrew.cmu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;University of Pittsburgh",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.pitt.edu",
        "aff_unique_abbr": "CMU;Pitt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6e798d4cbd",
        "title": "Non-Blind Deblurring for Fluorescence: A Deformable Latent Space Approach With Kernel Parameterization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Guan_Non-Blind_Deblurring_for_Fluorescence_A_Deformable_Latent_Space_Approach_With_WACV_2022_paper.html",
        "author": "Ziqiao Guan; Esther H. R. Tsai; Xiaojing Huang; Kevin G. Yager; Hong Qin",
        "abstract": "Non-blind deblurring (NBD) is a modeling method of the image deblurring problem in computer vision, where the blurring kernel is known or can be externally estimated. In this paper, we attempt to solve a parametric NBD problem, inspired by the simultaneous acquisition of ptychography and fluorescent imaging (FI). Ptychography is an imaging method that favors larger probes, i.e. convolutional kernels, while FI relies on a small probe for high resolution. Also, the kernel can be solved during ptychographic reconstruction. With Ptycho-FI using the same larger kernel, we can perform NBD on the blurred fluorescent images to achieve high-resolution FI, and thus speed up the experiments. To this end, we design a deep latent space deformation network that is directly parameterized by the kernel. The network consists of three components: encoder, deformer, and decoder, where the deformer is specifically meant to rectify the latent space representations of blurred images to a standard latent space, regardless of the kernel. The deformation network is trained with a two-stage training scheme. We conduct extensive experiments to confirm that our parametric model can adapt to drastically different blurring kernels and perform robust deblurring.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Guan_Non-Blind_Deblurring_for_Fluorescence_A_Deformable_Latent_Space_Approach_With_WACV_2022_paper.pdf",
        "aff": "Stony Brook University; Brookhaven National Laboratory; Brookhaven National Laboratory; Brookhaven National Laboratory; Stony Brook University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Guan_Non-Blind_Deblurring_for_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5063090,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3136854147551304793&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cs.stonybrook.edu;bnl.gov;bnl.gov;bnl.gov;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;bnl.gov;bnl.gov;bnl.gov;cs.stonybrook.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Stony Brook University;Brookhaven National Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stonybrook.edu;https://www.bnl.gov",
        "aff_unique_abbr": "SBU;BNL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4765684df9",
        "title": "Non-Local Attention Improves Description Generation for Retinal Images",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Huang_Non-Local_Attention_Improves_Description_Generation_for_Retinal_Images_WACV_2022_paper.html",
        "author": "Jia-Hong Huang; Ting-Wei Wu; C.-H. Huck Yang; Zenglin Shi; I-Hung Lin; Jesper Tegner; Marcel Worring",
        "abstract": "Automatically generating medical reports from retinal images is a difficult task in which an algorithm must generate semantically coherent descriptions for a given retinal image. Existing methods mainly rely on the input image to generate descriptions. However, many abstract medical concepts or descriptions cannot be generated based on image information only. In this work, we integrate additional information to help solve this task; we observe that early in the diagnosis process, ophthalmologists have usually written down a small set of keywords denoting important information. These keywords are then subsequently used to aid the later creation of medical reports for a patient. Since these keywords commonly exist and are useful for generating medical reports, we incorporate them into automatic report generation. Since we have two types of inputs - expert-defined unordered keywords and images - effectively fusing features from these different modalities is challenging. To that end, we propose a new keyword-driven medical report generation method based on a non-local attention-based multi-modal feature fusion approach, TransFuser, which is capable of fusing features from different types of inputs based on such attention. Our experiments show the proposed method successfully captures the mutual information of keywords and image content. We further show our proposed keyword-driven generation model reinforced by the TransFuser is superior to baselines under the popular text evaluation metrics BLEU, CIDEr, and ROUGE. TransFuser Github:https://github.com/Jhhuangkay/Non-local-Attention-Improves-Description-Generation-for-Retinal-Images.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Huang_Non-Local_Attention_Improves_Description_Generation_for_Retinal_Images_WACV_2022_paper.pdf",
        "aff": "University of Amsterdam; Georgia Institute of Technology; Georgia Institute of Technology; University of Amsterdam; Department of Ophthalmology, Tri-Service General Hospital, National Defense Medical Center, Taiwan; King Abdullah University of Science and Technology (KAUST); University of Amsterdam",
        "project": "",
        "github": "https://github.com/Jhhuangkay/Non-local-Attention-Improves-Description-Generation-for-Retinal-Images",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3948024,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15975515330623838025&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "uva.nl; ; ; ; ; ; ",
        "email": "uva.nl; ; ; ; ; ; ",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;2;3;0",
        "aff_unique_norm": "University of Amsterdam;Georgia Institute of Technology;National Defense Medical Center;King Abdullah University of Science and Technology",
        "aff_unique_dep": ";;Department of Ophthalmology;",
        "aff_unique_url": "https://www.uva.nl;https://www.gatech.edu;https://www.ndmc.edu.tw;https://www.kaust.edu.sa",
        "aff_unique_abbr": "UvA;Georgia Tech;NDMC;KAUST",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Taiwan",
        "aff_country_unique_index": "0;1;1;0;2;3;0",
        "aff_country_unique": "Netherlands;United States;China;Saudi Arabia"
    },
    {
        "id": "212ed00b9d",
        "title": "Non-Semantic Evaluation of Image Forensics Tools: Methodology and Database",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bammey_Non-Semantic_Evaluation_of_Image_Forensics_Tools_Methodology_and_Database_WACV_2022_paper.html",
        "author": "Quentin Bammey; Tina Nikoukhah; Marina Gardella; Rafael Grompone von Gioi; Miguel Colom; Jean-Michel Morel",
        "abstract": "We propose a new method to evaluate image forensics tools, that characterizes what image cues are being used by each detector. Our method enables effortless creation of an arbitrarily large dataset of carefully tampered images in which controlled detection cues are present. Starting with raw images, we alter aspects of the image formation pipeline inside a mask, while leaving the rest of the image intact. This does not change the image's interpretation; we thus call such alterations \"non-semantic\", as they yield no semantic inconsistencies. This method avoids the painful and often biased creation of convincing semantics. All aspects of image formation (noise, CFA, compression pattern and quality, etc.) can vary independently in both the authentic and tampered parts of the image. Alteration of a specific cue enables precise evaluation of the many forgery detectors that rely on this cue, and of the sensitivity of more generic forensic tools to each specific trace of forgery, and can be used to guide the combination of different methods. Based on this methodology, we create a database and conduct an evaluation of the main state-of-the-art image forensics tools, where we characterize the performance of each method with respect to each detection cue. Check qbammey.github.io/trace for the database and code.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bammey_Non-Semantic_Evaluation_of_Image_Forensics_Tools_Methodology_and_Database_WACV_2022_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Bammey_Non-Semantic_Evaluation_of_WACV_2022_supplemental.pdf",
        "arxiv": "2105.02700",
        "pdf_size": 4892518,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16304307392763323854&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "67af49550b",
        "title": "Nonnegative Low-Rank Tensor Completion via Dual Formulation With Applications to Image and Video Completion",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sinha_Nonnegative_Low-Rank_Tensor_Completion_via_Dual_Formulation_With_Applications_to_WACV_2022_paper.html",
        "author": "Tanmay Kumar Sinha; Jayadev Naram; Pawan Kumar",
        "abstract": "Recent approaches to the tensor completion problem have often overlooked the nonnegative structure of the data. We consider the problem of learning a nonnegative low-rank tensor, and using duality theory, we propose a novel factorization of such tensors. The factorization decouples the nonnegative constraints from the low-rank constraints. The resulting problem is an optimization problem on manifolds, and we propose a variant of Riemannian conjugate gradients to solve it. We test the proposed algorithm across various tasks such as colour image inpainting, video completion, and hyperspectral image completion. Experimental results show that the proposed method outperforms many state-of-the-art tensor completion algorithms.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sinha_Nonnegative_Low-Rank_Tensor_Completion_via_Dual_Formulation_With_Applications_to_WACV_2022_paper.pdf",
        "aff": "IIIT, Hyderabad; IIIT, Hyderabad; IIIT, Hyderabad",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8922679,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10527595236574967206&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "International Institute of Information Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": "IIIT-H",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "3a82018d79",
        "title": "Normalizing Flow as a Flexible Fidelity Objective for Photo-Realistic Super-Resolution",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lugmayr_Normalizing_Flow_as_a_Flexible_Fidelity_Objective_for_Photo-Realistic_Super-Resolution_WACV_2022_paper.html",
        "author": "Andreas Lugmayr; Martin Danelljan; Fisher Yu; Luc Van Gool; Radu Timofte",
        "abstract": "Super-resolution is an ill-posed problem, where a ground-truth high-resolution image represents only one possibility in the space of plausible solutions. Yet, the dominant paradigm is to employ pixel-wise losses, such as L_1, which drive the prediction towards a blurry average. This leads to fundamentally conflicting objectives when combined with adversarial losses, which degrades the final quality. We address this issue by revisiting the L_1 loss and show that it corresponds to a one-layer conditional flow. Inspired by this relation, we explore general flows as a fidelity-based alternative to the L_1 objective. We demonstrate that the flexibility of deeper flows leads to better visual quality and consistency when combined with adversarial losses. We conduct extensive user studies for three datasets and scale factors, where our approach is shown to outperform state-of-the-art methods for photo-realistic super-resolution.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lugmayr_Normalizing_Flow_as_a_Flexible_Fidelity_Objective_for_Photo-Realistic_Super-Resolution_WACV_2022_paper.pdf",
        "aff": "CVL, ETH Z\u00fcrich, Switzerland; CVL, ETH Z\u00fcrich, Switzerland; CVL, ETH Z\u00fcrich, Switzerland; CVL, ETH Z\u00fcrich, Switzerland; CVL, ETH Z\u00fcrich, Switzerland",
        "project": "",
        "github": "git.io/AdFlow",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Lugmayr_Normalizing_Flow_as_WACV_2022_supplemental.pdf",
        "arxiv": "2111.03649",
        "pdf_size": 5709502,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9054662940362972633&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "CVL",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "199a953a6c",
        "title": "Novel Ensemble Diversification Methods for Open-Set Scenarios",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Farber_Novel_Ensemble_Diversification_Methods_for_Open-Set_Scenarios_WACV_2022_paper.html",
        "author": "Miriam Farber; Roman Goldenberg; George Leifman; Gal Novich",
        "abstract": "We revisit existing ensemble diversification approaches and present two novel diversification methods tailored for open-set scenarios. The first method uses a new loss, designed to encourage models disagreement on outliers only, thus alleviating the intrinsic accuracy-diversity trade-off. The second method achieves diversity via automated feature engineering, by training each model to disregard input features learned by previously trained ensemble models. We conduct an extensive evaluation and analysis of the proposed techniques on seven datasets that cover image classification, re-identification and recognition domains. We compare to and demonstrate accuracy improvements over the existing state-of-the-art ensemble diversification methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Farber_Novel_Ensemble_Diversification_Methods_for_Open-Set_Scenarios_WACV_2022_paper.pdf",
        "aff": "Amazon; Google Research\u2217; Google Research\u2217; Amazon",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Farber_Novel_Ensemble_Diversification_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 712884,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16309822203956416510&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "amazon.com;google.com;google.com;amazon.com",
        "email": "amazon.com;google.com;google.com;amazon.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Amazon;Google",
        "aff_unique_dep": "Amazon.com, Inc.;Google Research",
        "aff_unique_url": "https://www.amazon.com;https://research.google",
        "aff_unique_abbr": "Amazon;Google Research",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1fadf0f6b2",
        "title": "Novel-View Synthesis of Human Tourist Photos",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Freer_Novel-View_Synthesis_of_Human_Tourist_Photos_WACV_2022_paper.html",
        "author": "Jonathan Freer; Kwang Moo Yi; Wei Jiang; Jongwon Choi; Hyung Jin Chang",
        "abstract": "We present a novel framework for performing novel-view synthesis on human tourist photos. Given a tourist photo from a known scene, we reconstruct the photo in 3D space through modeling the human and the background independently. We generate a deep buffer from a novel view point of the reconstruction and utilize a deep network to translate the buffer into a photo realistic rendering of the novel view. We additionally present a method to relight the renderings, allowing for relighting of both human and background to match either the provided input image or any other. The key contributions of our paper are: 1) a framework for performing novel view synthesis on human tourist photos, 2) an appearance transfer method for relighting of humans to match synthesized backgrounds, and 3) a method for estimating lighting properties from a single human photo.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Freer_Novel-View_Synthesis_of_Human_Tourist_Photos_WACV_2022_paper.pdf",
        "aff": "School of Computer Science, University of Birmingham; University of British Columbia; University of British Columbia; Department of Advanced Imaging, Chung-Ang University; School of Computer Science, University of Birmingham",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9584413,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12780017564903283394&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "student.bham.ac.uk;cs.ubc.ca;cs.ubc.ca;cau.ac.kr;bham.ac.uk",
        "email": "student.bham.ac.uk;cs.ubc.ca;cs.ubc.ca;cau.ac.kr;bham.ac.uk",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2;0",
        "aff_unique_norm": "University of Birmingham;University of British Columbia;Chung-Ang University",
        "aff_unique_dep": "School of Computer Science;;Department of Advanced Imaging",
        "aff_unique_url": "https://www.birmingham.ac.uk;https://www.ubc.ca;http://www.cau.ac.kr",
        "aff_unique_abbr": "UoB;UBC;CAU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Birmingham;",
        "aff_country_unique_index": "0;1;1;2;0",
        "aff_country_unique": "United Kingdom;Canada;South Korea"
    },
    {
        "id": "d67b6655c3",
        "title": "Occlusion Resistant Network for 3D Face Reconstruction",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tiwari_Occlusion_Resistant_Network_for_3D_Face_Reconstruction_WACV_2022_paper.html",
        "author": "Hitika Tiwari; Vinod K. Kurmi; K.S. Venkatesh; Yong-Sheng Chen",
        "abstract": "3D face reconstruction from a monocular face image is a mathematically ill-posed problem. Recently, we observed a surge of interest in deep learning-based approaches to address the issue. These methods possess extreme sensitivity towards occlusions. Thus, in this paper, we present a novel context-learning-based distillation approach to tackle the occlusions in the face images. Our training pipeline focuses on distilling the knowledge from a pre-trained occlusion-sensitive deep network. The proposed model learns the context of the target occluded face image. Hence our approach uses a weak model (unsuitable for occluded face images) to train a highly robust network towards partially and fully-occluded face images. We obtain a landmark accuracy of 0.77 against 5.84 of recent state-of-the-art-method for real-life challenging facial occlusions. Also, we propose a novel end-to-end training pipeline to reconstruct 3D faces from multiple variations of the target image per identity to emphasize the significance of visible facial features during learning. For this purpose, we leverage a novel composite multi-occlusion loss function. Our multi-occlusion per identity model shows a dip in the landmark error by a large margin of 6.67 in comparison to a recent state-of-the-art method. We deploy the occluded variations of the CelebA validation dataset and AFLW2000-3D face dataset: naturally-occluded and artificially occluded, for the comparisons. We comprehensively compare our results with the other approaches concerning the accuracy of the reconstructed 3D face mesh for occluded face images.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tiwari_Occlusion_Resistant_Network_for_3D_Face_Reconstruction_WACV_2022_paper.pdf",
        "aff": "National Yang Ming Chiao Tung University, Taiwan+Indian Institute of Technology Kanpur, India; KU Leuven, Belgium; Indian Institute of Technology Kanpur, India; National Yang Ming Chiao Tung University, Taiwan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10656683,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2905535277260059356&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iitk.ac.in;kuleuven.be;iitk.ac.in;cs.nycu.edu.tw",
        "email": "iitk.ac.in;kuleuven.be;iitk.ac.in;cs.nycu.edu.tw",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;1;0",
        "aff_unique_norm": "National Yang Ming Chiao Tung University;Indian Institute of Technology Kanpur;KU Leuven",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nycu.edu.tw;https://www.iitk.ac.in;https://www.kuleuven.be",
        "aff_unique_abbr": "NYCU;IIT Kanpur;KU Leuven",
        "aff_campus_unique_index": "0+1;1;0",
        "aff_campus_unique": "Taiwan;Kanpur;",
        "aff_country_unique_index": "0+1;2;1;0",
        "aff_country_unique": "China;India;Belgium"
    },
    {
        "id": "61090c4b41",
        "title": "Occlusion-Robust Object Pose Estimation With Holistic Representation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chen_Occlusion-Robust_Object_Pose_Estimation_With_Holistic_Representation_WACV_2022_paper.html",
        "author": "Bo Chen; Tat-Jun Chin; Marius Klimavicius",
        "abstract": "Practical object pose estimation demands robustness against occlusions to the target object. State-of-the-art (SOTA) object pose estimators take a two-stage approach, where the first stage predicts 2D landmarks using a deep network and the second stage solves for 6DOF pose from 2D-3D correspondences. Albeit widely adopted, such two-stage approaches could suffer from novel occlusions when generalising and weak landmark coherence due to disrupted features. To address these issues, we develop a novel occlude-and-blackout batch augmentation technique to learn occlusion-robust deep features, and a multi-precision supervision architecture to encourage holistic pose representation learning for accurate and coherent landmark predictions. We perform careful ablation tests to verify the impact of our innovations and compare our method to SOTA pose estimators. Without the need of any post-processing or refinement, our method exhibits superior performance on the LINEMOD dataset. On the YCB-Video dataset our method outperforms all non-refinement methods in terms of the ADD(-S) metric. We also demonstrate the high data-efficiency of our method. Our code is available at http://github.com/BoChenYS/ROPE",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chen_Occlusion-Robust_Object_Pose_Estimation_With_Holistic_Representation_WACV_2022_paper.pdf",
        "aff": "The University of Adelaide; The University of Adelaide; Blackswan Technologies",
        "project": "",
        "github": "http://github.com/BoChenYS/ROPE",
        "supp": "",
        "arxiv": "2110.11636",
        "pdf_size": 9785433,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10674981033316629642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;blackswan.ltd",
        "email": "adelaide.edu.au;adelaide.edu.au;blackswan.ltd",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Adelaide;Blackswan Technologies",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.adelaide.edu.au;https://www.blackswan.technology",
        "aff_unique_abbr": "Adelaide;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "ac59c9fcca",
        "title": "On Black-Box Explanation for Face Verification",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Mery_On_Black-Box_Explanation_for_Face_Verification_WACV_2022_paper.html",
        "author": "Domingo Mery; Bernardita Morris",
        "abstract": "Given a facial matcher, in explainable face verification, the task is to answer: how relevant are the parts of a probe image to establish the matching with an enrolled image. In many cases, however, the trained models cannot be manipulated and must be treated as \"black-boxes\". In this paper, we present six different saliency maps that can be used to explain any face verification algorithm with no manipulation inside of the face recognition model. The key idea of the methods is based on how the matching score of the two face images changes when the probe is perturbed. The proposed methods remove and aggregate different parts of the face, and measure contributions of these parts individually and in-collaboration as well. We test and compare our proposed methods in three different scenarios: synthetic images with different qualities and occlusions, real face images with different facial expressions, poses, and occlusions and faces from different demographic groups. In our experiments, five different face verification algorithms are used: ArcFace, Dlib, FaceNet (trained on VGGface2 and Casia-WebFace), and LBP. We conclude that one of the proposed methods achieves saliency maps that are stable and interpretable to humans. In addition, our method, in combination with a new visualization of saliency maps based on contours, shows promising results in comparison with other state-of-the-art art methods. This paper presents good insights into any face verification algorithm, in which it can be clearly appreciated which are the most relevant face areas that an algorithm takes into account to carry out the recognition process.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Mery_On_Black-Box_Explanation_for_Face_Verification_WACV_2022_paper.pdf",
        "aff": "Department of Computer Science, Pontificia Universidad Cat\u00f3lica de Chile; Department of Computer Science, Pontificia Universidad Cat\u00f3lica de Chile",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5145289,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9792939754341897220&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "uc.cl;uc.cl",
        "email": "uc.cl;uc.cl",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Pontificia Universidad Cat\u00f3lica de Chile",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.puc.cl",
        "aff_unique_abbr": "PUC Chile",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Chile"
    },
    {
        "id": "d04a0a75ec",
        "title": "On the Effectiveness of Small Input Noise for Defending Against Query-Based Black-Box Attacks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Byun_On_the_Effectiveness_of_Small_Input_Noise_for_Defending_Against_WACV_2022_paper.html",
        "author": "Junyoung Byun; Hyojun Go; Changick Kim",
        "abstract": "While deep neural networks show unprecedented performance in various tasks, the vulnerability to adversarial examples hinders their deployment in safety-critical systems. Many studies have shown that attacks are also possible even in a black-box setting where an adversary cannot access the target model's internal information. Most black-box attacks are based on queries, each of which obtains the target model's output for an input, and many recent studies focus on reducing the number of required queries. In this paper, we pay attention to an implicit assumption of query-based black-box adversarial attacks that the target model's output exactly corresponds to the query input. If some randomness is introduced into the model, it can break the assumption, and thus, query-based attacks may have tremendous difficulty in both gradient estimation and local search, which are the core of their attack process. From this motivation, we observe even a small additive input noise can neutralize most query-based attacks and name this simple yet effective approach Small Noise Defense (SND). We analyze how SND can defend against query-based black-box attacks and demonstrate its effectiveness against eight state-of-the-art attacks with CIFAR-10 and ImageNet datasets. Even with strong defense ability, SND almost maintains the original classification accuracy and computational speed. SND is readily applicable to pre-trained models by adding only one line of code at the inference.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Byun_On_the_Effectiveness_of_Small_Input_Noise_for_Defending_Against_WACV_2022_paper.pdf",
        "aff": "Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST)",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Byun_On_the_Effectiveness_WACV_2022_supplemental.pdf",
        "arxiv": "2101.04829",
        "pdf_size": 694271,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17856317072136349615&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "cb7d5eaa77",
        "title": "On the Maximum Radius of Polynomial Lens Distortion",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Leotta_On_the_Maximum_Radius_of_Polynomial_Lens_Distortion_WACV_2022_paper.html",
        "author": "Matthew J. Leotta; David Russell; Andrew Matrai",
        "abstract": "Polynomial radial lens distortion models are widely used in image processing and computer vision applications to compensate for when straight lines in the world appear curved in an image. While polynomial models are used pervasively in software ranging from PhotoShop to OpenCV to Blender, they have an often overlooked behavior: polynomial models can fold back onto themselves. This property often goes unnoticed when simply warping to undistort an image. However, in applications such as augmented reality where 3D scene geometry is projected and distorted to overlay an image, this folding can result in a surprising behavior. Points well outside the field of view can project into the middle of the image. The domain of a radial distortion model is only valid up to some (possibly infinite) maximum radius where this folding occurs. This paper derives the closed form expression for the maximum valid radius and demonstrates how this value can be used to filter invalid projections or validate the range of an estimated lens model. Experiments on the popular Lensfun database demonstrate that this folding problem exists on 30% of lens models used in the wild.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Leotta_On_the_Maximum_Radius_of_Polynomial_Lens_Distortion_WACV_2022_paper.pdf",
        "aff": "Kitware, Inc.; Carnegie Mellon University + Kitware, Inc.; Kitware, Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8157288,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2416359398783846233&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "kitware.com;andrew.cmu.edu;kitware.com",
        "email": "kitware.com;andrew.cmu.edu;kitware.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Kitware, Inc.;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kitware.com;https://www.cmu.edu",
        "aff_unique_abbr": "Kitware;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "74693d288d",
        "title": "One-Class Learned Encoder-Decoder Network With Adversarial Context Masking for Novelty Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Jewell_One-Class_Learned_Encoder-Decoder_Network_With_Adversarial_Context_Masking_for_Novelty_WACV_2022_paper.html",
        "author": "John Taylor Jewell; Vahid Reza Khazaie; Yalda Mohsenzadeh",
        "abstract": "Novelty detection is the task of recognizing samples that do not belong to the distribution of the target class. During training, the novelty class is absent, preventing the use of traditional classification approaches. Deep autoencoders have been widely used as a base of many novelty detection methods. In particular, context autoencoders have been successful in the novelty detection task because of the more effective representations they learn by reconstructing original images from randomly masked images. However, a significant drawback of context autoencoders is that random masking fails to consistently cover important structures of the input image, leading to suboptimal representations - especially for the novelty detection task. In this paper, to optimize input masking, we introduce a Mask Module that learns to generate optimal masks and a Reconstructor that aims to reconstruct masked images. The networks are trained in an adversarial setting in which the Mask Module seeks to maximize the reconstruction error that the Reconstructor is minimizing. When applied to novelty detection, the proposed approach learns semantically richer representations compared to context autoencoders and enhances novelty detection at test time through more optimal masking. Novelty detection experiments on the MNIST and CIFAR-10 image datasets demonstrate the proposed approach's superiority over cutting-edge methods. In a further experiment on the UCSD video dataset for novelty detection, the proposed approach achieves a frame-level Area Under the Curve (AUC) of 99.02% and an Equal Error Rate (EER) of 5.4%, exceeding recent state-of-the-art models.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Jewell_One-Class_Learned_Encoder-Decoder_Network_With_Adversarial_Context_Masking_for_Novelty_WACV_2022_paper.pdf",
        "aff": "Western University; Western University; Western University",
        "project": "",
        "github": "https://github.com/jewelltaylor/OLED",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Jewell_One-Class_Learned_Encoder-Decoder_WACV_2022_supplemental.pdf",
        "arxiv": "2103.14953",
        "pdf_size": 1502053,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5282089190181522706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uwo.ca;uwo.ca;uwo.ca",
        "email": "uwo.ca;uwo.ca;uwo.ca",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Western University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uwo.ca",
        "aff_unique_abbr": "Western",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "9aad3eeba3",
        "title": "One-Shot Compositional Data Generation for Low Resource Handwritten Text Recognition",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Souibgui_One-Shot_Compositional_Data_Generation_for_Low_Resource_Handwritten_Text_Recognition_WACV_2022_paper.html",
        "author": "Mohamed Ali Souibgui; Ali Furkan Biten; Sounak Dey; Alicia Forn\u00e9s; Yousri Kessentini; Llu\u00eds G\u00f3mez; Dimosthenis Karatzas; Josep Llad\u00f3s",
        "abstract": "Low resource Handwritten Text Recognition (HTR) is a hard problem due to the scarce annotated data and the very limited linguistic information (dictionaries and language models). For example, in the case of historical ciphered manuscripts, which are usually written with invented alphabets to hide the message contents. Thus, in this paper we address this problem through a data generation technique based on Bayesian Program Learning (BPL). Contrary to traditional generation approaches, which require a huge amount of annotated images, our method is able to generate human-like handwriting using only one sample of each symbol in the alphabet. After generating symbols, we create synthetic lines to train state-of-the-art HTR architectures in a segmentation free fashion. Quantitative and qualitative analyses were carried out and confirm the effectiveness of the proposed method.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Souibgui_One-Shot_Compositional_Data_Generation_for_Low_Resource_Handwritten_Text_Recognition_WACV_2022_paper.pdf",
        "aff": ";;;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Souibgui_One-Shot_Compositional_Data_WACV_2022_supplemental.pdf",
        "arxiv": "2105.05300",
        "pdf_size": 718793,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13426413445349214981&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "author_num": 8,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9b1909ae23",
        "title": "Online Continual Learning via Candidates Voting",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/He_Online_Continual_Learning_via_Candidates_Voting_WACV_2022_paper.html",
        "author": "Jiangpeng He; Fengqing Zhu",
        "abstract": "Continual learning in online scenario aims to learn a sequence of new tasks from data stream using each data only once for training, which is more realistic than in offline mode assuming data from new task are all available. However, this problem is still under-explored for the challenging class-incremental setting in which the model classifies all classes seen so far during inference. Particularly, performance struggles with increased number of tasks or additional classes to learn for each task. In addition, most existing methods require storing original data as exemplars for knowledge replay, which may not be feasible for certain applications with limited memory budget or privacy concerns. In this work, we introduce an effective and memory-efficient method for online continual learning under class-incremental setting through candidates selection from each learned task together with prior incorporation using stored feature embeddings instead of original data as exemplars. Our proposed method implemented for image classification task achieves the best results under different benchmark datasets for online continual learning including CIFAR-10, CIFAR-100 and CORE-50 while requiring much less memory resource compared with existing works.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/He_Online_Continual_Learning_via_Candidates_Voting_WACV_2022_paper.pdf",
        "aff": "School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.08855",
        "pdf_size": 1846826,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3770254477994817050&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "School of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "West Lafayette",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c13ee7ce67",
        "title": "Online Knowledge Distillation by Temporal-Spatial Boosting",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Li_Online_Knowledge_Distillation_by_Temporal-Spatial_Boosting_WACV_2022_paper.html",
        "author": "Chengcheng Li; Zi Wang; Hairong Qi",
        "abstract": "Online knowledge distillation (KD) mutually trains a group of student networks from scratch in a peer-teaching manner, eliminating the need for pre-trained teacher models. However, supervision from peers can be noisy, especially in the early stage of training. In this paper, we propose a novel method for online knowledge distillation by temporal-spatial boosting (TSB). The proposed method constructs superior \"teachers\" with two modules, temporal accumulator and spatial integrator. Specifically, the temporal accumulator leverages the previous outputs of networks during training and produces a representative prediction over all classes. Instead of merely imitating the outputs of other networks as in vanilla online KD, we further propose the so-called spatial integrator that consolidates the knowledge learned by all networks and yields a stronger instructor. The operations of these two modules are simple and straightforward, which can be computed efficiently on the fly during training. The proposed method can improve the efficiency of transferring effective knowledge as well as stabilize the training process. Experimental results on various benchmark datasets and network structures validate the effectiveness of the proposed method over the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Li_Online_Knowledge_Distillation_by_Temporal-Spatial_Boosting_WACV_2022_paper.pdf",
        "aff": "University of Tennessee, Knoxville, TN, USA; University of Tennessee, Knoxville, TN, USA; University of Tennessee, Knoxville, TN, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4232509,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8661223522183834848&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "vols.utk.edu;vols.utk.edu;utk.edu",
        "email": "vols.utk.edu;vols.utk.edu;utk.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tennessee",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utk.edu",
        "aff_unique_abbr": "UT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Knoxville",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f7ea5bf429",
        "title": "Ortho-Shot: Low Displacement Rank Regularization With Data Augmentation for Few-Shot Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Osahor_Ortho-Shot_Low_Displacement_Rank_Regularization_With_Data_Augmentation_for_Few-Shot_WACV_2022_paper.html",
        "author": "Uche Osahor; Nasser M. Nasrabadi",
        "abstract": "In few-shot classification, the primary goal is to learn representations from a few samples that generalize well for novel classes. In this paper, we propose an efficient low displacement rank (LDR) regularization strategy termed Ortho-Shot; a technique that imposes orthogonal regularization on the convolutional layers of a few-shot classifier, which is based on the doubly-block toeplitz (DBT) matrix structure. The regularized convolutional layers of the few-shot classifier enhances model generalization and intra-class feature embeddings that are crucial for few-shot learning. Overfitting is a typical issue for few-shot models, the lack of data diversity inhibits proper model inference which weakens the classification accuracy of few-shot learners to novel classes. In this regard, we broke down the pipeline of the few-shot classifier and established that the support, query and task data augmentation collectively alleviates overfitting in networks. With compelling results, we demonstrated that combining a DBT-based low-rank orthogonal regularizer with data augmentation strategies, significantly boosts the performance of a few-shot classifier. We perform our experiments on the miniImagenet, CIFAR-FS and Stanford datasets with performance values of about 5% when compared to state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Osahor_Ortho-Shot_Low_Displacement_Rank_Regularization_With_Data_Augmentation_for_Few-Shot_WACV_2022_paper.pdf",
        "aff": "West Virginia University; West Virginia University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4323618,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12355626887510925369&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mix.wvu.edu;mail.wvu.edu",
        "email": "mix.wvu.edu;mail.wvu.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "West Virginia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wvu.edu",
        "aff_unique_abbr": "WVU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b9097051cd",
        "title": "PERF-Net: Pose Empowered RGB-Flow Net",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Li_PERF-Net_Pose_Empowered_RGB-Flow_Net_WACV_2022_paper.html",
        "author": "Yinxiao Li; Zhichao Lu; Xuehan Xiong; Jonathan Huang",
        "abstract": "In recent years, many works in the video action recognition literature have shown that two stream models (combining spatial and temporal input streams) are necessary for achieving state-of-the-art performance. In this paper we show the benefits of including yet another stream based on human pose estimated from each frame --- specifically by rendering pose on input RGB frames. At first blush, this additional stream may seem redundant given that human pose is fully determined by RGB pixel values --- however we show (perhaps surprisingly) that this simple and flexible addition can provide complementary gains. Using this insight, we propose a new model, which we dub PERF-Net (short for Pose Empowered RGB-Flow Net), which combines this new pose stream with the standard RGB and flow based input streams via distillation techniques and show that our model outperforms the state-of-the-art by a large margin in a number of human action recognition datasets while not requiring flow or pose to be explicitly computed at inference time. The proposed pose stream is also part of the winner solution of the ActivityNet Kinetics Challenge 2020.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Li_PERF-Net_Pose_Empowered_RGB-Flow_Net_WACV_2022_paper.pdf",
        "aff": "Google Research; Google Research; Google Research; Google Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3198072,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3413206189815662820&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c2b3ff8e42",
        "title": "PICA: Point-Wise Instance and Centroid Alignment Based Few-Shot Domain Adaptive Object Detection With Loose Annotations",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zhong_PICA_Point-Wise_Instance_and_Centroid_Alignment_Based_Few-Shot_Domain_Adaptive_WACV_2022_paper.html",
        "author": "Chaoliang Zhong; Jie Wang; Cheng Feng; Ying Zhang; Jun Sun; Yasuto Yokota",
        "abstract": "In this work, we focus on supervised domain adaptation for object detection in few-shot loose annotation setting, where the source images are sufficient and fully labeled but the target images are few-shot and loosely annotated. As annotated objects exist in the target domain, instance level alignment can be utilized to improve the performance. Traditional methods conduct the instance level alignment by semantically aligning the distributions of paired object features with domain adversarial training. Although it is demonstrated that point-wise surrogates of distribution alignment provide a more effective solution in few-shot classification tasks across domains, this point-wise alignment approach has not yet been extended to object detection. In this work, we propose a method that extends the point-wise alignment from classification to object detection. Moreover, in the few-shot loose annotation setting, the background ROIs of target domain suffer from severe label noise problem, which may make the point-wise alignment fail. To this end, we exploit moving average centroids to mitigate the label noise problem of background ROIs. Meanwhile, we exploit point-wise alignment over instances and centroids to tackle the problem of scarcity of labeled target instances. Hence this method is not only robust against label noises of background ROIs but also robust against the scarcity of labeled target objects. Experimental results show that the proposed instance level alignment method brings significant improvement compared with the baseline and is superior to state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zhong_PICA_Point-Wise_Instance_and_Centroid_Alignment_Based_Few-Shot_Domain_Adaptive_WACV_2022_paper.pdf",
        "aff": "Fujitsu R & D Center, Co., Ltd.; Fujitsu R & D Center, Co., Ltd.; Fujitsu R & D Center, Co., Ltd.; Fujitsu R & D Center, Co., Ltd.; Fujitsu R & D Center, Co., Ltd.; Fujitsu LTD",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4998160,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9306211477814429389&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "fujitsu.com;fujitsu.com;fujitsu.com;fujitsu.com;fujitsu.com;fujitsu.com",
        "email": "fujitsu.com;fujitsu.com;fujitsu.com;fujitsu.com;fujitsu.com;fujitsu.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Fujitsu R & D Center;Fujitsu Limited",
        "aff_unique_dep": "R & D;",
        "aff_unique_url": "https://www.fujitsu.com/global/;https://www.fujitsu.com/",
        "aff_unique_abbr": "Fujitsu;Fujitsu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "f4f0bdb1bf",
        "title": "PPCD-GAN: Progressive Pruning and Class-Aware Distillation for Large-Scale Conditional GANs Compression",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Vo_PPCD-GAN_Progressive_Pruning_and_Class-Aware_Distillation_for_Large-Scale_Conditional_GANs_WACV_2022_paper.html",
        "author": "Duc Minh Vo; Akihiro Sugimoto; Hideki Nakayama",
        "abstract": "We push forward neural network compression research by exploiting a novel challenging task of large-scale conditional generative adversarial networks (GANs) compression. To this end, we propose a gradually shrinking GAN (PPCD-GAN) by introducing progressive pruning residual block (PP-Res) and class-aware distillation. The PP-Res is an extension of the conventional residual block where each convolutional layer is followed by a learnable mask layer to progressively prune network parameters as training proceeds. The class-aware distillation, on the other hand, enhances the stability of training by transferring immense knowledge from a well-trained teacher model through instructive attention maps. We train the pruning and distillation processes simultaneously on a well-known GAN architecture in an end-to-end manner. After training, all redundant parameters as well as the mask layers are discarded, yielding a lighter network while retaining the performance. We comprehensively illustrate, on ImageNet 128 x 128 dataset, PPCD-GAN reduces up to 5.2x (81%) parameters against state-of-the-arts while keeping better performance.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Vo_PPCD-GAN_Progressive_Pruning_and_Class-Aware_Distillation_for_Large-Scale_Conditional_GANs_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Vo_PPCD-GAN_Progressive_Pruning_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 33110942,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15925250151581240501&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "97e1bd5e1d",
        "title": "PRECODE - A Generic Model Extension To Prevent Deep Gradient Leakage",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Scheliga_PRECODE_-_A_Generic_Model_Extension_To_Prevent_Deep_Gradient_WACV_2022_paper.html",
        "author": "Daniel Scheliga; Patrick M\u00e4der; Marco Seeland",
        "abstract": "Collaborative training of neural networks leverages distributed data by exchanging gradient information between different clients. Although training data entirely resides with the clients, recent work shows that training data can be reconstructed from such exchanged gradient information. To enhance privacy, gradient perturbation techniques have been proposed. However, they come at the cost of reduced model performance, increased convergence time, or increased data demand. In this paper, we introduce PRECODE, a PRivacy EnhanCing mODulE that can be used as generic extension for arbitrary model architectures. We propose a simple yet effective realization of PRECODE using variational modeling. The stochastic sampling induced by variational modeling effectively prevents privacy leakage from gradients and in turn preserves privacy of data owners. We evaluate PRECODE using state of the art gradient inversion attacks on two different model architectures trained on three datasets. In contrast to commonly used defense mechanisms, we find that our proposed modification consistently reduces the attack success rate to 0% while having almost no negative impact on model training and final performance. As a result, PRECODE reveals a promising path towards privacy enhancing model extensions.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Scheliga_PRECODE_-_A_Generic_Model_Extension_To_Prevent_Deep_Gradient_WACV_2022_paper.pdf",
        "aff": "Technische Universit \u00a8at Ilmenau; Technische Universit \u00a8at Ilmenau + Friedrich Schiller Universit \u00a8at Jena; Technische Universit \u00a8at Ilmenau",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Scheliga_PRECODE_-_A_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1920495,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15126416555706304611&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "tu-ilmenau.de;tu-ilmenau.de;tu-ilmenau.de",
        "email": "tu-ilmenau.de;tu-ilmenau.de;tu-ilmenau.de",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Technische Universit\u00e4t Ilmenau;Friedrich Schiller University Jena",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tu-ilmenau.de/;https://www.uni-jena.de/",
        "aff_unique_abbr": "TU Ilmenau;FSU Jena",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "f10b990521",
        "title": "PROVES: Establishing Image Provenance Using Semantic Signatures",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Xie_PROVES_Establishing_Image_Provenance_Using_Semantic_Signatures_WACV_2022_paper.html",
        "author": "Mingyang Xie; Manav Kulshrestha; Shaojie Wang; Jinghan Yang; Ayan Chakrabarti; Ning Zhang; Yevgeniy Vorobeychik",
        "abstract": "Modern AI tools, such as generative adversarial networks, have transformed our ability to create and modify visual data with photorealistic results. However, one of the deleterious side-effects of these advances is the emergence of nefarious uses in manipulating information in visual data, such as through the use of deep fakes. We propose a novel architecture for preserving the provenance of semantic information in images to make them less susceptible to deep fake attacks. Our architecture includes semantic signing and verification steps. We apply this architecture to verifying two types of semantic information: individual identities (faces) and whether the photo was taken indoors or outdoors. Verification in both cases carefully accounts for a collection of common image transformation, such as translation, scaling, cropping, and small rotations, and rejects adversarial transformations, such as adversarially perturbed or, in the case of face verification, swapped faces. Experiments demonstrate that in the case of provenance of faces in an image, our approach is robust to black-box adversarial transformations (which are rejected) as well as benign transformations (which are accepted), with few false negatives and false positives. Background verification, on the other hand, is susceptible to black-box adversarial examples, but becomes significantly more robust after adversarial training.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Xie_PROVES_Establishing_Image_Provenance_Using_Semantic_Signatures_WACV_2022_paper.pdf",
        "aff": "Computer Science & Engineering, Washington University in St. Louis; Computer Science, University of Massachusetts at Amherst; Computer Science & Engineering, Washington University in St. Louis; Computer Science & Engineering, Washington University in St. Louis; Computer Science & Engineering, Washington University in St. Louis; Computer Science & Engineering, Washington University in St. Louis; Computer Science & Engineering, Washington University in St. Louis",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.11411",
        "pdf_size": 2801323,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16725125524246466060&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0;0;0",
        "aff_unique_norm": "Washington University in St. Louis;University of Massachusetts Amherst",
        "aff_unique_dep": "Computer Science & Engineering;Computer Science",
        "aff_unique_url": "https://wustl.edu;https://www.umass.edu",
        "aff_unique_abbr": "WashU;UMass Amherst",
        "aff_campus_unique_index": "0;1;0;0;0;0;0",
        "aff_campus_unique": "St. Louis;Amherst",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fe2e05d4a1",
        "title": "Parsing Line Chart Images Using Linear Programming",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kato_Parsing_Line_Chart_Images_Using_Linear_Programming_WACV_2022_paper.html",
        "author": "Hajime Kato; Mitsuru Nakazawa; Hsuan-Kung Yang; Mark Chen; Bj\u00f6rn Stenger",
        "abstract": "This paper proposes a method for automatically recovering data from chart images. In particular we focus on the task of estimating line charts, as the most common chart type, in a fully automatic way that handles line occlusions, as well as lines of different styles, e.g., dashed or dotted. For this, we first train a single semantic segmentation network to predict probability maps for each different line styles. We then construct a graph based on this output and formulate the line tracing task as a minimum-cost-flow problem, optimizing a cost function using linear programming. From the traced lines, the axes, and text labels, we recover the numerical values used to generate the chart. In experiments on six datasets, containing both synthesized and crawled images, we show significant improvements over prior work.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kato_Parsing_Line_Chart_Images_Using_Linear_Programming_WACV_2022_paper.pdf",
        "aff": "Rakuten Institute of Technology, Rakuten Group, Inc.; Rakuten Institute of Technology, Rakuten Group, Inc.; National Tsing Hua University + Rakuten Group, Inc.; The University of Tokyo + Rakuten Group, Inc.; Rakuten Institute of Technology, Rakuten Group, Inc.",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Kato_Parsing_Line_Chart_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 7190393,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1933304526015329044&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "rakuten.com;rakuten.com; ; ;rakuten.com",
        "email": "rakuten.com;rakuten.com; ; ;rakuten.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2;3+2;0",
        "aff_unique_norm": "Rakuten Institute of Technology;National Tsing Hua University;Rakuten Group;University of Tokyo",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://rit.rakuten.com;https://www.nthu.edu.tw;https://www.rakuten.com;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "RIT;NTHU;Rakuten;UTokyo",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Taiwan",
        "aff_country_unique_index": "0;0;1+0;0+0;0",
        "aff_country_unique": "Japan;China"
    },
    {
        "id": "2434e11a7c",
        "title": "Perceptual Consistency in Video Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zhang_Perceptual_Consistency_in_Video_Segmentation_WACV_2022_paper.html",
        "author": "Yizhe Zhang; Shubhankar Borse; Hong Cai; Ying Wang; Ning Bi; Xiaoyun Jiang; Fatih Porikli",
        "abstract": "In this paper, we present a novel perceptual consistency perspective on video semantic segmentation, which can capture both temporal consistency and pixel-wise correctness. Given two nearby video frames, perceptual consistency measures how much the segmentation decisions agree with the pixel correspondences obtained via matching general perceptual features. More specifically, for each pixel in one frame, we find the most perceptually correlated pixel in the other frame. Our intuition is that such a pair of pixels are highly likely to belong to the same class. Next, we assess how much the segmentation agrees with such perceptual correspondences, based on which we derive the perceptual consistency of the segmentation maps across these two frames. Utilizing perceptual consistency, we can evaluate the temporal consistency of video segmentation by measuring the perceptual consistency over consecutive pairs of segmentation maps in a video. Furthermore, given a sparsely labeled test video, perceptual consistency can be utilized to aid with predicting the pixel-wise correctness of the segmentation on an unlabeled frame. More specifically, by measuring the perceptual consistency between the predicted segmentation and the available ground truth on a nearby frame and combining it with the segmentation confidence, we can accurately assess the classification correctness on each pixel. Our experiments show that the proposed perceptual consistency can more accurately evaluate the temporal consistency of video segmentation as compared to flow-based measures. Furthermore, it can help more confidently predict segmentation accuracy on unlabeled test frames, as compared to using classification confidence alone. Finally, our proposed measure can be used as a regularizer during the training of segmentation models, which leads to more temporally consistent video segmentation while maintaining accuracy.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zhang_Perceptual_Consistency_in_Video_Segmentation_WACV_2022_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Zhang_Perceptual_Consistency_in_WACV_2022_supplemental.pdf",
        "arxiv": "2110.12385",
        "pdf_size": 9500695,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7528382303705762118&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "534342fc92",
        "title": "PhotoWCT2: Compact Autoencoder for Photorealistic Style Transfer Resulting From Blockwise Training and Skip Connections of High-Frequency Residuals",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chiu_PhotoWCT2_Compact_Autoencoder_for_Photorealistic_Style_Transfer_Resulting_From_Blockwise_WACV_2022_paper.html",
        "author": "Tai-Yin Chiu; Danna Gurari",
        "abstract": "Photorealistic style transfer is an image editing task with the goal to modify an image to match the style of another image while ensuring the result looks like a real photograph. A limitation of existing models is that they have many parameters, which in turn prevents their use for larger image resolutions and leads to slower run-times. We introduce two mechanisms that enable our design of a more compact model that we call PhotoWCT2, which preserves state-of-art stylization strength and photorealism. First, we introduce blockwise training to perform coarse-to-fine feature transformations that enable state-of-art stylization strength in a single autoencoder in place of the inefficient cascade of four autoencoders used in PhotoWCT. Second, we introduce skip connections of high-frequency residuals in order to preserve image quality when applying the sequential coarse-to-fine feature transformations. Our PhotoWCT2 model requires fewer parameters (e.g., 30.3% fewer) while supporting higher resolution images (e.g., 4K) and achieving faster stylization than existing models.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chiu_PhotoWCT2_Compact_Autoencoder_for_Photorealistic_Style_Transfer_Resulting_From_Blockwise_WACV_2022_paper.pdf",
        "aff": "The University of Texas at Austin; University of Colorado Boulder",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Chiu_PhotoWCT2_Compact_Autoencoder_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 10606278,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17884121910023255883&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Texas at Austin;University of Colorado",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;https://www.colorado.edu",
        "aff_unique_abbr": "UT Austin;CU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Austin;Boulder",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7ba504e47e",
        "title": "Physical Adversarial Attacks on an Aerial Imagery Object Detector",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Du_Physical_Adversarial_Attacks_on_an_Aerial_Imagery_Object_Detector_WACV_2022_paper.html",
        "author": "Andrew Du; Bo Chen; Tat-Jun Chin; Yee Wei Law; Michele Sasdelli; Ramesh Rajasegaran; Dillon Campbell",
        "abstract": "Deep neural networks (DNNs) have become essential for processing the vast amounts of aerial imagery collected using earth-observing satellite platforms. However, DNNs are vulnerable towards adversarial examples, and it is expected that this weakness also plagues DNNs for aerial imagery. In this work, we demonstrate one of the first efforts on physical adversarial attacks on aerial imagery, whereby adversarial patches were optimised, fabricated and installed on or near target objects (cars) to significantly reduce the efficacy of an object detector applied on overhead images. Physical adversarial attacks on aerial images, particularly those captured from satellite platforms, are challenged by atmospheric factors (lighting, weather, seasons) and the distance between the observer and target. To investigate the effects of these challenges, we devised novel experiments and metrics to evaluate the efficacy of physical adversarial attacks against object detectors in aerial scenes. Our results indicate the palpable threat posed by physical adversarial attacks towards DNNs for processing satellite imagery.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Du_Physical_Adversarial_Attacks_on_an_Aerial_Imagery_Object_Detector_WACV_2022_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Du_Physical_Adversarial_Attacks_WACV_2022_supplemental.pdf",
        "arxiv": "2108.11765",
        "pdf_size": 1543688,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1462593576673615910&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d446e2801d",
        "title": "Pixel-Level Bijective Matching for Video Object Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Cho_Pixel-Level_Bijective_Matching_for_Video_Object_Segmentation_WACV_2022_paper.html",
        "author": "Suhwan Cho; Heansung Lee; Minjung Kim; Sungjun Jang; Sangyoun Lee",
        "abstract": "Semi-supervised video object segmentation (VOS) aims to track a designated object present in the initial frame of a video at the pixel level. To fully exploit the appearance information of an object, pixel-level feature matching is widely used in VOS. Conventional feature matching runs in a surjective manner, i.e., only the best matches from the query frame to the reference frame are considered. Each location in the query frame refers to the optimal location in the reference frame regardless of how often each reference frame location is referenced. This works well in most cases and is robust against rapid appearance variations, but may cause critical errors when the query frame contains background distractors that look similar to the target object. To mitigate this concern, we introduce a bijective matching mechanism to find the best matches from the query frame to the reference frame and vice versa. Before finding the best matches for the query frame pixels, the optimal matches for the reference frame pixels are first considered to prevent each reference frame pixel from being overly referenced. As this mechanism operates in a strict manner, i.e., pixels are connected if and only if they are the sure matches for each other, it can effectively eliminate background distractors. In addition, we propose a mask embedding module to improve the existing mask propagation method. By utilizing multiple historic masks and their variations, it can effectively capture the position information of a target object.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Cho_Pixel-Level_Bijective_Matching_for_Video_Object_Segmentation_WACV_2022_paper.pdf",
        "aff": "Yonsei University; Yonsei University; Yonsei University; Yonsei University; Yonsei University",
        "project": "",
        "github": "https://github.com/suhwan-cho/BMVOS",
        "supp": "",
        "arxiv": "2110.01644",
        "pdf_size": 3145325,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3790138162159800165&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Yonsei University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yonsei.ac.kr",
        "aff_unique_abbr": "Yonsei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "975d58c964",
        "title": "Pixel-by-Pixel Cross-Domain Alignment for Few-Shot Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tavera_Pixel-by-Pixel_Cross-Domain_Alignment_for_Few-Shot_Semantic_Segmentation_WACV_2022_paper.html",
        "author": "Antonio Tavera; Fabio Cermelli; Carlo Masone; Barbara Caputo",
        "abstract": "In this paper we consider the task of semantic segmentation in autonomous driving applications. Specifically, we consider the cross-domain few-shot setting where training can use only few real-world annotated images and many annotated synthetic images. In this context, aligning the domains is made more challenging by the pixel-wise class imbalance that is intrinsic in the segmentation and that leads to ignoring the underrepresented classes and overfitting the well represented ones. We address this problem with a novel framework called Pixel-By-Pixel Cross-Domain Alignment (PixDA). We propose a novel pixel-by-pixel domain adversarial loss following three criteria: (i) align the source and the target domain for each pixel, (ii) avoid negative transfer on the correctly represented pixels, and (iii) regularize the training of infrequent classes to avoid overfitting. The pixel-wise adversarial training is assisted by a novel sample selection procedure, that handles the imbalance between source and target data, and a knowledge distillation strategy, that avoids overfitting towards the few target images. We demonstrate on standard synthetic-to-real benchmarks that PixDA outperforms previous state-of-the-art methods in (1-5)-shot settings.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tavera_Pixel-by-Pixel_Cross-Domain_Alignment_for_Few-Shot_Semantic_Segmentation_WACV_2022_paper.pdf",
        "aff": "Polytechnic University of Turin, Turin, Italy; Polytechnic University of Turin, Turin, Italy; CINI - Consorzio Interuniversitario Nazionale per l\u2019Informatica, Rome, Italy; Polytechnic University of Turin, Turin, Italy",
        "project": "",
        "github": "https://github.com/taveraantonio/PixDA",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Tavera_Pixel-by-Pixel_Cross-Domain_Alignment_WACV_2022_supplemental.pdf",
        "arxiv": "2110.11650",
        "pdf_size": 4986722,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13198089946343066235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "polito.it;polito.it; ;polito.it",
        "email": "polito.it;polito.it; ;polito.it",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Polytechnic University of Turin;Consorzio Interuniversitario Nazionale per l\u2019Informatica",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.polito.it;",
        "aff_unique_abbr": "Polito;CINI",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Turin;Rome",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "6cd94f962f",
        "title": "Plugging Self-Supervised Monocular Depth Into Unsupervised Domain Adaptation for Semantic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Cardace_Plugging_Self-Supervised_Monocular_Depth_Into_Unsupervised_Domain_Adaptation_for_Semantic_WACV_2022_paper.html",
        "author": "Adriano Cardace; Luca De Luigi; Pierluigi Zama Ramirez; Samuele Salti; Luigi Di Stefano",
        "abstract": "Although recent semantic segmentation methods have made remarkable progress, they still rely on large amounts of annotated training data, which are often infeasible to collect in the autonomous driving scenario. Previous works usually tackle this issue with Unsupervised Domain Adaptation (UDA), which entails training a network on synthetic images and applying the model to real ones while minimizing the discrepancy between the two domains. Yet, these techniques do not consider additional information that may be obtained from other tasks. Differently, we propose to exploit self-supervised monocular depth estimation to improve UDA for semantic segmentation. On one hand, we deploy depth to realize a plug-in component which can inject complementary geometric cues into any existing UDA method. We further rely on depth to generate a large and varied set of samples to Self-Train the final model. Our whole proposal allows for achieving state-of-the-art performance (58.8 mIoU) in the GTA5->CS benchmark benchmark. Code is available at https://github.com/CVLAB-Unibo/d4-dbst.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Cardace_Plugging_Self-Supervised_Monocular_Depth_Into_Unsupervised_Domain_Adaptation_for_Semantic_WACV_2022_paper.pdf",
        "aff": "Department of Computer Science and Engineering (DISI), University of Bologna, Italy; Department of Computer Science and Engineering (DISI), University of Bologna, Italy; Department of Computer Science and Engineering (DISI), University of Bologna, Italy; Department of Computer Science and Engineering (DISI), University of Bologna, Italy; Department of Computer Science and Engineering (DISI), University of Bologna, Italy",
        "project": "",
        "github": "https://github.com/CVLAB-Unibo/d4-dbst",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Cardace_Plugging_Self-Supervised_Monocular_WACV_2022_supplemental.pdf",
        "arxiv": "2110.06685",
        "pdf_size": 1525051,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15718486505654978184&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "unibo.it;unibo.it;unibo.it; ; ",
        "email": "unibo.it;unibo.it;unibo.it; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Bologna",
        "aff_unique_dep": "Department of Computer Science and Engineering (DISI)",
        "aff_unique_url": "https://www.unibo.it",
        "aff_unique_abbr": "UNIBO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "5c75bc70e0",
        "title": "PoP-Net: Pose Over Parts Network for Multi-Person 3D Pose Estimation From a Depth Image",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Guo_PoP-Net_Pose_Over_Parts_Network_for_Multi-Person_3D_Pose_Estimation_WACV_2022_paper.html",
        "author": "Yuliang Guo; Zhong Li; Zekun Li; Xiangyu Du; Shuxue Quan; Yi Xu",
        "abstract": "In this paper, a real-time method called PoP-Net is proposed to predict multi-person 3D poses from a depth image. PoP-Net learns to predict bottom-up part representations and top-down global poses in a single shot. Specifically, a new part-level representation, called Truncated Part Displacement Field (TPDF), is introduced which enables an explicit fusion process to unify the advantages of bottom-up part detection and global pose detection. Meanwhile, an effective mode selection scheme is introduced to automatically resolve the conflicting cases between global pose and part detections. Finally, due to the lack of high-quality depth datasets for developing multi-person 3D pose estimation, we introduce Multi-Person 3D Human Pose Dataset (MP-3DHP) as a new benchmark. MP-3DHP is designed to enable effective multi-person and background data augmentation in model training, and to evaluate 3D human pose estimators under uncontrolled multi-person scenarios. We show that PoP-Net achieves the state-of-the-art results both on MP-3DHP and on the widely used ITOP dataset, and has significant advantages in efficiency for multi-person processing. MP-3DHP Dataset and the evaluation code have been made available at: https://github.com/oppo-us-research/PoP-Net.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Guo_PoP-Net_Pose_Over_Parts_Network_for_Multi-Person_3D_Pose_Estimation_WACV_2022_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "https://github.com/oppo-us-research/PoP-Net",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Guo_PoP-Net_Pose_Over_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3223636,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=844744975335572332&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e444fcdc59",
        "title": "Pose and Joint-Aware Action Recognition",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Shah_Pose_and_Joint-Aware_Action_Recognition_WACV_2022_paper.html",
        "author": "Anshul Shah; Shlok Mishra; Ankan Bansal; Jun-Cheng Chen; Rama Chellappa; Abhinav Shrivastava",
        "abstract": "Recent progress on action recognition has mainly focused on RGB and optical flow features. In this paper, we approach the problem of joint-based action recognition. Unlike other modalities, constellation of joints and their motion generate models with succinct human motion information for activity recognition. We present a new model for joint-based action recognition, which first extracts motion features from each joint separately through a shared motion encoder before performing collective reasoning. Our joint selector module re-weights the joint information to select the most discriminative joints for the task. We also propose a novel joint-contrastive loss that pulls together groups of joint features which convey the same action. We strengthen the joint-based representations by using a geometry-aware data augmentation technique which jitters pose heatmaps while retaining the dynamics of the action. We show large improvements over the current state-of-the-art joint-based approaches on JHMDB, HMDB, Charades, AVA action recognition datasets. A late fusion with RGB and Flow-based approaches yields additional improvements. Our model also outperforms the existing baseline on Mimetics, a dataset with out-of-context actions.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Shah_Pose_and_Joint-Aware_Action_Recognition_WACV_2022_paper.pdf",
        "aff": "Johns Hopkins University; University of Maryland, College Park; University of Maryland, College Park; Research Center for Information Technology Innovation, Academia Sinica; Johns Hopkins University; University of Maryland, College Park",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Shah_Pose_and_Joint-Aware_WACV_2022_supplemental.pdf",
        "arxiv": "2010.08164",
        "pdf_size": 610762,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1865374364358866886&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "jhu.edu;umd.edu;umd.edu;citi.sinica.edu.tw;jhu.edu;umd.edu",
        "email": "jhu.edu;umd.edu;umd.edu;citi.sinica.edu.tw;jhu.edu;umd.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2;0;1",
        "aff_unique_norm": "Johns Hopkins University;University of Maryland;Academia Sinica",
        "aff_unique_dep": ";;Research Center for Information Technology Innovation",
        "aff_unique_url": "https://www.jhu.edu;https://www/umd.edu;https://www.sinica.edu.tw",
        "aff_unique_abbr": "JHU;UMD;Academia Sinica",
        "aff_campus_unique_index": "1;1;2;1",
        "aff_campus_unique": ";College Park;Taiwan",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "867958eba4",
        "title": "Pose-Guided Generative Adversarial Net for Novel View Action Synthesis",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Li_Pose-Guided_Generative_Adversarial_Net_for_Novel_View_Action_Synthesis_WACV_2022_paper.html",
        "author": "Xianhang Li; Junhao Zhang; Kunchang Li; Shruti Vyas; Yogesh S. Rawat",
        "abstract": "We focus on the problem of novel-view human action synthesis. Given an action video, the goal is to generate the same action from an unseen viewpoint. Naturally, novel view video synthesis is more challenging than image synthesis. It requires the synthesis of a sequence of realistic frames with temporal coherency. Besides, transferring the different actions to a novel target view requires awareness of action category and viewpoint change simultaneously. To address these challenges we propose a novel framework named Pose-guided Action Separable Generative Adversarial Net (PAS-GAN), which utilizes pose to alleviate the difficulty of this task. First, we propose a recurrent pose-transformation module which transforms actions from the source view to the target view and generates novel view pose sequence in 2D coordinate space. Second, a well-transformed pose sequence enables us to separatethe action and background in the target view. We employ a novel local-global spatial transformation module to effectively generate sequential video features in the target view using these action and background features. Finally, the generated video features are used to synthesize human action with the help of a 3D decoder. Moreover, to focus on dynamic action in the video, we propose a novel multi-scale action-separable loss which further improves the video quality. We conduct extensive experiments on two large-scale multi-view human action datasets, NTU-RGBD and PKU-MMD, demonstrating the effectiveness of PAS-GAN which outperforms existing approaches.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Li_Pose-Guided_Generative_Adversarial_Net_for_Novel_View_Action_Synthesis_WACV_2022_paper.pdf",
        "aff": "CRCV, University of Central Florida; National University of Singapore; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; CRCV, University of Central Florida; CRCV, University of Central Florida",
        "project": "https://xhl-video.github.io/xianhangli/pas gan.html",
        "github": "https://github.com/xhl-video/PAS-GAN",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Li_Pose-Guided_Generative_Adversarial_WACV_2022_supplemental.pdf",
        "arxiv": "2110.07993",
        "pdf_size": 3000491,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12255629913014459702&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ucsc.edu;u.nus.edu;siat.ac.cn;crcv.ucf.edu;crcv.ucf.edu",
        "email": "ucsc.edu;u.nus.edu;siat.ac.cn;crcv.ucf.edu;crcv.ucf.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "University of Central Florida;National University of Singapore;Chinese Academy of Sciences",
        "aff_unique_dep": "Center for Research in Computer Vision;;Shenzhen Institutes of Advanced Technology",
        "aff_unique_url": "https://www.ucf.edu;https://www.nus.edu.sg;http://www.siat.cas.cn",
        "aff_unique_abbr": "UCF;NUS;SIAT",
        "aff_campus_unique_index": "0;2;0;0",
        "aff_campus_unique": "Orlando;;Shenzhen",
        "aff_country_unique_index": "0;1;2;0;0",
        "aff_country_unique": "United States;Singapore;China"
    },
    {
        "id": "42e9cd3bae",
        "title": "Post-OCR Paragraph Recognition by Graph Convolutional Networks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wang_Post-OCR_Paragraph_Recognition_by_Graph_Convolutional_Networks_WACV_2022_paper.html",
        "author": "Renshen Wang; Yasuhisa Fujii; Ashok C. Popat",
        "abstract": "We propose a new approach for paragraph recognition in document images by spatial graph convolutional networks (GCN) applied on OCR text boxes. Two steps, namely line splitting and line clustering, are performed to extract paragraphs from the lines in OCR results. Each step uses a beta-skeleton graph constructed from bounding boxes, where the graph edges provide efficient support for graph convolution operations. With pure layout input features, the GCN model size is 3 4 orders of magnitude smaller compared to R-CNN based models, while achieving comparable or better accuracies on PubLayNet and other datasets. Furthermore, the GCN models show good generalization from synthetic training data to real-world images, and good adaptivity for variable document styles.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wang_Post-OCR_Paragraph_Recognition_by_Graph_Convolutional_Networks_WACV_2022_paper.pdf",
        "aff": "Google Research; Google Research; Google Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2101.12741",
        "pdf_size": 4941578,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10528260849607875974&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "google.com;google.com;google.com",
        "email": "google.com;google.com;google.com",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e1ce0ee5d1",
        "title": "PredStereo: An Accurate Real-Time Stereo Vision System",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Moolchandani_PredStereo_An_Accurate_Real-Time_Stereo_Vision_System_WACV_2022_paper.html",
        "author": "Diksha Moolchandani; Nivedita Shrivastava; Anshul Kumar; Smruti R. Sarangi",
        "abstract": "Stereo vision algorithms are important building blocks of self-driving applications. The two primary requirements of a self-driving vehicle are real-time operation and nearly 100% accuracy in constructing the 3D scene regardless of the weather conditions and the degree of ambient light. Sadly, most real-time systems as of today provide a level of accuracy that is inadequate and this endangers the life of the passengers; consequently, it is necessary to supplement such systems with expensive LiDAR-based sensors. We observe that for a given scene, different stereo matching algorithms can have vastly different accuracies, and among these algorithms, there is no clear winner. This makes the case for a hybrid stereo vision system where the best stereo vision algorithm for a stereo image pair is chosen by a predictor dynamically, in real-time. We implement such a system called PredStereo in ASIC that combines two diametrically different stereo vision algorithms, CNN-based and traditional, and chooses the best one at runtime. In addition, it associates a confidence with the chosen algorithm, such that the higher-level control system can be switched on in case of a low confidence value. We show that designing a predictor that is explainable and a system that respects soft real-time constraints is non-trivial. Hence, we propose a variety of hardware optimizations that enable our system to work in real-time. Overall, PredStereo improves the disparity estimation error over a state-of-the-art CNN-based stereo vision system by up to 18% (on average 6.25%) with a negligible area overhead (0.003 mm^2) while respecting real-time constraints.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Moolchandani_PredStereo_An_Accurate_Real-Time_Stereo_Vision_System_WACV_2022_paper.pdf",
        "aff": "Indian Institute of Technology Delhi; Indian Institute of Technology Delhi; Indian Institute of Technology Delhi; Indian Institute of Technology Delhi",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Moolchandani_PredStereo_An_Accurate_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 526724,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5325892592860789278&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cse.iitd.ac.in;ee.iitd.ac.in;cse.iitd.ac.in;cse.iitd.ac.in",
        "email": "cse.iitd.ac.in;ee.iitd.ac.in;cse.iitd.ac.in;cse.iitd.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Delhi",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitd.ac.in",
        "aff_unique_abbr": "IIT Delhi",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Delhi",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "348e4fd9dc",
        "title": "Predicting Levels of Household Electricity Consumption in Low-Access Settings",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Fobi_Predicting_Levels_of_Household_Electricity_Consumption_in_Low-Access_Settings_WACV_2022_paper.html",
        "author": "Simone Fobi; Joel Mugyenyi; Nathaniel J. Williams; Vijay Modi; Jay Taneja",
        "abstract": "In low-income settings, the most critical piece of information for electric utilities is the anticipated consumption of a customer. Electricity consumption assessment is difficult to do in settings where a significant fraction of households do not yet have an electricity connection. In such settings the absolute levels of anticipated consumption can range from 5-100 kWh/month, leading to high variability amongst these customers. Precious resources are at stake if a significant fraction of low consumers are connected over those with higher consumption. This is the first study of it's kind in low-income settings that attempts to predict a building's consumption and not that of an aggregate administrative area. We train a Convolutional Neural Network (CNN) over pre-electrification daytime satellite imagery with a sample of utility bills from 20,000 geo-referenced electricity customers in Kenya (0.01% of Kenya's residential customers). This is made possible with a two-stage approach that uses a novel building segmentation approach to leverage much larger volumes of no-cost satellite imagery to make the most of scarce and expensive customer data. Our method shows that competitive accuracies can be achieved at the building level, addressing the challenge of consumption variability. This work shows that the building's characteristics and it's surrounding context are both important in predicting consumption levels. We also evaluate the addition of lower resolution geospatial datasets into the training process, including nighttime lights and census-derived data. The results are already helping inform site selection and distribution-level planning, through granular predictions at the level of individual structures in Kenya and there is no reason this cannot be extended to other countries.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Fobi_Predicting_Levels_of_Household_Electricity_Consumption_in_Low-Access_Settings_WACV_2022_paper.pdf",
        "aff": "Department of Mechanical Engineering, Columbia University; Department of Electrical & Computer Engineering, University of Massachusetts Amherst; Golisano Institute for Sustainability, Rochester Institute of Technology; Department of Mechanical Engineering, Columbia University; Department of Electrical & Computer Engineering, University of Massachusetts Amherst",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Fobi_Predicting_Levels_of_WACV_2022_supplemental.pdf",
        "arxiv": "2112.08497",
        "pdf_size": 2083736,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1183404630486966170&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "columbia.edu;columbia.edu;umass.edu;umass.edu;rit.edu",
        "email": "columbia.edu;columbia.edu;umass.edu;umass.edu;rit.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;1",
        "aff_unique_norm": "Columbia University;University of Massachusetts Amherst;Rochester Institute of Technology",
        "aff_unique_dep": "Department of Mechanical Engineering;Department of Electrical & Computer Engineering;Golisano Institute for Sustainability",
        "aff_unique_url": "https://www.columbia.edu;https://www.umass.edu;https://www.rit.edu",
        "aff_unique_abbr": "Columbia;UMass Amherst;RIT",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Amherst",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7214154c48",
        "title": "Preventing Catastrophic Forgetting and Distribution Mismatch in Knowledge Distillation via Synthetic Data",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Binici_Preventing_Catastrophic_Forgetting_and_Distribution_Mismatch_in_Knowledge_Distillation_via_WACV_2022_paper.html",
        "author": "Kuluhan Binici; Nam Trung Pham; Tulika Mitra; Karianto Leman",
        "abstract": "With the increasing popularity of deep learning on edge devices, compressing large neural networks to meet the hardware requirements of resource-constrained devices became a significant research direction. Numerous compression methodologies are currently being used to reduce the memory sizes and energy consumption of neural networks. Knowledge distillation (KD) is among such methodologies and it functions by using data samples to transfer the knowledge captured by a large model (teacher) to a smaller one (student). However, due to various reasons, the original training data might not be accessible at the compression stage. Therefore, data-free model compression is an ongoing research problem that has been addressed by various works. In this paper, we point out that catastrophic forgetting is a problem that can potentially be observed in existing data-free distillation methods. Moreover, the sample generation strategies in some of these methods could result in a mismatch between the synthetic and real data distributions. To prevent such problems, we propose a data-free KD framework that maintains a dynamic collection of generated samples over time. Additionally, we add the constraint of matching the real data distribution in sample generation strategies that target maximum information gain. Our experiments demonstrate that we can improve the accuracy of the student models obtained via KD when compared with state-of-the-art approaches on the SVHN, Fashion MNIST and CIFAR100 datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Binici_Preventing_Catastrophic_Forgetting_and_Distribution_Mismatch_in_Knowledge_Distillation_via_WACV_2022_paper.pdf",
        "aff": "Institute for Infocomm Research, A*STAR, Singapore+School of Computing, National University of Singapore, Singapore; Institute for Infocomm Research, A*STAR, Singapore; School of Computing, National University of Singapore, Singapore; Institute for Infocomm Research, A*STAR, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2108.05698",
        "pdf_size": 4876747,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1817353942094333499&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "comp.nus.edu.sg;i2r.a-star.edu.sg;comp.nus.edu.sg;i2r.a-star.edu.sg",
        "email": "comp.nus.edu.sg;i2r.a-star.edu.sg;comp.nus.edu.sg;i2r.a-star.edu.sg",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;1;0",
        "aff_unique_norm": "Institute for Infocomm Research;National University of Singapore",
        "aff_unique_dep": ";School of Computing",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg;https://www.nus.edu.sg",
        "aff_unique_abbr": "I2R;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "655f1bb2cc",
        "title": "Pro-CCaps: Progressively Teaching Colourisation to Capsules",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Pucci_Pro-CCaps_Progressively_Teaching_Colourisation_to_Capsules_WACV_2022_paper.html",
        "author": "Rita Pucci; Christian Micheloni; Gian Luca Foresti; Niki Martinel",
        "abstract": "Automatic image colourisation studies how to colourise greyscale images. Existing approaches exploit convolutional layers that extract image-level features learning the colourisation on the entire image, but miss entities-level ones due to pooling strategies. We believe that entity-level features are of paramount importance to deal with the intrinsic multimodality of the problem (i.e., the same object can have different colours, and the same colour can have different properties). Models based on capsule layers aim to identify entity-level features in the image from different points of view, but they do not keep track of global features. Our network architecture integrates entity-level features into the image-level features to generate a plausible image colourisation. We observed that results obtained with direct integration of such two representations are largely dominated by the image-level features, thus resulting in unsaturated colours for the entities. To limit such an issue, we propose a gradual growth of the reconstruction phase of the model while training. By advantaging of prior knowledge from each growing step, we obtain a stable collaboration between image-level and entity-level features that ultimately generates stable and vibrant colourisations. Experimental results on three benchmark datasets, and a user study, demonstrate that our approach has competitive performance with respect to the state-of-the-art and provides more consistent colourisation. Code available at omitted-for-reviewing-purposes.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Pucci_Pro-CCaps_Progressively_Teaching_Colourisation_to_Capsules_WACV_2022_paper.pdf",
        "aff": "Universit `a degli Studi di Udine, Italia; Universit `a degli Studi di Udine, Italia; Universit `a degli Studi di Udine, Italia; Universit `a degli Studi di Udine, Italia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3456008,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9636954064204901531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "uniud.it;uniud.it;uniud.it;uniud.it",
        "email": "uniud.it;uniud.it;uniud.it;uniud.it",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Universit\u00e0 degli Studi di Udine",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uniud.it",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "60343e7820",
        "title": "Progressive Automatic Design of Search Space for One-Shot Neural Architecture Search",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Xia_Progressive_Automatic_Design_of_Search_Space_for_One-Shot_Neural_Architecture_WACV_2022_paper.html",
        "author": "Xin Xia; Xuefeng Xiao; Xing Wang; Min Zheng",
        "abstract": "Neural Architecture Search (NAS) has attracted growing interest. To reduce the search cost, recent work has explored weight sharing across models and made major progress in One-Shot NAS. However, it has been observed that a model with higher one-shot model accuracy does not necessarily perform better when stand-alone trained. To address this issue, in this paper, we propose Progressive Automatic Design of search space, named PAD-NAS. Unlike previous approaches where the same operation search space is shared by all the layers in the supernet, we formulate a progressive search strategy based on operation pruning and build a layer-wise operation search space. In this way, PAD-NAS can automatically design the operations for each layer and achieve a trade-off between search space quality and model diversity. During the search, we also take the hardware platform constraints into consideration for efficient neural network model deployment. Extensive experiments on ImageNet show that our method can achieve state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Xia_Progressive_Automatic_Design_of_Search_Space_for_One-Shot_Neural_Architecture_WACV_2022_paper.pdf",
        "aff": "ByteDance Inc.; ByteDance Inc.; ByteDance Inc.; ByteDance Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2005.07564",
        "pdf_size": 1691735,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13810793798610698821&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "bytedance.com;bytedance.com;bytedance.com;bytedance.com",
        "email": "bytedance.com;bytedance.com;bytedance.com;bytedance.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ByteDance",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bytedance.com",
        "aff_unique_abbr": "ByteDance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "9cdb2f6e9a",
        "title": "QUALIFIER: Question-Guided Self-Attentive Multimodal Fusion Network for Audio Visual Scene-Aware Dialog",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ye_QUALIFIER_Question-Guided_Self-Attentive_Multimodal_Fusion_Network_for_Audio_Visual_Scene-Aware_WACV_2022_paper.html",
        "author": "Muchao Ye; Quanzeng You; Fenglong Ma",
        "abstract": "Audio video scene-aware dialog (AVSD) is a new but more challenging visual question answering (VQA) task because of the higher complexity of feature extraction and fusion brought by the additional modalities. Although recent methods have achieved early success in improving feature extraction technique for AVSD, the technique of feature fusion still needs further investigation. In this paper, inspired by the success of self-attention mechanism and the importance of understanding questions for VQA answering, we propose a question-guided self-attentive multi-modal fusion network (QUALIFIER) (QUALIFIER) to improve the AVSD practice in the stage of feature fusion and answer generation. Specifically, after extracting features and learning a comprehensive feature for each modality, we first use the designed self-attentive multi-modal fusion (SMF) module to aggregate each feature with the correlated information learned from others. Later, by prioritizing the question feature, we concatenate it with each fused feature to guide the generation of a natural language response to the question. As for experimental results, QUALIFIER shows better performance than other baseline methods in the large-scale AVSD dataset named DSTC7. Additionally, the human evaluation and ablation study results also demonstrate the effectiveness of our network architecture.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ye_QUALIFIER_Question-Guided_Self-Attentive_Multimodal_Fusion_Network_for_Audio_Visual_Scene-Aware_WACV_2022_paper.pdf",
        "aff": "The Pennsylvania State University; Microsoft Azure Computer Vision; The Pennsylvania State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1062783,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=737874966263324984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "psu.edu;microsoft.com;psu.edu",
        "email": "psu.edu;microsoft.com;psu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Pennsylvania State University;Microsoft",
        "aff_unique_dep": ";Azure Computer Vision",
        "aff_unique_url": "https://www.psu.edu;https://azure.microsoft.com/en-us/services/computer-vision/",
        "aff_unique_abbr": "PSU;Microsoft Azure CV",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "269d7e6788",
        "title": "Quantified Facial Expressiveness for Affective Behavior Analytics",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Uddin_Quantified_Facial_Expressiveness_for_Affective_Behavior_Analytics_WACV_2022_paper.html",
        "author": "Md Taufeeq Uddin; Shaun Canavan",
        "abstract": "The quantified measurement of facial expressiveness is crucial to analyze human affective behavior at scale. Unfortunately, methods for expressiveness quantification at the video frame-level are largely unexplored, unlike the study of discrete expression. In this work, we propose an algorithm that quantifies facial expressiveness using a bounded, continuous expressiveness score using multimodal facial features, such as action units (AUs), landmarks, head pose, and gaze. The proposed algorithm more heavily weights AUs with high intensities and large temporal changes. The proposed algorithm can compute the expressiveness in terms of discrete expression, and can be used to perform tasks including facial behavior tracking and subjectivity quantification in context. Our results on benchmark datasets show the proposed algorithm is effective in terms of capturing temporal changes and expressiveness, measuring subjective differences in context, and extracting useful insight.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Uddin_Quantified_Facial_Expressiveness_for_Affective_Behavior_Analytics_WACV_2022_paper.pdf",
        "aff": "University of South Florida, Tampa, FL, US; University of South Florida, Tampa, FL, US",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.01758",
        "pdf_size": 2729603,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16048518968943937962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "usf.edu;usf.edu",
        "email": "usf.edu;usf.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of South Florida",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usf.edu",
        "aff_unique_abbr": "USF",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tampa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "aa9dc88c13",
        "title": "REFICS: A Step Towards Linking Vision With Hardware Assurance",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wilson_REFICS_A_Step_Towards_Linking_Vision_With_Hardware_Assurance_WACV_2022_paper.html",
        "author": "Ronald Wilson; Hangwei Lu; Mengdi Zhu; Domenic Forte; Damon L. Woodard",
        "abstract": "Hardware assurance is a key process in ensuring the integrity, security and functionality of a hardware device. Its heavy reliance on images, especially on Scanning Electron Microscopy images, makes it an excellent candidate for the vision community. The goal of this paper is to provide a pathway for inter-community collaboration by introducing the existing challenges for hardware assurance on integrated circuits in the context of computer vision and support further development using a large-scale dataset with 800,000 images. A detailed benchmark of existing vision approaches in hardware assurance on the dataset is also included for quantitative insights into the problem.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wilson_REFICS_A_Step_Towards_Linking_Vision_With_Hardware_Assurance_WACV_2022_paper.pdf",
        "aff": ";;;;",
        "project": "https://trust-hub.org",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Wilson_REFICS_A_Step_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1173579,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12337015369176736898&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "bf03620704",
        "title": "REGroup: Rank-Aggregating Ensemble of Generative Classifiers for Robust Predictions",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tiwari_REGroup_Rank-Aggregating_Ensemble_of_Generative_Classifiers_for_Robust_Predictions_WACV_2022_paper.html",
        "author": "Lokender Tiwari; Anish Madan; Saket Anand; Subhashis Banerjee",
        "abstract": "Deep Neural Networks (DNNs) are often criticized for being susceptible to adversarial attacks. Most successful defense strategies adopt adversarial training or random input transformations that typically require retraining or fine-tuning the model to achieve reasonable performance. In this work, our investigations of intermediate representations of a pre-trained DNN lead to an interesting discovery pointing to intrinsic robustness to adversarial attacks. We find that we can learn a generative classifier by statistically characterizing the neural response of an intermediate layer to clean training samples. The predictions of multiple such intermediate-layer based classifiers, when aggregated, show unexpected robustness to adversarial attacks. Specifically, we devise an ensemble of these generative classifiers that rank-aggregates their predictions via a Borda count-based consensus. Our proposed approach uses a subset of the clean training data and a pre-trained model, and yet is agnostic to network architectures or the adversarial attack generation method. We show extensive experiments to establish that our defense strategy achieves state-of-the-art performance on the ImageNet validation set.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tiwari_REGroup_Rank-Aggregating_Ensemble_of_Generative_Classifiers_for_Robust_Predictions_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Tiwari_REGroup_Rank-Aggregating_Ensemble_WACV_2022_supplemental.pdf",
        "arxiv": "2006.10679",
        "pdf_size": 3159486,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17663269560142389809&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "15754a832f",
        "title": "RGL-NET: A Recurrent Graph Learning Framework for Progressive Part Assembly",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Narayan_RGL-NET_A_Recurrent_Graph_Learning_Framework_for_Progressive_Part_Assembly_WACV_2022_paper.html",
        "author": "Abhinav Narayan; Rajendra Nagar; Shanmuganathan Raman",
        "abstract": "Autonomous assembly of objects is an essential task in robotics and 3D computer vision. It has been studied extensively in robotics as a problem of motion planning, actuator control and obstacle avoidance. However, the task of developing a generalized framework for assembly robust to structural variants remains relatively unexplored. In this work, we tackle this problem using a recurrent graph learning framework considering inter-part relations and the progressive update of the part pose. Our network can learn more plausible predictions of shape structure by accounting for priorly assembled parts. Compared to the current state-of-the-art, our network yields up to 10% improvement in part accuracy and up to 15% improvement in connectivity accuracy on the PartNet dataset. Moreover, our resulting latent space facilitates exciting applications such as shape recovery from the point-cloud components. We conduct extensive experiments to justify our design choices and demonstrate the effectiveness of the proposed framework.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Narayan_RGL-NET_A_Recurrent_Graph_Learning_Framework_for_Progressive_Part_Assembly_WACV_2022_paper.pdf",
        "aff": "CVIG Lab, IIT Gandhinagar; IIT Jodhpur; CVIG Lab, IIT Gandhinagar",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Narayan_RGL-NET_A_Recurrent_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1898042,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16352475064824487010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": "iitgn.ac.in;iitj.ac.in;iitgn.ac.in",
        "email": "iitgn.ac.in;iitj.ac.in;iitgn.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Indian Institute of Technology Gandhinagar;Indian Institute of Technology Jodhpur",
        "aff_unique_dep": "CVIG Lab;",
        "aff_unique_url": "https://www.iitgn.ac.in;https://www.iitj.ac.in",
        "aff_unique_abbr": "IITGN;IITJ",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Gandhinagar;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "868db6f7a8",
        "title": "RLSS: A Deep Reinforcement Learning Algorithm for Sequential Scene Generation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ostonov_RLSS_A_Deep_Reinforcement_Learning_Algorithm_for_Sequential_Scene_Generation_WACV_2022_paper.html",
        "author": "Azimkhon Ostonov; Peter Wonka; Dominik L. Michels",
        "abstract": "We present RLSS: a reinforcement learning algorithm for sequential scene generation. This is based on employing the proximal policy optimization (PPO) algorithm for generative problems. In particular, we consider how to effectively reduce the action space by including a greedy search algorithm in the learning process. Our experiments demonstrate that our method converges for a relatively large number of actions and learns to generate scenes with predefined design objectives. This approach is placing objects iteratively in the virtual scene. In each step, the network chooses which objects to place and selects positions which result in maximal reward. A high reward is assigned if the last action resulted in desired properties whereas the violation of constraints is penalized. We demonstrate the capability of our method to generate plausible and diverse scenes efficiently by solving indoor planning problems and generating Angry Birds levels.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ostonov_RLSS_A_Deep_Reinforcement_Learning_Algorithm_for_Sequential_Scene_Generation_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ostonov_RLSS_A_Deep_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6362404,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1280654400756118681&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "fdc6448efe",
        "title": "Re-Compose the Image by Evaluating the Crop on More Than Just a Score",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Cheng_Re-Compose_the_Image_by_Evaluating_the_Crop_on_More_Than_WACV_2022_paper.html",
        "author": "Yang Cheng; Qian Lin; Jan P. Allebach",
        "abstract": "Image re-composition has always been regarded as one of the most important steps during the post-processing of a photo. The quality of an image re-composition mainly depends on a person's taste in aesthetics, which is not an effortless task for those who have no abundant experience in photography. Besides, while re-composing one image does not require much of a person's time, it could be quite time-consuming when there are hundreds of images to be re-composed. To solve these problems, we propose a method that automates the process of re-composing an image to the desired aspect ratio. Although there already exist many image re-composition methods, they only provide a score to their predicted best crop but fail to explain why the score is high or low. Conversely, we succeed in designing an explainable method by introducing a novel 10-layer aesthetic score map, which represents how the position of the saliency in the original uncropped image, relative to that of the crop region, contributes to the overall score of the crop, so that the crop is not just represented by a single score. We conducted experiments to show that the proposed score map boosts the performance of our algorithm, which achieves a state-of-the-art performance on both public and our own datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Cheng_Re-Compose_the_Image_by_Evaluating_the_Crop_on_More_Than_WACV_2022_paper.pdf",
        "aff": "Purdue University; HP Labs, HP Inc.; Purdue University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4405132,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3917512035439748745&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "purdue.edu;hp.com;purdue.edu",
        "email": "purdue.edu;hp.com;purdue.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Purdue University;HP Inc.",
        "aff_unique_dep": ";HP Labs",
        "aff_unique_url": "https://www.purdue.edu;https://www.hp.com",
        "aff_unique_abbr": "Purdue;HP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4957e62a1e",
        "title": "Reconstructing Training Data From Diverse ML Models by Ensemble Inversion",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wang_Reconstructing_Training_Data_From_Diverse_ML_Models_by_Ensemble_Inversion_WACV_2022_paper.html",
        "author": "Qian Wang; Daniel Kurz",
        "abstract": "Model Inversion (MI), in which an adversary abuses access to a trained Machine Learning (ML) model attempting to infer sensitive information about its original training data, has attracted increasing research attention. During MI, the trained model under attack (MUA) is usually frozen and used to guide the training of a generator, such as a Generative Adversarial Network (GAN), to reconstruct the distribution prior of that model. This might cause leakage of original training samples, and if successful, the privacy of dataset subjects will be at risk if the training data contains Personally Identifiable Information (PII). Therefore, an in-depth investigation of the potentials of MI techniques is crucial for the development of corresponding defense techniques. High-quality reconstruction of training data based on a single model is challenging. However, existing MI literature does not explore targeting multiple trained models simultaneously, which may provide additional information and diverse perspectives to the adversary. In this work, we propose the ensemble inversion technique that estimates the distribution of original training data, by training a generator constrained by an ensemble (or set) of trained models with shared subjects or entities. This technique leads to noticeable improvements of the quality of the generated samples with distinguishable features of the dataset entities compared to MI of a single model. We utilize an auxiliary dataset that's similar to the presumed training data, but we also demonstrate high quality data-free model inversion without such dataset. The impact of model diversity in the ensemble is thoroughly investigated in this work, and additional constraints are utilized to further encourage sharp predictions and high activations for the reconstructed samples, leading to more accurate reconstruction of training images.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wang_Reconstructing_Training_Data_From_Diverse_ML_Models_by_Ensemble_Inversion_WACV_2022_paper.pdf",
        "aff": "Apple; Apple",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2111.03702",
        "pdf_size": 5213284,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15863237771450325814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "apple.com;apple.com",
        "email": "apple.com;apple.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Apple",
        "aff_unique_dep": "Apple Inc.",
        "aff_unique_url": "https://www.apple.com",
        "aff_unique_abbr": "Apple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ece08087c3",
        "title": "Recursive Contour-Saliency Blending Network for Accurate Salient Object Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ke_Recursive_Contour-Saliency_Blending_Network_for_Accurate_Salient_Object_Detection_WACV_2022_paper.html",
        "author": "Yun Yi Ke; Takahiro Tsubono",
        "abstract": "Contour information plays a vital role in salient object detection. However, excessive false positives remain in predictions from existing contour-based models due to insufficient contour-saliency fusion. In this work, we designed a network for better edge quality in salient object detection. We proposed a contour-saliency blending module to exchange information between contour and saliency. We adopted recursive CNN to increase contour-saliency fusion while keeping the total trainable parameters the same. Furthermore, we designed a stage-wise feature extraction module to help the model pick up the most helpful features from previous intermediate saliency predictions. Besides, we proposed two new loss functions, namely Dual Confinement Loss and Confidence Loss, for our model to generate better boundary predictions. Evaluation results on five common benchmark datasets reveal that our model achieves competitive state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ke_Recursive_Contour-Saliency_Blending_Network_for_Accurate_Salient_Object_Detection_WACV_2022_paper.pdf",
        "aff": "Computer Vision & AI Technology Lab, Open8 Singapore; Computer Vision & AI Technology Lab, Open8 Singapore",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ke_Recursive_Contour-Saliency_Blending_WACV_2022_supplemental.pdf",
        "arxiv": "2105.13865",
        "pdf_size": 6428702,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13602008804648541142&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;open8.com",
        "email": "gmail.com;open8.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Open8 Singapore",
        "aff_unique_dep": "Computer Vision & AI Technology Lab",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "836edb17e8",
        "title": "Registration of Human Point Set Using Automatic Key Point Detection and Region-Aware Features",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Maharjan_Registration_of_Human_Point_Set_Using_Automatic_Key_Point_Detection_WACV_2022_paper.html",
        "author": "Amar Maharjan; Xiaohui Yuan",
        "abstract": "Non-rigid point set registration is challenging when point sets have large deformations and different numbers of points. Examples of such point sets include human point sets representing complex human poses captured by different types of depth cameras. In this work, we present a probabilistic, non-rigid registration method to deal with these issues. Two regularization terms are used: key point correspondences and local neighborhood preservation. Our method detects key points in the point sets based on geodesic distance. Correspondences are established using a new cluster-based, region-aware feature descriptor. This feature descriptor encodes the association of a cluster to the left-right (symmetry) or upper-lower regions of the point sets. We use the Stochastic Neighbor Embedding (SNE) constraint to preserve the local neighborhood of the point set. Experimental results on challenging 3D human poses demonstrate that our method outperforms the state-of-the-art methods. Our method achieved highly competitive performance with a slight increase of error by 3.9% in comparison with the method using manually specified key point correspondences.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Maharjan_Registration_of_Human_Point_Set_Using_Automatic_Key_Point_Detection_WACV_2022_paper.pdf",
        "aff": "University of North Texas, Denton; University of North Texas, Denton",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9007650,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2496273165210888104&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "my.unt.edu;unt.edu",
        "email": "my.unt.edu;unt.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of North Texas",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unt.edu",
        "aff_unique_abbr": "UNT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Denton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2f9ff8d1a5",
        "title": "Resolution-Robust Large Mask Inpainting With Fourier Convolutions",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Suvorov_Resolution-Robust_Large_Mask_Inpainting_With_Fourier_Convolutions_WACV_2022_paper.html",
        "author": "Roman Suvorov; Elizaveta Logacheva; Anton Mashikhin; Anastasia Remizova; Arsenii Ashukha; Aleksei Silvestrov; Naejin Kong; Harshith Goka; Kiwoong Park; Victor Lempitsky",
        "abstract": "Modern image inpainting systems, despite the significant progress, often struggle with large missing areas, complex geometric structures, and high-resolution images. We find that one of the main reasons for that is the lack of an effective receptive field in both the inpainting network and the loss function. To alleviate this issue, we propose a new method called large mask inpainting (LaMa). LaMa is based on i) a new inpainting network architecture that uses fast Fourier convolutions (FFCs), which have the image-wide receptive field; ii) a high receptive field perceptual loss; iii) large training masks, which unlocks the potential of the first two components. Our inpainting network improves the state-of-the-art across a range of datasets and achieves excellent performance even in challenging scenarios, e.g. completion of periodic structures. Our model generalizes surprisingly well to resolutions that are higher than those seen at train time, and achieves this at lower parameter & time costs than the competitive baselines. The code is available at https://github.com/saic-mdal/lama.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Suvorov_Resolution-Robust_Large_Mask_Inpainting_With_Fourier_Convolutions_WACV_2022_paper.pdf",
        "aff": "Samsung AI Center Moscow; Samsung AI Center Moscow; Samsung AI Center Moscow; Swiss Federal Institute of Technology Lausanne (EPFL); Samsung AI Center Moscow; Samsung AI Center Moscow; Samsung Research; Samsung Research; Samsung Research; Samsung AI Center Moscow + Skolkovo Institute of Science and Technology, Moscow, Russia",
        "project": "",
        "github": "https://github.com/saic-mdal/lama",
        "supp": "",
        "arxiv": "2109.07161",
        "pdf_size": 4554481,
        "gs_citation": 1112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9821294211947478445&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com; ; ; ; ; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ; ; ; ; ",
        "author_num": 10,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0;0;0;0;0;0+2",
        "aff_unique_norm": "Samsung;Swiss Federal Institute of Technology Lausanne;Skolkovo Institute of Science and Technology",
        "aff_unique_dep": "AI Center;;",
        "aff_unique_url": "https://www.samsung.com/global/innovation/ai-research/;https://www.epfl.ch;https://www.skoltech.ru",
        "aff_unique_abbr": "Samsung AI;EPFL;Skoltech",
        "aff_campus_unique_index": "0;0;0;1;0;0;0+0",
        "aff_campus_unique": "Moscow;Lausanne;",
        "aff_country_unique_index": "0;0;0;1;0;0;2;2;2;0+0",
        "aff_country_unique": "Russian Federation;Switzerland;South Korea"
    },
    {
        "id": "0fea688c7a",
        "title": "Resource-Efficient Hybrid X-Formers for Vision",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Jeevan_Resource-Efficient_Hybrid_X-Formers_for_Vision_WACV_2022_paper.html",
        "author": "Pranav Jeevan; Amit Sethi",
        "abstract": "Although transformers have become the neural architectures of choice for natural language processing, they require orders of magnitude more training data, GPU memory, and computations in order to compete with convolutional neural networks for computer vision. The attention mechanism of transformers scales quadratically with the length of the input sequence, and unrolled images have long sequence lengths. Plus, transformers lack an inductive bias that is appropriate for images. We tested three modifications to vision transformer (ViT) architectures that address these shortcomings. Firstly, we alleviate the quadratic bottleneck by using linear attention mechanisms, called X-formers (such that, X in  Performer, Linformer, Nystromformer ), thereby creating Vision X-formers (ViXs). This resulted in up to a seven times reduction in the GPU memory requirement. We also compared their performance with FNet and multi-layer perceptron mixers, which further reduced the GPU memory requirement. Secondly, we introduced an inductive prior for images by replacing the initial linear embedding layer by convolutional layers in ViX, which significantly increased classification accuracy without increasing the model size. Thirdly, we replaced the learnable 1D position embeddings in ViT with Rotary Position Embedding (RoPE), which increases the classification accuracy for the same model size. We believe that incorporating such changes can democratize transformers by making them accessible to those with limited data and computing resources.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Jeevan_Resource-Efficient_Hybrid_X-Formers_for_Vision_WACV_2022_paper.pdf",
        "aff": "Indian Institute of Technology Bombay; Indian Institute of Technology Bombay",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 481309,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8478491652400788199&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ee.iitb.ac.in;iitb.ac.in",
        "email": "ee.iitb.ac.in;iitb.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitb.ac.in",
        "aff_unique_abbr": "IIT Bombay",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bombay",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "cacb5b350e",
        "title": "Rethinking Video Anomaly Detection - A Continual Learning Approach",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Doshi_Rethinking_Video_Anomaly_Detection_-_A_Continual_Learning_Approach_WACV_2022_paper.html",
        "author": "Keval Doshi; Yasin Yilmaz",
        "abstract": "While video anomaly detection has been an active area of research for several years, recent progress is limited to improving the state-of-the-art results on small datasets using an inadequate evaluation criterion. In this work, we take a new comprehensive look at the video anomaly detection problem from a more realistic perspective. Specifically, we consider practical challenges such as continual learning and few-shot learning, which humans can easily do but remains to be a significant challenge for machines. A novel algorithm designed for such practical challenges is also proposed. For performance evaluation in this new framework, we introduce a new dataset which is significantly more comprehensive than the existing benchmark datasets, and a new performance metric which takes into account the fundamental temporal aspect of video anomaly detection. The experimental results show that the existing state-of-the-art methods are not suitable for the considered practical challenges, and the proposed algorithm outperforms them with a large margin in continual learning and few-shot learning tasks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Doshi_Rethinking_Video_Anomaly_Detection_-_A_Continual_Learning_Approach_WACV_2022_paper.pdf",
        "aff": "University of South FLorida; University of South FLorida",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Doshi_Rethinking_Video_Anomaly_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 7051974,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7048843744639645049&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "usf.edu;usf.edu",
        "email": "usf.edu;usf.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of South Florida",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usf.edu",
        "aff_unique_abbr": "USF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9178a7d49c",
        "title": "Revealing Disocclusions in Temporal View Synthesis Through Infilling Vector Prediction",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kanchana_Revealing_Disocclusions_in_Temporal_View_Synthesis_Through_Infilling_Vector_Prediction_WACV_2022_paper.html",
        "author": "Vijayalakshmi Kanchana; Nagabhushan Somraj; Suraj Yadwad; Rajiv Soundararajan",
        "abstract": "We consider the problem of temporal view synthesis, where the goal is to predict a future video frame from the past frames using knowledge of the depth and relative camera motion. In contrast to revealing the disoccluded regions through intensity based infilling, we study the idea of an infilling vector to infill by pointing to a non-disoccluded region in the synthesized view. To exploit the structure of disocclusions created by camera motion during their infilling, we rely on two important cues, temporal correlation of infilling directions and depth. We design a learning framework to predict the infilling vector by computing a temporal prior that reflects past infilling directions and a normalized depth map as input to the network. We conduct extensive experiments on a large scale dataset we build for evaluating temporal view synthesis in addition to the SceneNet RGB-D dataset. Our experiments demonstrate that our infilling vector prediction approach achieves superior quantitative and qualitative infilling performance compared to other approaches in literature. Our dataset and code can be found at https://nagabhushansn95.github.io/publications/2021/ivp.html",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kanchana_Revealing_Disocclusions_in_Temporal_View_Synthesis_Through_Infilling_Vector_Prediction_WACV_2022_paper.pdf",
        "aff": "Indian Institute of Science, Bengaluru, India; Indian Institute of Science, Bengaluru, India; Indian Institute of Science, Bengaluru, India; Indian Institute of Science, Bengaluru, India",
        "project": "https://nagabhushansn95.github.io/publications/2021/ivp.html",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Kanchana_Revealing_Disocclusions_in_WACV_2022_supplemental.zip",
        "arxiv": "2110.08805",
        "pdf_size": 8053531,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2754094498656198955&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "iisc.ac.in;iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "email": "iisc.ac.in;iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Bengaluru",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "d9d9cc4e51",
        "title": "Robust 3D Garment Digitization From Monocular 2D Images for 3D Virtual Try-On Systems",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Majithia_Robust_3D_Garment_Digitization_From_Monocular_2D_Images_for_3D_WACV_2022_paper.html",
        "author": "Sahib Majithia; Sandeep N. Parameswaran; Sadbhavana Babar; Vikram Garg; Astitva Srivastava; Avinash Sharma",
        "abstract": "In this paper, we develop a robust 3D garment digitization solution that can generalize well on real-world fashion catalog images with cloth texture occlusions and large body pose variations. We assumed fixed topology parametric template mesh models for known types of garments (e.g., T-shirts, Trousers) and perform mapping of high-quality texture from an input catalog image to UV map panels corresponding to the parametric mesh model of the garment. We achieve this by first predicting a sparse set of 2D landmarks on the boundary of the garments. Subsequently, we use these landmarks to perform Thin-Plate-Spline-based texture transfer on UV map panels. Subsequently, we employ a deep texture inpainting network to fill the large holes (due to view variations & self-occlusions) in TPS output to generate consistent UV maps. Furthermore, to train the supervised deep networks for landmark prediction & texture inpainting tasks, we generated a large set of synthetic data with varying texture and lighting imaged from various views with the human present in a wide variety of poses. Additionally, we manually annotated a small set of fashion catalog images crawled from online fashion e-commerce platforms to finetune. We conduct thorough empirical evaluations and show impressive qualitative results of our proposed 3D garment texture solution on fashion catalog images. Such 3D garment digitization helps us solve the challenging task of enabling 3D Virtual Try-on.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Majithia_Robust_3D_Garment_Digitization_From_Monocular_2D_Images_for_3D_WACV_2022_paper.pdf",
        "aff": "Myntra Designs Pvt. Ltd., Bangalore, India; Myntra Designs Pvt. Ltd., Bangalore, India; Myntra Designs Pvt. Ltd., Bangalore, India; Myntra Designs Pvt. Ltd., Bangalore, India; IIIT-Hyderabad, India; IIIT-Hyderabad, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Majithia_Robust_3D_Garment_WACV_2022_supplemental.zip",
        "arxiv": "2111.15140",
        "pdf_size": 7773959,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=622062559904341037&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "myntra.com;myntra.com;myntra.com;myntra.com;research.iiit.ac.in;iiit.ac.in",
        "email": "myntra.com;myntra.com;myntra.com;myntra.com;research.iiit.ac.in;iiit.ac.in",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "Myntra Designs Pvt. Ltd.;International Institute of Information Technology, Hyderabad",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": ";IIIT-H",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hyderabad",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "706a4710a9",
        "title": "Robust High-Resolution Video Matting With Temporal Guidance",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lin_Robust_High-Resolution_Video_Matting_With_Temporal_Guidance_WACV_2022_paper.html",
        "author": "Shanchuan Lin; Linjie Yang; Imran Saleemi; Soumyadip Sengupta",
        "abstract": "We introduce a robust, real-time, high-resolution human video matting method that achieves new state-of-the-art performance. Our method is much lighter than previous approaches and can process 4K at 76 FPS and HD at 104 FPS on an Nvidia GTX 1080Ti GPU. Unlike most existing methods that perform video matting frame-by-frame as independent images, our method uses a recurrent architecture to exploit temporal information in videos and achieves significant improvements in temporal coherence and matting quality. Furthermore, we propose a novel training strategy that enforces our network on both matting and segmentation objectives. This significantly improves our model's robustness. Our method does not require any auxiliary inputs such as a trimap or a pre-captured background image, so it can be widely applied to existing human matting applications.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lin_Robust_High-Resolution_Video_Matting_With_Temporal_Guidance_WACV_2022_paper.pdf",
        "aff": "University of Washington; ByteDance Inc.; ByteDance Inc.; University of Washington",
        "project": "",
        "github": "https://peterl1n.github.io/RobustVideoMatting/",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Lin_Robust_High-Resolution_Video_WACV_2022_supplemental.pdf",
        "arxiv": "2108.11515",
        "pdf_size": 1678081,
        "gs_citation": 201,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=710744047325669652&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cs.washington.edu;bytedance.com;bytedance.com;cs.washington.edu",
        "email": "cs.washington.edu;bytedance.com;bytedance.com;cs.washington.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Washington;ByteDance",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.bytedance.com",
        "aff_unique_abbr": "UW;ByteDance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "5cd7615bc7",
        "title": "Robust Lane Detection via Expanded Self Attention",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lee_Robust_Lane_Detection_via_Expanded_Self_Attention_WACV_2022_paper.html",
        "author": "Minhyeok Lee; Junhyeop Lee; Dogyoon Lee; Woojin Kim; Sangwon Hwang; Sangyoun Lee",
        "abstract": "The image-based lane detection algorithm is one of the key technologies in autonomous vehicles. Modern deep learning methods achieve high performance in lane detection, but it is still difficult to accurately detect lanes in challenging situations such as congested roads and extreme lighting conditions. To be robust on these challenging situations, it is important to extract global contextual information even from limited visual cues. In this paper, we propose a simple but powerful self-attention mechanism optimized for lane detection called the Expanded Self Attention (ESA) module. Inspired by the simple geometric structure of lanes, the proposed method predicts the confidence of a lane along the vertical and horizontal directions in an image. The prediction of the confidence enables estimating occluded locations by extracting global contextual information. ESA module can be easily implemented and applied to any encoder-decoder-based model without increasing the inference time. The performance of our method is evaluated on three popular lane detection benchmarks (TuSimple, CULane and BDD100K). We achieve state-of-the-art performance in CULane and BDD100K and distinct improvement on TuSimple dataset. The experimental results show that our approach is robust to occlusion and extreme lighting conditions.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lee_Robust_Lane_Detection_via_Expanded_Self_Attention_WACV_2022_paper.pdf",
        "aff": "Yonsei University School of Electrical and Electronic Engineering; Yonsei University School of Electrical and Electronic Engineering; Yonsei University School of Electrical and Electronic Engineering; Yonsei University School of Electrical and Electronic Engineering; Yonsei University School of Electrical and Electronic Engineering; Yonsei University School of Electrical and Electronic Engineering",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2102.07037",
        "pdf_size": 7070738,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=350276598145903959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "email": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Yonsei University",
        "aff_unique_dep": "School of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.yonsei.ac.kr",
        "aff_unique_abbr": "Yonsei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "7ae62992ed",
        "title": "Robustly Recognizing Irregular Scene Text by Rectifying Principle Irregularities",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Xu_Robustly_Recognizing_Irregular_Scene_Text_by_Rectifying_Principle_Irregularities_WACV_2022_paper.html",
        "author": "Changsheng Xu; Yang Wang; Fan Bai; Jihong Guan; Shuigeng Zhou",
        "abstract": "Reading irregular scene text is a challenging problem in scene text recognition. Rectification is a popular measure to reduce irregularities of text in images. Existing rectification methods seek to rectify text images into a strictly regular form via free parametric transformation functions. However, they always suffer from information loss or severe deformation due to their poor constraints to the transformation functions. In our investigation, we found that CNN and attention are robust to many slight irregularities. That inspires us to propose a novel and effective rectification method that mainly rectifies the principle regularities, and leaves the slight irregularities to the CNNLSTM-attention recognizer. Our rectification method first estimates the character densities and directions of the input image in a down-sampled map, then finds a best fitting curve from a small predefined Bezier curve set, and finally rectifies the input image with a transformation function corresponding to the selected curve. Transformation functions are carefully designed so that they neither lose important visual information nor cause severe deformation. Extensive experiments on seven benchmark datasets show that our method achieves the state of the art performance in most cases, especially in curved text recognition.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Xu_Robustly_Recognizing_Irregular_Scene_Text_by_Rectifying_Principle_Irregularities_WACV_2022_paper.pdf",
        "aff": "School of Computer Science, Fudan University; Dept. of Computer Sci. & Tech., Tongji University; School of Computer Science, Fudan University; Dept. of Computer Sci. & Tech., Tongji University; School of Computer Science, Fudan University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 670040,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16336971381650809071&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "fudan.edu.cn;tongji.edu.cn;fudan.edu.cn;tongji.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;tongji.edu.cn;fudan.edu.cn;tongji.edu.cn;fudan.edu.cn",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "Fudan University;Tongji University",
        "aff_unique_dep": "School of Computer Science;Department of Computer Science and Technology",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.tongji.edu.cn",
        "aff_unique_abbr": "Fudan;Tongji",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "175a056af8",
        "title": "S2-MLP: Spatial-Shift MLP Architecture for Vision",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yu_S2-MLP_Spatial-Shift_MLP_Architecture_for_Vision_WACV_2022_paper.html",
        "author": "Tan Yu; Xu Li; Yunfeng Cai; Mingming Sun; Ping Li",
        "abstract": "Recently, visual Transformer (ViT) and its following works abandon the convolution and exploit the self-attention operation, attaining a comparable or even higher accuracy than CNN. More recently, MLP-mixer abandons both the convolution and the self-attention operation, proposing an architecture containing only MLP layers. To achieve cross-patch communications, it devises an additional token-mixing MLP besides the channel-mixing MLP. It achieves promising results when training on an extremely large-scale dataset such as JFT-300M. But it cannot achieve as outstanding performance as its CNN and ViT counterparts when training on medium-scale datasets such as ImageNet-1K and ImageNet-21K. The performance drop of MLP-mixer motivates us to rethink the token-mixing MLP. We discover that token-mixing operation in MLP-mixer is a variant of depthwise convolution with a global reception field and spatial-specific configuration. It is the global reception field and the spatial-specific property that make token-mixing MLP prone to over-fitting. In this paper, we propose a novel pure MLP architecture, spatial-shift MLP (S^2-MLP). Different from MLP-mixer, our S^2-MLP only contains channel-mixing MLP. We devise a spatial-shift operation for achieving the communication between patches. It has a local reception field and is spatial-agnostic. Meanwhile, it is parameter-free and efficient for computation. The proposed S^2-MLP attains higher recognition accuracy than MLP-mixer when training on ImageNet-1K dataset. Meanwhile, S^2-MLP accomplishes as excellent performance as ViT on ImageNet-1K dataset with considerably simpler architecture and fewer FLOPs and parameters.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yu_S2-MLP_Spatial-Shift_MLP_Architecture_for_Vision_WACV_2022_paper.pdf",
        "aff": "Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, Washington 98004, USA + No.10 Xibeiwang East Road, Beijing 100193, China; Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, Washington 98004, USA + No.10 Xibeiwang East Road, Beijing 100193, China; Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, Washington 98004, USA + No.10 Xibeiwang East Road, Beijing 100193, China; Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, Washington 98004, USA + No.10 Xibeiwang East Road, Beijing 100193, China; Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, Washington 98004, USA + No.10 Xibeiwang East Road, Beijing 100193, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 604682,
        "gs_citation": 258,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7701660998804727435&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Baidu;Tsinghua University",
        "aff_unique_dep": "Cognitive Computing Lab;",
        "aff_unique_url": "https://research.baidu.com;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Baidu;THU",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_campus_unique": "Bellevue;Beijing",
        "aff_country_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "8c10245660",
        "title": "S2FGAN: Semantically Aware Interactive Sketch-To-Face Translation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yang_S2FGAN_Semantically_Aware_Interactive_Sketch-To-Face_Translation_WACV_2022_paper.html",
        "author": "Yan Yang; Md Zakir Hossain; Tom Gedeon; Shafin Rahman",
        "abstract": "Interactive facial image manipulation attempts to edit single and multiple face attributes using a photo-realistic face and/or semantic mask as input. In the absence of the photo-realistic image (only sketch/mask available), previous methods only retrieve the original face but ignore the potential of aiding model controllability and diversity in the translation process. This paper proposes a sketch-to-image generation framework called S2FGAN, aiming to improve users' ability to interpret and flexibility of face attribute editing from a simple sketch. First, to restore a vivid face from a sketch, we propose semantic level perceptual loss to increase the translation quality. Second, we dedicate the theoretic analysis of attribute editing and build attribute mapping networks with latent semantic loss to modify latent space semantics of Generative Adversarial Networks (GANs). The users can command the model to retouch the generated images by involving the semantic information in the generation process. In this way, our method can manipulate single or multiple face attributes by only specifying attributes to be changed. Extensive experimental results on the CelebAMask-HQ dataset empirically show our superior performance and effectiveness on this task. Our method successfully outperforms state-of-the-art sketch-to-image generation and attribute manipulation methods by exploiting greater control of attribute intensity.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yang_S2FGAN_Semantically_Aware_Interactive_Sketch-To-Face_Translation_WACV_2022_paper.pdf",
        "aff": "BDSI, Australian National University, Australia + A&F, CSIRO, Australia; BDSI, Australian National University, Australia + A&F, CSIRO, Australia; EECMS, Curtin University, Australia; ECE, North South University, Bangladesh",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yang_S2FGAN_Semantically_Aware_WACV_2022_supplemental.pdf",
        "arxiv": "2011.14785",
        "pdf_size": 4501283,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18340607306897139197&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "anu.edu.au;anu.edu.au;curtin.edu.au;northsouth.edu",
        "email": "anu.edu.au;anu.edu.au;curtin.edu.au;northsouth.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2;3",
        "aff_unique_norm": "Australian National University;CSIRO;Curtin University;North South University",
        "aff_unique_dep": "BDSI;Agriculture and Food;EECMS;Electrical and Computer Engineering",
        "aff_unique_url": "https://www.anu.edu.au;https://www.csiro.au;https://www.curtin.edu.au;https://www.nsu.edu.bd",
        "aff_unique_abbr": "ANU;CSIRO;;NSU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;1",
        "aff_country_unique": "Australia;Bangladesh"
    },
    {
        "id": "1caae27a7b",
        "title": "SAC: Semantic Attention Composition for Text-Conditioned Image Retrieval",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Jandial_SAC_Semantic_Attention_Composition_for_Text-Conditioned_Image_Retrieval_WACV_2022_paper.html",
        "author": "Surgan Jandial; Pinkesh Badjatiya; Pranit Chawla; Ayush Chopra; Mausoom Sarkar; Balaji Krishnamurthy",
        "abstract": "The ability to efficiently search for images is essential for improving the user experiences across various products. Incorporating user feedback, via multi-modal inputs, to navigate visual search can help tailor retrieved results to specific user queries. We focus on the task of text-conditioned image retrieval that utilizes support text feedback alongside a reference image to retrieve images that concurrently satisfy constraints imposed by both inputs. The task is challenging since it requires learning composite image-text features by incorporating multiple cross-granular semantic edits from text feedback and then applying the same to visual features. To address this, we propose a novel framework SAC which resolves the above in two major steps: \"where to see\" (Semantic Feature Attention) and \"how to change\" (Semantic Feature Modification). We systematically show how our architecture streamlines the generation of text-aware image features by removing the need for various modules required by other state-of-art techniques. We present extensive quantitative, qualitative analysis, and ablation studies, to show that our architecture SAC outperforms existing techniques by achieving state-of-the-art performance on 3 benchmark datasets: FashionIQ, Shoes, and Birds-to-Words while supporting natural language feedback of varying lengths.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Jandial_SAC_Semantic_Attention_Composition_for_Text-Conditioned_Image_Retrieval_WACV_2022_paper.pdf",
        "aff": "Media and Data Science Research Lab, Adobe; Microsoft, India; Indian Institute of Technology, Kharagpur; Media Lab, Massachusetts Institute of Technology; Media and Data Science Research Lab, Adobe; Media and Data Science Research Lab, Adobe",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Jandial_SAC_Semantic_Attention_WACV_2022_supplemental.pdf",
        "arxiv": "2009.01485",
        "pdf_size": 1941186,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17469549746825864485&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;0;0",
        "aff_unique_norm": "Adobe;Microsoft;Indian Institute of Technology;Massachusetts Institute of Technology",
        "aff_unique_dep": "Media and Data Science Research Lab;Microsoft Corporation;;Media Lab",
        "aff_unique_url": "https://www.adobe.com;https://www.microsoft.com/en-in;https://www.iitkgp.ac.in;https://www.mit.edu",
        "aff_unique_abbr": "Adobe;Microsoft;IIT Kharagpur;MIT",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Kharagpur;Cambridge",
        "aff_country_unique_index": "0;1;1;0;0;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "90e9e68886",
        "title": "SBEVNet: End-to-End Deep Stereo Layout Estimation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gupta_SBEVNet_End-to-End_Deep_Stereo_Layout_Estimation_WACV_2022_paper.html",
        "author": "Divam Gupta; Wei Pu; Trenton Tabor; Jeff Schneider",
        "abstract": "Accurate layout estimation is crucial for planning and navigation in robotics applications, such as self-driving. In this paper, we introduce the Stereo Bird's Eye ViewNetwork (SBEVNet), a novel supervised end-to-end framework for estimation of bird's eye view layout from a pair of stereo images. Although our network reuses some of the building blocks from the state-of-the-art deep learning networks for disparity estimation, we show that explicit depth estimation is neither sufficient nor necessary. Instead, the learning of a good internal bird's eye view feature representation is effective for layout estimation. Specifically, we first generate a disparity feature volume using the features of the stereo images and then project it to the bird's eye view coordinates. This gives us coarse-grained information about the scene structure. We also apply inverse perspective mapping (IPM) to map the input images and their features to the bird's eye view. This gives us fine-grained texture information. Concatenating IPM features with the projected feature volume creates a rich bird's eye view representation which is useful for spatial reasoning. We use this representation to estimate the BEV semantic map. Additionally, we show that using the IPM features as a supervisory signal for stereo features can give an improvement in performance. We demonstrate our approach on two datasets:the KITTI dataset and a synthetically generated dataset from the CARLA simulator. For both of these datasets, we establish state-of-the-art performance compared to baseline techniques.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gupta_SBEVNet_End-to-End_Deep_Stereo_Layout_Estimation_WACV_2022_paper.pdf",
        "aff": "Carnegie Mellon University; National Robotics Engineering Center; National Robotics Engineering Center; Carnegie Mellon University",
        "project": "",
        "github": "https://github.com/divamgupta/sbevnet-stereo-layout-estimation",
        "supp": "",
        "arxiv": "2105.11705",
        "pdf_size": 1807198,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10508857987247345469&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cmu.edu;nrec.ri.cmu.edu;nrec.ri.cmu.edu;cs.cmu.edu",
        "email": "cmu.edu;nrec.ri.cmu.edu;nrec.ri.cmu.edu;cs.cmu.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2d8cc73124",
        "title": "SC-UDA: Style and Content Gaps Aware Unsupervised Domain Adaptation for Object Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yu_SC-UDA_Style_and_Content_Gaps_Aware_Unsupervised_Domain_Adaptation_for_WACV_2022_paper.html",
        "author": "Fuxun Yu; Di Wang; Yinpeng Chen; Nikolaos Karianakis; Tong Shen; Pei Yu; Dimitrios Lymberopoulos; Sidi Lu; Weisong Shi; Xiang Chen",
        "abstract": "Current state-of-the-art object detectors can have a significant performance drop when deployed in the wild due to domain gaps with training data. Unsupervised Domain Adaptation (UDA) is a promising approach to adapt detectors for new domains/environments without any expensive label cost. Previous mainstream UDA works for object detection usually focused on image-level and/or feature-level adaptation by using adversarial learning methods. In this work, we show that such adversarial-based methods can only reduce the domain style gap, but cannot address the domain content gap that is also important for object detectors. To overcome this limitation, we propose the SC-UDA framework to concurrently reduce both gaps: We propose fine-grained domain style transfer to reduce the style gaps with finer image details preserved for detecting small objects; Then we leverage the pseudo-label-based self-training to reduce content gaps; To address pseudo label error accumulation during self-training, novel optimizations are proposed, including uncertainty-based pseudo labeling and imbalanced mini-batch sampling strategy. Experiment results show that our approach consistently outperforms prior stat-of-the-art methods (up to 8.6%, 2.7%, and 2.5% mAP on three UDA benchmarks).",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yu_SC-UDA_Style_and_Content_Gaps_Aware_Unsupervised_Domain_Adaptation_for_WACV_2022_paper.pdf",
        "aff": "George Mason University; Microsoft; Microsoft; Microsoft; Microsoft; Microsoft; Microsoft; Wayne State University; Wayne State University; George Mason University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5838518,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5673018872829545429&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "author_num": 10,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;1;1;2;2;0",
        "aff_unique_norm": "George Mason University;Microsoft;Wayne State University",
        "aff_unique_dep": ";Microsoft Corporation;",
        "aff_unique_url": "https://www.gmu.edu;https://www.microsoft.com;https://wayne.edu",
        "aff_unique_abbr": "GMU;Microsoft;WSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9437f1da73",
        "title": "SEGA: Semantic Guided Attention on Visual Prototype for Few-Shot Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yang_SEGA_Semantic_Guided_Attention_on_Visual_Prototype_for_Few-Shot_Learning_WACV_2022_paper.html",
        "author": "Fengyuan Yang; Ruiping Wang; Xilin Chen",
        "abstract": "Teaching machines to recognize a new category based on few training samples especially only one remains challenging owing to the incomprehensive understanding of the novel category caused by the lack of data. However, human can learn new classes quickly even given few samples since human can tell what discriminative features should be focused on about each category based on both the visual and semantic prior knowledge. To better utilize those prior knowledge, we propose the SEmantic Guided Attention (SEGA) mechanism where the semantic knowledge is used to guide the visual perception in a top-down manner about what visual features should be paid attention to when distinguishing a category from the others. As a result, the embedding of the novel class even with few samples can be more discriminative. Concretely, a feature extractor is trained to embed few images of each novel class into a visual prototype with the help of transferring visual prior knowledge from base classes. Then we learn a network that maps semantic knowledge to category-specific attention vectors which will be used to perform feature selection to enhance the visual prototypes. Extensive experiments on miniImageNet, tieredImageNet, CIFAR-FS, and CUB indicate that our semantic guided attention realizes anticipated function and outperforms state-of-the-art results.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yang_SEGA_Semantic_Guided_Attention_on_Visual_Prototype_for_Few-Shot_Learning_WACV_2022_paper.pdf",
        "aff": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China + Beijing Academy of Artificial Intelligence, Beijing, 100084, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yang_SEGA_Semantic_Guided_WACV_2022_supplemental.pdf",
        "arxiv": "2111.04316",
        "pdf_size": 1614410,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2266347117265262255&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1+2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Institute of Computing Technology;;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn;https://www.baaic.cn",
        "aff_unique_abbr": "CAS;UCAS;BAAI",
        "aff_campus_unique_index": "0+0;0+0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2a1d7061c0",
        "title": "SIDE: Center-Based Stereo 3D Detector With Structure-Aware Instance Depth Estimation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Peng_SIDE_Center-Based_Stereo_3D_Detector_With_Structure-Aware_Instance_Depth_Estimation_WACV_2022_paper.html",
        "author": "Xidong Peng; Xinge Zhu; Tai Wang; Yuexin Ma",
        "abstract": "3D detection plays an indispensable role in environment perception. Due to the high cost of commonly used LiDAR sensor, stereo vision based 3D detection, as an economical yet effective setting, attracts more attention recently. For these approaches based on 2D images, accurate depth information is the key to achieve 3D detection, and most existing methods resort to a preliminary stage for depth estimation. They mainly focus on the global depth and neglect the property of depth information in this specific task, namely, sparsity and locality, where exactly accurate depth is only needed for these 3D bounding boxes. Motivated by this finding, we propose a stereo-image based anchor-free 3D detection method, called structure-aware stereo 3D detector (termed as SIDE), where we explore the instance-level depth information via constructing the cost volume from RoIs of each object. Due to the information sparsity of local cost volume, we further introduce match reweighting and structure-aware attention, to make the depth information more concentrated. Experiments conducted on the KITTI dataset show that our method achieves the state-of-the-art performance compared to existing methods without depth map supervision.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Peng_SIDE_Center-Based_Stereo_3D_Detector_With_Structure-Aware_Instance_Depth_Estimation_WACV_2022_paper.pdf",
        "aff": "ShanghaiTech University; The Chinese University of Hong Kong; The Chinese University of Hong Kong; ShanghaiTech University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Peng_SIDE_Center-Based_Stereo_WACV_2022_supplemental.pdf",
        "arxiv": "2108.09663",
        "pdf_size": 2379266,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3820913184664333252&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;gmail.com;gmail.com;shanghaitech.edu.cn",
        "email": "gmail.com;gmail.com;gmail.com;shanghaitech.edu.cn",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "ShanghaiTech University;Chinese University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.shanghaitech.edu.cn;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "ShanghaiTech;CUHK",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "1349019387",
        "title": "SIGNAV: Semantically-Informed GPS-Denied Navigation and Mapping in Visually-Degraded Environments",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Krasner_SIGNAV_Semantically-Informed_GPS-Denied_Navigation_and_Mapping_in_Visually-Degraded_Environments_WACV_2022_paper.html",
        "author": "Alex Krasner; Mikhail Sizintsev; Abhinav Rajvanshi; Han-Pang Chiu; Niluthpol Mithun; Kevin Kaighn; Philip Miller; Ryan Villamil; Supun Samarasekera",
        "abstract": "Understanding the perceived scene during navigation enables intelligent robot behaviors. Current vision-based semantic SLAM (Simultaneous Localization and Mapping) systems provide these capabilities. However, their performance decreases in visually-degraded environments, that are common places for critical robotic applications, such as search and rescue missions. In this paper, we present SIGNAV, a real-time semantic SLAM system to operate in perceptually-challenging situations. To improve the robustness for navigation in dark environments, SIGNAV leverages a multi-sensor navigation architecture to fuse vision with additional sensing modalities, including an inertial measurement unit (IMU), LiDAR, and wheel odometry. A new 2.5D semantic segmentation method is also developed to combine both images and LiDAR depth maps to generate semantic labels of 3D mapped points in real time. We demonstrate that the navigation accuracy from SIGNAV in a variety of indoor environments under both normal lighting and dark conditions. SIGNAV also provides semantic scene understanding capabilities in visually-degraded environments. We also show the benefits of semantic information to SIGNAV's performance.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Krasner_SIGNAV_Semantically-Informed_GPS-Denied_Navigation_and_Mapping_in_Visually-Degraded_Environments_WACV_2022_paper.pdf",
        "aff": "SRI Internaional, USA; SRI Internaional, USA; SRI Internaional, USA; SRI Internaional, USA; SRI Internaional, USA; SRI Internaional, USA; SRI Internaional, USA; SRI Internaional, USA; SRI Internaional, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Krasner_SIGNAV_Semantically-Informed_GPS-Denied_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3190110,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6471935658682879596&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "sri.com; ; ; ; ; ; ; ; ",
        "email": "sri.com; ; ; ; ; ; ; ; ",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "SRI International",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sri.com",
        "aff_unique_abbr": "SRI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "50cea534fb",
        "title": "SSCAP: Self-Supervised Co-Occurrence Action Parsing for Unsupervised Temporal Action Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wang_SSCAP_Self-Supervised_Co-Occurrence_Action_Parsing_for_Unsupervised_Temporal_Action_Segmentation_WACV_2022_paper.html",
        "author": "Zhe Wang; Hao Chen; Xinyu Li; Chunhui Liu; Yuanjun Xiong; Joseph Tighe; Charless Fowlkes",
        "abstract": "Temporal action segmentation is a task to classify each frame in the video with an action label. However, it is quite expensive to annotate every frame in a large corpus of videos to construct a comprehensive supervised training dataset. Thus in this work we propose an unsupervised method, namely SSCAP, that operates on a corpus of unlabeled videos and predicts a likely set of temporal segments across the videos. SSCAP leverages Self-Supervised learning to extract distinguishable features and then applies a novel Co-occurrence Action Parsing algorithm to not only capture the correlation among sub-actions underlying the structure of activities, but also estimate the temporal path of the sub-actions in an accurate and general way. We evaluate on both classic datasets (Breakfast, 50Salads) and the emerging fine-grained action dataset (FineGym) with more complex activity structures and similar sub-actions. Results show that SSCAP achieves state-of-the-art performance on all datasets and can even outperform some weakly-supervised approaches, demonstrating its effectiveness and generalizability.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wang_SSCAP_Self-Supervised_Co-Occurrence_Action_Parsing_for_Unsupervised_Temporal_Action_Segmentation_WACV_2022_paper.pdf",
        "aff": "Dept. of CS, UC Irvine+Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Dept. of CS, UC Irvine+Amazon Web Services",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2105.14158",
        "pdf_size": 1550726,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3929537418761845214&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;1;1;1;0+1",
        "aff_unique_norm": "University of California, Irvine;Amazon",
        "aff_unique_dep": "Department of Computer Science;Amazon Web Services",
        "aff_unique_url": "https://www.uci.edu;https://aws.amazon.com",
        "aff_unique_abbr": "UCI;AWS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d66f565ba1",
        "title": "SWAG-V: Explanations for Video Using Superpixels Weighted by Average Gradients",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hartley_SWAG-V_Explanations_for_Video_Using_Superpixels_Weighted_by_Average_Gradients_WACV_2022_paper.html",
        "author": "Thomas Hartley; Kirill Sidorov; Christopher Willis; David Marshall",
        "abstract": "CNN architectures that take videos as an input are often overlooked when it comes to the development of explanation techniques. This is despite their use in critical domains such as surveillance and healthcare. Explanation techniques developed for these networks must take into account the additional temporal domain if they are to be successful. In this paper we introduce SWAG-V, an extension of SWAG for use with networks that take video as an input. By creating superpixels that incorporate individual frames of the input video we are able to create explanations that better locate regions of the input that are important to the networks prediction. We demonstrate using Kinetics-400 with both the C3D and R(2+1)D network architectures that SWAG-V outperforms Grad-CAM, Grad-CAM++ and Saliency Tubes over a range of common metrics such as explanation accuracy and localisation.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hartley_SWAG-V_Explanations_for_Video_Using_Superpixels_Weighted_by_Average_Gradients_WACV_2022_paper.pdf",
        "aff": "Cardiff University; Cardiff University; BAE Systems Applied Intelligence; Cardiff University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hartley_SWAG-V_Explanations_for_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2102095,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5202003330086447231&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cardiff.ac.uk;cardiff.ac.uk;baesystems.com;cardiff.ac.uk",
        "email": "cardiff.ac.uk;cardiff.ac.uk;baesystems.com;cardiff.ac.uk",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Cardiff University;BAE Systems",
        "aff_unique_dep": ";Applied Intelligence",
        "aff_unique_url": "https://www.cardiff.ac.uk;https://www.baesystems.com/en/product-and-services/applied-intelligence",
        "aff_unique_abbr": "Cardiff;BAE AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "70d04d5f3c",
        "title": "Sandwich Batch Normalization: A Drop-In Replacement for Feature Distribution Heterogeneity",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gong_Sandwich_Batch_Normalization_A_Drop-In_Replacement_for_Feature_Distribution_Heterogeneity_WACV_2022_paper.html",
        "author": "Xinyu Gong; Wuyang Chen; Tianlong Chen; Zhangyang Wang",
        "abstract": "We present Sandwich Batch Normalization (SaBN), a frustratingly easy improvement of Batch Normalization (BN) with only a few lines of code changes. SaBN is motivated by addressing the inherent feature distribution heterogeneity that one can be identified in many tasks, which can arise from data heterogeneity (multiple input domains) or model heterogeneity (dynamic architectures, model conditioning, etc.). Our SaBN factorizes the BN affine layer into one shared sandwich affine layer, cascaded by several parallel independent affine layers. Concrete analysis reveals that, during optimization, SaBN promotes balanced gradient norms while still preserving diverse gradient directions -- a property that many application tasks seem to favor. We demonstrate the prevailing effectiveness of SaBN as a drop-in replacement in four tasks: conditional image generation, neural architecture search (NAS), adversarial training, and arbitrary style transfer. Leveraging SaBN immediately achieves better Inception Score and FID on CIFAR-10 and ImageNet conditional image generation with three state-of-the-art GANs; boosts the performance of a state-of-the-art weight-sharing NAS algorithm significantly on NAS-Bench-201; substantially improves the robust and standard accuracies for adversarial defense; and produces superior arbitrary stylized results. We also provide visualizations and analysis to help understand why SaBN works. Codes are available at: https://github.com/VITA-Group/Sandwich-Batch-Normalization.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gong_Sandwich_Batch_Normalization_A_Drop-In_Replacement_for_Feature_Distribution_Heterogeneity_WACV_2022_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, the University of Texas at Austin; Department of Electrical and Computer Engineering, the University of Texas at Austin; Department of Electrical and Computer Engineering, the University of Texas at Austin; Department of Electrical and Computer Engineering, the University of Texas at Austin",
        "project": "",
        "github": "https://github.com/VITA-Group/Sandwich-Batch-Normalization",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Gong_Sandwich_Batch_Normalization_WACV_2022_supplemental.pdf",
        "arxiv": "2102.11382",
        "pdf_size": 16935245,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6745230712394527172&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "utexas.edu;utexas.edu;utexas.edu;utexas.edu",
        "email": "utexas.edu;utexas.edu;utexas.edu;utexas.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8bdc9ac1b2",
        "title": "SeaDronesSee: A Maritime Benchmark for Detecting Humans in Open Water",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Varga_SeaDronesSee_A_Maritime_Benchmark_for_Detecting_Humans_in_Open_Water_WACV_2022_paper.html",
        "author": "Leon Amadeus Varga; Benjamin Kiefer; Martin Messmer; Andreas Zell",
        "abstract": "Unmanned Aerial Vehicles (UAVs) are of crucial importance in search and rescue missions in maritime environments due to their flexible and fast operation capabilities. Modern computer vision algorithms are of great interest in aiding such missions. However, they are dependent on large amounts of real-case training data from UAVs, which is only available for traffic scenarios on land. Moreover, current object detection and tracking data sets only provide limited environmental information or none at all, neglecting a valuable source of information. Therefore, this paper introduces a large-scaled visual object detection and tracking benchmark (SeaDronesSee) aiming to bridge the gap from land-based vision systems to sea-based ones. We collect and annotate over 54,000 frames with 400,000 instances captured from various altitudes and viewing angles ranging from 5 to 260 meters and 0 to 90 degrees while providing the respective meta information for altitude, viewing angle and other meta data. We evaluate multiple state-of-the-art computer vision algorithms on this newly established benchmark serving as baselines. We provide an evaluation server where researchers can upload their prediction and compare their results on a central leaderboard.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Varga_SeaDronesSee_A_Maritime_Benchmark_for_Detecting_Humans_in_Open_Water_WACV_2022_paper.pdf",
        "aff": "Cognitive Systems Group, University of Tuebingen, Tuebingen, Germany; Cognitive Systems Group, University of Tuebingen, Tuebingen, Germany; Cognitive Systems Group, University of Tuebingen, Tuebingen, Germany; Cognitive Systems Group, University of Tuebingen, Tuebingen, Germany",
        "project": "https://seadronessee.cs.uni-tuebingen.de",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Varga_SeaDronesSee_A_Maritime_WACV_2022_supplemental.zip",
        "arxiv": "2105.01922",
        "pdf_size": 4184266,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17087670394889351382&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de",
        "email": "uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Tuebingen",
        "aff_unique_dep": "Cognitive Systems Group",
        "aff_unique_url": "https://www.uni-tuebingen.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Tuebingen",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "3321f8e29b",
        "title": "SeeTek: Very Large-Scale Open-Set Logo Recognition With Text-Aware Metric Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Li_SeeTek_Very_Large-Scale_Open-Set_Logo_Recognition_With_Text-Aware_Metric_Learning_WACV_2022_paper.html",
        "author": "Chenge Li; Istv\u00e1n Feh\u00e9rv\u00e1ri; Xiaonan Zhao; Ives Macedo; Srikar Appalaraju",
        "abstract": "Recent advances in deep learning and computer vision have set new state of the art in logo recognition. Logo recognition has mostly been approached as a closed-set object recognition problem and more recently as an open-set retrieval problem. Current approaches suffer from distinguishing visually similar logos, especially in open-set retrieval for very large-scale applications with thousands of brands. To address the problem, we propose a multi-task learning architecture of deep metric learning and scene text recognition. We use brand names as weak labels and enforce the model to simultaneously extract distinct visual features as well as predict brand name text. To achieve it, we collected a dataset with 3 Million logos cropped from Amazon Product Catalog images across nearly 8K brands, named PL8K. Our experiments show that adding the task of text recognition during training boosts the model's retrieval performance both on our PL8K dataset and on five other public logo datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Li_SeeTek_Very_Large-Scale_Open-Set_Logo_Recognition_With_Text-Aware_Metric_Learning_WACV_2022_paper.pdf",
        "aff": "Amazon; Amazon; Amazon; Amazon; Amazon",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Li_SeeTek_Very_Large-Scale_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1408894,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8190740703506838606&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "amazon.com;fehervari.org;amazon.com;ivesmacedo.com;amazon.com",
        "email": "amazon.com;fehervari.org;amazon.com;ivesmacedo.com;amazon.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon.com, Inc.",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "74f7bcd531",
        "title": "Seeing Implicit Neural Representations As Fourier Series",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Benbarka_Seeing_Implicit_Neural_Representations_As_Fourier_Series_WACV_2022_paper.html",
        "author": "Nuri Benbarka; Timon H\u00f6fer; Hamd ul-Moqeet Riaz; Andreas Zell",
        "abstract": "Implicit Neural Representations (INR) use multilayer perceptrons to represent high-frequency functions in low-dimensional problem domains. Recently these representations achieved state-of-the-art results on tasks related to complex 3D objects and scenes. A core problem is the representation of highly detailed signals, which is tackled using networks with periodic activation functions (SIRENs) or applying Fourier mappings to the input. This work analyzes the connection between the two methods and shows that a Fourier mapped perceptron is structurally like one hidden layer SIREN. Furthermore, we identify the relationship between the previously proposed Fourier mapping and the general d-dimensional Fourier series, leading to an integer lattice mapping. Moreover, we modify a progressive training strategy to work on arbitrary Fourier mappings and show that it improves the generalization of the interpolation task. Lastly, we compare the different mappings on the image regression and novel view synthesis tasks. We confirm the previous finding that the main contributor to the mapping performance is the size of the embedding and standard deviation of its elements.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Benbarka_Seeing_Implicit_Neural_Representations_As_Fourier_Series_WACV_2022_paper.pdf",
        "aff": "University of T \u00a8ubingen, Wilhelm-Schickard-Institute for Computer Science, Sand 1, 72076, T \u00a8ubingen; University of T \u00a8ubingen, Wilhelm-Schickard-Institute for Computer Science, Sand 1, 72076, T \u00a8ubingen; University of T \u00a8ubingen, Wilhelm-Schickard-Institute for Computer Science, Sand 1, 72076, T \u00a8ubingen; University of T \u00a8ubingen, Wilhelm-Schickard-Institute for Computer Science, Sand 1, 72076, T \u00a8ubingen",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Benbarka_Seeing_Implicit_Neural_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3573734,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12560599481700600045&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de",
        "email": "uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of T\u00fcbingen",
        "aff_unique_dep": "Wilhelm-Schickard-Institute for Computer Science",
        "aff_unique_url": "https://www.uni-tuebingen.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "cac2bcbe4e",
        "title": "Self-Guidance: Improve Deep Neural Network Generalization via Knowledge Distillation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zheng_Self-Guidance_Improve_Deep_Neural_Network_Generalization_via_Knowledge_Distillation_WACV_2022_paper.html",
        "author": "Zhenzhu Zheng; Xi Peng",
        "abstract": "We present Self-Guidance, a simple way to train deep neural networks via knowledge distillation. The basic idea is to train sub-network to match the prediction of the full network, so-called \"Self-Guidance\". Under the \"teacher-student\" framework, we construct both teacher and student within the same target network. Student network is the sub-networks that randomly skip some portions of the full network. The teacher network is the full network, can be considered as the ensemble of all possible student networks. The training process is performed in a closed-loop: (1) Forward prediction contains two passes that generate student and teacher predictions. (2) Backward distillation allows knowledge transfer from the teacher back to students. Comprehensive evaluations show that our approach improves the generalization ability of deep neural networks to a significant margin. The results prove our superior performance in both image classification on CIFAR10, CIFAR100, and facial expression recognition on FER-2013 and RAF.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zheng_Self-Guidance_Improve_Deep_Neural_Network_Generalization_via_Knowledge_Distillation_WACV_2022_paper.pdf",
        "aff": "University of Delaware; University of Delaware",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 863899,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2969554318512573060&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "udel.edu;udel.edu",
        "email": "udel.edu;udel.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Delaware",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.udel.edu",
        "aff_unique_abbr": "UD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b5d39d4bf2",
        "title": "Self-Supervised Domain Adaptation for Visual Navigation With Global Map Consistency",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lee_Self-Supervised_Domain_Adaptation_for_Visual_Navigation_With_Global_Map_Consistency_WACV_2022_paper.html",
        "author": "Eun Sun Lee; Junho Kim; Young Min Kim",
        "abstract": "We propose a light-weight, self-supervised adaptation for a visual navigation agent to generalize to unseen environment. Given an embodied agent trained in a noiseless environment, our objective is to transfer the agent to a noisy environment where actuation and odometry sensor noise is present. Our method encourages the agent to maximize the consistency between the global maps generated at different time steps in a round-trip trajectory. The proposed task is completely self-supervised, not requiring any supervision from ground-truth pose data or explicit noise model. In addition, optimization of the task objective is extremely light-weight, as training terminates within a few minutes on a commodity GPU. Our experiments show that the proposed task helps the agent to successfully transfer to new, noisy environments. The transferred agent exhibits improved localization and mapping accuracy, further leading to enhanced performance in downstream visual navigation tasks. Moreover, we demonstrate test-time adaptation with our self-supervised task to show its potential applicability in real-world deployment.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lee_Self-Supervised_Domain_Adaptation_for_Visual_Navigation_With_Global_Map_Consistency_WACV_2022_paper.pdf",
        "aff": "Dept. of Electrical and Computer Engineering, Seoul National University, Korea; Dept. of Electrical and Computer Engineering, Seoul National University, Korea; Dept. of Electrical and Computer Engineering, Seoul National University, Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Lee_Self-Supervised_Domain_Adaptation_WACV_2022_supplemental.pdf",
        "arxiv": "2110.07184",
        "pdf_size": 1610817,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2314655326072228162&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Dept. of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "b5b047cb44",
        "title": "Self-Supervised Generative Style Transfer for One-Shot Medical Image Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tomar_Self-Supervised_Generative_Style_Transfer_for_One-Shot_Medical_Image_Segmentation_WACV_2022_paper.html",
        "author": "Devavrat Tomar; Behzad Bozorgtabar; Manana Lortkipanidze; Guillaume Vray; Mohammad Saeed Rad; Jean-Philippe Thiran",
        "abstract": "In medical image segmentation, supervised deep networks' success comes at the cost of requiring abundant labeled data. While asking domain experts to annotate only one or a few of the cohort's images is feasible, annotating all available images is impractical. This issue is further exacerbated when pre-trained deep networks are exposed to a new image dataset from an unfamiliar distribution. Using available open-source data for ad-hoc transfer learning or hand-tuned techniques for data augmentation only provides suboptimal solutions. Motivated by atlas-based segmentation, we propose a novel volumetric self-supervised learning for data augmentation capable of synthesizing volumetric image-segmentation pairs via learning transformations from a single labeled atlas to the unlabeled data. Our work's central tenet benefits from a combined view of one-shot generative learning and the proposed self-supervised training strategy that cluster unlabeled volumetric images with similar styles together. Unlike previous methods, our method does not require input volumes at inference time to synthesize new images. Instead, it can generate diversified volumetric image-segmentation pairs from a prior distribution given a single or multi-site dataset. Augmented data generated by our method used to train the segmentation network provide significant improvements over state-of-the-art deep one-shot learning methods on the task of brain MRI segmentation. Ablation studies further exemplified that the proposed appearance model and joint training are crucial to synthesize realistic examples compared to existing medical registration methods. The code, data, and models are available at https://github.com/devavratTomar/SST/.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tomar_Self-Supervised_Generative_Style_Transfer_for_One-Shot_Medical_Image_Segmentation_WACV_2022_paper.pdf",
        "aff": "LTS5, EPFL, Switzerland+CHUV, Switzerland+CIBM, Switzerland; LTS5, EPFL, Switzerland+CHUV, Switzerland+CIBM, Switzerland; ; ; LTS5, EPFL, Switzerland; LTS5, EPFL, Switzerland+CHUV, Switzerland+CIBM, Switzerland",
        "project": "",
        "github": "https://github.com/devavratTomar/SST/",
        "supp": "",
        "arxiv": "2110.02117",
        "pdf_size": 2490510,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9428052229223703342&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "epfl.ch; ; ; ; ; ",
        "email": "epfl.ch; ; ; ; ; ",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;0+1+2;0;0+1+2",
        "aff_unique_norm": "EPFL;University Hospital of Lausanne (CHUV);Center for Biomedical Imaging (CIBM)",
        "aff_unique_dep": "LTS5;;",
        "aff_unique_url": "https://www.epfl.ch;https://www.chuv.ch;https://cibm.ch",
        "aff_unique_abbr": "EPFL;CHUV;CIBM",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0+0+0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "39e62c21c0",
        "title": "Self-Supervised Knowledge Transfer via Loosely Supervised Auxiliary Tasks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hong_Self-Supervised_Knowledge_Transfer_via_Loosely_Supervised_Auxiliary_Tasks_WACV_2022_paper.html",
        "author": "Seungbum Hong; Jihun Yoon; Min-Kook Choi; Junmo Kim",
        "abstract": "Knowledge transfer using convolutional neural networks (CNNs) can help efficiently train a CNN with fewer parameters or maximize the generalization performance under limited supervision. To enable a more efficient transfer of pretrained knowledge under relaxed conditions, we propose a simple yet powerful knowledge transfer methodology without any restrictions regarding the network structure or dataset used, namely self-supervised knowledge transfer (SSKT), via loosely supervised auxiliary tasks. For this, we devise a training methodology that transfers previously learned knowledge to the current training process as an auxiliary task for the target task through self-supervision using a soft label. The SSKT is independent of the network structure and dataset, and is trained differently from existing knowledge transfer methods; hence, it has an advantage in that the prior knowledge acquired from various tasks can be naturally transferred during the training process to the target task. Furthermore, it can improve the generalization performance on most datasets through the proposed knowledge transfer between different problem domains from multiple source networks. SSKT outperforms the other transfer learning methods (KD, DML, MAXL) through experiments under various knowledge transfer settings. The source code will be made available to the public",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hong_Self-Supervised_Knowledge_Transfer_via_Loosely_Supervised_Auxiliary_Tasks_WACV_2022_paper.pdf",
        "aff": "VisionAI, hutom; VisionAI, hutom; VisionAI, hutom; KAIST",
        "project": "",
        "github": "https://github.com/generation21/generation6011",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hong_Self-Supervised_Knowledge_Transfer_WACV_2022_supplemental.pdf",
        "arxiv": "2110.12696",
        "pdf_size": 1795359,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6064135829312192599&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "hutom.io;hutom.io;hutom.io;kaist.ac.kr",
        "email": "hutom.io;hutom.io;hutom.io;kaist.ac.kr",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "VisionAI;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.kaist.ac.kr",
        "aff_unique_abbr": ";KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";South Korea"
    },
    {
        "id": "c9a3fc9b33",
        "title": "Self-Supervised Learning of Domain Invariant Features for Depth Estimation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Akada_Self-Supervised_Learning_of_Domain_Invariant_Features_for_Depth_Estimation_WACV_2022_paper.html",
        "author": "Hiroyasu Akada; Shariq Farooq Bhat; Ibraheem Alhashim; Peter Wonka",
        "abstract": "We tackle the problem of unsupervised synthetic-to-real domain adaptation for single image depth estimation. An essential building block of single image depth estimation is an encoder-decoder task network that takes RGB images as input and produces depth maps as output. In this paper, we propose a novel training strategy to force the task network to learn domain invariant representations in a self-supervised manner. Specifically, we extend self-supervised learning from traditional representation learning, which works on images from a single domain, to domain invariant representation learning, which works on images from two different domains by utilizing an image-to-image translation network. Firstly, we use an image-to-image translation network to transfer domain-specific styles between synthetic and real domains. This style transfer operation allows us to obtain similar images from the different domains. Secondly, we jointly train our task network and Siamese network with the same images from the different domains to obtain domain invariance for the task network. Finally, we fine-tune the task network using labeled synthetic and unlabeled real-world data. Our training strategy yields improved generalization capability in the real-world domain. We carry out an extensive evaluation on two popular datasets for depth estimation, KITTI and Make3D. The results demonstrate that our proposed method outperforms the state-of-the-art on all metrics, e.g. by 14.7% on Sq Rel on KITTI. The source code and model weights will be made available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Akada_Self-Supervised_Learning_of_Domain_Invariant_Features_for_Depth_Estimation_WACV_2022_paper.pdf",
        "aff": "KAUST + Keio University; KAUST; National Center for Arti\ufb01cial Intelligence (NCAI), Saudi Data and Arti\ufb01cial Intelligence Authority (SDAIA); KAUST",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Akada_Self-Supervised_Learning_of_WACV_2022_supplemental.pdf",
        "arxiv": "2106.02594",
        "pdf_size": 2033151,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16985436332592857726&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "keio.jp;kaust.edu.sa;gmail.com;gmail.com",
        "email": "keio.jp;kaust.edu.sa;gmail.com;gmail.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;2;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology;Keio University;National Center for Arti\ufb01cial Intelligence",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.kaust.edu.sa;https://www.keio.ac.jp;",
        "aff_unique_abbr": "KAUST;Keio;NCAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;0",
        "aff_country_unique": "Saudi Arabia;Japan"
    },
    {
        "id": "49d3538099",
        "title": "Self-Supervised Pretraining Improves Self-Supervised Pretraining",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Reed_Self-Supervised_Pretraining_Improves_Self-Supervised_Pretraining_WACV_2022_paper.html",
        "author": "Colorado J Reed; Xiangyu Yue; Ani Nrusimha; Sayna Ebrahimi; Vivek Vijaykumar; Richard Mao; Bo Li; Shanghang Zhang; Devin Guillory; Sean Metzger; Kurt Keutzer; Trevor Darrell",
        "abstract": "While self-supervised pretraining has proven beneficial for many computer vision tasks, it requires expensive and lengthy computation, large amounts of data, and is sensitive to data augmentation. Prior work demonstrates that models pretrained on datasets dissimilar to their target data, such as chest X-ray models trained on ImageNet, underperform models trained from scratch. Users that lack the resources to pretrain must use existing models with lower performance. This paper explores Hierarchical PreTraining (HPT), which decreases convergence time and improves accuracy by initializing the pretraining process with an existing pretrained model. Through experimentation on 16 diverse vision datasets, we show HPT converges up to 80x faster, improves accuracy across tasks, and improves the robustness of the self-supervised pretraining process to changes in the image augmentation policy or amount of pretraining data. Taken together, HPT provides a simple framework for obtaining better pretrained representations with less computational resources.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Reed_Self-Supervised_Pretraining_Improves_Self-Supervised_Pretraining_WACV_2022_paper.pdf",
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; Georgia Tech; UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley + UCSF; UC Berkeley; UC Berkeley",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Reed_Self-Supervised_Pretraining_Improves_WACV_2022_supplemental.pdf",
        "arxiv": "2103.12718",
        "pdf_size": 870504,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13234600128400561216&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "berkeley.edu; ; ; ; ; ; ; ; ; ; ;",
        "email": "berkeley.edu; ; ; ; ; ; ; ; ; ; ;",
        "author_num": 12,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;0;0;0;0;0+2;0;0",
        "aff_unique_norm": "University of California, Berkeley;Georgia Institute of Technology;University of California, San Francisco",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.gatech.edu;https://www.ucsf.edu",
        "aff_unique_abbr": "UC Berkeley;Georgia Tech;UCSF",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0+2;0;0",
        "aff_campus_unique": "Berkeley;;San Francisco",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c3b592304b",
        "title": "Self-Supervised Shape Alignment for Sports Field Registration",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Shi_Self-Supervised_Shape_Alignment_for_Sports_Field_Registration_WACV_2022_paper.html",
        "author": "Feng Shi; Paul Marchwica; Juan Camilo Gamboa Higuera; Michael Jamieson; Mehrsan Javan; Parthipan Siva",
        "abstract": "This paper presents an end-to-end self-supervised learning approach for cross-modality image registration and homography estimation, with a particular emphasis on registering sports field templates onto broadcast videos as a practical application. Rather then using any pairwise labelled data for training, we propose a self-supervised data mining method to train the registration network with a natural image and its edge map. Using an iterative estimation process controlled by a score regression network (SRN) to measure the registration error, the network can learn to estimate any homography transformation regardless of how misaligned the image and the template is. We further show the benefits of using pretrained weights to finetune the network for sports field calibration with few training data. We demonstrate the effectiveness of our proposed method by applying it to real-world sports broadcast videos where we achieve state-of-the-art results and real-time processing.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Shi_Self-Supervised_Shape_Alignment_for_Sports_Field_Registration_WACV_2022_paper.pdf",
        "aff": "SLiQ Labs; Sportlogiq, Montreal, QC., Canada; Sportlogiq, Montreal, QC., Canada; Sportlogiq, Montreal, QC., Canada; Sportlogiq, Montreal, QC., Canada; Sportlogiq, Montreal, QC., Canada",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Shi_Self-Supervised_Shape_Alignment_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4903356,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8605737471365670037&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "sportlogiq.com;sportlogiq.com;sportlogiq.com;sportlogiq.com;sportlogiq.com;sportlogiq.com",
        "email": "sportlogiq.com;sportlogiq.com;sportlogiq.com;sportlogiq.com;sportlogiq.com;sportlogiq.com",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "SLiQ Labs;Sportlogiq",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Montreal",
        "aff_country_unique_index": "1;1;1;1;1",
        "aff_country_unique": ";Canada"
    },
    {
        "id": "9237280f0e",
        "title": "Self-Supervised Test-Time Adaptation on Video Data",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Azimi_Self-Supervised_Test-Time_Adaptation_on_Video_Data_WACV_2022_paper.html",
        "author": "Fatemeh Azimi; Sebastian Palacio; Federico Raue; J\u00f6rn Hees; Luca Bertinetto; Andreas Dengel",
        "abstract": "In typical computer vision problems revolving around video data, pre-trained models are simply evaluated at test time, without adaptation. This general approach clearly cannot capture the shifts that will likely arise between the distributions from which training and test data have been sampled. Adapting a pre-trained model to a new video encountered at test time could be essential to avoid the potentially catastrophic effects of such shifts. However, given the inherent impossibility of labeling data only available at test-time, traditional fine-tuning techniques cannot be leveraged in this highly practical scenario. This paper explores whether the recent progress in test-time adaptation in the image domain and self-supervised learning can be leveraged to adapt a model to previously unseen and unlabelled videos presenting both mild (but arbitrary) and severe covariate shifts. In our experiments, we show that test-time adaptation approaches applied to self-supervised methods are always beneficial, but also that the extent of their effectiveness largely depends on the specific combination of the algorithms used for adaptation and self-supervision, and also on the type of covariate shift taking place.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Azimi_Self-Supervised_Test-Time_Adaptation_on_Video_Data_WACV_2022_paper.pdf",
        "aff": "German Research Center for Arti\ufb01cial Intelligence (DFKI) + TU Kaiserslautern; German Research Center for Arti\ufb01cial Intelligence (DFKI) + TU Kaiserslautern; German Research Center for Arti\ufb01cial Intelligence (DFKI); German Research Center for Arti\ufb01cial Intelligence (DFKI) + TU Kaiserslautern; Five AI Ltd.; German Research Center for Arti\ufb01cial Intelligence (DFKI) + TU Kaiserslautern",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Azimi_Self-Supervised_Test-Time_Adaptation_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1816827,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11449570492704365767&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "dfki.de;dfki.de;dfki.de;dfki.de;robots.ox.ac.uk; ",
        "email": "dfki.de;dfki.de;dfki.de;dfki.de;robots.ox.ac.uk; ",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0;0+1;2;0+1",
        "aff_unique_norm": "German Research Center for Artificial Intelligence;Technische Universit\u00e4t Kaiserslautern;Five AI",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.dFKI.de;https://www.tu-kl.de;https://www.five.ai",
        "aff_unique_abbr": "DFKI;TU Kaiserslautern;Five AI",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0;1;0+0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "f337604cbe",
        "title": "Self-Supervised Video Representation Learning With Cross-Stream Prototypical Contrasting",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Toering_Self-Supervised_Video_Representation_Learning_With_Cross-Stream_Prototypical_Contrasting_WACV_2022_paper.html",
        "author": "Martine Toering; Ioannis Gatopoulos; Maarten Stol; Vincent Tao Hu",
        "abstract": "Instance-level contrastive learning techniques, which rely on data augmentation and a contrastive loss function, have found great success in the domain of visual representation learning. They are not suitable for exploiting the rich dynamical structure of video however, as operations are done on many augmented instances. In this paper we propose \"Video Cross-Stream Prototypical Contrasting\", a novel method which predicts consistent prototype assignments from both RGB and optical flow views, operating on sets of samples. Specifically, we alternate the optimization process; while optimizing one of the streams, all views are mapped to one set of stream prototype vectors. Each of the assignments is predicted with all views except the one matching the prediction, pushing representations closer to their assigned prototypes. As a result, more efficient video embeddings with ingrained motion information are learned, without the explicit need for optical flow computation during inference. We obtain state-of-the-art results on nearest-neighbour video retrieval and action recognition, outperforming previous best by +3.2% on UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and +15.1% on HMDB51 using the R(2+1)D backbone.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Toering_Self-Supervised_Video_Representation_Learning_With_Cross-Stream_Prototypical_Contrasting_WACV_2022_paper.pdf",
        "aff": "University of Amsterdam; BrainCreators B. V.; BrainCreators B. V.; University of Amsterdam",
        "project": "",
        "github": "https://github.com/martinetoering/ViCC",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Toering_Self-Supervised_Video_Representation_WACV_2022_supplemental.pdf",
        "arxiv": "2106.10137",
        "pdf_size": 1126089,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11854837664385178026&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com; ; ; ",
        "email": "gmail.com; ; ; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Amsterdam;BrainCreators",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uva.nl;https://www.braincreators.nl",
        "aff_unique_abbr": "UvA;BrainCreators",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "501879e4fb",
        "title": "Semantically Stealthy Adversarial Attacks Against Segmentation Models",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chen_Semantically_Stealthy_Adversarial_Attacks_Against_Segmentation_Models_WACV_2022_paper.html",
        "author": "Zhenhua Chen; Chuhua Wang; David Crandall",
        "abstract": "Segmentation models have been found to be vulnerable to targeted/non-targeted adversarial attacks. However, damaged predictions make it easy to unearth an attack. In this paper, we propose semantically stealthy adversarial attacks which can manipulate targeted labels as designed and preserve non-targeted labels at the same time. In this way, we may hide the corresponding attack behaviors. One challenge is making semantically meaningful manipulations across datasets/models. Another challenge is avoiding damaging non-targeted labels. To solve the above challenges, we consider each input image as prior knowledge to generate perturbations. We also design a special regularizer to help extract features. To evaluate our model's performance, we design three basic attack types, namely `vanishing into the context', `embedding fake labels', and `displacing target objects'. The experiments show that our stealthy adversarial model can attack segmentation models with a relatively high success rate on Cityscapes, Mapillary, and BDD100K. Finally, our framework also shows good generalizations across datasets/models empirically.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chen_Semantically_Stealthy_Adversarial_Attacks_Against_Segmentation_Models_WACV_2022_paper.pdf",
        "aff": "Indiana University Bloomington; Indiana University Bloomington; Indiana University Bloomington",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Chen_Semantically_Stealthy_Adversarial_WACV_2022_supplemental.pdf",
        "arxiv": "2104.01732",
        "pdf_size": 8079681,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17527459273599054030&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "iu.edu;iu.edu;iu.edu",
        "email": "iu.edu;iu.edu;iu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indiana University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.indiana.edu",
        "aff_unique_abbr": "IU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bloomington",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eb191872bf",
        "title": "Semi-Supervised Domain Adaptation via Sample-to-Sample Self-Distillation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Yoon_Semi-Supervised_Domain_Adaptation_via_Sample-to-Sample_Self-Distillation_WACV_2022_paper.html",
        "author": "Jeongbeen Yoon; Dahyun Kang; Minsu Cho",
        "abstract": "Semi-supervised domain adaptation (SSDA) is to adapt a learner to a new domain with only a small set of labeled samples when a large labeled dataset is given on a source domain. In this paper, we propose a pair-based SSDA method that adapts a model to the target domain using self-distillation with sample pairs. Each sample pair is composed of a teacher sample from a labeled dataset (i.e., source or labeled target) and its student sample from an unlabeled dataset (i.e., unlabeled target). Our method generates an assistant feature by transferring an intermediate style between the teacher and the student, and then train the model by minimizing the output discrepancy between the student and the assistant. During training, the assistants gradually bridge the discrepancy between the two domains, thus allowing the student to easily learn from the teacher. Experimental evaluation on standard benchmarks shows that our method effectively minimizes both the inter-domain and intra-domain discrepancies, thus achieving significant improvements over recent methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Yoon_Semi-Supervised_Domain_Adaptation_via_Sample-to-Sample_Self-Distillation_WACV_2022_paper.pdf",
        "aff": "Pohang University of Science and Technology (POSTECH), South Korea; Pohang University of Science and Technology (POSTECH), South Korea; Pohang University of Science and Technology (POSTECH), South Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Yoon_Semi-Supervised_Domain_Adaptation_WACV_2022_supplemental.pdf",
        "arxiv": "2111.14353",
        "pdf_size": 2058272,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11506315287527373320&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Pohang University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pohang",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "f207d3caf5",
        "title": "Semi-Supervised Multi-Task Learning for Semantics and Depth",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wang_Semi-Supervised_Multi-Task_Learning_for_Semantics_and_Depth_WACV_2022_paper.html",
        "author": "Yufeng Wang; Yi-Hsuan Tsai; Wei-Chih Hung; Wenrui Ding; Shuo Liu; Ming-Hsuan Yang",
        "abstract": "Multi-Task Learning (MTL) aims to enhance the model generalization by sharing representations between related tasks for better performance. Typical MTL methods are jointly trained with the complete multitude of ground-truths for all tasks simultaneously. However, one single dataset may not contain the annotations for each task of interest. To address this issue, we propose the Semi-supervised Multi-Task Learning (SemiMTL) method to leverage the available supervisory signals from different datasets, particularly for semantic segmentation and depth estimation tasks. To this end, we design an adversarial learning scheme in our semi-supervised training by leveraging unlabeled data to optimize all the task branches simultaneously and accomplish all tasks across datasets with partial annotations. We further present a domain-aware discriminator structure with various alignment formulations to mitigate the domain discrepancy issue among datasets. Finally, we demonstrate the effectiveness of the proposed method to learn across different datasets on challenging street view and remote sensing benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wang_Semi-Supervised_Multi-Task_Learning_for_Semantics_and_Depth_WACV_2022_paper.pdf",
        "aff": "Beihang University; Phiar Technologies; Waymo; Beihang University; Beihang University; University of California at Merced",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Wang_Semi-Supervised_Multi-Task_Learning_WACV_2022_supplemental.pdf",
        "arxiv": "2110.07197",
        "pdf_size": 5202901,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17694898722109784417&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "buaa.edu.cn;gmail.com;waymo.com;buaa.edu.cn;buaa.edu.cn;ucmerced.edu",
        "email": "buaa.edu.cn;gmail.com;waymo.com;buaa.edu.cn;buaa.edu.cn;ucmerced.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0;3",
        "aff_unique_norm": "Beihang University;Phiar Technologies;Waymo;University of California, Merced",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "http://www.buaa.edu.cn/;;https://www.waymo.com;https://www.ucmerced.edu",
        "aff_unique_abbr": "BUAA;;Waymo;UC Merced",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Merced",
        "aff_country_unique_index": "0;1;1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "ed5200884a",
        "title": "Semi-Supervised Semantic Segmentation of Vessel Images Using Leaking Perturbations",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hou_Semi-Supervised_Semantic_Segmentation_of_Vessel_Images_Using_Leaking_Perturbations_WACV_2022_paper.html",
        "author": "Jinyong Hou; Xuejie Ding; Jeremiah D. Deng",
        "abstract": "Semantic segmentation based on deep learning methods can attain appealing accuracy provided large amounts of annotated samples. However, it remains a challenging task when only limited labelled data are available, which is especially common in medical imaging. In this paper, we propose to use Leaking GAN, a GAN-based semi-supervised architecture for retina vessel semantic segmentation. Our key idea is to pollute the discriminator by leaking information from the generator. This leads to more moderate generations that benefit the training of GAN. As a result, the unlabelled examples can be better utilized to boost the learning of the discriminator, which eventually leads to stronger classification performance. In addition, to overcome the variations in medical images, the mean-teacher mechanism is utilized as an auxiliary regularization of the discriminator. Further, we modify the focal loss to fit it as the consistency objective for mean-teacher regularizer. Extensive experiments demonstrate that the Leaking GAN framework achieves competitive performance compared to the state-of-the-art methods when evaluated on benchmark datasets including DRIVE, STARE and CHASE_DB1, using as few as 8 labelled images in the semi-supervised setting. It also outperforms existing algorithms on cross-domain segmentation tasks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hou_Semi-Supervised_Semantic_Segmentation_of_Vessel_Images_Using_Leaking_Perturbations_WACV_2022_paper.pdf",
        "aff": "Department of Information Science, University of Otago; Department of Information Science, University of Otago; Department of Information Science, University of Otago",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hou_Semi-Supervised_Semantic_Segmentation_WACV_2022_supplemental.pdf",
        "arxiv": "2110.11998",
        "pdf_size": 5410049,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17919449865396973024&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "postgrad.otago.ac.nz;otago.ac.nz;otago.ac.nz",
        "email": "postgrad.otago.ac.nz;otago.ac.nz;otago.ac.nz",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Otago",
        "aff_unique_dep": "Department of Information Science",
        "aff_unique_url": "https://www.otago.ac.nz",
        "aff_unique_abbr": "UO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "New Zealand"
    },
    {
        "id": "610bd83884",
        "title": "Shadow Art Revisited: A Differentiable Rendering Based Approach",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sadekar_Shadow_Art_Revisited_A_Differentiable_Rendering_Based_Approach_WACV_2022_paper.html",
        "author": "Kaustubh Sadekar; Ashish Tiwari; Shanmuganathan Raman",
        "abstract": "While recent learning-based methods have been observed to be superior for several vision-related applications, their potential in generating artistic effects has not been explored much. One such exciting application is Shadow Art - a unique form of sculptural art that produces artistic effects through 2D shadows cast by a 3D sculpture. In this work, we revisit shadow art using differentiable rendering-based optimization frameworks to obtain the 3D sculpture from a set of shadow (binary) images and their corresponding projection information. Specifically, we discuss shape optimization through voxel as well as mesh-based differentiable renderers. Our choice of using differentiable rendering for generating shadow art sculptures can be attributed to its ability to learn the underlying 3D geometry solely from image data, thus reducing the dependence on 3D ground truth. The qualitative and quantitative results demonstrate the potential of the proposed framework in generating complex 3D sculptures that transcend the ones seen in contemporary art pieces using just a set of shadow images as input. Further, we demonstrate the generation of 3D sculptures to cast shadows of faces, animated movie characters, and the applicability of the proposed framework to sketch-based 3D reconstruction of the underlying shapes.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sadekar_Shadow_Art_Revisited_A_Differentiable_Rendering_Based_Approach_WACV_2022_paper.pdf",
        "aff": "CVIG Lab, IIT Gandhinagar; CVIG Lab, IIT Gandhinagar; CVIG Lab, IIT Gandhinagar",
        "project": "kaustubh-sadekar.github.io/ShadowArt-Revisited/",
        "github": "https://github.com/kaustubh-sadekar/ShadowArt-Revisited",
        "supp": "",
        "arxiv": "2107.14539",
        "pdf_size": 4206319,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17369818661428519682&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "iitgn.ac.in;iitgn.ac.in;iitgn.ac.in",
        "email": "iitgn.ac.in;iitgn.ac.in;iitgn.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Gandhinagar",
        "aff_unique_dep": "CVIG Lab",
        "aff_unique_url": "https://www.iitgn.ac.in",
        "aff_unique_abbr": "IITGN",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Gandhinagar",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "fb043b7b4a",
        "title": "Shallow Features Guide Unsupervised Domain Adaptation for Semantic Segmentation at Class Boundaries",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Cardace_Shallow_Features_Guide_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_at_WACV_2022_paper.html",
        "author": "Adriano Cardace; Pierluigi Zama Ramirez; Samuele Salti; Luigi Di Stefano",
        "abstract": "Although deep neural networks have achieved remarkable results for the task of semantic segmentation, they usually fail to generalize towards new domains, especially when performing synthetic-to-real adaptation. Such domain shift is particularly noticeable along class boundaries, invalidating one of the main goals of semantic segmentation that consists in obtaining sharp segmentation masks. In this work, we specifically address this core problem in the context of Unsupervised Domain Adaptation and present a novel low-level adaptation strategy that allows us to obtain sharp predictions. Moreover, inspired by recent self-training techniques, we introduce an effective data augmentation that alleviates the noise typically present at semantic boundaries when employing pseudo-labels for self-training. Our contributions can be easily integrated into other popular adaptation frameworks, and extensive experiments show that they effectively improve performance along class boundaries.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Cardace_Shallow_Features_Guide_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_at_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Cardace_Shallow_Features_Guide_WACV_2022_supplemental.pdf",
        "arxiv": "2110.02833",
        "pdf_size": 2541491,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3334839130138965161&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f907d39050",
        "title": "Shape-Coded ArUco: Fiducial Marker for Bridging 2D and 3D Modalities",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Makabe_Shape-Coded_ArUco_Fiducial_Marker_for_Bridging_2D_and_3D_Modalities_WACV_2022_paper.html",
        "author": "Lilika Makabe; Hiroaki Santo; Fumio Okura; Yasuyuki Matsushita",
        "abstract": "We introduce a fiducial marker for the registration of two-dimensional (2D) images and untextured three-dimensional (3D) shapes that are recorded by commodity laser scanners. Specifically, we design a 3D-version of the ArUco marker that retains exactly the same appearance as its 2D counterpart from any viewpoint above the marker but contains shape information. The shape-coded ArUco can naturally work with off-the-shelf ArUco marker detectors in the 2D image domain. For detection in the 3D domain, we develop a method for detecting the marker in an untextured 3D point cloud. Experiments demonstrate accurate 2D-3D registration using our shape-coded ArUco markers in comparison to baseline methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Makabe_Shape-Coded_ArUco_Fiducial_Marker_for_Bridging_2D_and_3D_Modalities_WACV_2022_paper.pdf",
        "aff": "Graduate School of Information Science and Technology, Osaka University; Graduate School of Information Science and Technology, Osaka University; Graduate School of Information Science and Technology, Osaka University; Graduate School of Information Science and Technology, Osaka University",
        "project": "",
        "github": "https://github.com/lilika-makabe/shape-coded-aruco",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Makabe_Shape-Coded_ArUco_Fiducial_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 6703326,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3661034917888477844&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ist.osaka-u.ac.jp;ist.osaka-u.ac.jp;ist.osaka-u.ac.jp;ist.osaka-u.ac.jp",
        "email": "ist.osaka-u.ac.jp;ist.osaka-u.ac.jp;ist.osaka-u.ac.jp;ist.osaka-u.ac.jp",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Osaka University",
        "aff_unique_dep": "Graduate School of Information Science and Technology",
        "aff_unique_url": "https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "OU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Osaka",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "866086bacb",
        "title": "Sharing Decoders: Network Fission for Multi-Task Pixel Prediction",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hickson_Sharing_Decoders_Network_Fission_for_Multi-Task_Pixel_Prediction_WACV_2022_paper.html",
        "author": "Steven Hickson; Karthik Raveendran; Irfan Essa",
        "abstract": "We examine the benefits of splitting encoder-decoders for multitask learning and showcase results on three tasks (semantics, surface normals, and depth) while adding very few FLOPS per task. Current hard parameter sharing methods for multi-task pixel-wise labeling use one shared encoder with separate decoders for each task. We generalize this notion and term the splitting of encoder-decoder architectures at different points as fission. Our ablation studies on fission show that sharing most of the decoder layers in multi-task encoder-decoder networks results in improvement while adding far fewer parameters per task. Our proposed method trains faster, uses less memory, results in better accuracy, and uses significantly fewer floating point operations (FLOPS) than conventional multi-task methods, with additional tasks only requiring 0.017% more FLOPS than the single-task network. We show results with a real-time model on a Pixel phone with released source code.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hickson_Sharing_Decoders_Network_Fission_for_Multi-Task_Pixel_Prediction_WACV_2022_paper.pdf",
        "aff": "Google/Georgia Tech; Google; Google/Georgia Tech",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hickson_Sharing_Decoders_Network_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3890746,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7436316426793362569&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "google.com; ; ",
        "email": "google.com; ; ",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3de7f6686b",
        "title": "Short-Term Solar Irradiance Prediction From Sky Images With a Clear Sky Model",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gao_Short-Term_Solar_Irradiance_Prediction_From_Sky_Images_With_a_Clear_WACV_2022_paper.html",
        "author": "Huiyu Gao; Miaomiao Liu",
        "abstract": "Integrating the solar power into the power grid system while maintaining its stability is essential for utilising such type of clean energy widely. It renders the solar irradiance (determining the solar power) forecasting a critical task. This paper tackles the problem of solar irradiance prediction from a history of sky image sequence. Most existing machine learning methods directly regress the solar irradiance values from a historical image sequence and/or solar irradiance observations. By contrast, we propose a novel deep neural network for short-term solar irradiance forecasting by leveraging a clear sky model. In particular, we build our network structure on the vision transformer to encode the spatial as well as the temporal information in the sky video sequence. We then aim to predict the solar irradiance residual from the learned representation by explicitly using a clear sky model. We evaluated our approach extensively on the existing benchmark datasets, such as TSI880 and ASI16. Results on the nowcasting task, namely estimation of the solar irradiance from the observations, and the forecasting task, which is up to 4-hour ahead-of-time prediction, demonstrate the superior performance of our method compared with existing machine learning algorithms.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gao_Short-Term_Solar_Irradiance_Prediction_From_Sky_Images_With_a_Clear_WACV_2022_paper.pdf",
        "aff": "Australian National University, Canberra, Australia; Australian National University, Canberra, Australia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Gao_Short-Term_Solar_Irradiance_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6358192,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=724982702413268733&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Canberra",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "e314958edc",
        "title": "Siamese Transformer Pyramid Networks for Real-Time UAV Tracking",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Xing_Siamese_Transformer_Pyramid_Networks_for_Real-Time_UAV_Tracking_WACV_2022_paper.html",
        "author": "Daitao Xing; Nikolaos Evangeliou; Athanasios Tsoukalas; Anthony Tzes",
        "abstract": "Recent object tracking methods depend upon deep networks or convoluted architectures. Most of those trackers can hardly meet real-time processing requirements on mobile platforms with limited computing resources. In this work, we introduce the Siamese Transformer Pyramid Network (SiamTPN), which inherits the advantages from both CNN and Transformer architectures. Specifically, we exploit the inherent feature pyramid of a lightweight network (ShuffleNetV2) and reinforce it with a Transformer to construct a robust target-specific appearance model. A centralized architecture with lateral cross attention is developed for building augmented high-level feature maps. To avoid the computation and memory intensity while fusing pyramid representations with the Transformer, we further introduce the pooling attention module, which significantly reduces memory and time complexity while improving the robustness. Comprehensive experiments on both aerial and prevalent tracking benchmarks achieve competitive results while operating at high speed, demonstrating the effectiveness of SiamTPN. Moreover, our fastest variant tracker operates over 30 Hz on a single CPU-core and obtaining an AUC score of 58.1% on the LaSOT dataset.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Xing_Siamese_Transformer_Pyramid_Networks_for_Real-Time_UAV_Tracking_WACV_2022_paper.pdf",
        "aff": "New York University, USA; New York University Abu Dhabi, UAE; New York University Abu Dhabi, UAE; New York University Abu Dhabi, UAE",
        "project": "",
        "github": "https://github.com/RISC-NYUAD/SiamTPNTracker",
        "supp": "",
        "arxiv": "2110.08822",
        "pdf_size": 3363996,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9900777501165283704&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "nyu.edu;nyu.edu;nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu;nyu.edu;nyu.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Abu Dhabi",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "United States;United Arab Emirates"
    },
    {
        "id": "7a0b951682",
        "title": "Sign Language Translation With Hierarchical Spatio-Temporal Graph Neural Network",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kan_Sign_Language_Translation_With_Hierarchical_Spatio-Temporal_Graph_Neural_Network_WACV_2022_paper.html",
        "author": "Jichao Kan; Kun Hu; Markus Hagenbuchner; Ah Chung Tsoi; Mohammed Bennamoun; Zhiyong Wang",
        "abstract": "Sign language translation (SLT), which generates text in a spoken language from visual content in a sign language, is important to assist the hard-of-hearing community for their communications. Inspired by neural machine translation (NMT), most existing SLT studies adopt a general sequence to sequence learning strategy. However, SLT is significantly different from conventional NMT tasks since sign languages convey messages through multiple aspects simultaneously such as hand poses, relative positions and body movements. Therefore, in this paper, the unique characteristics of the signing poses of sign languages is utilized to formulate hierarchical spatio-temporal graph representations of signing poses, including both high-level and fine-level graphs of which each vertex characterizes a specified body part and the edges represent the interactions between any two vertices. Specifically, high-level graphs represent the interactions between key regions such as hands and face, and fine-level graphs represent relationships between the joints of each hand and landmarks of facial regions. To this end, a novel deep learning architecture, namely hierarchical spatio-temporal graph neural network (HST-GNN), is proposed to learn such graph representations. In addition, graph convolutions and graph self-attentions with neighborhood context are proposed to characterize both the local and the global graph properties. Experimental results on benchmark datasets demonstrated the the performance.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kan_Sign_Language_Translation_With_Hierarchical_Spatio-Temporal_Graph_Neural_Network_WACV_2022_paper.pdf",
        "aff": "School of Computer Science, The University of Sydney + Data Science Institute, University of Technology Sydney; School of Computer Science, The University of Sydney; School of Computing and Information Technology, University of Wollongong; School of Computing and Information Technology, University of Wollongong; Department of Computer Science and Software Engineering, The University of Western Australia; School of Computer Science, The University of Sydney",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1877692,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3140523399304346067&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "student.uts.edu.au;uni.sydney.edu.au;uow.edu.au;uow.edu.au;uwa.edu.au;sydney.edu.au",
        "email": "student.uts.edu.au;uni.sydney.edu.au;uow.edu.au;uow.edu.au;uwa.edu.au;sydney.edu.au",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;2;2;3;0",
        "aff_unique_norm": "University of Sydney;University of Technology Sydney;University of Wollongong;University of Western Australia",
        "aff_unique_dep": "School of Computer Science;Data Science Institute;School of Computing and Information Technology;Department of Computer Science and Software Engineering",
        "aff_unique_url": "https://www.sydney.edu.au;https://www.uts.edu.au;https://www.uow.edu.au;https://www.uwa.edu.au",
        "aff_unique_abbr": "USYD;UTS;UOW;UWA",
        "aff_campus_unique_index": "0+0;0;1;1;0",
        "aff_campus_unique": "Sydney;Wollongong;",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "f998b653f7",
        "title": "Single Image Deraining Network With Rain Embedding Consistency and Layered LSTM",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Li_Single_Image_Deraining_Network_With_Rain_Embedding_Consistency_and_Layered_WACV_2022_paper.html",
        "author": "Yizhou Li; Yusuke Monno; Masatoshi Okutomi",
        "abstract": "Single image deraining is typically addressed as residual learning to predict the rain layer from an input rainy image. For this purpose, an encoder-decoder network draws wide attention, where the encoder is required to encode a high-quality rain embedding which determines the performance of the subsequent decoding stage to reconstruct the rain layer. However, most of existing studies ignore the significance of rain embedding quality, thus leading to limited performance with over/under-deraining. In this paper, with our observation of the high rain layer reconstruction performance by an rain-to-rain autoencoder, we introduce the idea of \"Rain Embedding Consistency\" by regarding the encoded embedding by the autoencoder as an ideal rain embedding and aim at enhancing the deraining performance by improving the consistency between the ideal rain embedding and the rain embedding derived by the encoder of the deraining network. To achieve this, a Rain Embedding Loss is applied to directly supervise the encoding process, with a Rectified Local Contrast Normalization (RLCN) as the guide that effectively extracts the candidate rain pixels. We also propose Layered LSTM for recurrent deraining and fine-grained encoder feature refinement considering different scales. Qualitative and quantitative experiments demonstrate that our proposed method outperforms previous state-of-the-art methods particularly on a real-world dataset. Our source code is available at http://www.ok.sc.e.titech.ac.jp/res/SIR/.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Li_Single_Image_Deraining_Network_With_Rain_Embedding_Consistency_and_Layered_WACV_2022_paper.pdf",
        "aff": "Tokyo Institute of Technology, Japan; Tokyo Institute of Technology, Japan; Tokyo Institute of Technology, Japan",
        "project": "http://www.ok.sc.e.titech.ac.jp/res/SIR/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Li_Single_Image_Deraining_WACV_2022_supplemental.pdf",
        "arxiv": "2111.03615",
        "pdf_size": 3968815,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2235410368258675878&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ok.sc.e.titech.ac.jp;ok.sc.e.titech.ac.jp;ctrl.titech.ac.jp",
        "email": "ok.sc.e.titech.ac.jp;ok.sc.e.titech.ac.jp;ctrl.titech.ac.jp",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2f76fc7186",
        "title": "Single Image Object Counting and Localizing Using Active-Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Huberman-Spiegelglas_Single_Image_Object_Counting_and_Localizing_Using_Active-Learning_WACV_2022_paper.html",
        "author": "Inbar Huberman-Spiegelglas; Raanan Fattal",
        "abstract": "The need to count and localize repeating objects in an image arises in different scenarios, such as biological microscopy studies, production-lines inspection, and surveillance recordings analysis. The use of supervised Convolutional Neural Networks (CNNs) achieves accurate object detection when trained over large class-specific datasets. The labeling effort in this approach does not pay-off when the counting is required over few images of a unique object class. We present a new method for counting and localizing repeating objects in single-image scenarios, assuming no pre-trained classifier is available. Our method trains a CNN over a small set of labels carefully collected from the input image in few active-learning iterations. At each iteration, the latent space of the network is analyzed to extract a minimal number of user-queries that strives to both sample the in-class manifold as thoroughly as possible as well as avoid redundant labels. Compared with existing user-assisted counting methods, our active-learning iterations achieve state-of-the-art performance in terms of counting and localizing accuracy, number of user mouse clicks, and running-time. This evaluation was performed through a large user study over a wide range of image classes with diverse conditions of illumination and occlusions.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Huberman-Spiegelglas_Single_Image_Object_Counting_and_Localizing_Using_Active-Learning_WACV_2022_paper.pdf",
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel; School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1506749,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12282235323341591525&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "mail.huji.ac.il;mail.huji.ac.il",
        "email": "mail.huji.ac.il;mail.huji.ac.il",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "f9574ef360",
        "title": "Single Source One Shot Reenactment Using Weighted Motion From Paired Feature Points",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tripathy_Single_Source_One_Shot_Reenactment_Using_Weighted_Motion_From_Paired_WACV_2022_paper.html",
        "author": "Soumya Tripathy; Juho Kannala; Esa Rahtu",
        "abstract": "Image reenactment is a task where the target object in the source image imitates the motion represented in the driving image. One of the most common reenactment tasks is face image animation. The major challenge in the current face reenactment approaches is to distinguish between facial motion and identity. For this reason, the previous models struggle to produce high-quality animations if the driving and source identities are different (cross-person reenactment). We propose a new (face) reenactment model that learns shape-independent motion features in a self-supervised setup. The motion is represented using a set of paired feature points extracted from the source and driving images simultaneously. The model is generalized to multiple reenactment tasks including faces and non-face objects using only a single source image. The extensive experiments show that the model faithfully transfers the driving motion to the source while retaining the source identity intact.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tripathy_Single_Source_One_Shot_Reenactment_Using_Weighted_Motion_From_Paired_WACV_2022_paper.pdf",
        "aff": "Computer Vision Group, Tampere University; Aalto University of Technology; Computer Vision Group, Tampere University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2104.03117",
        "pdf_size": 3001893,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3654975933032828741&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "tuni.fi;aalto.fi;tuni.fi",
        "email": "tuni.fi;aalto.fi;tuni.fi",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Tampere University;Aalto University",
        "aff_unique_dep": "Computer Vision Group;",
        "aff_unique_url": "https://www.tuni.fi;https://www.aalto.fi",
        "aff_unique_abbr": "Tuni;Aalto",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "dafe2193e4",
        "title": "Single-Photon Camera Guided Extreme Dynamic Range Imaging",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Liu_Single-Photon_Camera_Guided_Extreme_Dynamic_Range_Imaging_WACV_2022_paper.html",
        "author": "Yuhao Liu; Felipe Gutierrez-Barragan; Atul Ingle; Mohit Gupta; Andreas Velten",
        "abstract": "Reconstruction of high-resolution extreme dynamic range images from a small number of low dynamic range (LDR) images is crucial for many computer vision applications. Current high dynamic range (HDR) cameras based on CMOS image sensor technology rely on multiexposure bracketing which suffers from motion artifacts and signal-to-noise (SNR) dip artifacts in extreme dynamic range scenes. Recently, single-photon cameras (SPCs) have been shown to achieve orders of magnitude higher dynamic range for passive imaging than conventional CMOS sensors. SPCs are becoming increasingly available commercially, even in some consumer devices. Unfortunately, current SPCs suffer from low spatial resolution. To overcome the limitations of CMOS and SPC sensors, we propose a learning-based CMOS-SPC fusion method to recover high-resolution extreme dynamic range images. We compare the performance of our method against various traditional and state-of-the-art baselines using both synthetic and experimental data. Our method outperforms these baselines, both in terms of visual quality and quantitative metrics.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Liu_Single-Photon_Camera_Guided_Extreme_Dynamic_Range_Imaging_WACV_2022_paper.pdf",
        "aff": "University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Liu_Single-Photon_Camera_Guided_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5042137,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1788705388547328145&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu",
        "email": "wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Wisconsin-Madison",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW-Madison",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4c579c1580",
        "title": "Single-Shot Dense Active Stereo With Pixel-Wise Phase Estimation Based on Grid-Structure Using CNN and Correspondence Estimation Using GCN",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Furukawa_Single-Shot_Dense_Active_Stereo_With_Pixel-Wise_Phase_Estimation_Based_on_WACV_2022_paper.html",
        "author": "Ryo Furukawa; Michihiro Mikamo; Ryusuke Sagawa; Hiroshi Kawasaki",
        "abstract": "Active stereo systems based on static pattern projection,a.k.a. oneshot scan, have been widely used for measuring dynamic scenes. Many patterns used for oneshot active stereo have grid structures and grid-wise codes. For such systems, the grid structure is first detected, and graph matching methods are applied to estimate correspondences.However, such graph matching is often vulnerable to graph connection errors caused by grid structure analysis based on image features. Also, dense reconstruction for such systems is an open problem, where pixel-wise correspondence estimation from sparse image features is required. We propose a learning based method to capture grid structure information and pixel-wise positional information simultaneously. We also propose to represent the grid structure by graphs with augmented connections other than 4-neighborconnections and applying them to a graph convolutional network (GCN). Experiments are conducted to confirm the effectiveness of the method by comparing with the existing methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Furukawa_Single-Shot_Dense_Active_Stereo_With_Pixel-Wise_Phase_Estimation_Based_on_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Furukawa_Single-Shot_Dense_Active_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 67113983,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4896898287504954584&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "25dbf64412",
        "title": "Single-Shot Path Integrated Panoptic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hwang_Single-Shot_Path_Integrated_Panoptic_Segmentation_WACV_2022_paper.html",
        "author": "Sukjun Hwang; Seoung Wug Oh; Seon Joo Kim",
        "abstract": "Panoptic segmentation, which is a novel task of unifying instance segmentation and semantic segmentation, has attracted a lot of attention lately. However, most of the previous methods are composed of multiple pathways with each pathway specialized to a designated segmentation task. In this paper, we propose to resolve panoptic segmentation in single-shot by integrating the execution flows. With the integrated pathway, a unified feature map called Panoptic-Feature is generated, which includes the information of both things and stuffs. Panoptic-Feature becomes more sophisticated by auxiliary problems that guide to cluster pixels that belong to the same instance and differentiate between objects of different classes. A collection of convolutional filters, where each filter represents either a thing or stuff, is applied to Panoptic-Feature at once, materializing the single-shot panoptic segmentation. Taking the advantages of both top-down and bottom-up approaches, our method, named SPINet, enjoys high efficiency and accuracy on major panoptic segmentation benchmarks: COCO and Cityscapes.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hwang_Single-Shot_Path_Integrated_Panoptic_Segmentation_WACV_2022_paper.pdf",
        "aff": "Yonsei University; Adobe Research; Yonsei University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hwang_Single-Shot_Path_Integrated_WACV_2022_supplemental.pdf",
        "arxiv": "2012.01632",
        "pdf_size": 1855841,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12226012441689388727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Yonsei University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.yonsei.ac.kr;https://research.adobe.com",
        "aff_unique_abbr": "Yonsei;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "51898bd7e4",
        "title": "Skeleton-DML: Deep Metric Learning for Skeleton-Based One-Shot Action Recognition",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Memmesheimer_Skeleton-DML_Deep_Metric_Learning_for_Skeleton-Based_One-Shot_Action_Recognition_WACV_2022_paper.html",
        "author": "Raphael Memmesheimer; Simon H\u00e4ring; Nick Theisen; Dietrich Paulus",
        "abstract": "One-shot action recognition allows the recognition of human-performed actions with only a single training example. This can influence human-robot-interaction positively by enabling the robot to react to previously unseen behavior. We formulate the one-shot action recognition problem as a deep metric learning problem and propose a novel image-based skeleton representation that performs well in a metric learning setting. Therefore, we train a model that projects the image representations into an em-bedding space. In embedding space, similar actions have a low euclidean distance while dissimilar actions have a higher distance. The one-shot action recognition problem becomes a nearest-neighbor search in a set of activity reference samples. We evaluate the performance of our pro-posed representation against a variety of other skeleton-based image representations. In addition, we present an ablation study that shows the influence of different embedding vector sizes, losses and augmentation. Our approach lifts the state-of-the-art by 3.3% for the one-shot action recognition protocol on the NTU RGB+D 120 dataset under a comparable training setup. With additional augmentation, our result improved over 7.7%",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Memmesheimer_Skeleton-DML_Deep_Metric_Learning_for_Skeleton-Based_One-Shot_Action_Recognition_WACV_2022_paper.pdf",
        "aff": "University of Koblenz-Landau; University of Koblenz-Landau; University of Koblenz-Landau; University of Koblenz-Landau",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Memmesheimer_Skeleton-DML_Deep_Metric_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 4372656,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14476594094477492835&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "uni-koblenz.de;uni-koblenz.de;uni-koblenz.de;uni-koblenz.de",
        "email": "uni-koblenz.de;uni-koblenz.de;uni-koblenz.de;uni-koblenz.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Koblenz-Landau",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-koblenz-landau.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "c71be038c2",
        "title": "Spatial-Temporal Transformer for 3D Point Cloud Sequences",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wei_Spatial-Temporal_Transformer_for_3D_Point_Cloud_Sequences_WACV_2022_paper.html",
        "author": "Yimin Wei; Hao Liu; Tingting Xie; Qiuhong Ke; Yulan Guo",
        "abstract": "Effective learning of spatial-temporal information within a point cloud sequence is highly important for many down-stream tasks such as 4D semantic segmentation and 3D action recognition. In this paper, we propose a novel framework named Point Spatial-Temporal Transformer (PST2) to learn spatial-temporal representations from dynamic 3D point cloud sequences. Our PST2 consists of two major modules: a Spatio-Temporal Self-Attention (STSA) module and a Resolution Embedding (RE) module. Our STSA module is introduced to capture the spatial-temporal context information across adjacent frames, while the RE module is proposed to aggregate features across neighbors to enhance the resolution of feature maps. We test the effectiveness our PST2 with two different tasks on point cloud sequences, i.e., 4D semantic segmentation and 3D action recognition. Extensive experiments on three benchmarks show that our PST2 outperforms existing methods on all datasets. The effectiveness of our STSA and RE modules have also been justified with ablation experiments.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wei_Spatial-Temporal_Transformer_for_3D_Point_Cloud_Sequences_WACV_2022_paper.pdf",
        "aff": "Sun Yat-sen University+Shenzhen Campus of Sun Yat-sen University; Sun Yat-sen University+Shenzhen Campus of Sun Yat-sen University; Queen Mary University of London; The University of Melbourne; Sun Yat-sen University+Shenzhen Campus of Sun Yat-sen University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.09783",
        "pdf_size": 10416833,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7036931491985599680&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mail2.sysu.edu.cn; ; ; ;sysu.edu.cn",
        "email": "mail2.sysu.edu.cn; ; ; ;sysu.edu.cn",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0;1;2;0+0",
        "aff_unique_norm": "Sun Yat-sen University;Queen Mary University of London;University of Melbourne",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.qmul.ac.uk;https://www.unimelb.edu.au",
        "aff_unique_abbr": "SYSU;QMUL;UniMelb",
        "aff_campus_unique_index": "1;1;2;1",
        "aff_campus_unique": ";Shenzhen;London",
        "aff_country_unique_index": "0+0;0+0;1;2;0+0",
        "aff_country_unique": "China;United Kingdom;Australia"
    },
    {
        "id": "67e7b15531",
        "title": "Spatiotemporal Initialization for 3D CNNs With Generated Motion Patterns",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kataoka_Spatiotemporal_Initialization_for_3D_CNNs_With_Generated_Motion_Patterns_WACV_2022_paper.html",
        "author": "Hirokatsu Kataoka; Kensho Hara; Ryusuke Hayashi; Eisuke Yamagata; Nakamasa Inoue",
        "abstract": "The paper proposes a framework of Formula-Driven Supervised Learning (FDSL) for spatiotemporal initialization. Our FDSL approach enables to automatically and simultaneously generate motion patterns and their video labels with a simple formula which is based on Perlin noise. We designed a dataset of generated motion patterns adequate for the 3D CNNs to learn a better basis set of natural videos. The constructed Video Perlin Noise (VPN) dataset can be applied to initialize a model before pre-training with large-scale video datasets such as Kinetics-400/700, to enhance target task performance. Our spatiotemporal initialization with VPN dataset (VPN initialization) outperforms the previous initialization method with the inflated 3D ConvNet (I3D) using 2D ImageNet dataset. Our proposed method increased the top-1 video-level accuracy of Kinetics-400 pre-trained model on  Kinetics-400, UCF-101, HMDB-51, ActivityNet  datasets. Especially, the proposed method increased the performance rate of Kinetics-400 pre-trained model by 10.3 pt on ActivityNet. We also report that the relative performance improvements from the baseline are greater in 3D CNNs rather than other models.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kataoka_Spatiotemporal_Initialization_for_3D_CNNs_With_Generated_Motion_Patterns_WACV_2022_paper.pdf",
        "aff": "National Institute of Advanced Industrial Science and Technology (AIST); National Institute of Advanced Industrial Science and Technology (AIST); National Institute of Advanced Industrial Science and Technology (AIST); Tokyo Institute of Technology; Tokyo Institute of Technology",
        "project": "https://hirokatsukataoka16.github.io/Spatiotemporal-Initialization-for-3DCNNs/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 18608460,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2826046190142444997&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "aist.go.jp;aist.go.jp;aist.go.jp;gmail.com;c.titech.ac.jp",
        "email": "aist.go.jp;aist.go.jp;aist.go.jp;gmail.com;c.titech.ac.jp",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "National Institute of Advanced Industrial Science and Technology;Tokyo Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.aist.go.jp;https://www.titech.ac.jp",
        "aff_unique_abbr": "AIST;Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "0203eb44eb",
        "title": "SpectraNet: Learned Recognition of Artificial Satellites From High Contrast Spectroscopic Imagery",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gazak_SpectraNet_Learned_Recognition_of_Artificial_Satellites_From_High_Contrast_Spectroscopic_WACV_2022_paper.html",
        "author": "J. Zachary Gazak; Ian McQuaid; Ryan Swindle; Matthew Phelps; Justin Fletcher",
        "abstract": "Effective space traffic management requires positive identification of artificial satellites. Current methods for extracting object identification from observed data require spatially resolved imagery which limits identification to objects in low earth orbits. Most artificial satellites, however, operate in geostationary orbits at distances which prohibit ground based observatories from resolving spatial information. This paper demonstrates an object identification solution leveraging modified residual convolutional neural networks to map distance-invariant spectroscopic data to object identity. We report classification accuracies exceeding 80% for a simulated 64-class satellite problem--even in the case of satellites undergoing constant, random re-orientation. An astronomical observing campaign driven by these results returned accuracies of  72% for a nine-class problem with an average of 100 examples per class, performing as expected from simulation. We demonstrate the application of variational Bayesian inference by dropout, stochastic weight averaging (SWA), and SWA-focused deep ensembling to measure classification uncertainties--critical components in space traffic management where routine decisions risk expensive space assets and carry geopolitical consequences.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gazak_SpectraNet_Learned_Recognition_of_Artificial_Satellites_From_High_Contrast_Spectroscopic_WACV_2022_paper.pdf",
        "aff": "United States Space Force Space Systems Command; Air Force Research Laboratory; United States Space Force Space Systems Command; United States Space Force Space Systems Command; United States Space Force Space Systems Command",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1179196,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=248306673391909972&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "us.af.mil; ; ; ;us.af.mil",
        "email": "us.af.mil; ; ; ;us.af.mil",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "United States Space Force;Air Force Research Laboratory",
        "aff_unique_dep": "Space Systems Command;",
        "aff_unique_url": "https://www.ssf.mil;https://www.afrl.af.mil/",
        "aff_unique_abbr": "USSF;AFRL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "be9bbe390c",
        "title": "SporeAgent: Reinforced Scene-Level Plausibility for Object Pose Refinement",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Bauer_SporeAgent_Reinforced_Scene-Level_Plausibility_for_Object_Pose_Refinement_WACV_2022_paper.html",
        "author": "Dominik Bauer; Timothy Patten; Markus Vincze",
        "abstract": "Observational noise, inaccurate segmentation and ambiguity due to symmetry and occlusion lead to inaccurate object pose estimates. While depth- and RGB-based pose refinement approaches increase the accuracy of the resulting pose estimates, they are susceptible to ambiguity in the observation as they consider visual alignment. We propose to leverage the fact that we often observe static, rigid scenes. Thus, the objects therein need to be under physically plausible poses. We show that considering plausibility reduces ambiguity and, in consequence, allows poses to be more accurately predicted in cluttered environments. To this end, we extend a recent RL-based registration approach towards iterative refinement of object poses. Experiments on the LINEMOD and YCB-VIDEO datasets demonstrate the state-of-the-art performance of our depth-based refinement approach.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Bauer_SporeAgent_Reinforced_Scene-Level_Plausibility_for_Object_Pose_Refinement_WACV_2022_paper.pdf",
        "aff": "TU Wien, Austria; TU Wien, Austria + University of Technology Sydney, Australia; TU Wien, Austria",
        "project": "",
        "github": "github.com/dornik/sporeagent",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Bauer_SporeAgent_Reinforced_Scene-Level_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3476104,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10669294345910613246&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "acin.tuwien.ac.at;acin.tuwien.ac.at;acin.tuwien.ac.at",
        "email": "acin.tuwien.ac.at;acin.tuwien.ac.at;acin.tuwien.ac.at",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Technische Universit\u00e4t Wien;University of Technology Sydney",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tuwien.ac.at;https://www.uts.edu.au",
        "aff_unique_abbr": "TU Wien;UTS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "Austria;Australia"
    },
    {
        "id": "a49082f937",
        "title": "StickyLocalization: Robust End-to-End Relocalization on Point Clouds Using Graph Neural Networks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Fischer_StickyLocalization_Robust_End-to-End_Relocalization_on_Point_Clouds_Using_Graph_Neural_WACV_2022_paper.html",
        "author": "Kai Fischer; Martin Simon; Stefan Milz; Patrick M\u00e4der",
        "abstract": "Relocalization inside pre-built maps provides a big benefit in the course of today's autonomous driving tasks where the map can be considered as an additional sensor for refining the estimated current pose of the vehicle. Due to potentially large drifts in the initial pose guess as well as maps containing unfiltered dynamic and temporal static objects (e.g. parking cars), traditional methods like ICP tend to fail and show high computation times. We propose a novel and fast relocalization method for accurate pose estimation inside a pre-built map based on 3D point clouds. The method is robust against inaccurate initialization caused by low performance GPS systems and tolerates the presence of unfiltered objects by specifically learning to extract significant features from current scans and adjacent map sections. More specifically, we introduce a novel distance-based matching loss enabling us to simultaneously extract important information from raw point clouds and aggregating inner- and inter-cloud context by utilizing self- and cross-attention inside a Graph Neural Network. We evaluate StickyLocalization's (SL) performance through an extensive series of experiments using two benchmark datasets in terms of Relocalization on NuScenes and Loop Closing using KITTI's Odometry dataset. We found that SL outperforms state-of-the art point cloud registration and relocalization methods in terms of transformation errors and runtime.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Fischer_StickyLocalization_Robust_End-to-End_Relocalization_on_Point_Clouds_Using_Graph_Neural_WACV_2022_paper.pdf",
        "aff": "Valeo Schalter und Sensoren GmbH+Ilmenau University of Technology; Valeo Schalter und Sensoren GmbH+Ilmenau University of Technology; Spleenlab GmbH+Ilmenau University of Technology; Ilmenau University of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Fischer_StickyLocalization_Robust_End-to-End_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1727606,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5123143739512768933&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "valeo.com;valeo.com;tu-ilmenau.de;tu-ilmenau.de",
        "email": "valeo.com;valeo.com;tu-ilmenau.de;tu-ilmenau.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2+1;1",
        "aff_unique_norm": "Valeo;Ilmenau University of Technology;Spleenlab GmbH",
        "aff_unique_dep": "Schalter und Sensoren;;",
        "aff_unique_url": "https://www.valeo.com;https://www.tu-ilmenau.de/;",
        "aff_unique_abbr": "Valeo;TU Ilmenau;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "a1277a0d56",
        "title": "Strumming to the Beat: Audio-Conditioned Contrastive Video Textures",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Narasimhan_Strumming_to_the_Beat_Audio-Conditioned_Contrastive_Video_Textures_WACV_2022_paper.html",
        "author": "Medhini Narasimhan; Shiry Ginosar; Andrew Owens; Alexei A. Efros; Trevor Darrell",
        "abstract": "We introduce a non-parametric approach for infinite video texture synthesis using a representation learned via contrastive learning. We take inspiration from Video Textures, which showed that plausible new videos could be generated from a single one by stitching its frames together in a novel yet consistent order. This classic work, however, was constrained by its use of hand-designed distance metrics, limiting its use to simple, repetitive videos. We draw on recent techniques from self-supervised learning to learn this distance metric, allowing us to compare frames in a manner that scales to more challenging dynamics, and to condition on other data, such as audio. We learn representations for video frames and frame-to-frame transition probabilities by fitting a video-specific model trained using contrastive learning. To synthesize a texture, we randomly sample frames with high transition probabilities to generate diverse temporally smooth videos with novel sequences and transitions. The model naturally extends to an audio-conditioned setting without requiring any fine-tuning. Our model outperforms baselines on human perceptual scores, can handle a diverse range of input videos, and can combine semantic and audio-visual cues in order to synthesize videos that synchronize well with an audio signal.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Narasimhan_Strumming_to_the_Beat_Audio-Conditioned_Contrastive_Video_Textures_WACV_2022_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of Michigan; University of California, Berkeley; University of California, Berkeley",
        "project": "",
        "github": "https://medhini.github.io/audio_video_textures",
        "supp": "",
        "arxiv": "2104.02687",
        "pdf_size": 7771338,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2221160607184730067&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "berkeley.edu;berkeley.edu;umich.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;umich.edu;berkeley.edu;berkeley.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of California, Berkeley;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.umich.edu",
        "aff_unique_abbr": "UC Berkeley;UM",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c82412b70c",
        "title": "Style Agnostic 3D Reconstruction via Adversarial Style Transfer",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Petersen_Style_Agnostic_3D_Reconstruction_via_Adversarial_Style_Transfer_WACV_2022_paper.html",
        "author": "Felix Petersen; Bastian Goldluecke; Oliver Deussen; Hilde Kuehne",
        "abstract": "Reconstructing the 3D geometry of an object from an image is a major challenge in computer vision. Recently introduced differentiable renderers can be leveraged to learn the 3D geometry of objects from 2D images, but those approaches require additional supervision to enable the renderer to produce an output that can be compared to the input image. This can be scene information or constraints such as object silhouettes, uniform backgrounds, material, texture, and lighting. In this paper, we propose an approach that enables a differentiable rendering-based learning of 3D objects from images with backgrounds without the need for silhouette supervision. Instead of trying to render an image close to the input, we propose an adversarial style-transfer and domain adaptation pipeline that allows to translate the input image domain to the rendered image domain. This allows us to directly compare between a translated image and the differentiable rendering of a 3D object reconstruction in order to train the 3D object reconstruction network. We show that the approach learns 3D geometry from images with backgrounds and provides a better performance than constrained methods for single-view 3D object reconstruction on this task.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Petersen_Style_Agnostic_3D_Reconstruction_via_Adversarial_Style_Transfer_WACV_2022_paper.pdf",
        "aff": "University of Konstanz; University of Konstanz; University of Konstanz; University of Frankfurt + IBM-MIT Watson AI Lab",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Petersen_Style_Agnostic_3D_WACV_2022_supplemental.pdf",
        "arxiv": "2110.10784",
        "pdf_size": 2379172,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1204813946837749913&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uni-konstanz.de;uni-konstanz.de;uni-konstanz.de; ",
        "email": "uni-konstanz.de;uni-konstanz.de;uni-konstanz.de; ",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1+2",
        "aff_unique_norm": "University of Konstanz;Goethe University Frankfurt;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;IBM-MIT Watson AI Lab",
        "aff_unique_url": "https://www.uni-konstanz.de;https://www.uni-frankfurt.de;https://www.mit.edu",
        "aff_unique_abbr": "Uni Konstanz;Goethe Uni;MIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "488e234f5b",
        "title": "StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and Manipulation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kocasari_StyleMC_Multi-Channel_Based_Fast_Text-Guided_Image_Generation_and_Manipulation_WACV_2022_paper.html",
        "author": "Umut Kocasari; Alara Dirik; Mert Tiftikci; Pinar Yanardag",
        "abstract": "Discovering meaningful directions in the latent space of GANs to manipulate semantic attributes typically requires large amounts of labeled data. Recent work aims to overcome this limitation by leveraging the power of Contrastive Language-Image Pre-training (CLIP), a joint text-image model. While promising, these methods require several hours of preprocessing or training to achieve the desired manipulations. In this paper, we present StyleMC, a fast and efficient method for text-driven image generation and manipulation. StyleMC uses a CLIP-based loss and an identity loss to manipulate images via a single text prompt without significantly affecting other attributes. Unlike prior work, StyleMC requires only a few seconds of training per text prompt to find stable global directions, does not require prompt engineering and can be used with any pre-trained StyleGAN2 model. We demonstrate the effectiveness of our method and compare it to state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kocasari_StyleMC_Multi-Channel_Based_Fast_Text-Guided_Image_Generation_and_Manipulation_WACV_2022_paper.pdf",
        "aff": "Bo\u011fazi\u00e7i University; Bo\u011fazi\u00e7i University; Bo\u011fazi\u00e7i University; Bo\u011fazi\u00e7i University",
        "project": "",
        "github": "http://catlab-team.github.io/stylemc",
        "supp": "",
        "arxiv": "2112.08493",
        "pdf_size": 1842684,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18118005620219042349&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "boun.edu.tr;boun.edu.tr;boun.edu.tr;gmail.com",
        "email": "boun.edu.tr;boun.edu.tr;boun.edu.tr;gmail.com",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Bo\u011fazi\u00e7i University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.boun.edu.tr",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "T\u00fcrkiye"
    },
    {
        "id": "5f75a6f792",
        "title": "Stylizing 3D Scene via Implicit Representation and HyperNetwork",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chiang_Stylizing_3D_Scene_via_Implicit_Representation_and_HyperNetwork_WACV_2022_paper.html",
        "author": "Pei-Ze Chiang; Meng-Shiun Tsai; Hung-Yu Tseng; Wei-Sheng Lai; Wei-Chen Chiu",
        "abstract": "In this work, we aim to address the 3D scene stylization problem - generating stylized images of the scene at arbitrary novel view angles. A straightforward solution is to combine existing novel view synthesis and image/video style transfer approaches, which often leads to blurry results or inconsistent appearance. Inspired by the high quality results of the neural radiance fields (NeRF) method, we propose a joint framework to directly render novel views with the desired style. Our framework consists of two components: an implicit representation of the 3D scene with the neural radiance field model, and a hypernetwork to transfer the style information into the scene representation. To alleviate the training difficulties and memory burden, we propose a two-stage training procedure and a patch sub-sampling approach to optimize the style and content losses with the neural radiance field model. After optimization, our model is able to render consistent novel views at arbitrary view angles with arbitrary style. Both quantitative evaluation and human subject study have demonstrated that the proposed method generates faithful stylization results with consistent appearance across different views.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chiang_Stylizing_3D_Scene_via_Implicit_Representation_and_HyperNetwork_WACV_2022_paper.pdf",
        "aff": "National Yang Ming Chiao Tung University, Taiwan + MediaTek-NCTU Research Center, Taiwan; National Yang Ming Chiao Tung University, Taiwan + MediaTek-NCTU Research Center, Taiwan; University of California, Merced; University of California, Merced; National Yang Ming Chiao Tung University, Taiwan + MediaTek-NCTU Research Center, Taiwan",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Chiang_Stylizing_3D_Scene_WACV_2022_supplemental.pdf",
        "arxiv": "2105.13016",
        "pdf_size": 8060201,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16014582695982923610&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2;2;0+1",
        "aff_unique_norm": "National Yang Ming Chiao Tung University;National Chiao Tung University;University of California, Merced",
        "aff_unique_dep": ";MediaTek-NCTU Research Center;",
        "aff_unique_url": "https://www.nycu.edu.tw;https://www.nctu.edu.tw;https://www.ucmerced.edu",
        "aff_unique_abbr": "NYCU;NCTU;UC Merced",
        "aff_campus_unique_index": "0+0;0+0;1;1;0+0",
        "aff_campus_unique": "Taiwan;Merced",
        "aff_country_unique_index": "0+0;0+0;1;1;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "fa93389cdc",
        "title": "Supervised Compression for Resource-Constrained Edge Computing Systems",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Matsubara_Supervised_Compression_for_Resource-Constrained_Edge_Computing_Systems_WACV_2022_paper.html",
        "author": "Yoshitomo Matsubara; Ruihan Yang; Marco Levorato; Stephan Mandt",
        "abstract": "There has been much interest in deploying deep learning algorithms on low-powered devices, including smartphones, drones, and medical sensors. However, full-scale deep neural networks are often too resource-intensive in terms of energy and storage. As a result, the bulk part of the machine learning operation is therefore often carried out on an edge server, where the data is compressed and transmitted. However, compressing data (such as images) leads to transmitting information irrelevant to the supervised task. Another popular approach is to split the deep network between the device and the server while compressing intermediate features. To date, however, such split computing strategies have barely outperformed the aforementioned naive data compression baselines due to their inefficient approaches to feature compression. This paper adopts ideas from knowledge distillation and neural image compression to compress intermediate feature representations more efficiently. Our supervised compression approach uses a teacher model and a student model with a stochastic bottleneck and learnable prior for entropy coding (Entropic Student). We compare our approach to various neural image and feature compression baselines in three vision tasks and found that it achieves better supervised rate-distortion performance while maintaining smaller end-to-end latency. We furthermore show that the learned feature representations can be tuned to serve multiple downstream tasks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Matsubara_Supervised_Compression_for_Resource-Constrained_Edge_Computing_Systems_WACV_2022_paper.pdf",
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Matsubara_Supervised_Compression_for_WACV_2022_supplemental.pdf",
        "arxiv": "2108.11898",
        "pdf_size": 2235321,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7412791889700403208&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "uci.edu;uci.edu;uci.edu;uci.edu",
        "email": "uci.edu;uci.edu;uci.edu;uci.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c6ef59c507",
        "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tan_Surrogate_Model-Based_Explainability_Methods_for_Point_Cloud_NNs_WACV_2022_paper.html",
        "author": "Hanxiao Tan; Helena Kotthaus",
        "abstract": "In the field of autonomous driving and robotics, point clouds are showing their excellent real-time performance as raw data from most of the mainstream 3D sensors. Therefore, point cloud neural networks have become a popular research direction in recent years. So far, however, there has been little discussion about the explainability of deep neural networks for point clouds. In this paper, we propose a point cloud-applicable explainability approaches based on local surrogate model-based methods to show which components make the main contribution to the classification. Moreover, we propose quantitative fidelity validations for generated explanations that enhance the persuasive power of explainability and compare the plausibility of different existing point cloud-applicable explainability methods. Our new explainability approach provides a fairly accurate, more intuitive and widely applicable explanation for point cloud classification tasks. Our code is available at https://github.com/Explain3D/LIME-3D",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tan_Surrogate_Model-Based_Explainability_Methods_for_Point_Cloud_NNs_WACV_2022_paper.pdf",
        "aff": "AI Group, TU Dortmund; AI Group, TU Dortmund",
        "project": "",
        "github": "https://github.com/Explain3D/LIME-3D",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Tan_Surrogate_Model-Based_Explainability_WACV_2022_supplemental.pdf",
        "arxiv": "2107.13459",
        "pdf_size": 1049756,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2217056889858243422&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "tu-dortmund.de;tu-dortmund.de",
        "email": "tu-dortmund.de;tu-dortmund.de",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technische Universit\u00e4t Dortmund",
        "aff_unique_dep": "AI Group",
        "aff_unique_url": "https://www.tu-dortmund.de",
        "aff_unique_abbr": "TU Dortmund",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2603d6fa70",
        "title": "Symmetric-Light Photometric Stereo",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Minami_Symmetric-Light_Photometric_Stereo_WACV_2022_paper.html",
        "author": "Kazuma Minami; Hiroaki Santo; Fumio Okura; Yasuyuki Matsushita",
        "abstract": "This paper presents symmetric-light photometric stereo for surface normal estimation, in which directional lights are distributed symmetrically with respect to the optic center. Unlike previous studies of ring-light settings that required the information of ring radius, we show that even without the knowledge of the exact light source locations or their distances from the optic center, the symmetric configuration provides us sufficient information for recovering unique surface normals without ambiguity. Specifically, under the symmetric lights, measurements of a pair of scene points having distinct surface normals but the same albedo yield a system of constrained quadratic equations about the surface normal, which has a unique solution. Experiments demonstrate that the proposed method alleviates the need for geometric light source calibration while maintaining the accuracy of calibrated photometric stereo.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Minami_Symmetric-Light_Photometric_Stereo_WACV_2022_paper.pdf",
        "aff": "Graduate School of Information Science and Technology, Osaka University; Graduate School of Information Science and Technology, Osaka University; Graduate School of Information Science and Technology, Osaka University; Graduate School of Information Science and Technology, Osaka University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Minami_Symmetric-Light_Photometric_Stereo_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 8245437,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9989236522333955857&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ist.osaka-u.ac.jp;ist.osaka-u.ac.jp;ist.osaka-u.ac.jp;ist.osaka-u.ac.jp",
        "email": "ist.osaka-u.ac.jp;ist.osaka-u.ac.jp;ist.osaka-u.ac.jp;ist.osaka-u.ac.jp",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Osaka University",
        "aff_unique_dep": "Graduate School of Information Science and Technology",
        "aff_unique_url": "https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "OU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Osaka",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "7d4d5ea339",
        "title": "T-Net: A Resource-Constrained Tiny Convolutional Neural Network for Medical Image Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Khan_T-Net_A_Resource-Constrained_Tiny_Convolutional_Neural_Network_for_Medical_Image_WACV_2022_paper.html",
        "author": "Tariq M. Khan; Antonio Robles-Kelly; Syed S. Naqvi",
        "abstract": "In this paper, we present T-Net, a fully convolutional net-work particularly well suited for resource constrained andmobile devices, which cannot cater for the computationalresources often required by much larger networks. T-NET's design allows for dual-stream information flow both insideas well as outside of the encoder-decoder pair. Here, weuse group convolutions to increase the width of the networkand, in doing so, learn a larger number of low and inter-mediate level features. We have also employed skip connec-tions in order to keep spatial information loss to a minimum.T-Net uses a dice loss for pixel-wise classification which al-leviates the effect of class imbalance. We have performedexperiments with three different applications, retinal vesselsegmentation, skin lesion segmentation and digestive tractpolyp segmentation. In our experiments, T-Net is quite com-petitive, outperforming alternatives with two or even threeorders of magnitude more trainable parameters.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Khan_T-Net_A_Resource-Constrained_Tiny_Convolutional_Neural_Network_for_Medical_Image_WACV_2022_paper.pdf",
        "aff": "School of IT, Faculty of Sci. Eng. & Built Env., Deakin University, Waurn Ponds, VIC 3216, Australia; School of IT, Faculty of Sci. Eng. & Built Env., Deakin University, Waurn Ponds, VIC 3216, Australia; Dept. of Electrical and Computer Eng., COMSATS University Islamabad, Islamabad, Pakistan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6667052,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10093273022744162693&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;deakin.edu.au;comsats.edu.pk",
        "email": "gmail.com;deakin.edu.au;comsats.edu.pk",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Deakin University;COMSATS University Islamabad",
        "aff_unique_dep": "School of IT;Dept. of Electrical and Computer Eng.",
        "aff_unique_url": "https://www.deakin.edu.au;https://www.comsats.edu.pk",
        "aff_unique_abbr": "Deakin;CUI",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Waurn Ponds;Islamabad",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Australia;Pakistan"
    },
    {
        "id": "bc3d349e58",
        "title": "TA-Net: Topology-Aware Network for Gland Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Wang_TA-Net_Topology-Aware_Network_for_Gland_Segmentation_WACV_2022_paper.html",
        "author": "Haotian Wang; Min Xian; Aleksandar Vakanski",
        "abstract": "Gland segmentation is a critical step to quantitatively assess the morphology of glands in histopathology image analysis. However, it is challenging to separate densely clustered glands accurately. Existing deep learning-based approaches attempted to use contour-based techniques to alleviate this issue but only achieved limited success. To address this challenge, we propose a novel topology-aware network (TA-Net) to accurately separate densely clustered and severely deformed glands. The proposed TA-Net has a multitask learning architecture and enhances the generalization of gland segmentation by learning shared representation from two tasks: instance segmentation and gland topology estimation. The proposed topology loss computes gland topology using gland skeletons and markers. It drives the network to generate segmentation results that comply with the true gland topology. We validate the proposed approach on the GlaS and CRAG datasets using three quantitative metrics, F1-score, object-level Dice coefficient, and object-level Hausdorff distance. Extensive experiments demonstrate that TA-Net achieves state-of-the-art performance on the two datasets. TA-Net outperforms other approaches in the presence of densely clustered glands.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Wang_TA-Net_Topology-Aware_Network_for_Gland_Segmentation_WACV_2022_paper.pdf",
        "aff": "University of Idaho, Idaho, USA; University of Idaho, Idaho, USA; University of Idaho, Idaho, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10357292,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7187618042878448043&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "uidaho.edu;uidaho.edu;uidaho.edu",
        "email": "uidaho.edu;uidaho.edu;uidaho.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Idaho",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uidaho.edu",
        "aff_unique_abbr": "UI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "386de3277c",
        "title": "Tailor Me: An Editing Network for Fashion Attribute Shape Manipulation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kwon_Tailor_Me_An_Editing_Network_for_Fashion_Attribute_Shape_Manipulation_WACV_2022_paper.html",
        "author": "Youngjoong Kwon; Stefano Petrangeli; Dahun Kim; Haoliang Wang; Viswanathan Swaminathan; Henry Fuchs",
        "abstract": "Fashion attribute editing aims to manipulate fashion images based on a user-specified attribute, while preserving the details of the original image as intact as possible. Recent works in this domain have mainly focused on direct manipulation of the raw RGB pixels, which only allows to perform edits involving relatively small shape changes (e.g., sleeves). The goal of our Virtual Personal Tailoring Network (VPTNet) is to extend the editing capabilities to much larger shape changes of fashion items, such as cloth length. To achieve this goal, we decouple the fashion attribute editing task into two conditional stages: shape-then-appearance editing. To this aim, we propose a shape editing network that employs a semantic parsing of the fashion image as an interface for manipulation. Compared to operating on the raw RGB image, our parsing map editing enables performing more complex shape editing operations. Second, we introduce an appearance completion network that takes the previous stage results and completes the shape difference regions to produce the final RGB image. Qualitative and quantitative experiments on the DeepFashion-Synthesis dataset confirm that VPTNet outperforms state-of-the-art methods for both small and large shape attribute editing.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kwon_Tailor_Me_An_Editing_Network_for_Fashion_Attribute_Shape_Manipulation_WACV_2022_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1533418,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15796426415605985916&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "055f8682d8",
        "title": "Temporally Stable Video Segmentation Without Video Annotations",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Azulay_Temporally_Stable_Video_Segmentation_Without_Video_Annotations_WACV_2022_paper.html",
        "author": "Aharon Azulay; Tavi Halperin; Orestis Vantzos; Nadav Bornstein; Ofir Bibi",
        "abstract": "Temporally consistent dense video annotations are scarce and hard to collect. In contrast, image segmentation datasets (and pre-trained models) are ubiquitous, and easier to label for any novel task. In this paper, we introduce a method to adapt still image segmentation models to video in an unsupervised manner, by using an optical flow-based consistency measure. To ensure that the inferred segmented videos appear more stable in practice, we verify that the consistency measure is well correlated with human judgement via a user study. Training a new multi-input multi-output decoder using this measure as a loss, together with a technique for refining current image segmentation datasets and a temporal weighted-guided filter, we observe stability improvements in the generated segmented videos with minimal loss of accuracy.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Azulay_Temporally_Stable_Video_Segmentation_Without_Video_Annotations_WACV_2022_paper.pdf",
        "aff": "Lightricks+The Hebrew University; Lightricks; Lightricks; Lightricks; Lightricks",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.08893",
        "pdf_size": 1615581,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13593671755635189235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "lightricks.com;lightricks.com;lightricks.com;lightricks.com;lightricks.com",
        "email": "lightricks.com;lightricks.com;lightricks.com;lightricks.com;lightricks.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0;0",
        "aff_unique_norm": "Lightricks;Hebrew University of Jerusalem",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.lightricks.com;https://www.huji.ac.il",
        "aff_unique_abbr": ";HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "6389d9d374",
        "title": "Tensor Feature Hallucination for Few-Shot Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Lazarou_Tensor_Feature_Hallucination_for_Few-Shot_Learning_WACV_2022_paper.html",
        "author": "Michalis Lazarou; Tania Stathaki; Yannis Avrithis",
        "abstract": "Few-shot learning addresses the challenge of learning how to address novel tasks given not just limited supervision but limited data as well. An attractive solution is synthetic data generation. However, most such methods are overly sophisticated, focusing on high-quality, realistic data in the input space. It is unclear whether adapting them to the few-shot regime and using them for the downstream task of classification is the right approach. Previous works on synthetic data generation for few-shot classification focus on exploiting complex models, e.g. a Wasserstein GAN with multiple regularizers or a network that transfers latent diversities from known to novel classes. We follow a different approach and investigate how a simple and straightforward synthetic data generation method can be used effectively. We make two contributions, namely we show that: (1) using a simple loss function is more than enough for training a feature generator in the few-shot setting; and (2) learning to generate tensor features instead of vector features is superior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show that our method sets a new state of the art, outperforming more sophisticated few-shot data augmentation methods. The source code can be found at https://github.com/MichalisLazarou/TFH_fewshot.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Lazarou_Tensor_Feature_Hallucination_for_Few-Shot_Learning_WACV_2022_paper.pdf",
        "aff": "Imperial College London; Imperial College London; Athena RC",
        "project": "",
        "github": "https://github.com/MichalisLazarou/TFH_fewshot",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Lazarou_Tensor_Feature_Hallucination_WACV_2022_supplemental.pdf",
        "arxiv": "2106.05321",
        "pdf_size": 911533,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14985959446469864706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Imperial College London;Athena RC",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.imperial.ac.uk;",
        "aff_unique_abbr": "ICL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "de6a056edf",
        "title": "Tensor-Based Non-Rigid Structure From Motion",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Grasshof_Tensor-Based_Non-Rigid_Structure_From_Motion_WACV_2022_paper.html",
        "author": "Stella Gra\u00dfhof; Sami Sebastian Brandt",
        "abstract": "In this work we present a method that combines tensor-based face modelling and analysis and non-rigid structure-from-motion (NRSFM). The core idea is to see that the conventional tensor formulation for the face structure and expression analysis can be utilised while the structure component can be directly analysed as the non-rigid structure-from-motion problem. To the NRSFM problem part we further present a novel prior-free approach that factorises the 2D input shapes into affine projection matrices, rank-one 3D affine basis shapes, and the basis shape coefficients. The linear combination of the basis shapes thus yields the recovered 3D shapes upto an affine transformation. In contrast to most works in literature, no calibration information of the cameras or structure prior is required. Experiments on challenging face datasets show that our method, with and without the metric upgrade, is accurate and fast when compared to the state-of-the-art and is well suitable for dense reconstruction and face editing.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Grasshof_Tensor-Based_Non-Rigid_Structure_From_Motion_WACV_2022_paper.pdf",
        "aff": "IT University of Copenhagen, Denmark; IT University of Copenhagen, Denmark",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Grasshof_Tensor-Based_Non-Rigid_Structure_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5713904,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11477160281299494928&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IT University of Copenhagen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://itu.dk",
        "aff_unique_abbr": "ITU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "5e818fa1ac",
        "title": "The Hitchhiker's Guide to Prior-Shift Adaptation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sipka_The_Hitchhikers_Guide_to_Prior-Shift_Adaptation_WACV_2022_paper.html",
        "author": "Tom\u00e1\u0161 \u0160ipka; Milan \u0160ulc; Ji\u0159\u00ed Matas",
        "abstract": "In many computer vision classification tasks, class priors at test time often differ from priors on the training set. In the case of such prior shift, classifiers must be adapted correspondingly to maintain close to optimal performance. This paper analyzes methods for adaptation of probabilistic classifiers to new priors and for estimating new priors on an unlabeled test set. We propose a novel method to address a known issue of prior estimation methods based on confusion matrices, where inconsistent estimates of decision probabilities and confusion matrices lead to negative values in the estimated priors. Experiments on fine-grained image classification datasets provide insight into the best practice of prior shift estimation and classifier adaptation, and show that the proposed method achieves state-of-the-art results in prior adaptation. Applying the best practice to two tasks with naturally imbalanced priors, learning from web-crawled images and plant species classification, increased the recognition accuracy by 1.1% and 3.4% respectively.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sipka_The_Hitchhikers_Guide_to_Prior-Shift_Adaptation_WACV_2022_paper.pdf",
        "aff": "Czech Technical University in Prague; Czech Technical University in Prague; Czech Technical University in Prague",
        "project": "",
        "github": "https://github.com/sipkatom/The-Hitchhiker-s-Guide-to-Prior-Shift-Adaptation",
        "supp": "",
        "arxiv": "",
        "pdf_size": 590172,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17936670814133771054&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "seznam.cz;gmail.com;fel.cvut.cz",
        "email": "seznam.cz;gmail.com;fel.cvut.cz",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Czech Technical University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ctu.cz",
        "aff_unique_abbr": "CTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Prague",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Czech Republic"
    },
    {
        "id": "3b79da82f3",
        "title": "The Untapped Potential of Off-the-Shelf Convolutional Neural Networks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Inkawhich_The_Untapped_Potential_of_Off-the-Shelf_Convolutional_Neural_Networks_WACV_2022_paper.html",
        "author": "Matthew Inkawhich; Nathan Inkawhich; Eric Davis; Hai Li; Yiran Chen",
        "abstract": "Over recent years, a myriad of novel convolutional network architectures have been developed to advance state-of-the-art performance on challenging recognition tasks. As computational resources improve, a great deal of effort has been placed on efficiently scaling up existing designs and generating new architectures with Neural Architecture Search (NAS) algorithms. While network topology has proven to be a critical factor for model performance, we show that significant gains are being left on the table by keeping topology static at inference-time. Due to challenges such as scale variation, we should not expect static models configured to perform well across a training dataset to be optimally configured to handle all test data. In this work, we expose the exciting potential of inference-time-dynamic models. We show that by allowing just four layers to dynamically change configuration at inference-time, off-the-shelf models like ResNet-50 have an upper bound accuracy of over 95% on ImageNet. This level of performance currently exceeds that of models with over 20x more parameters and significantly more complex training procedures. While this upper bound of performance may be practically difficult to achieve for a real dynamic model, it indicates a significant source of untapped potential for current models.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Inkawhich_The_Untapped_Potential_of_Off-the-Shelf_Convolutional_Neural_Networks_WACV_2022_paper.pdf",
        "aff": "Duke University; Duke University; SRC Inc.; Duke University; Duke University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Inkawhich_The_Untapped_Potential_WACV_2022_supplemental.pdf",
        "arxiv": "2103.09891",
        "pdf_size": 1023454,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Nr2O-rMMa9kJ:scholar.google.com/&scioq=The+Untapped+Potential+of+Off-the-Shelf+Convolutional+Neural+Networks&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff_domain": "duke.edu; ; ; ; ",
        "email": "duke.edu; ; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Duke University;SRC Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.duke.edu;",
        "aff_unique_abbr": "Duke;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "681efe5ace",
        "title": "Time-Space Transformers for Video Panoptic Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Petrovai_Time-Space_Transformers_for_Video_Panoptic_Segmentation_WACV_2022_paper.html",
        "author": "Andra Petrovai; Sergiu Nedevschi",
        "abstract": "We propose a novel solution for the task of video panoptic segmentation, that simultaneously predicts pixel-level semantic and instance segmentation and generates clip-level instance tracks. Our network, named VPS-Transformer, with a hybrid architecture based on the state-of-the-art panoptic segmentation network Panoptic-DeepLab, combines a convolutional architecture for single-frame panoptic segmentation and a novel video module based on an instantiation of the pure Transformer block. The Transformer, equipped with attention mechanisms, models spatio-temporal relations between backbone output features of current and past frames for more accurate and consistent panoptic estimates. As the pure Transformer block introduces large computation overhead when processing high resolution images, we propose a few design changes for a more efficient compute. We study how to aggregate information more effectively over the space-time volume and we compare several variants of the Transformer block with different attention schemes. Extensive experiments on the Cityscapes-VPS dataset demonstrate that our best model improves the temporal consistency and video panoptic quality by a margin of 2.2%, with little extra computation.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Petrovai_Time-Space_Transformers_for_Video_Panoptic_Segmentation_WACV_2022_paper.pdf",
        "aff": "Technical University of Cluj-Napoca; Technical University of Cluj-Napoca",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9355298,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3881338203574984918&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cs.utcluj.ro;cs.utcluj.ro",
        "email": "cs.utcluj.ro;cs.utcluj.ro",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technical University of Cluj-Napoca",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tucluj.ro",
        "aff_unique_abbr": "TUCN",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cluj-Napoca",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Romania"
    },
    {
        "id": "c6b772b6e5",
        "title": "To Miss-Attend Is to Misalign! Residual Self-Attentive Feature Alignment for Adapting Object Detectors",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Khindkar_To_Miss-Attend_Is_to_Misalign_Residual_Self-Attentive_Feature_Alignment_for_WACV_2022_paper.html",
        "author": "Vaishnavi Khindkar; Chetan Arora; Vineeth N Balasubramanian; Anbumani Subramanian; Rohit Saluja; C.V. Jawahar",
        "abstract": "Advancements in adaptive object detection can lead to tremendous improvements in applications like autonomous navigation, as they alleviate the distributional shifts along the detection pipeline. Prior works adopt adversarial learning to align image features at global and local levels, yet the instance-specific misalignment persists. Also, adaptive object detection remains challenging due to visual diversity in background scenes and intricate combinations of objects. Motivated by structural importance, we aim to attend prominent instance-specific regions, overcoming the feature misalignment issue. We propose a novel resIduaL seLf-attentive featUre alignMEnt ( ILLUME ) method for adaptive object detection. ILLUME comprises Self-Attention Feature Map (SAFM) module that enhances structural attention to object-related regions and thereby generates domain invariant features. Our approach significantly reduces the domain distance with the improved feature alignment of the instances. Qualitative results demonstrate the ability of ILLUME to attend important object instances required for alignment. Experimental results on several benchmark datasets show that our method outperforms the existing state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Khindkar_To_Miss-Attend_Is_to_Misalign_Residual_Self-Attentive_Feature_Alignment_for_WACV_2022_paper.pdf",
        "aff": "CVIT - IIIT Hyderabad, India; IIT Delhi, India; IIT Hyderabad, India; CVIT - IIIT Hyderabad, India; CVIT - IIIT Hyderabad, India; CVIT - IIIT Hyderabad, India",
        "project": "",
        "github": "https://github.com/Vaishnvi/ILLUME",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Khindkar_To_Miss-Attend_Is_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1449339,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3539228942985225395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;cse.iitd.ac.in;iith.ac.in;iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "email": "gmail.com;cse.iitd.ac.in;iith.ac.in;iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0;0",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad;Indian Institute of Technology Delhi;Indian Institute of Technology Hyderabad",
        "aff_unique_dep": "Center for Visual Information Technology;;",
        "aff_unique_url": "https://www.iiit Hyderabad.ac.in;https://www.iitd.ac.in;https://www.iith.ac.in",
        "aff_unique_abbr": "IIIT Hyderabad;IITD;IIT Hyderabad",
        "aff_campus_unique_index": "0;1;0;0;0;0",
        "aff_campus_unique": "Hyderabad;Delhi",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "3f969a413b",
        "title": "Towards Active Vision for Action Localization With Reactive Control and Predictive Learning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Trehan_Towards_Active_Vision_for_Action_Localization_With_Reactive_Control_and_WACV_2022_paper.html",
        "author": "Shubham Trehan; Sathyanarayanan N. Aakur",
        "abstract": "Visual event perception tasks such as action localization have primarily focused on supervised learning settings under a static observer, i.e., the camera is static and cannot be controlled by an algorithm. They are often restricted by the quality, quantity, and diversity of annotated training data and do not often generalize to out-of-domain samples. In this work, we tackle the problem of active action localization where the goal is to localize an action while controlling the geometric and physical parameters of an active camera to keep the action in the field of view without training data. We formulate an energy-based mechanism that combines predictive learning and reactive control to perform active action localization without rewards, which can be sparse or non-existent in real-world environments. We perform extensive experiments in both simulated and real-world environments on two tasks - active object tracking and active action localization. We demonstrate that the proposed approach can generalize to different tasks and environments in a streaming fashion, requiring only a single pass through the video, working in real-time. We show that the proposed approach outperforms unsupervised baselines and obtains competitive performance compared to those trained with reinforcement learning.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Trehan_Towards_Active_Vision_for_Action_Localization_With_Reactive_Control_and_WACV_2022_paper.pdf",
        "aff": "Department of Computer Science, Oklahoma State University; Department of Computer Science, Oklahoma State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2111.05448",
        "pdf_size": 1742271,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=326791102011123721&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "okstate.edu;okstate.edu",
        "email": "okstate.edu;okstate.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oklahoma State University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.okstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8bb7079a40",
        "title": "Towards Class-Oriented Poisoning Attacks Against Neural Networks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zhao_Towards_Class-Oriented_Poisoning_Attacks_Against_Neural_Networks_WACV_2022_paper.html",
        "author": "Bingyin Zhao; Yingjie Lao",
        "abstract": "Poisoning attacks on machine learning systems compromise the model performance by deliberately injecting malicious samples in the training dataset to influence the training process. Prior works focus on either availability attacks (i.e., lowering the overall model accuracy) or integrity attacks (i.e., enabling specific instance-based backdoor). In this paper, we advance the adversarial objectives of the availability attacks to a per-class basis, which we refer to as class-oriented poisoning attacks. We demonstrate that the proposed attack is capable of forcing the corrupted model to predict in two specific ways: (i) classify unseen new images to a targeted \"supplanter\" class, and (ii) misclassify images from a \"victim\" class while maintaining the classification accuracy on other non-victim classes. To maximize the adversarial effect as well as reduce the computational complexity of poisoned data generation, we propose a gradient-based framework that crafts poisoning images with carefully manipulated feature information for each scenario. Using newly defined metrics at the class level, we demonstrate the effectiveness of the proposed class-oriented poisoning attacks on various models (e.g., LeNet-5, Vgg-9, and ResNet-50) over a wide range of datasets (e.g., MNIST, CIFAR-10, and ImageNet-ILSVRC2012) in an end-to-end training setting.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zhao_Towards_Class-Oriented_Poisoning_Attacks_Against_Neural_Networks_WACV_2022_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, Clemson University, SC, 29634, USA; Department of Electrical and Computer Engineering, Clemson University, SC, 29634, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Zhao_Towards_Class-Oriented_Poisoning_WACV_2022_supplemental.pdf",
        "arxiv": "2008.00047",
        "pdf_size": 1991281,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17576156922437950342&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "clemson.edu;clemson.edu",
        "email": "clemson.edu;clemson.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Clemson University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.clemson.edu",
        "aff_unique_abbr": "Clemson",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Clemson",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3b8658c074",
        "title": "Towards Durability Estimation of Bioprosthetic Heart Valves via Motion Symmetry Analysis",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Alizadeh_Towards_Durability_Estimation_of_Bioprosthetic_Heart_Valves_via_Motion_Symmetry_WACV_2022_paper.html",
        "author": "Maryam Alizadeh; Melissa Cote; Alexandra Branzan Albu",
        "abstract": "This paper addresses bioprosthetic heart valve (BHV) durability estimation via computer vision (CV)-based analyses of the visual symmetry of valve leaflet motion. BHVs are routinely implanted in patients suffering from valvular heart diseases. Valve designs are rigorously tested using cardiovascular equipment, but once implanted, more than 50% of BHVs encounter a structural failure within 15 years. We investigate the correlation between the visual dynamic symmetry of BHV leaflets and the functional symmetry of the valves. We hypothesize that an asymmetry in the valve leaflet motion will generate an asymmetry in the flow patterns, resulting in added local stress and forces on some of the leaflets, which can accelerate the failure of the valve. We propose two different pair-wise leaflet symmetry scores based on the diagonals of orthogonal projection matrices (DOPM) and on dynamic time warping (DTW), computed from videos recorded during pulsatile flow tests. We compare the symmetry score profiles with those of fluid dynamic parameters (velocity and vorticity values) at the leaflet borders, obtained from valve-specific numerical simulations. Experiments on four cases that include three different tricuspid BHVs yielded promising results, with the DTW scores showing a good coherence with respect to the simulations. With a link between visual and functional symmetries established, this approach paves the way towards BHV durability estimation using CV techniques.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Alizadeh_Towards_Durability_Estimation_of_Bioprosthetic_Heart_Valves_via_Motion_Symmetry_WACV_2022_paper.pdf",
        "aff": "Electrical and Computer Engineering, University of Victoria; Electrical and Computer Engineering, University of Victoria; Electrical and Computer Engineering, University of Victoria",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7219138,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:zlJTVrbDCCAJ:scholar.google.com/&scioq=Towards+Durability+Estimation+of+Bioprosthetic+Heart+Valves+via+Motion+Symmetry+Analysis&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff_domain": "uvic.ca;uvic.ca;uvic.ca",
        "email": "uvic.ca;uvic.ca;uvic.ca",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Victoria",
        "aff_unique_dep": "Electrical and Computer Engineering",
        "aff_unique_url": "https://www.uvic.ca",
        "aff_unique_abbr": "UVic",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Victoria",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2857c06cb2",
        "title": "Towards a Robust Differentiable Architecture Search Under Label Noise",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Simon_Towards_a_Robust_Differentiable_Architecture_Search_Under_Label_Noise_WACV_2022_paper.html",
        "author": "Christian Simon; Piotr Koniusz; Lars Petersson; Yan Han; Mehrtash Harandi",
        "abstract": "We all have experienced the difficulty of designing appropriate neural architectures due to the lack of general principles and best practices. The game changer might be touse Neural Architecture Search (NAS) where a machine does all the hard work for us based on the data at its disposal. Invarious problems and in particular in classification, architectures designed by NAS outperform or compete with the best manual network designs in terms of accuracy, size, memory footprint and FLOPs. That said, previous studies focus ondeveloping NAS algorithms for clean high quality data, a restrictive and somewhat unrealistic assumption. In this paper, focusing on the differentiable NAS algorithms, we show that vanilla NAS algorithms suffer from a performance loss if class labels are noisy. To combat this issue, we propose tomake use of the principle of information bottleneck as a regularizer. This leads us to develop a noise injecting operation that is included during the learning process, preventing the network from learning from noisy samples. Our empirical evaluations show that the noise injecting operation does not degrade the performance of the NAS algorithm if the data is indeed clean. In contrast, if the data is noisy, the architecture learned by our algorithm comfortably outperforms algorithms specifically equipped with sophisticated mechanisms to learn in the presence of label noise. In contrast to many algorithms designed to work in the presence of noisylabels, prior knowledge about the properties of the noise and its characteristics are not required for our algorithm.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Simon_Towards_a_Robust_Differentiable_Architecture_Search_Under_Label_Noise_WACV_2022_paper.pdf",
        "aff": "The Australian National University+Data61-CSIRO; Data61-CSIRO+The Australian National University; Data61-CSIRO+The Australian National University; The Australian National University+Data61-CSIRO; Monash University+Data61-CSIRO",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2110.12197",
        "pdf_size": 1134427,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11721669331160992461&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "anu.edu.au;data61.csiro.au;data61.csiro.au;anu.edu.au;monash.edu",
        "email": "anu.edu.au;data61.csiro.au;data61.csiro.au;anu.edu.au;monash.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1+0;1+0;0+1;2+1",
        "aff_unique_norm": "Australian National University;Commonwealth Scientific and Industrial Research Organisation;Monash University",
        "aff_unique_dep": ";Data61;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.csiro.au;https://www.monash.edu",
        "aff_unique_abbr": "ANU;CSIRO;Monash",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "0ae6542db5",
        "title": "Trading-Off Information Modalities in Zero-Shot Classification",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Sanchez_Trading-Off_Information_Modalities_in_Zero-Shot_Classification_WACV_2022_paper.html",
        "author": "Jorge S\u00e1nchez; Mat\u00edas Molina",
        "abstract": "Zero-shot classification is the task of learning predictors for classes not seen during training. A practical way to deal with the lack of annotations for the target categories is to encode not only the inputs (images) but also the outputs (object classes) into a suitable representation space. We can use these representations to measure the degree at which images and categories agree by fitting a compatibility measure using the information available during training. One way to define such a measure is by a two step process in which we first project the elements of either space visual or semantic) onto the other and then compute a similarity score in the target space. Although projections onto the visual space has shown better general performance, little attention has been paid to the degree at which the visual and semantic information contribute to the final predictions. In this paper, we build on this observation and propose two different formulations that allow us to explicitly trade-off the relative importance of the visual and semantic spaces for classification in a zero-shot setting. Our formulations are based on redefinition of the similarity scoring and loss function used to learn the projections. Experiments on six different datasets show that our approach lead to improve performance compared to similar methods. Moreover, combined with synthetic features, our approach competes favorably with the state of the art on both the standard and generalized settings.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Sanchez_Trading-Off_Information_Modalities_in_Zero-Shot_Classification_WACV_2022_paper.pdf",
        "aff": "CIEM-CONICET; CONICET and Universidad Nacional de C \u00b4ordoba",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 484985,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11713857574299540140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "unc.edu.ar;unc.edu.ar",
        "email": "unc.edu.ar;unc.edu.ar",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Centro de Investigaciones Econ\u00f3micas, CIEM;CONICET",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ciem-conicet.gob.ar;https://www.conicet.gov.ar",
        "aff_unique_abbr": "CIEM;CONICET",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Argentina"
    },
    {
        "id": "93ab98e8c4",
        "title": "Training a Task-Specific Image Reconstruction Loss",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Mustafa_Training_a_Task-Specific_Image_Reconstruction_Loss_WACV_2022_paper.html",
        "author": "Aamir Mustafa; Aliaksei Mikhailiuk; Dan Andrei Iliescu; Varun Babbar; Rafa\u0142 K. Mantiuk",
        "abstract": "The choice of a loss function is an important factor when training neural networks for image restoration problems, such as single image super resolution. The loss function should encourage natural and perceptually pleasing results. A popular choice for a loss is a pre-trained network, such as VGG, which is used as a feature extractor for computing the difference between restored and reference images. However, such an approach has multiple drawbacks: it is computationally expensive, requires regularization and hyper-parameter tuning, and involves a large network trained on an unrelated task. Furthermore, it has been observed that there is no single loss function that works best across all applications and across different datasets. In this work, we instead propose to train a set of loss functions that are application specific in nature. Our loss function comprises a series of discriminators that are trained to detect and penalize the presence of application-specific artefacts. We show that a single natural image and corresponding distortions are sufficient to train our feature extractor that outperforms state-of-the-art loss functions in applications like single image super resolution, denoising, and JPEG artefact removal. Finally, we conclude that an effective loss function does not have to be a good predictor of perceived image quality, but instead needs to be specialized in identifying the distortions for a given restoration method.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Mustafa_Training_a_Task-Specific_Image_Reconstruction_Loss_WACV_2022_paper.pdf",
        "aff": "University of Cambridge, UK; University of Cambridge, UK; University of Cambridge, UK; University of Cambridge, UK; University of Cambridge, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2103.14616",
        "pdf_size": 12943767,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16425086322482069564&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "768eda1498",
        "title": "Transductive Weakly-Supervised Player Detection Using Soccer Broadcast Videos",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gadde_Transductive_Weakly-Supervised_Player_Detection_Using_Soccer_Broadcast_Videos_WACV_2022_paper.html",
        "author": "Chris Andrew Gadde; C.V. Jawahar",
        "abstract": "Player detection lays the foundation for many applications in the field of sports analytics including player recognition, player tracking, and activity detection. In this work we study player detection in continuous long shot broadcast videos. Broadcast match videos are easy to obtain, and detection on these videos is much more challenging. We propose a transductive approach for player detection that treats it as a domain adaptation problem. We show that instance-level domain labels are significant for sufficient adaptation in the case of soccer broadcast videos. An efficient multi-model greedy labelling scheme based on visual features is proposed to annotate domain labels on bounding box predictions made by our inductive model. We use reliable instances from the inductive model inferences to train a transductive copy of the model. We create and release a fully annotated player detection dataset comprising soccer broadcast videos from the FIFA 2018 World Cup matches to evaluate our method. Our method shows significant improvements in player detection to the baseline and existing state-of-the-art methods on our dataset. We show, on average, a 16 point improvement in mAP for soccer broadcast videos by annotating domain labels for around a 100 samples per video.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gadde_Transductive_Weakly-Supervised_Player_Detection_Using_Soccer_Broadcast_Videos_WACV_2022_paper.pdf",
        "aff": "International Institute of Information Technology, Hyderabad, India; International Institute of Information Technology, Hyderabad, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Gadde_Transductive_Weakly-Supervised_Player_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6016319,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9281838328955317381&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "research.iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;iiit.ac.in",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "International Institute of Information Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": "IIIT Hyderabad",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "be66612474",
        "title": "Transfer Learning for Pose Estimation of Illustrated Characters",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chen_Transfer_Learning_for_Pose_Estimation_of_Illustrated_Characters_WACV_2022_paper.html",
        "author": "Shuhong Chen; Matthias Zwicker",
        "abstract": "Human pose information is a critical component in many downstream image processing tasks, such as activity recognition and motion tracking. Likewise, a pose estimator for the illustrated character domain would provide a valuable prior for assistive content creation tasks, such as reference pose retrieval and automatic character animation. But while modern data-driven techniques have substantially improved pose estimation performance on natural images, little work has been done for illustrations. In our work, we bridge this domain gap by efficiently transfer-learning from both domain-specific and task-specific source models. Additionally, we upgrade and expand an existing illustrated pose estimation dataset, and introduce two new datasets for classification and segmentation subtasks. We then apply the resultant state-of-the-art character pose estimator to solve the novel task of pose-guided illustration retrieval. All data, models, and code will be made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chen_Transfer_Learning_for_Pose_Estimation_of_Illustrated_Characters_WACV_2022_paper.pdf",
        "aff": "University of Maryland - College Park; University of Maryland - College Park",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Chen_Transfer_Learning_for_WACV_2022_supplemental.zip",
        "arxiv": "2108.01819",
        "pdf_size": 1942675,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2526353444062781679&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cs.umd.edu;cs.umd.edu",
        "email": "cs.umd.edu;cs.umd.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "435e12457a",
        "title": "Transferable 3D Adversarial Textures Using End-to-End Optimization",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Pestana_Transferable_3D_Adversarial_Textures_Using_End-to-End_Optimization_WACV_2022_paper.html",
        "author": "Camilo Pestana; Naveed Akhtar; Nazanin Rahnavard; Mubarak Shah; Ajmal Mian",
        "abstract": "Deep visual models are known to be vulnerable to adversarial attacks. The last few years have seen numerous techniques to compute adversarial inputs for these models. However, there are still under-explored avenues in this critical research direction. Among those is the estimation of adversarial textures for 3D models in an end-to-end optimization scheme. In this paper, we propose such a scheme to generate adversarial textures for 3D models that are highly transferable and invariant to different camera views and lighting conditions. Our method makes use of neural rendering with explicit control over the model texture and background. We ensure transferability of the adversarial textures by employing an ensemble of robust and non-robust models. Our technique utilizes 3D models as a proxy to simulate closer to real-life conditions, in contrast to conventional use of 2D images for adversarial attacks. We show the efficacy of our method with extensive experiments.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Pestana_Transferable_3D_Adversarial_Textures_Using_End-to-End_Optimization_WACV_2022_paper.pdf",
        "aff": "The University of Western Australia; The University of Western Australia; University of Central Florida; University of Central Florida; The University of Western Australia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Pestana_Transferable_3D_Adversarial_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 6529407,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14683006350116907241&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "uwa.edu.au;uwa.edu.au;ucf.edu;crcv.ucf.edu;uwa.edu.au",
        "email": "uwa.edu.au;uwa.edu.au;ucf.edu;crcv.ucf.edu;uwa.edu.au",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "University of Western Australia;University of Central Florida",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uwa.edu.au;https://www.ucf.edu",
        "aff_unique_abbr": "UWA;UCF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "be4ec16aef",
        "title": "TricubeNet: 2D Kernel-Based Object Representation for Weakly-Occluded Oriented Object Detection",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kim_TricubeNet_2D_Kernel-Based_Object_Representation_for_Weakly-Occluded_Oriented_Object_Detection_WACV_2022_paper.html",
        "author": "Beomyoung Kim; Janghyeon Lee; Sihaeng Lee; Doyeon Kim; Junmo Kim",
        "abstract": "We present a novel approach for oriented object detection, named TricubeNet, which localizes oriented objects using visual cues (i.e., heatmap) instead of oriented box offsets regression. We represent each object as a 2D Tricube kernel and extract bounding boxes using simple image-processing algorithms. Our approach is able to (1) obtain well-arranged boxes from visual cues, (2) solve the angle discontinuity problem, and (3) can save computational complexity due to our anchor-free modeling. To further boost the performance, we propose some effective techniques for size-invariant loss, reducing false detections, extracting rotation-invariant features, and heatmap refinement. To demonstrate the effectiveness of our TricubeNet, we experiment on various tasks for weakly-occluded oriented object detection: detection in an aerial image, densely packed object image, and text image. The extensive experimental results show that our TricubeNet is quite effective for oriented object detection. Code is available at https://github.com/qjadud1994/TricubeNet.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kim_TricubeNet_2D_Kernel-Based_Object_Representation_for_Weakly-Occluded_Oriented_Object_Detection_WACV_2022_paper.pdf",
        "aff": "NA VER CLOV A; LG AI Research; KAIST+NA VER CLOV A; KAIST; KAIST",
        "project": "",
        "github": "https://github.com/qjadud1994/TricubeNet",
        "supp": "",
        "arxiv": "2104.11435",
        "pdf_size": 1295606,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12384032617461356547&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "1;2;2;2",
        "aff_unique_norm": ";LG;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": ";LG AI Research;",
        "aff_unique_url": ";https://www.lgaires.com;https://www.kaist.ac.kr",
        "aff_unique_abbr": ";LG AI;KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";South Korea"
    },
    {
        "id": "36462ff886",
        "title": "Typenet: Towards Camera Enabled Touch Typing on Flat Surfaces Through Self-Refinement",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Maman_Typenet_Towards_Camera_Enabled_Touch_Typing_on_Flat_Surfaces_Through_WACV_2022_paper.html",
        "author": "Ben Maman; Amit Bermano",
        "abstract": "Text entry for mobile devices nowadays is an equally crucial and time-consuming task, with no practical solution available for natural typing speeds without extra hardware. In this paper, we introduce a real-time method that is a significant step towards enabling touch typing on arbitrary flat surfaces (e.g., tables). The method employs only a simple video camera, placed in front of the user on the flat surface --- at an angle practical for mobile usage. To achieve this, we adopt a classification framework, based on the observation that, in touch typing, similar hand configurations imply the same typed character across users. Importantly, this approach allows the convenience of un-calibrated typing, where the hand positions, with respect to the camera and each other, are not dictated. To improve accuracy, we propose a Language Processing scheme, which corrects the typed text and is specifically designed for real-time performance and integration with the vision-based signal. To enable feasible data collection and training, we propose a self-refinement approach that allows training on unlabeled flat-surface-typing footage; A network trained on (labeled) keyboard footage labels flat-surface videos using dynamic time warping, and is trained on them, in an Expectation Maximization (EM) manner. Using these techniques, we introduce the TypingHands26 Dataset, comprising videos of 26 different users typing on a keyboard, and 10 users typing on a flat surface, labeled at the frame level. We validate our approach and present a single camera-based system with character-level accuracy of 93.5% on average for known users, and 85.7% for unknown ones, outperforming pose-estimation-based methods by a large margin, despite performing at natural typing speeds of up to 80 Words Per Minute. Our method is the first to rely on a simple camera alone, and runs in interactive speeds, while still maintaining accuracy comparable to systems employing non-commodity equipment.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Maman_Typenet_Towards_Camera_Enabled_Touch_Typing_on_Flat_Surfaces_Through_WACV_2022_paper.pdf",
        "aff": "Tel Aviv University; Tel Aviv University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Maman_Typenet_Towards_Camera_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2672095,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4715870737398157306&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tel Aviv University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "18852e87f4",
        "title": "UNETR: Transformers for 3D Medical Image Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hatamizadeh_UNETR_Transformers_for_3D_Medical_Image_Segmentation_WACV_2022_paper.html",
        "author": "Ali Hatamizadeh; Yucheng Tang; Vishwesh Nath; Dong Yang; Andriy Myronenko; Bennett Landman; Holger R. Roth; Daguang Xu",
        "abstract": "Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful \"U-shaped\" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hatamizadeh_UNETR_Transformers_for_3D_Medical_Image_Segmentation_WACV_2022_paper.pdf",
        "aff": ";;;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "2103.10504",
        "pdf_size": 912757,
        "gs_citation": 2681,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4072300244718161964&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "author_num": 8,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1713488baf",
        "title": "Uncertainty Learning Towards Unsupervised Deformable Medical Image Registration",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Gong_Uncertainty_Learning_Towards_Unsupervised_Deformable_Medical_Image_Registration_WACV_2022_paper.html",
        "author": "Xuan Gong; Luckyson Khaidem; Wentao Zhu; Baochang Zhang; David Doermann",
        "abstract": "Uncertainty estimation in medical image registration enables surgeons to evaluate the operative risk based on the trustworthiness of the registered image data thus of paramount importance for practical clinical applications. Despite the recent promising results obtained with deep unsupervised learning-based registration methods, reasoning about uncertainty of unsupervised registration models remains largely unexplored. In this work, we propose a predictive module to learn the registration and uncertainty in correspondence simultaneously. Our framework introduces empirical randomness and registration error based uncertainty prediction. We systematically assess the performances on two MRI datasets with different ensemble paradigms. Experimental results highlight that our proposed framework significantly improves the registration accuracy and uncertainty compared with the baseline.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Gong_Uncertainty_Learning_Towards_Unsupervised_Deformable_Medical_Image_Registration_WACV_2022_paper.pdf",
        "aff": "University at Buffalo; University at Buffalo; Kuaishou Technology; Beihang University; University at Buffalo",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 745900,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14573185079772312030&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "buffalo.edu;buffalo.edu;gmail.com;buaa.edu.cn;buffalo.edu",
        "email": "buffalo.edu;buffalo.edu;gmail.com;buaa.edu.cn;buffalo.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University at Buffalo;Kuaishou Technology;Beihang University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.buffalo.edu;https://www.kuaishou.com;http://www.buaa.edu.cn/",
        "aff_unique_abbr": "UB;Kuaishou;BUAA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "c4a9c3abfa",
        "title": "Unsupervised Learning for Human Sensing Using Radio Signals",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Li_Unsupervised_Learning_for_Human_Sensing_Using_Radio_Signals_WACV_2022_paper.html",
        "author": "Tianhong Li; Lijie Fan; Yuan Yuan; Dina Katabi",
        "abstract": "There is a growing literature demonstrating the feasibility of using Radio Frequency (RF) signals to enable key computer vision tasks in the presence of occlusions and poor lighting. It leverages that RF signals traverse walls and occlusions to deliver through-wall pose estimation, action recognition, scene captioning, and human re-identification. However, unlike RGB datasets which can be labeled by human workers, labeling RF signals is a daunting task because such signals are not human interpretable. Yet, it is fairly easy to collect unlabelled RF signals. It would be highly beneficial to use such unlabeled RF data to learn useful representations in an unsupervised manner. Thus, in this paper, we explore the feasibility of adapting RGB-based unsupervised representation learning to RF signals. We show that while contrastive learning has emerged as the main technique for unsupervised representation learning from images and videos, such methods produce poor performance when applied to sensing humans using RF signals. In contrast, predictive unsupervised learning methods learn high-quality representations that can be used for multiple downstream RF-based sensing tasks. Our empirical results show that this approach outperforms state-of-the-art RF-based human sensing on various tasks, opening the possibility of unsupervised representation learning from this novel modality.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Li_Unsupervised_Learning_for_Human_Sensing_Using_Radio_Signals_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2170815,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13288749669711966512&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6b968ef73b",
        "title": "Unsupervised Robust Domain Adaptation Without Source Data",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Agarwal_Unsupervised_Robust_Domain_Adaptation_Without_Source_Data_WACV_2022_paper.html",
        "author": "Peshal Agarwal; Danda Pani Paudel; Jan-Nico Zaech; Luc Van Gool",
        "abstract": "We study the problem of robust domain adaptation in the context of unavailable target labels and source data. The considered robustness is against adversarial perturbations. This paper aims at answering the question of finding the right strategy to make the target model robust and accurate in the setting of unsupervised domain adaptation without source data. The major findings of this paper are: (i) robust source models can be transferred robustly to the target; (ii) robust domain adaptation can greatly benefit from non-robust pseudo-labels and the pair-wise contrastive loss. The proposed method of using non-robust pseudo-labels performs surprisingly well on both clean and adversarial samples, for the task of image classification. We show a consistent performance improvement of over 10% in accuracy against the tested baselines on four benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Agarwal_Unsupervised_Robust_Domain_Adaptation_Without_Source_Data_WACV_2022_paper.pdf",
        "aff": "Computer Vision Laboratory, ETH Zurich, Switzerland; Computer Vision Laboratory, ETH Zurich, Switzerland; Computer Vision Laboratory, ETH Zurich, Switzerland; Computer Vision Laboratory, ETH Zurich, Switzerland+KU Leuven, Belgium",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Agarwal_Unsupervised_Robust_Domain_WACV_2022_supplemental.pdf",
        "arxiv": "2103.14577",
        "pdf_size": 2912912,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17077601123730740232&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "student.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "student.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "ETH Zurich;KU Leuven",
        "aff_unique_dep": "Computer Vision Laboratory;",
        "aff_unique_url": "https://www.ethz.ch;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Switzerland;Belgium"
    },
    {
        "id": "19e11c8cf2",
        "title": "Unsupervised Sounding Object Localization With Bottom-Up and Top-Down Attention",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Shi_Unsupervised_Sounding_Object_Localization_With_Bottom-Up_and_Top-Down_Attention_WACV_2022_paper.html",
        "author": "Jiayin Shi; Chao Ma",
        "abstract": "Learning to localize sounding objects in visual scenes without manual annotations has drawn increasing attention recently. In this paper, we propose an unsupervised sounding object localization algorithm by using bottom-up and top-down attention in visual scenes. The bottom-up attention module generates an objectness confidence map, while the top-down attention draws the similarity between sound and visual regions. Moreover, we propose a bottom-up attention loss function, which models the correlation relationship between bottom-up and top-down attention. Extensive experimental results demonstrate that our proposed unsupervised method significantly advances the state-of-the-art unsupervised methods. The source code is available at https://github.com/VISION-SJTU/usol/.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Shi_Unsupervised_Sounding_Object_Localization_With_Bottom-Up_and_Top-Down_Attention_WACV_2022_paper.pdf",
        "aff": "MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
        "project": "",
        "github": "https://github.com/VISION-SJTU/USOL",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2049089,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6533664973990377551&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "AI Institute",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "5c72240b00",
        "title": "Unveiling Real-Life Effects of Online Photo Sharing",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Nguyen_Unveiling_Real-Life_Effects_of_Online_Photo_Sharing_WACV_2022_paper.html",
        "author": "Van-Khoa Nguyen; Adrian Popescu; J\u00e9r\u00f4me Deshayes-Chossart",
        "abstract": "Social networks give free access to their services in exchange for the right to exploit their users' data. Data sharing is done in an initial context which is chosen by the users. However, data are used by social networks and third parties in different contexts which are often not transparent. In order to unveil such usages, we propose an approach which focuses on the effects of data sharing in impactful real-life situations. Focus is put on visual content because of its strong influence in shaping online user profiles. The approach relies on three components: (1) a set of concepts with associated situation impact ratings obtained by crowdsourcing, (2) a corresponding set of object detectors for mining users' photos and (3) a ground truth dataset made of 500 visual user profiles which are manually rated per situation. These components are combined in LERVUP, a method which learns to rate visual user profiles in each situation. LERVUP exploits a new image descriptor which aggregates concept ratings and object detections at user level and an attention mechanism which boosts highly-rated concepts to prevent them from being overwhelmed by low-rated ones. Performance is evaluated per situation by measuring the correlation between the automatic ranking of profile ratings and a manual ground truth. Results indicate that LERVUP is effective since a strong correlation of the two rankings is obtained. A practical implementation of the approach in a mobile app which raises user awareness about shared data usage is also discussed.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Nguyen_Unveiling_Real-Life_Effects_of_Online_Photo_Sharing_WACV_2022_paper.pdf",
        "aff": "Universit\u00e9 Paris-Saclay, CEA, List, F-91120, Palaiseau, France; Universit\u00e9 Paris-Saclay, CEA, List, F-91120, Palaiseau, France; Universit\u00e9 Paris-Saclay, CEA, List, F-91120, Palaiseau, France",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Nguyen_Unveiling_Real-Life_Effects_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 895707,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5997246204568201277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;cea.fr;cea.fr",
        "email": "gmail.com;cea.fr;cea.fr",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universit\u00e9 Paris-Saclay",
        "aff_unique_dep": "CEA List",
        "aff_unique_url": "https://www.universite-paris-saclay.fr",
        "aff_unique_abbr": "UPS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Palaiseau",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "9c87bbdf17",
        "title": "V-SlowFast Network for Efficient Visual Sound Separation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zhu_V-SlowFast_Network_for_Efficient_Visual_Sound_Separation_WACV_2022_paper.html",
        "author": "Lingyu Zhu; Esa Rahtu",
        "abstract": "The objective of this paper is to perform visual sound separation: i) we study visual sound separation on spectrograms of different temporal resolutions; ii) we propose a new light yet efficient three-stream framework V-SlowFast that operates on Visual frame, Slow spectrogram, and Fast spectrogram. The Slow spectrogram captures the coarse temporal resolution while the Fast spectrogram contains the fine-grained temporal resolution; iii) we introduce two contrastive objectives to encourage the network to learn discriminative visual features for separating sounds; iv) we propose an audio-visual global attention module for audio and visual feature fusion; v) the introduced V-SlowFast model outperforms previous state-of-the-art in single-frame based visual sound separation on small- and large-scale datasets: MUSIC-21, AVE, and VGG-Sound. We also propose a small V-SlowFast architecture variant, which achieves 74.2% reduction in the number of model parameters and 81.4% reduction in GMACs compared to the previous multi-stage models. Project page: https://ly-zhu.github.io/V-SlowFast",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zhu_V-SlowFast_Network_for_Efficient_Visual_Sound_Separation_WACV_2022_paper.pdf",
        "aff": "Tampere University, Finland; Tampere University, Finland",
        "project": "https://ly-zhu.github.io/V-SlowFast",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Zhu_V-SlowFast_Network_for_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2577957,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16845016936552725023&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tuni.fi;tuni.fi",
        "email": "tuni.fi;tuni.fi",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tampere University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tuni.fi",
        "aff_unique_abbr": "Tuni",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "821f1fb2ab",
        "title": "VCSeg: Virtual Camera Adaptation for Road Segmentation",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Cheng_VCSeg_Virtual_Camera_Adaptation_for_Road_Segmentation_WACV_2022_paper.html",
        "author": "Gong Cheng; James H. Elder",
        "abstract": "Domain shift limits generalization in many problem domains. For road segmentation, one of the principal causes of domain shift is variation in the geometric camera parameters, which results in misregistration of scene structure between images. To address this issue, we decompose the shift into two components: Between-camera shift and within-camera shift. To handle between-camera shift, we assume that average camera parameters are known or can be estimated and use this knowledge to rectify both source and target domain images to a standard virtual camera model. To handle within-camera shift, we use estimates of road vanishing points to correct for shifts in camera pan and tilt. While this approach improves alignment, it produces gaps in the virtual image that complicates network training. To solve this problem, we introduce a novel projective image completion method that fills these gaps in a plausible way. Using five diverse and challenging road segmentation datasets, we demonstrate that our virtual camera method dramatically improves road segmentation performance when generalizing across cameras, and propose that this be integrated as a standard component of road segmentation systems to improve generalization",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Cheng_VCSeg_Virtual_Camera_Adaptation_for_Road_Segmentation_WACV_2022_paper.pdf",
        "aff": "York University; York University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Cheng_VCSeg_Virtual_Camera_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2603304,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=525394290053356330&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "eecs.yorku.ca;yorku.ca",
        "email": "eecs.yorku.ca;yorku.ca",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yorku.ca",
        "aff_unique_abbr": "York U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "cda4939358",
        "title": "Variational Stacked Local Attention Networks for Diverse Video Captioning",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Deb_Variational_Stacked_Local_Attention_Networks_for_Diverse_Video_Captioning_WACV_2022_paper.html",
        "author": "Tonmoay Deb; Akib Sadmanee; Kishor Kumar Bhaumik; Amin Ahsan Ali; M Ashraful Amin; A K M Mahbubur Rahman",
        "abstract": "While describing spatio-temporal events in natural language, video captioning models mostly rely on the encoder's latent visual representation. Recent progress on the encoder-decoder model attends encoder features mainly in linear interaction with the decoder. However, growing model complexity for visual data encourages more explicit feature interaction for fine-grained information, which is currently absent in the video captioning domain. Moreover, feature aggregations methods have been used to unveil richer visual representation, either by the concatenation or using a linear layer. Though feature sets for a video semantically overlap to some extent, these approaches result in objective mismatch and feature redundancy. In addition, diversity in captions is a fundamental component of expressing one event from several meaningful perspectives, currently missing in the temporal, i.e., video captioning domain. To this end, we propose Variational Stacked Local Attention Network (VSLAN), which exploits low-rank bilinear pooling for self-attentive feature interaction and stacking multiple video feature streams in a discount fashion. Each feature stack's learned attributes contribute to our proposed diversity encoding module, followed by the decoding query stage to facilitate end-to-end diverse and natural captions without any explicit supervision on attributes. We evaluate VSLAN on MSVD and MSR-VTT datasets in terms of syntax and diversity. The CIDEr score of VSLAN outperforms current off-the-shelf methods by 7.8% on MSVD and 4.5% on MSR-VTT, respectively. On the same datasets, VSLAN achieves competitive results in caption diversity metrics.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Deb_Variational_Stacked_Local_Attention_Networks_for_Diverse_Video_Captioning_WACV_2022_paper.pdf",
        "aff": "Arti\ufb01cial Intelligence and Cybernetics (AGenCy) Lab, Independent University, Bangladesh; Arti\ufb01cial Intelligence and Cybernetics (AGenCy) Lab, Independent University, Bangladesh; Arti\ufb01cial Intelligence and Cybernetics (AGenCy) Lab, Independent University, Bangladesh; Arti\ufb01cial Intelligence and Cybernetics (AGenCy) Lab, Independent University, Bangladesh; Arti\ufb01cial Intelligence and Cybernetics (AGenCy) Lab, Independent University, Bangladesh; Arti\ufb01cial Intelligence and Cybernetics (AGenCy) Lab, Independent University, Bangladesh",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Deb_Variational_Stacked_Local_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1592653,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11052078087638170436&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;iub.edu.bd;iub.edu.bd;iub.edu.bd;iub.edu.bd;iub.edu.bd",
        "email": "gmail.com;iub.edu.bd;iub.edu.bd;iub.edu.bd;iub.edu.bd;iub.edu.bd",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Independent University, Bangladesh",
        "aff_unique_dep": "Arti\ufb01cial Intelligence and Cybernetics (AGenCy) Lab",
        "aff_unique_url": "https://www.independent.edu.bd",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Bangladesh"
    },
    {
        "id": "f678bb7665",
        "title": "Video Salient Object Detection via Contrastive Features and Attention Modules",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Chen_Video_Salient_Object_Detection_via_Contrastive_Features_and_Attention_Modules_WACV_2022_paper.html",
        "author": "Yi-Wen Chen; Xiaojie Jin; Xiaohui Shen; Ming-Hsuan Yang",
        "abstract": "Video salient object detection aims to find the most visually distinct objects in a video. To explore the temporal dependencies, existing methods usually resort to recurrent neural networks or optical flow. However, these approaches require high computational cost, and tend to accumulate inaccuracies over time. In this paper, we propose a network with attention modules to learn contrastive features for video salient object detection without the high computational temporal modeling techniques. We develop a non-local self-attention scheme to capture the global information in the video frame. A co-attention formulation is utilized to combine the low-level and high-level features. We further apply the contrastive learning to improve the feature representations, where foreground region pairs from the same video are pulled together, and foreground-background region pairs are pushed away in the latent space. The intra-frame contrastive loss helps separate the foreground and background features, and the inter-frame contrastive loss improves the temporal consistency. We conduct extensive experiments on several benchmark datasets for video salient object detection and unsupervised video object segmentation, and show that the proposed method requires less computation, and performs favorably against the state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Chen_Video_Salient_Object_Detection_via_Contrastive_Features_and_Attention_Modules_WACV_2022_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Chen_Video_Salient_Object_WACV_2022_supplemental.zip",
        "arxiv": "2111.02368",
        "pdf_size": 2908296,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5987827155395722059&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d54559c2e1",
        "title": "Video and Text Matching With Conditioned Embeddings",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ali_Video_and_Text_Matching_With_Conditioned_Embeddings_WACV_2022_paper.html",
        "author": "Ameen Ali; Idan Schwartz; Tamir Hazan; Lior Wolf",
        "abstract": "We present a method for matching a text sentence from a given corpus to a given video clip and vice versa. Traditionally video and text matching is done by learning a shared embedding space and the encoding of one modality is independent of the other. In this work, we encode the dataset data in a way that takes into account the query's relevant information. The power of the method is demonstrated to arise from pooling the interaction data between words and frames. Since the encoding of the video clip depends on the sentence compared to it, the representation needs to be recomputed for each potential match. To this end, we propose an efficient shallow neural network. Its training employs a hierarchical triplet loss that is extendable to paragraph/video matching. The method is simple, provide explainability, and achieves a state-of-the-art-results, for both sentence-clip and video-text by a sizable margin across five different datasets: ActivityNet, DiDeMo, YouCook2, MSR-VTT, and LSMDC. We also show that our conditioned representation can be transferred to video-guided machine translation, where we improved the current results on VATEX. Source code is available at https://github.com/AmeenAli/VideoMatch.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ali_Video_and_Text_Matching_With_Conditioned_Embeddings_WACV_2022_paper.pdf",
        "aff": "Tel Aviv University; Technion + NetApp; Technion; Tel Aviv University",
        "project": "",
        "github": "https://github.com/AmeenAli/VideoMatch",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ali_Video_and_Text_WACV_2022_supplemental.pdf",
        "arxiv": "2110.11298",
        "pdf_size": 9943167,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=366065270813233737&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1;0",
        "aff_unique_norm": "Tel Aviv University;Technion - Israel Institute of Technology;NetApp",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tau.ac.il;https://www.technion.ac.il/en/;https://www.netapp.com",
        "aff_unique_abbr": "TAU;Technion;NetApp",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "060f2766ef",
        "title": "Visual Understanding of Complex Table Structures From Document Images",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Raja_Visual_Understanding_of_Complex_Table_Structures_From_Document_Images_WACV_2022_paper.html",
        "author": "Sachin Raja; Ajoy Mondal; C.V. Jawahar",
        "abstract": "Table structure recognition is necessary for a comprehensive understanding of documents. Tables in unstructured business documents are tough to parse due to the high diversity of layouts, varying alignments of contents, and the presence of empty cells. The problem is particularly difficult because of challenges in identifying individual cells using visual or linguistic contexts or both. Accurate detection of table cells (including empty cells) simplifies structure extraction and hence, it becomes the prime focus of our work. We propose a novel object-detection-based deep model that captures the inherent alignments of cells within tables and is fine-tuned for fast optimization. Despite accurate detection of cells, recognizing structures for dense tables may still be challenging because of difficulties in capturing long-range row/column dependencies in presence of multi-row/column spanning cells. Therefore, we also aim to improve structure recognition by deducing a novel rectilinear graph-based formulation. From a semantics perspective, we highlight the significance of empty cells in a table. To take these cells into account, we suggest an enhancement to a popular evaluation criterion. Finally, we introduce a modestly sized evaluation dataset with an annotation style inspired by human cognition to encourage new approaches to the problem. Our framework improves the previous state-of-the-art performance by a 2.7% average F1 score on benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Raja_Visual_Understanding_of_Complex_Table_Structures_From_Document_Images_WACV_2022_paper.pdf",
        "aff": "IIIT-Hyderabad; IIIT-Hyderabad; IIIT-Hyderabad",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Raja_Visual_Understanding_of_WACV_2022_supplemental.pdf",
        "arxiv": "2111.07129",
        "pdf_size": 851143,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3180134973974764826&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "research.iiit.ac.in;iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;iiit.ac.in;iiit.ac.in",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad",
        "aff_unique_dep": "",
        "aff_unique_url": "https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": "IIIT-H",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "105371e689",
        "title": "Visualizing Paired Image Similarity in Transformer Networks",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Black_Visualizing_Paired_Image_Similarity_in_Transformer_Networks_WACV_2022_paper.html",
        "author": "Samuel Black; Abby Stylianou; Robert Pless; Richard Souvenir",
        "abstract": "Transformer architectures have shown promise for a wide range of computer vision tasks, including image embedding. As was the case with convolutional neural networks and other models, explainability of the predictions is a key concern, but visualization approaches tend to be architecture-specific. In this paper, we introduce a new method for producing interpretable visualizations that, given a pair of images encoded with a Transformer, show which regions contributed to their similarity. Additionally, for the task of image retrieval, we compare the performance of Transformer and ResNet models of similar capacity and show that while they have similar performance in aggregate, the retrieved results and the visual explanations for those results are quite different. Code is available at https://github.com/vidarlab/xformer-paired-viz.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Black_Visualizing_Paired_Image_Similarity_in_Transformer_Networks_WACV_2022_paper.pdf",
        "aff": "Temple University; Saint Louis University; George Washington University; Temple University",
        "project": "",
        "github": "https://github.com/vidarlab/xformer-paired-viz",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10999618,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3458923431404043076&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff_domain": "temple.edu;slu.edu;gwu.edu;temple.edu",
        "email": "temple.edu;slu.edu;gwu.edu;temple.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Temple University;Saint Louis University;George Washington University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.temple.edu;https://www.slu.edu;https://www.gwu.edu",
        "aff_unique_abbr": "Temple;SLU;GWU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9de51dbc79",
        "title": "Visually Guided Sound Source Separation and Localization Using Self-Supervised Motion Representations",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Zhu_Visually_Guided_Sound_Source_Separation_and_Localization_Using_Self-Supervised_Motion_WACV_2022_paper.html",
        "author": "Lingyu Zhu; Esa Rahtu",
        "abstract": "In this paper, we perform audio-visual sound source separation, i.e. to separate component audios from a mixture based on the videos of sound sources. Moreover, we aim to pinpoint the source location in the input video sequence. Recent works have shown impressive audio-visual separation results when using prior knowledge of the source type (e.g. human playing instrument) and pre-trained motion detectors (e.g. keypoints or optical flows). However, at the same time, the models are limited to a certain application domain. In this paper, we address these limitations and make the following contributions: i) we propose a two-stage architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. The entire system is trained in a self-supervised manner; ii) we introduce an Audio-Motion Embedding (AME) framework to explicitly represent the motions that related to sound; iii) we propose an audio-motion transformer architecture for audio and motion feature fusion; iv) we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained keypoint detectors or optical flow estimators. Project page: https://ly-zhu.github.io/self-supervised-motion-representations",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Zhu_Visually_Guided_Sound_Source_Separation_and_Localization_Using_Self-Supervised_Motion_WACV_2022_paper.pdf",
        "aff": "Tampere University, Finland; Tampere University, Finland",
        "project": "https://ly-zhu.github.io/self-supervised-motion-representations",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Zhu_Visually_Guided_Sound_WACV_2022_supplemental.pdf",
        "arxiv": "2104.08506",
        "pdf_size": 3603866,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7627499810331469138&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tuni.fi;tuni.fi",
        "email": "tuni.fi;tuni.fi",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tampere University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tuni.fi",
        "aff_unique_abbr": "Tuni",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "ad88c8c72c",
        "title": "WEPDTOF: A Dataset and Benchmark Algorithms for In-the-Wild People Detection and Tracking From Overhead Fisheye Cameras",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Tezcan_WEPDTOF_A_Dataset_and_Benchmark_Algorithms_for_In-the-Wild_People_Detection_WACV_2022_paper.html",
        "author": "Ozan Tezcan; Zhihao Duan; Mertcan Cokbas; Prakash Ishwar; Janusz Konrad",
        "abstract": "Owing to their large field of view, overhead fisheye cameras are becoming a surveillance modality of choice for large indoor spaces. However, traditional people detection and tracking algorithms developed for side-mounted, rectilinear-lens cameras do not work well on images from overhead fisheye cameras due to their viewpoint and unique optics. While several people-detection algorithms have been recently developed for such cameras, they have all been tested on datasets consisting of \"staged\" recordings with a limited variety of people, scenes and challenges. Clearly, the performance of these algorithms \"in the wild\", i.e., on recordings with real-world challenges, remains unknown. In this paper, we introduce a new benchmark dataset of in-the-Wild Events for People Detection and Tracking from Overhead Fisheye cameras (WEPDTOF). The dataset features 14 YouTube videos captured in a wide range of scenes, 188 distinct person identities consistently labeled across time, and real-world challenges such as extreme occlusions and camouflage. Also, we propose 3 spatio-temporal extensions of a state-of-the-art people-detection algorithm to enhance the coherence of detections across time. Compared to top-performing algorithms, that are purely spatial, the new algorithms offer a significant performance improvement on the new dataset. Finally, we compare the people tracking performance of these algorithms on WEPDTOF.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Tezcan_WEPDTOF_A_Dataset_and_Benchmark_Algorithms_for_In-the-Wild_People_Detection_WACV_2022_paper.pdf",
        "aff": "Boston University, Department of Electrical and Computer Engineering, Boston, MA; Purdue University, Department of Electrical and Computer Engineering, West Lafayette, IN; Boston University, Department of Electrical and Computer Engineering, Boston, MA; Boston University, Department of Electrical and Computer Engineering, Boston, MA; Boston University, Department of Electrical and Computer Engineering, Boston, MA",
        "project": "vip.bu.edu/wepdtof",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Tezcan_WEPDTOF_A_Dataset_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4066813,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4118549353065016494&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "bu.edu;purdue.edu;bu.edu;bu.edu;bu.edu",
        "email": "bu.edu;purdue.edu;bu.edu;bu.edu;bu.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Boston University;Purdue University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.bu.edu;https://www.purdue.edu",
        "aff_unique_abbr": "BU;Purdue",
        "aff_campus_unique_index": "0;1;0;0;0",
        "aff_campus_unique": "Boston;West Lafayette",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "91ce65d62b",
        "title": "Weakly Supervised Branch Network With Template Mask for Classifying Masses in 3D Automated Breast Ultrasound",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Kim_Weakly_Supervised_Branch_Network_With_Template_Mask_for_Classifying_Masses_WACV_2022_paper.html",
        "author": "Daekyung Kim; Chang-Mo Nam; Haesol Park; Mijung Jang; Kyong Joon Lee",
        "abstract": "Automated breast ultrasound (ABUS) is being rapidly utilized for screening and diagnosing breast cancer. Breast masses, including cancers shown in ABUS scans, often appear as irregular hypoechoic areas that are hard to distinguish from background shadings. We propose a novel branch network architecture incorporating segmentation information of masses in the training process. By providing the spatial attention effect, the branch network boosts the performance of existing neural network classifiers, helping to learn meaningful features around the mass. For the segmentation information, we leverage the existing radiology reports without additional labeling efforts. The reports should include the characteristics of breast masses, such as shape and orientation, and a template mask can be created in a rule-based manner. Experimental results show that the proposed branch network with a template mask significantly improves the performance of existing classifiers.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Kim_Weakly_Supervised_Branch_Network_With_Template_Mask_for_Classifying_Masses_WACV_2022_paper.pdf",
        "aff": "Seoul National University+Monitor Corporation; Monitor Corporation; Korea Institute of Science and Technology; Seoul National University Bundang Hospital; Seoul National University+Seoul National University Bundang Hospital",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10112105,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6336659677696236735&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "snu.ac.kr; ; ; ; ",
        "email": "snu.ac.kr; ; ; ; ",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;2;0;0+0",
        "aff_unique_norm": "Seoul National University;Monitor Corporation;Korea Institute of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.snu.ac.kr;;https://www.kist.re.kr",
        "aff_unique_abbr": "SNU;;KIST",
        "aff_campus_unique_index": ";1;1",
        "aff_campus_unique": ";Bundang",
        "aff_country_unique_index": "0+1;1;0;0;0+0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "9e0a769040",
        "title": "Weakly Supervised Learning for Joint Image Denoising and Protein Localization in Cryo-Electron Microscopy",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Huang_Weakly_Supervised_Learning_for_Joint_Image_Denoising_and_Protein_Localization_WACV_2022_paper.html",
        "author": "Qinwen Huang; Ye Zhou; Hsuan-Fu Liu; Alberto Bartesaghi",
        "abstract": "Deep learning-based object detection methods have shown promising results in various fields ranging from autonomous driving to video surveillance where input images have relatively high signal-to-noise ratios (SNR). On low SNR images such as biological electron microscopy (EM) data, however, the performance of these algorithms is significantly lower. Moreover, biological data typically lacks standardized annotations further complicating the training of detection algorithms. Accurate identification of proteins from EM images is a critical task, as the detected positions serve as inputs for the downstream 3D structure determination process. To overcome the low SNR and lack of image annotations, we propose a joint weakly-supervised learning framework that performs image denoising while detecting objects of interest. By leveraging per-pixel soft segmentation and consistency regularization, our framework denoises images without the need of clean images and is able to detect particles of interest even when less than 0.5% of the data are labeled. We validate our approach on real single-particle cryo-EM and cryo-electron tomography (ET) images which are known to suffer from extremely low SNR, and show that our strategy outperforms existing state-of-the-art (SofA) methods used in the cryo-EM field by a significant margin. We also evaluate the performance of our algorithm under decreasing SNR conditions and show that our method is more robust to noise than competing methods.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Huang_Weakly_Supervised_Learning_for_Joint_Image_Denoising_and_Protein_Localization_WACV_2022_paper.pdf",
        "aff": "Duke University; Duke University; Duke University; Duke University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Huang_Weakly_Supervised_Learning_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 7582000,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7713538503156331698&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "403a2f2caf",
        "title": "Weakly-Supervised Convolutional Neural Networks for Vessel Segmentation in Cerebral Angiography",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Vepa_Weakly-Supervised_Convolutional_Neural_Networks_for_Vessel_Segmentation_in_Cerebral_Angiography_WACV_2022_paper.html",
        "author": "Arvind Vepa; Andrew Choi; Noor Nakhaei; Wonjun Lee; Noah Stier; Andrew Vu; Greyson Jenkins; Xiaoyan Yang; Manjot Shergill; Moira Desphy; Kevin Delao; Mia Levy; Cristopher Garduno; Lacy Nelson; Wandi Liu; Fan Hung; Fabien Scalzo",
        "abstract": "Automated vessel segmentation in cerebral digital subtraction angiography (DSA) has significant clinical utility in the management of cerebrovascular diseases such as stroke diagnosis and detection of aneurysms. While deep learning is state-of-the-art in segmentation, a significant amount of labeled data is needed for training. Because of domain differences, pretrained networks cannot be applied to DSA data out-of-the-box. We propose a novel learning framework, which utilizes an active contour model for weak supervision and low-cost human-in-the-loop strategies to improve weak label quality. Our study produces several significant results, including state-of-the-art results for cerebral DSA vessel segmentation, which exceed human annotator quality, and an analysis of annotation cost and model performance trade-offs utilizing weak supervision strategies. Additionally, we will be publicly releasing code to reproduce our methodology and our dataset, the largest known high-quality annotated cerebral DSA vessel segmentation dataset.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Vepa_Weakly-Supervised_Convolutional_Neural_Networks_for_Vessel_Segmentation_in_Cerebral_Angiography_WACV_2022_paper.pdf",
        "aff": "University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Santa Barbara; Cal State University, Fullerton; Pepperdine University; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles+Pepperdine University; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles+Pepperdine University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6902988,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9766626935346895568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;",
        "author_num": 17,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;2;3;0;0;0;0;0;0+3;0;0;0;0+3",
        "aff_unique_norm": "University of California, Los Angeles;University of California, Santa Barbara;California State University, Fullerton;Pepperdine University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ucla.edu;https://www.ucsb.edu;https://www.fullerton.edu;https://www.pepperdine.edu",
        "aff_unique_abbr": "UCLA;UCSB;CSUF;Pepperdine",
        "aff_campus_unique_index": "0;0;0;0;1;2;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Los Angeles;Santa Barbara;Fullerton;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0+0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dfd73ff499",
        "title": "What Makes for Effective Few-Shot Point Cloud Classification?",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ye_What_Makes_for_Effective_Few-Shot_Point_Cloud_Classification_WACV_2022_paper.html",
        "author": "Chuangguan Ye; Hongyuan Zhu; Yongbin Liao; Yanggang Zhang; Tao Chen; Jiayuan Fan",
        "abstract": "Due to the emergence of powerful computing resources and large-scale annotated datasets, deep learning has seen wide applications in our daily life. However, most current methods require extensive data collection and retraining when dealing with novel classes never seen before. On the other hand, we humans can quickly recognize new classes by looking at a few samples, which motivates the recent popularity of few-shot learning (FSL) in machine learning communities. Most current FSL approaches work on 2D image domain, however, its implication in 3D perception is relatively under-explored. Not only needs to recognize the unseen examples as in 2D domain, 3D few-shot learning is more challenging with unordered- structures, high intra-class variances and subtle inter-class differences. Moreover, different architectures and learning algorithms make it difficult to study the effectiveness of existing 2D methods when migrating to 3D domain. In this work, for the first time, we perform systematic and extensive studies of recent 2D FSL and 3D backbone networks for benchmarking few-shot point cloud classification, and we suggest a strong baseline and learning architectures for 3D FSL. Then, we propose a novel plug-an and lay component called Cross-Instance Adaptation (CIA) module, to address the subtle inter-class differences and high intra-class variances issues, which can be easily inserted into current baselines with significant performance improvement. Extensive experiments on two newly introduced benchmark datasets, ModelNet40-FS and ShapeNet70-FS, demonstrate the superiority of our proposed network for 3D FSL. Codes and datasets will be released for facilitating future research in this area.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ye_What_Makes_for_Effective_Few-Shot_Point_Cloud_Classification_WACV_2022_paper.pdf",
        "aff": "School of Information Science and Technology, Fudan University, China; Institute for Infocomm Research, A*STAR, Singapore; School of Information Science and Technology, Fudan University, China; School of Information Science and Technology, Fudan University, China; School of Information Science and Technology, Fudan University, China; Academy for Engineering and Technology, Fudan University, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Ye_What_Makes_for_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1426948,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17996701228765823252&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "fudan.edu.cn;gmail.com;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;gmail.com;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "Fudan University;Institute for Infocomm Research",
        "aff_unique_dep": "School of Information Science and Technology;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "Fudan;I2R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "d4bbcc9baa",
        "title": "X-MIR: EXplainable Medical Image Retrieval",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Hu_X-MIR_EXplainable_Medical_Image_Retrieval_WACV_2022_paper.html",
        "author": "Brian Hu; Bhavan Vasu; Anthony Hoogs",
        "abstract": "Despite significant progress in the past few years, machine learning systems are still often viewed as \"black boxes\", which lack the ability to explain their output decisions. In high-stakes situations such as healthcare, there is a need for explainable AI (XAI) tools that can help open up this black box. In contrast to approaches which largely tackle classification problems in the medical imaging domain, we address the less-studied problem of explainable image retrieval. We test our approach on a COVID-19 chest X-ray dataset and the ISIC 2017 skin lesion dataset, showing that saliency maps help reveal the image features used by models to determine image similarity. We evaluated three different saliency algorithms, which were either occlusion-based, attention-based, or relied on a form of activation mapping. We also develop quantitative evaluation metrics that allow us to go beyond simple qualitative comparisons of the different saliency algorithms. Our results have the potential to aid clinicians when viewing medical images and addresses an urgent need for interventional tools in response to COVID-19. The source code is publicly available at: https://gitlab.kitware.com/brianhhu/x-mir.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Hu_X-MIR_EXplainable_Medical_Image_Retrieval_WACV_2022_paper.pdf",
        "aff": ";;",
        "project": "https://gitlab.kitware.com/brianhhu/x-mir",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Hu_X-MIR_EXplainable_Medical_WACV_2022_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 9177783,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10595034527693146321&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "525dd9db2f",
        "title": "YOLO-ReT: Towards High Accuracy Real-Time Object Detection on Edge GPUs",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Ganesh_YOLO-ReT_Towards_High_Accuracy_Real-Time_Object_Detection_on_Edge_GPUs_WACV_2022_paper.html",
        "author": "Prakhar Ganesh; Yao Chen; Yin Yang; Deming Chen; Marianne Winslett",
        "abstract": "Performance of object detection models has been growing rapidly on two major fronts, model accuracy and efficiency. However, in order to map deep neural network (DNN) based object detection models to edge devices, one typically needs to compress such models significantly, thus compromising the model accuracy. In this paper, we propose a novel edge GPU friendly module for multi-scale feature interaction by exploiting missing combinatorial connections between various feature scales in existing state-of-the-art methods. Additionally, we propose a novel transfer learning backbone adoption inspired by the changing translational information flow across various tasks, designed to complement our feature interaction module and together improve both accuracy as well as execution speed on various edge GPU devices available in the market. For instance, YOLO-ReT with MobileNetV2x0.75 backbone runs real-time on Jetson Nano, and achieves 68.75 mAP on Pascal VOC and 34.91 mAP on COCO, beating its peers by 3.05 mAP and 0.91 mAP respectively, while executing faster by 3.05 FPS. Furthermore, introducing our multi-scale feature interaction module in YOLOv4-tiny and YOLOv4-tiny (3l) improves their performance to 41.5 and 48.1 mAP respectively on COCO, outperforming the original versions by 1.3 and 0.9 mAP.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Ganesh_YOLO-ReT_Towards_High_Accuracy_Real-Time_Object_Detection_on_Edge_GPUs_WACV_2022_paper.pdf",
        "aff": "Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore; College of Science and Engineering, Hamad Bin Khalifa University, Qatar; University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1139967,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3716397009677337300&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "adsc-create.edu.sg;adsc-create.edu.sg;hbku.edu.qa;illinois.edu;illinois.edu",
        "email": "adsc-create.edu.sg;adsc-create.edu.sg;hbku.edu.qa;illinois.edu;illinois.edu",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;2",
        "aff_unique_norm": "Advanced Digital Sciences Center;Hamad Bin Khalifa University;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";College of Science and Engineering;",
        "aff_unique_url": ";https://www.hbku.edu.qa;https://illinois.edu",
        "aff_unique_abbr": ";;UIUC",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;0;1;2;2",
        "aff_country_unique": "Singapore;Qatar;United States"
    },
    {
        "id": "b101e16e5f",
        "title": "edge-SR: Super-Resolution for the Masses",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Michelini_edge-SR_Super-Resolution_for_the_Masses_WACV_2022_paper.html",
        "author": "Pablo Navarrete Michelini; Yunhua Lu; Xingqun Jiang",
        "abstract": "Classic image scaling (e.g. bicubic) can be seen as one convolutional layer and a single upscaling filter. Its implementation is ubiquitous in all display devices and image processing software. In the last decade deep learning systems have been introduced for the task of image super-resolution (SR), using several convolutional layers and numerous filters. These methods have taken over the benchmarks of image quality for upscaling tasks. Would it be possible to replace classic upscalers with deep learning architectures on edge devices such as display panels, tablets, laptop computers, etc.? On one hand, the current trend in Edge-AI chips shows a promising future in this direction, with rapid development of hardware that can run deep-learning tasks efficiently. On the other hand, in image SR only few architectures have pushed the limit to extreme small sizes that can actually run on edge devices at real-time. We explore possible solutions to this problem with the aim to fill the gap between classic upscalers and small deep learning configurations. As a transition from classic to deep-learning upscaling we propose edge-SR (eSR), a set of one-layer architectures that use interpretable mechanisms to upscale images. Certainly, a one-layer architecture cannot reach the quality of deep learning systems. Nevertheless, we find that for high speed requirements, eSR becomes better at trading-off image quality and runtime performance. Filling the gap between classic and deep-learning architectures for image upscaling is critical for massive adoption of this technology. It is equally important to have an interpretable system that can reveal the inner strategies to solve this problem and guide us to future improvements and better understanding of larger networks.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Michelini_edge-SR_Super-Resolution_for_the_Masses_WACV_2022_paper.pdf",
        "aff": "BOE Technology Group Co., Ltd.; ;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Michelini_edge-SR_Super-Resolution_for_WACV_2022_supplemental.zip",
        "arxiv": "",
        "pdf_size": 7389606,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9656858713494070460&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "BOE Technology Group",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.boe.com.cn/",
        "aff_unique_abbr": "BOE",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "id": "9264457778",
        "title": "mToFNet: Object Anti-Spoofing With Mobile Time-of-Flight Data",
        "site": "https://openaccess.thecvf.com/content/WACV2022/html/Jeong_mToFNet_Object_Anti-Spoofing_With_Mobile_Time-of-Flight_Data_WACV_2022_paper.html",
        "author": "Yonghyun Jeong; Doyeon Kim; Jaehyeon Lee; Minki Hong; Solbi Hwang; Jongwon Choi",
        "abstract": "In online markets, sellers can maliciously recapture others' images on display screens to utilize as spoof images, which can be challenging to distinguish in human eyes. To prevent such harm, we propose an anti-spoofing method using the pairs of RGB images and depth maps provided by the mobile camera with a time-of-fight sensor. When images are recaptured on display screens, various patterns differing by the screens as known as the moire patterns can be also captured in spoof images. These patterns lead the anti-spoofing model to be overfitted and unable to detect spoof images recaptured on unseen media. To avoid the issue, we build a novel representation model composed of two embedding models, which can be trained without considering the recaptured images. Also, we newly introduce mToF dataset, the largest and most diverse object anti-spoofing dataset, and the first to utilize the time-of-flight (ToF) data. Experimental results confirm that our model achieves robust generalization even across unseen domains.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Jeong_mToFNet_Object_Anti-Spoofing_With_Mobile_Time-of-Flight_Data_WACV_2022_paper.pdf",
        "aff": "Samsung SDS, Seoul, Korea; Samsung SDS, Seoul, Korea; Samsung SDS, Seoul, Korea; Samsung SDS, Seoul, Korea; Samsung SDS, Seoul, Korea; Dept. of Advanced Imaging, Chung-Ang University, Seoul, Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Jeong_mToFNet_Object_Anti-Spoofing_WACV_2022_supplemental.zip",
        "arxiv": "2110.04066",
        "pdf_size": 1517030,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15300971517475023408&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "samsung.com;samsung.com;samsung.com;samsung.com;samsung.com;cau.ac.kr",
        "email": "samsung.com;samsung.com;samsung.com;samsung.com;samsung.com;cau.ac.kr",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Samsung;Chung-Ang University",
        "aff_unique_dep": "Samsung SDS;Dept. of Advanced Imaging",
        "aff_unique_url": "https://www.samsungsds.com;http://www.cau.ac.kr",
        "aff_unique_abbr": "Samsung SDS;CAU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seoul",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    }
]